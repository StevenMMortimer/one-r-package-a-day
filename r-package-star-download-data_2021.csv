name,description,published,author,url,github_ind,github_url,downloads,stars,last_commit,downloads_per_star
AATtools,"Compute approach bias scores using different scoring algorithms,
    compute bootstrapped and exact split-half reliability estimates,
    and compute confidence intervals for individual participant scores.",2020-06-14,Sercan Kahveci,NA,TRUE,https://github.com/spiritspeak/aattools,7109,0,2021-08-25T08:38:43Z,NA
ABCoptim,"An implementation of Karaboga (2005) Artificial Bee Colony
    Optimization algorithm <http://mf.erciyes.edu.tr/abc/pub/tr06_2005.pdf>.
    This (working) version is a Work-in-progress, which is
    why it has been implemented using pure R code. This was developed upon the basic
    version programmed in C and distributed at the algorithm's official website.",2017-11-06,George Vega Yon,"http://github.com/gvegayon/ABCoptim, http://mf.erciyes.edu.tr/abc/",TRUE,https://github.com/gvegayon/abcoptim,28315,21,2021-06-14T06:23:28Z,1348.3333333333333
abess,"Extremely efficient toolkit for solving the best subset selection problem in linear regression, logistic regression, Poisson regression, Cox proportional hazard model, multiple-response regression, multinomial logistic regression, and (sequential) principal component analysis. It implements and generalizes algorithms designed in <doi:10.1073/pnas.2014241117> that exploits a novel sequencing-and-splicing technique to guarantee exact support recovery and globally optimal solution in polynomial times.  ",2021-07-31,Jin Zhu,"https://github.com/abess-team/abess,
https://abess-team.github.io/abess/,
https://abess.readthedocs.io",TRUE,https://github.com/abess-team/abess,2519,140,2021-09-03T13:07:41Z,17.992857142857144
abjutils,"The Brazilian Jurimetrics Association (ABJ in
    Portuguese, see <https://abj.org.br/> for more information) is
    a non-profit organization which aims to investigate and promote the
    use of statistics and probability in the study of Law and its
    institutions.  This package implements general purpose tools used by
    ABJ, such as functions for sampling and basic manipulation of
    Brazilian lawsuits identification number. It also implements functions
    for text cleaning, such as accentuation removal.",2020-08-12,Caio Lente,https://github.com/abjur/abjutils,TRUE,https://github.com/abjur/abjutils,47919,33,2021-02-05T18:04:45Z,1452.090909090909
abstr,"Provides functions to convert origin-destination data,
    represented as straight 'desire lines' in the 'sf' Simple Features
    class system, into JSON files that can be directly imported into
    A/B Street <https://www.abstreet.org>, a free and open source tool
    for simulating urban transport systems and scenarios of change
    <doi:10.1007/s10109-020-00342-2>.",2021-08-19,Nathanael Sheehan,"https://github.com/a-b-street/abstr,
https://a-b-street.github.io/abstr/",TRUE,https://github.com/a-b-street/abstr,209,13,2021-08-31T23:25:46Z,16.076923076923077
academictwitteR,"Package to query the Twitter Academic Research Product Track,
    providing access to full-archive search and other v2 API endpoints. Functions
    are written with academic research in mind. They provide flexibility in how 
    the user wishes to store collected data, and encourage regular storage of data
    to mitigate loss when collecting large volumes of tweets. They also provide
    workarounds to manage and reshape the format in which data is provided on
    the client side.",2021-07-07,Christopher Barrie,https://github.com/cjbarrie/academictwitteR,TRUE,https://github.com/cjbarrie/academictwitter,4057,163,2021-08-25T10:56:48Z,24.88957055214724
accucor,"An isotope natural abundance correction algorithm that
  is needed especially for high resolution mass spectrometers. Supports
  correction for 13C, 2H and 15N.",2021-04-12,Xiaoyang Su,https://github.com/XiaoyangSu/AccuCor,TRUE,https://github.com/xiaoyangsu/accucor,2117,9,2021-04-12T14:29:02Z,235.22222222222223
aceEditor,"Wraps the 'Ace' editor in a HTML widget. The 'Ace' editor has support for many languages. It can be opened in the viewer pane of 'RStudio', and this provides a second source editor.",2021-03-06,Stéphane Laurent,https://github.com/stla/aceEditor,TRUE,https://github.com/stla/aceeditor,5210,3,2021-03-06T09:48:47Z,1736.6666666666667
ActCR,"Circadian rhythms are rhythms that oscillate about every 24 h, which has been observed in multiple physiological processes including core body temperature, hormone secretion, heart rate, blood pressure, and many others. Measuring circadian rhythm with wearables is based on a principle that there is increased movement during wake periods and reduced movement during sleep periods, and has been shown to be reliable and valid. This package can be used to extract nonparametric circadian metrics like intradaily variability (IV), interdaily stability (IS), and relative amplitude (RA); and parametric cosinor model and extended cosinor model coefficient. Details can be found in Junrui Di et al (2019) <doi:10.1007/s12561-019-09236-4>.",2021-01-28,Junrui Di,https://github.com/junruidi/ActCR,TRUE,https://github.com/junruidi/actcr,6560,0,2021-01-29T21:40:48Z,NA
actel,"Designed for studies where animals tagged with acoustic tags are expected
    to move through receiver arrays. This package combines the advantages of automatic sorting and checking 
    of animal movements with the possibility for user intervention on tags that deviate from expected 
    behaviour. The three analysis functions (explore(), migration() and residency()) 
    allow the users to analyse their data in a systematic way, making it easy to compare results from 
    different studies.
    CJS calculations are based on Perry et al. (2012) <https://www.researchgate.net/publication/256443823_Using_mark-recapture_models_to_estimate_survival_from_telemetry_data>.",2021-01-05,Hugo Flávio,https://github.com/hugomflavio/actel,TRUE,https://github.com/hugomflavio/actel,10792,18,2021-06-10T18:55:44Z,599.5555555555555
activatr,"This contains helpful functions for parsing, managing, plotting, and
    visualizing activities, most often from GPX (GPS Exchange Format) files
    recorded by GPS devices. It allows easy parsing of the source files into
    standard R data formats, along with functions to compute derived data for
    the activity, and to plot the activity in a variety of ways.",2021-01-15,Daniel Schafer,https://github.com/dschafer/activatr,TRUE,https://github.com/dschafer/activatr,2669,2,2021-01-15T16:01:05Z,1334.5
ActivePathways,"Framework for analysing multiple omics datasets in the context of molecular pathways, biological processes and other types of gene sets. The package uses p-value merging to combine gene- or protein-level signals, followed by ranked hypergeometric tests to determine enriched pathways and processes. This approach allows researchers to interpret a series of omics datasets in the context of known biology and gene function, and discover associations that are only apparent when several datasets are combined. The package is part of the following publication: Integrative Pathway Enrichment Analysis of Multivariate Omics Data. Paczkowska M^, Barenboim J^, Sintupisut N, Fox NS, Zhu H, Abd-Rabbo D, Mee MW, Boutros PC, PCAWG Drivers and Functional Interpretation Working Group; Reimand J, PCAWG Consortium. Nature Communications (2020) <doi:10.1038/s41467-019-13983-9>.",2020-07-09,Juri Reimand,NA,TRUE,https://github.com/reimandlab/activepathways,12797,50,2020-09-11T02:03:53Z,255.94
acumos,"Create, upload and run 'Acumos' R models.
  'Acumos' (<https://www.acumos.org>) is a platform and open source framework intended to make it easy to build,
  share, and deploy AI apps. 'Acumos' is part of the 'LF AI Foundation', an umbrella
  organization within 'The Linux Foundation'. With this package, user can create a
  component, and push it to an 'Acumos' platform.",2021-07-02,Alassane Samba,"https://www.acumos.org (Acumos project website)
https://gerrit.acumos.org/r/gitweb?p=acumos-r-client.git (code
repository) https://github.com/acumos/acumos-r-client (mirror
repository)
https://docs.acumos.org/en/latest/submodules/acumos-r-client/docs
(docs)",TRUE,https://github.com/acumos/acumos-r-client,6388,3,2021-07-08T08:22:07Z,2129.3333333333335
adaptDiag,"Simulate clinical trials for diagnostic test devices and evaluate 
    the operating characteristics under an adaptive design with futility
    assessment determined via the posterior predictive probabilities.",2021-08-17,Graeme L. Hickey,https://github.com/graemeleehickey/adaptDiag,TRUE,https://github.com/graemeleehickey/adaptdiag,256,1,2021-08-16T16:39:26Z,256
adaptMCMC,Enables sampling from arbitrary distributions if the log density is known up to a constant; a common situation in the context of Bayesian inference. The implemented sampling algorithm was proposed by Vihola (2012) <DOI:10.1007/s11222-011-9269-5> and achieves often a high efficiency by tuning the proposal distributions to a user defined acceptance rate.,2021-03-29,Andreas Scheidegger,https://github.com/scheidan/adaptMCMC,TRUE,https://github.com/scheidan/adaptmcmc,35788,8,2021-03-29T07:38:05Z,4473.5
adaptMT,"Implementation of adaptive p-value thresholding (AdaPT), including both a framework that allows the user to specify any 
  algorithm to learn local false discovery rate and a pool of convenient functions that implement specific 
  algorithms. See Lei, Lihua and Fithian, William (2016) <arXiv:1609.06035>.",2018-07-31,Lihua Lei,"https://arxiv.org/abs/1609.06035,
https://github.com/lihualei71/adaptMT",TRUE,https://github.com/lihualei71/adaptmt,17650,6,2021-06-12T18:08:38Z,2941.6666666666665
addinslist,"Browse through a continuously updated list of existing RStudio 
    addins and install/uninstall their corresponding packages.",2021-01-10,Dean Attali,https://github.com/daattali/addinslist,TRUE,https://github.com/daattali/addinslist,42375,672,2021-08-13T22:44:36Z,63.058035714285715
additive,"Fit Generalized Additive Models (GAM) using 'mgcv' with 'parsnip'/'tidymodels'
    via 'additive' <doi:10.5281/zenodo.5091179>. 'tidymodels' is a collection of
    packages for machine learning; see Kuhn and Wickham (2020) <https://www.tidymodels.org>).
    The technical details of 'mgcv' are described in Wood (2017)
    <doi:10.1201/9781315370279>.",2021-07-12,Hamada S. Badr,"https://hsbadr.github.io/additive/,
https://github.com/hsbadr/additive",TRUE,https://github.com/hsbadr/additive,1556,0,2021-09-02T19:25:36Z,NA
ade4,"Tools for multivariate data analysis. Several methods are provided for the analysis (i.e., ordination) of one-table (e.g., principal component analysis, correspondence analysis), two-table (e.g., coinertia analysis, redundancy analysis), three-table (e.g., RLQ analysis) and K-table (e.g., STATIS, multiple coinertia analysis). The philosophy of the package is described in Dray and Dufour (2007) <doi:10.18637/jss.v022.i04>.",2021-06-17,Stéphane Dray,http://pbil.univ-lyon1.fr/ADE-4/,TRUE,https://github.com/sdray/ade4,1280837,18,2021-07-07T13:07:03Z,71157.61111111111
ade4TkGUI,A Tcl/Tk GUI for some basic functions in the 'ade4' package.,2020-12-03,Jean Thioulouse,"http://pbil.univ-lyon1.fr/ade4TkGUI/, Mailing list:
https://listes.univ-lyon1.fr/sympa/info/adelist",TRUE,https://github.com/aursiber/ade4tkgui,86579,0,2021-02-18T08:03:50Z,NA
adegenet,"Toolset for the exploration of genetic and genomic
    data. Adegenet provides formal (S4) classes for storing and handling
    various genetic data, including genetic markers with varying ploidy
    and hierarchical population structure ('genind' class), alleles counts
    by populations ('genpop'), and genome-wide SNP data ('genlight'). It
    also implements original multivariate methods (DAPC, sPCA), graphics,
    statistical tests, simulation tools, distance and similarity measures,
    and several spatial methods. A range of both empirical and simulated
    datasets is also provided to illustrate various methods.",2021-07-17,Thibaut Jombart,https://github.com/thibautjombart/adegenet,TRUE,https://github.com/thibautjombart/adegenet,296785,128,2021-07-17T18:07:47Z,2318.6328125
adegraphics,Graphical functionalities for the representation of multivariate data. It is a complete re-implementation of the functions available in the 'ade4' package.,2018-12-18,Stéphane Dray,"http://pbil.univ-lyon1.fr/ADE-4, Mailing list:
http://listes.univ-lyon1.fr/wws/info/adelist",TRUE,https://github.com/sdray/adegraphics,134944,6,2021-07-07T13:09:51Z,22490.666666666668
adept,"Designed for optimal use in performing fast, 
    accurate walking strides segmentation from high-density 
    data collected from a wearable accelerometer worn 
    during continuous walking activity.",2021-02-01,Marta Karas,https://github.com/martakarass/adept,TRUE,https://github.com/martakarass/adept,14580,3,2021-01-31T23:35:49Z,4860
adespatial,"Tools for the multiscale spatial analysis of multivariate data.
    Several methods are based on the use of a spatial weighting matrix and its
    eigenvector decomposition (Moran's Eigenvectors Maps, MEM). 
    Several approaches are described in the review Dray et al (2012)
    <doi:10.1890/11-1183.1>.",2021-04-07,Stéphane Dray,https://github.com/sdray/adespatial,TRUE,https://github.com/sdray/adespatial,78217,17,2021-08-04T08:33:43Z,4601
adheRenceRX,"A (mildly) opinionated set of functions to help assess medication adherence for researchers working with medication claims data.
    Medication adherence analyses have several complex steps that are often convoluted and can be time-intensive. The focus is to create a 
    set of functions using ""tidy principles"" geared towards transparency, speed, and flexibility while working with adherence metrics. All functions perform exactly one task 
    with an intuitive name so that a researcher can handle details (often achieved with vectorized solutions) while we handle non-vectorized tasks common to most 
    adherence calculations such as adjusting fill dates and determining episodes of care. The methodologies in referenced in this package come from
    Canfield SL, et al (2019) ""Navigating the Wild West of Medication Adherence Reporting in Specialty Pharmacy"" <doi:10.18553/jmcp.2019.25.10.1073>.",2020-11-20,Brennan Beal,https://github.com/btbeal/adheRenceRX,TRUE,https://github.com/btbeal/adherencerx,4045,3,2020-12-01T19:17:16Z,1348.3333333333333
adjclust,"Implements a constrained version of hierarchical agglomerative 
    clustering, in which each observation is associated to a position, and 
    only adjacent clusters can be merged. Typical application fields in 
    bioinformatics include Genome-Wide Association Studies or Hi-C data 
    analysis, where the similarity between items is a decreasing function of 
    their genomic distance. Taking advantage of this feature, the implemented 
    algorithm is time and memory efficient. This algorithm is described in 
    Ambroise et al (2019) <https://almob.biomedcentral.com/articles/10.1186/s13015-019-0157-4>.",2021-07-26,Pierre Neuvial,https://github.com/pneuvial/adjclust,TRUE,https://github.com/pneuvial/adjclust,22518,14,2021-07-26T21:44:52Z,1608.4285714285713
admisc,"Contains functions used across packages 'declared', 'DDIwR', 'mixed', 'QCA'
    and 'venn'. Interprets and translates, factorizes and negates SOP - Sum of Products
    expressions, for both binary and multi-value crisp sets, and extracts information
    (set names, set values) from those expressions. Other functions perform various
    other checks if possibly numeric (even if all numbers reside in a character vector)
    and coerce to numeric, or check if the numbers are whole. It also offers, among
    many others, a highly flexible recoding routine.",2021-09-01,Adrian Dusa,https://github.com/dusadrian/admisc,TRUE,https://github.com/dusadrian/admisc,71594,0,2021-08-31T22:49:35Z,NA
AdMit,"Provides functions to perform the fitting of an adaptive mixture
    of Student-t distributions to a target density through its kernel function as described in
    Ardia et al. (2009) <doi:10.18637/jss.v029.i03>. The
    mixture approximation can then be used as the importance density in importance
    sampling or as the candidate density in the Metropolis-Hastings algorithm to
    obtain quantities of interest for the target density itself. ",2021-05-16,David Ardia,https://github.com/ArdiaD/AdMit,TRUE,https://github.com/ardiad/admit,33539,3,2021-05-16T14:05:58Z,11179.666666666666
admix,"Implements several methods to estimate the unknown quantities related 
    to two-component admixture models, where the two components can belong to any
    distribution (note that in the case of multinomial mixtures, the two components
    must belong to the same family). Estimation methods depend on the assumptions 
    made on the unknown component density (see Bordes and Vandekerkhove (2010) <doi:10.3103/S1066530710010023>; 
    Patra and Sen (2016) <doi:10.1111/rssb.12148>); Milhaud, Pommeret, Salhi and Vandekerkhove 
    (2021) <https://hal.archives-ouvertes.fr/hal-03201760>). In practice, one can estimate both the 
    mixture weight and the unknown component density in a wide variety of frameworks.
    On top of that, hypothesis tests can be performed in one and two-samples contexts 
    to test the unknown component density. Finally, clustering of unknown
    mixture components is also feasible in a K-samples setting.",2021-06-14,Xavier Milhaud,https://github.com/XavierMilhaud/admix,TRUE,https://github.com/xaviermilhaud/admix,1092,1,2021-07-29T09:39:02Z,1092
ADMUR,"Provides tools to directly model underlying population dynamics using date datasets (radiocarbon and other) with a Continuous Piecewise Linear (CPL) model framework. Various other model types included. Taphonomic loss included optionally as a power function. Model comparison framework using BIC. Package also calibrates 14C samples, generates Summed Probability Distributions (SPD), and performs SPD simulation analysis to generate a Goodness-of-fit test for the best selected model. Details about the method can be found in Timpson A., Barberena R., Thomas M. G., Mendez C., Manning K. (2020) <doi:10.1098/rstb.2019.0723>.",2021-03-23,Adrian Timpson,https://github.com/UCL/ADMUR,TRUE,https://github.com/ucl/admur,3226,6,2021-03-19T18:53:55Z,537.6666666666666
adnuts,"Bayesian inference using the no-U-turn (NUTS) algorithm by 
 Hoffman and Gelman (2014) <https://www.jmlr.org/papers/v15/hoffman14a.html>. 
 Designed for 'AD Model Builder' ('ADMB') models,
 or when R functions for log-density and log-density gradient
 are available, such as 'Template Model Builder'
 models and other special cases. Functionality is similar to 'Stan', 
 and the 'rstan' and 'shinystan' packages are used for diagnostics and 
 inference.",2021-03-02,Cole Monnahan,https://github.com/Cole-Monnahan-NOAA/adnuts,TRUE,https://github.com/cole-monnahan-noaa/adnuts,18966,15,2021-03-02T22:15:00Z,1264.4
adobeanalyticsr,"Connect to the 'Adobe Analytics' API v2.0 <https://github.com/AdobeDocs/analytics-2.0-apis>
             which powers 'Analysis Workspace'. The package was developed
             with the analyst in  mind, and it will continue to be
             developed with the guiding principles of iterative,
             repeatable, timely analysis.",2021-03-01,Ben Woodard,https://github.com/benrwoodard/adobeanalyticsr,TRUE,https://github.com/benrwoodard/adobeanalyticsr,2819,7,2021-03-30T13:35:26Z,402.7142857142857
adoptr,"Optimize one or two-arm, two-stage designs for clinical trials with 
    respect to several pre-implemented objective criteria or implement custom 
    objectives.
    Optimization under uncertainty and conditional (given stage-one outcome) 
    constraints are supported.
    See <doi:10.1002/sim.8291> and <doi:10.18637/jss.v098.i09> for details.",2021-06-28,Kevin Kunzmann,https://github.com/kkmann/adoptr,TRUE,https://github.com/kkmann/adoptr,19544,6,2021-06-28T12:15:29Z,3257.3333333333335
afdx,"Estimate diagnosis performance (Sensitivity, Specificity, 
    Positive predictive value, Negative predicted value) of a diagnostic test
    where can not measure the golden standard but can estimate it using the 
    attributable fraction. ",2021-05-25,John J. Aponte,https://github.com/johnaponte/afdx,TRUE,https://github.com/johnaponte/afdx,1411,0,2021-05-24T09:35:27Z,NA
afex,"Convenience functions for analyzing factorial experiments using ANOVA or
         mixed models. aov_ez(), aov_car(), and aov_4() allow specification of
         between, within (i.e., repeated-measures), or mixed (i.e., split-plot) 
         ANOVAs for data in long format (i.e., one observation per row),
         automatically aggregating multiple observations per individual and cell 
         of the design. mixed() fits mixed models using lme4::lmer() and computes 
         p-values for all fixed effects using either Kenward-Roger or Satterthwaite 
         approximation for degrees of freedom (LMM only), parametric bootstrap 
         (LMMs and GLMMs), or likelihood ratio tests (LMMs and GLMMs). 
         afex_plot() provides a high-level interface for interaction or one-way 
         plots using ggplot2, combining raw data and model estimates. afex uses 
         type 3 sums of squares as default (imitating commercial statistical software).",2021-07-22,Henrik Singmann,"http://afex.singmann.science/, https://github.com/singmann/afex",TRUE,https://github.com/singmann/afex,305079,98,2021-07-21T19:04:32Z,3113.0510204081634
affinity,"Tools for raster georeferencing, grid affine transforms, and general raster logic. 
 These functions provide converters between raster specifications, world vector, geotransform, 
 'RasterIO' window, and 'RasterIO window' in 'sf' package list format. There are functions to offset
 a matrix by padding any of four corners (useful for vectorizing neighbourhood operations), and
 helper functions to harvesting user clicks on a graphics device to use for simple georeferencing
 of images.  Methods used are available from <https://en.wikipedia.org/wiki/World_file> and
 <https://gdal.org/user/raster_data_model.html>. ",2021-06-02,Michael D. Sumner,https://github.com/hypertidy/affinity,TRUE,https://github.com/hypertidy/affinity,1396,13,2021-06-15T01:49:13Z,107.38461538461539
aftgee,"A collection of methods for both the rank-based estimates and least-square estimates to the Accelerated Failure Time (AFT) model. For rank-based estimation, it provides approaches that include the computationally efficient Gehan's weight and the general's weight such as the logrank weight. Details of the rank-based estimation can be found in Chiou et al. (2014) <doi:10.1007/s11222-013-9388-2> and Chiou et al. (2015) <doi:10.1002/sim.6415>. For the least-square estimation, the estimating equation is solved with generalized estimating equations (GEE). Moreover, in multivariate cases, the dependence working correlation structure can be specified in GEE's setting. Details on the least-squares estimation can be found in Chiou et al. (2014) <doi:10.1007/s10985-014-9292-x>.",2021-07-12,Sy Han Chiou,https://github.com/stc04003/aftgee,TRUE,https://github.com/stc04003/aftgee,24702,0,2021-08-24T21:26:22Z,NA
AGD,"Tools for the analysis of growth data: to extract an 
    LMS table from a gamlss object, to calculate the standard 
    deviation scores and its inverse, and to superpose two wormplots 
    from different models. The package contains a some varieties of 
    reference tables, especially for The Netherlands.",2018-05-29,Stef van Buuren,https://github.com/stefvanbuuren/AGD,TRUE,https://github.com/stefvanbuuren/agd,60406,1,2021-05-31T13:13:15Z,60406
AGHmatrix,"Computation of A (pedigree), G (genomic-base), and H (A corrected
    by G) relationship matrices for diploid and autopolyploid species. Several methods
    are implemented considering additive and non-additive models.",2020-09-21,Rodrigo Amadeu,https://github.com/prmunoz/AGHmatrix,TRUE,https://github.com/prmunoz/aghmatrix,22652,6,2021-04-09T13:30:13Z,3775.3333333333335
aglm,"Provides functions to fit Accurate Generalized Linear Model (AGLM) models, visualize them, and predict for new data. AGLM is defined as a regularized GLM which applies a sort of feature transformations using a discretization of numerical features and specific coding methodologies of dummy variables. For more information on AGLM, see Suguru Fujita, Toyoto Tanaka, Kenji Kondo and Hirokazu Iwasawa (2020) <https://www.institutdesactuaires.com/global/gene/link.php?doc_id=16273&fg=1>.",2021-06-09,Kenji Kondo,https://github.com/kkondo1981/aglm,TRUE,https://github.com/kkondo1981/aglm,1108,9,2021-06-09T13:53:51Z,123.11111111111111
AGread,"Standardize the process of bringing various modes of output files
    into R. Additionally, processes are provided to read and minimally pre-
    process raw data from primary accelerometer and inertial measurement unit files,
    as well as binary .gt3x files. ActiGraph monitors are used to estimate physical
    activity outcomes via body-worn sensors that measure (e.g.) acceleration or
    rotational velocity.",2020-02-26,Paul R. Hibbing,https://github.com/paulhibbing/AGread,TRUE,https://github.com/paulhibbing/agread,21867,11,2021-07-15T15:23:45Z,1987.909090909091
agricolaeplotr,"Visualization of Design of Experiments from the 'agricolae' package with 'ggplot2' framework
    The user provides an experiment design from the 'agricolae' package, calls the corresponding function and will receive a 
    visualization with 'ggplot2' based functions that are specific for each design. As there are many different designs, each design is tested on its type.
    The output can be modified with standard 'ggplot2' commands or with other packages with 'ggplot2' function extensions.",2021-07-20,Jens Harbers,https://github.com/jensharbers/agricolaeplotr,TRUE,https://github.com/jensharbers/agricolaeplotr,1134,0,2021-07-20T18:22:05Z,NA
agridat,"Datasets from books, papers, and websites related to agriculture.
    Example graphics and analyses are included. Data come from small-plot trials,
    multi-environment trials, uniformity trials, yield monitors, and more.",2021-01-12,Kevin Wright,http://kwstat.github.io/agridat/,TRUE,https://github.com/kwstat/agridat,53681,73,2021-02-22T20:20:18Z,735.3561643835617
AHMbook,"Provides functions and data sets to accompany the two volume publication 'Applied Hierarchical Modeling in Ecology: Analysis of distribution, abundance and species richness in R and BUGS' by Marc Kéry and Andy Royle: volume 1 (2016, ISBN: 978-0-12-801378-6) and volume 2 (2021, ISBN: 978-0-12-809585-0), <https://www.mbr-pwrc.usgs.gov/pubanalysis/keryroylebook/>.",2021-05-17,Mike Meredith,https://sites.google.com/site/appliedhierarchicalmodeling/home,TRUE,https://github.com/mikemeredith/ahmbook,25300,21,2021-07-30T00:39:36Z,1204.7619047619048
AhoCorasickTrie,"Aho-Corasick is an optimal algorithm for finding many
    keywords in a text. It can locate all matches in a text in O(N+M) time; i.e.,
    the time needed scales linearly with the number of keywords (N) and the size of
    the text (M). Compare this to the naive approach which takes O(N*M) time to loop
    through each pattern and scan for it in the text. This implementation builds the
    trie (the generic name of the data structure) and runs the search in a single
    function call. If you want to search multiple texts with the same trie, the
    function will take a list or vector of texts and return a list of matches to
    each text. By default, all 128 ASCII characters are allowed in both the keywords
    and the text. A more efficient trie is possible if the alphabet size can be
    reduced. For example, DNA sequences use at most 19 distinct characters and
    usually only 4; protein sequences use at most 26 distinct characters and usually
    only 20. UTF-8 (Unicode) matching is not currently supported.",2020-09-29,Matt Chambers,https://github.com/chambm/AhoCorasickTrie,TRUE,https://github.com/chambm/ahocorasicktrie,23342,8,2020-09-29T13:51:24Z,2917.75
AIPW,"The 'AIPW' pacakge implements the augmented inverse probability weighting, a doubly robust estimator, for average causal effect estimation with user-defined stacked machine learning algorithms. To cite the 'AIPW' package, please use: ""Yongqi Zhong, Edward H. Kennedy, Lisa M. Bodnar, Ashley I. Naimi (2021, In Press). AIPW: An R Package for Augmented Inverse Probability Weighted Estimation of Average Causal Effects. American Journal of Epidemiology"". Visit: <https://yqzhong7.github.io/AIPW/> for more information.",2021-06-11,Yongqi Zhong,https://github.com/yqzhong7/AIPW,TRUE,https://github.com/yqzhong7/aipw,1268,6,2021-07-20T21:17:52Z,211.33333333333334
aire.zmvm,"Tools for downloading hourly averages, daily maximums and minimums from each of the 
    pollution, wind, and temperature measuring stations or geographic zones in the Mexico City 
    metro area. The package also includes the locations of each of the stations and zones. See 
    <http://aire.cdmx.gob.mx/> for more information.",2019-03-30,Diego Valle-Jones,"https://hoyodesmog.diegovalle.net/aire.zmvm/,
https://github.com/diegovalle/aire.zmvm",TRUE,https://github.com/diegovalle/aire.zmvm,24742,10,2021-08-13T20:52:14Z,2474.2
airports,"Geographic, use, and property related data on airports.",2020-06-29,Mine Çetinkaya-Rundel,https://github.com/OpenIntroStat/airports,TRUE,https://github.com/openintrostat/airports,75120,1,2021-09-02T13:40:53Z,75120
airr,"Schema definitions and read, write and validation tools for data 
    formatted in accordance with the AIRR Data Representation schemas defined 
    by the AIRR Community <http://docs.airr-community.org>.",2020-05-27,Jason Vander Heiden,http://docs.airr-community.org,TRUE,https://github.com/airr-community/airr-standards,28245,28,2021-08-05T22:32:01Z,1008.75
AirSensor,"Process and display data from air quality sensors.
    Initial focus is on PM2.5 measurements from sensors produced by 'PurpleAir'
    <https://www2.purpleair.com>.",2021-03-12,Jonathan Callahan,https://github.com/MazamaScience/AirSensor,TRUE,https://github.com/mazamascience/airsensor,5843,22,2021-04-02T00:20:36Z,265.59090909090907
akc,"A tidy framework for automatic knowledge classification and visualization. Currently, the core functionality of the framework is mainly supported by modularity-based clustering (community detection) in keyword co-occurrence network, and focuses on co-word analysis of bibliometric research. However, the designed functions in 'akc' are general, and could be extended to solve other tasks in text mining as well.",2020-12-05,Tian-Yuan Huang,https://github.com/hope-data-science/akc,TRUE,https://github.com/hope-data-science/akc,12930,9,2020-12-05T08:35:36Z,1436.6666666666667
akiFlagger,Flagger to detect acute kidney injury (AKI) in a patient dataset.,2021-04-07,Ishan Saran,https://github.com/isaranwrap/akiFlagger,TRUE,https://github.com/isaranwrap/akiflagger,4212,0,2021-03-10T20:31:51Z,NA
akmedoids,"Advances a novel adaptation of longitudinal k-means clustering 
    technique (Genolini et al. (2015) <doi:10.18637/jss.v065.i04>) 
    for grouping trajectories based on the similarities of their 
    long-term trends and determines the optimal solution based 
    on either the average silhouette width (Rousseeuw P. J. 1987) 
    or the Calinski-Harabatz criterion (Calinski and Harabatz (1974) 
    <doi:10.1080/03610927408827101>). Includes functions to extract 
    descriptive statistics and generate a visualisation of the 
    resulting groups, drawing methods from the 'ggplot2' library (Wickham H. (2016) 
    <doi:10.1007/978-3-319-24277-4>). The package also includes a number of 
    other useful functions for exploring and manipulating longitudinal 
    data prior to the clustering process.",2021-04-13,Monsuru Adepeju,https://cran.r-project.org/package=akmedoids,TRUE,https://github.com/manalytics/akmedoids,19118,0,2021-08-25T17:15:00Z,NA
aldvmm,"The goal of the package 'aldvmm' is to fit adjusted limited 
    dependent variable mixture models of health state utilities. Adjusted 
    limited dependent variable mixture models are finite mixtures of normal 
    distributions with an accumulation of density mass at the limits, and a gap 
    between 100% quality of life and the next smaller utility value. The 
    package 'aldvmm' uses the likelihood and expected value functions proposed 
    by Hernandez Alava and Wailoo (2015) <doi:10.1177/1536867X1501500307> using 
    normal component distributions and a multinomial logit model of 
    probabilities of component membership.",2021-07-19,Mark Pletscher,https://github.com/pletschm/aldvmm/,TRUE,https://github.com/pletschm/aldvmm,1306,1,2021-07-18T12:34:07Z,1306
alfred,"Provides direct access to the ALFRED (<https://alfred.stlouisfed.org>) and FRED (<https://fred.stlouisfed.org>) databases.
    Its functions return tidy data frames for different releases of the specified time series. 
    Note that this product uses the FRED© API but is not endorsed or certified by the Federal Reserve Bank of St. Louis.",2021-07-26,Onno Kleen,https://github.com/onnokleen/alfred/,TRUE,https://github.com/onnokleen/alfred,31162,13,2021-07-30T14:25:43Z,2397.076923076923
algorithmia,"The company, Algorithmia, houses the largest marketplace of online
    algorithms. This package essentially holds a bunch of REST wrappers that
    make it very easy to call algorithms in the Algorithmia platform and access
    files and directories in the Algorithmia data API. To learn more about the
    services they offer and the algorithms in the platform visit
    <http://algorithmia.com>. More information for developers can be found at
    <https://algorithmia.com/developers>.",2020-10-12,James Sutton,NA,TRUE,https://github.com/algorithmiaio/algorithmia-r,24436,14,2021-03-08T22:03:26Z,1745.4285714285713
aliases2entrez,"Queries multiple resources authors HGNC (2019) <https://www.genenames.org>, authors limma (2015) <doi:10.1093/nar/gkv007> 
    to find the correspondence between evolving nomenclature of human gene symbols, aliases, previous symbols or synonyms with 
    stable, curated gene entrezID from NCBI database. This allows fast, accurate and up-to-date correspondence
    between human gene expression datasets from various date and platform (e.g: gene symbol: BRCA1 - ID: 672).",2021-02-15,Raphael Bonnet,NA,TRUE,https://github.com/peyronlab/aliases2entrez,11633,1,2021-02-16T10:18:02Z,11633
alignfigR,"Create extensible figures of multiple sequence alignments, using the 'ggplot2' plotting engine. 'alignfigr' will create a baseline figure of a multiple sequence alignment which can be fully customized to the user's liking with standard 'ggplot2' features.",2018-07-05,Stephanie J. Spielman,https://github.com/sjspielman/alignfigR,TRUE,https://github.com/sjspielman/alignfigr,17486,6,2021-07-26T14:04:49Z,2914.3333333333335
allcontributors,"Acknowledge all contributors to a project via a
    single function call. The function appends to a 'README' or other
    specified file(s) a table with names of all individuals who
    contributed via code or repository issues.  The package also includes
    several additional functions to extract and quantify contributions to
    any repository.",2020-12-02,Mark Padgham,https://github.com/ropenscilabs/allcontributors,TRUE,https://github.com/ropenscilabs/allcontributors,3973,13,2021-05-28T09:50:50Z,305.61538461538464
alookr,"A collection of tools that support data splitting, predictive modeling, and model evaluation. 
    A typical function is to split a dataset into a training dataset and a test dataset. 
    Then compare the data distribution of the two datasets.
    Another feature is to support the development of predictive models and to compare the performance of several predictive models, 
    helping to select the best model. ",2021-02-22,Choonghyun Ryu,NA,TRUE,https://github.com/choonghyunryu/alookr,11064,8,2021-06-12T08:21:52Z,1383
alpaca,"Provides a routine to partial out factors with many levels during the
  optimization of the log-likelihood function of the corresponding generalized linear model (glm).
  The package is based on the algorithm described in Stammann (2018) <arXiv:1707.01815> and is
  restricted to glm's that are based on maximum likelihood estimation and non-linear. It also offers
  an efficient algorithm to recover estimates of the fixed effects in a post-estimation routine and 
  includes robust and multi-way clustered standard errors. Further the package provides analytical 
  bias corrections for binary choice models (logit and probit) derived by Fernandez-Val 
  and Weidner (2016) <doi:10.1016/j.jeconom.2015.12.014> and Hinz, Stammann, and Wanner (2020) 
  <arXiv:2004.12655>.",2020-10-30,Amrei Stammann,https://github.com/amrei-stammann/alpaca,TRUE,https://github.com/amrei-stammann/alpaca,57395,34,2021-07-18T16:35:36Z,1688.0882352941176
AlphaSimR,"The successor to the 'AlphaSim' software for breeding program 
  simulation [Faux et al. (2016) <doi:10.3835/plantgenome2016.02.0013>]. 
  Used for stochastic simulations of breeding programs to the level of DNA 
  sequence for every individual. Contained is a wide range of functions for 
  modeling common tasks in a breeding program, such as selection and crossing. 
  These functions allow for constructing simulations of highly complex plant and 
  animal breeding programs via scripting in the R software environment. Such 
  simulations can be used to evaluate overall breeding program performance and 
  conduct research into breeding program design, such as implementation of 
  genomic selection. Included is the 'Markovian Coalescent Simulator' ('MaCS') 
  for fast simulation of biallelic sequences according to a population 
  demographic history [Chen et al. (2009) <doi:10.1101/gr.083634.108>].",2021-07-29,Chris Gaynor,https://github.com/gaynorr/AlphaSimR,TRUE,https://github.com/gaynorr/alphasimr,28482,4,2021-07-28T15:04:02Z,7120.5
AlphaVantageClient,"Download data from the Alpha Vantage API (<https://www.alphavantage.co/>).
    Alpha Vantage is a RESTful API which provides various financial data, 
    including stock prices and technical indicators. 
    There is documentation for the underlying API available 
    here: <https://www.alphavantage.co/documentation/>. To get access to this API,
    the user needs to first claim an API key: <https://www.alphavantage.co/support/>.",2017-09-02,Alex Thompson,https://github.com/athompson1991/AlphaVantageClient,TRUE,https://github.com/athompson1991/alphavantageclient,19777,7,2021-01-10T18:18:01Z,2825.285714285714
alphavantager,"
    Alpha Vantage has free historical financial information. 
    All you need to do is get a free API key at <https://www.alphavantage.co>.
    Then you can use the R interface to retrieve free equity information.
    Refer to the Alpha Vantage website for more information.",2020-03-01,Matt Dancho,https://github.com/business-science/alphavantager,TRUE,https://github.com/business-science/alphavantager,349601,62,2020-11-02T17:44:02Z,5638.725806451613
altadata,"Functions for interacting directly with the 'ALTADATA' API. With this R package, developers can build applications around the 'ALTADATA' API without having to deal with accessing and managing requests and responses. 'ALTADATA' is a curated data marketplace for more information go to <https://www.altadata.io>.",2020-12-07,Emre Durukan,https://github.com/altabering/altadata-r,TRUE,https://github.com/altabering/altadata-r,3748,3,2020-12-29T14:50:17Z,1249.3333333333333
altair,"Interface to 'Altair' <https://altair-viz.github.io>, which itself 
  is a 'Python' interface to 'Vega-Lite' <https://vega.github.io/vega-lite/>.
  This package uses the 'Reticulate' framework 
  <https://rstudio.github.io/reticulate/> to manage the interface between R
  and 'Python'.",2021-01-14,Ian Lyttle,https://github.com/vegawidget/altair,TRUE,https://github.com/vegawidget/altair,15162,77,2021-01-14T13:39:11Z,196.9090909090909
altR2,"Provides alternatives to the normal adjusted R-squared estimator for the estimation of the multiple squared correlation in regression models, 
              as fitted by the lm() function. The alternative estimators are described in Karch (2016) <DOI:10.31234/osf.io/v8dz5>.",2019-09-23,Julian Karch,https://github.com/karchjd/altR2,TRUE,https://github.com/karchjd/altr2,11990,0,2021-01-21T18:49:59Z,NA
amanida,Combination of results for meta-analysis using significance and effect size only. P-values and fold-change are combined to obtain a global significance on each metabolite. Produces a volcano plot summarising the relevant results from meta-analysis. Vote-counting reports for metabolites. And explore plot to detect discrepancies between studies at a first glance. ,2021-08-03,Maria Llambrich,https://github.com/mariallr/amanida,TRUE,https://github.com/mariallr/amanida,1663,0,2021-08-03T08:47:32Z,NA
amapGeocode,Getting and parsing data of location geocode/reverse-geocode and administrative regions from 'AutoNavi Maps'<https://lbs.amap.com/api/webservice/summary> API.,2021-04-19,Han Chen,https://github.com/womeimingzi11/amapGeocode,TRUE,https://github.com/womeimingzi11/amapgeocode,5594,8,2021-04-21T03:31:15Z,699.25
AMAPVox,"Read, manipulate and write voxel spaces. Voxel spaces are read
    from text-based output files of the 'AMAPVox' software. 'AMAPVox' is a
    LiDAR point cloud voxelisation software that aims at estimating
    leaf area through several theoretical/numerical approaches. See more in the
    article Vincent et al. (2017) <doi:10.23708/1AJNMP> and the technical note
    Vincent et al. (2021) <doi:10.23708/1AJNMP>.",2021-07-09,Philippe Verley,https://github.com/umr-amap/AMAPVox,TRUE,https://github.com/umr-amap/amapvox,864,0,2021-09-02T16:00:10Z,NA
ambient,"Generation of natural looking noise has many application within 
    simulation, procedural generation, and art, to name a few. The 'ambient' 
    package provides an interface to the 'FastNoise' C++ library and allows for
    efficient generation of perlin, simplex, worley, cubic, value, and white 
    noise with optional pertubation in either 2, 3, or 4 (in case of simplex and
    white noise) dimensions.",2020-03-21,Thomas Lin Pedersen,"https://ambient.data-imaginist.com,
https://github.com/thomasp85/ambient",TRUE,https://github.com/thomasp85/ambient,20459,69,2021-01-20T11:35:02Z,296.5072463768116
ambiorix,"A web framework inspired by 'express.js' to build
  any web service from multi-page websites to 'RESTful' 
  application programming interfaces.",2021-01-27,John Coene,"https://github.com/JohnCoene/ambiorix,
https://ambiorix.john-coene.com",TRUE,https://github.com/johncoene/ambiorix,3406,87,2021-04-02T18:04:16Z,39.14942528735632
amen,"Analysis of dyadic network and relational data using additive and
    multiplicative effects (AME) models. The basic model includes
    regression terms, the covariance structure of the social relations model
    (Warner, Kenny and Stoto (1979) <DOI:10.1037/0022-3514.37.10.1742>, 
    Wong (1982) <DOI:10.2307/2287296>), and multiplicative factor
    models (Hoff(2009) <DOI:10.1007/s10588-008-9040-4>). 
    Several different link functions accommodate different
    relational data structures, including binary/network data, normal
    relational data, zero-inflated positive outcomes using a tobit model, ordinal relational data and data from
    fixed-rank nomination schemes. Several of these link functions are
    discussed in Hoff, Fosdick, Volfovsky and Stovel (2013) 
    <DOI:10.1017/nws.2013.17>. Development of this
    software was supported in part by NIH grant R01HD067509.",2020-12-16,Peter Hoff,https://github.com/pdhoff/amen,TRUE,https://github.com/pdhoff/amen,25933,14,2020-12-16T19:17:52Z,1852.357142857143
amerika,"A color palette generator inspired by American politics, with colors ranging from blue on the 
    left to gray in the middle and red on the right. A variety of palettes allow for a range of applications 
    from brief discrete scales (e.g., three colors for Democrats, Independents, and Republicans) to 
    continuous interpolated arrays including dozens of shades graded from blue (left) to red (right). This
    package greatly benefitted from building on the source code (with permission) from Ram and Wickham (2015).",2019-05-03,Philip Waggoner,NA,TRUE,https://github.com/pdwaggoner/amerika,19729,3,2020-12-08T13:33:12Z,6576.333333333333
AmpGram,"Predicts antimicrobial peptides using random forests trained on the
    n-gram encoded peptides. The implemented algorithm can be accessed from
    both the command line and shiny-based GUI. The AmpGram model is too large 
    for CRAN and it has to be downloaded separately from the repository:
    <https://github.com/michbur/AmpGramModel>.",2020-05-31,Michal Burdukiewicz,https://github.com/michbur/AmpGram,TRUE,https://github.com/michbur/ampgram,6067,1,2021-05-03T11:17:21Z,6067
ampir,"A toolkit to predict antimicrobial peptides from protein sequences on a genome-wide scale.
    It incorporates two support vector machine models (""precursor"" and ""mature"") trained on publicly available antimicrobial peptide data using calculated
    physico-chemical and compositional sequence properties described in Meher et al. (2017) <doi:10.1038/srep42362>.
    In order to support genome-wide analyses, these models are designed to accept any type of protein as input
    and calculation of compositional properties has been optimised for high-throughput use. For best results it is important to select the model that accurately 
    represents your sequence type: for full length proteins, it is recommended to use the default ""precursor"" model. The alternative, ""mature"", model is best suited
    for mature peptide sequences that represent the final antimicrobial peptide sequence after post-translational processing. For details see Fingerhut et al. (2020) <doi:10.1093/bioinformatics/btaa653>.
    The 'ampir' package is also available via a Shiny based GUI at <https://ampir.marine-omics.net/>.",2021-06-29,Legana Fingerhut,https://github.com/Legana/ampir,TRUE,https://github.com/legana/ampir,11894,17,2021-06-29T04:15:52Z,699.6470588235294
AMR,"Functions to simplify and standardise antimicrobial resistance (AMR)
    data analysis and to work with microbial and antimicrobial properties by
    using evidence-based methods and reliable reference data such as LPSN
    <doi:10.1099/ijsem.0.004332>.",2021-06-03,Matthijs S. Berends,"https://msberends.github.io/AMR/, https://github.com/msberends/AMR",TRUE,https://github.com/msberends/amr,48939,30,2021-09-01T14:52:55Z,1631.3
amt,"Manage and analyze animal movement data. The functionality of 'amt' includes methods to calculate home ranges <doi:10.1101/2020.08.19.256859v2>, track statistics (e.g. step lengths, speed, or turning angles), prepare data for fitting habitat selection analyses <doi:10.1101/2020.11.12.379834v4>, and simulation of space-use from fitted step-selection functions <doi:10.1002/ecs2.1771>.",2021-01-18,Johannes Signer,https://github.com/jmsigner/amt,TRUE,https://github.com/jmsigner/amt,33229,19,2021-07-26T11:41:38Z,1748.8947368421052
AnaCoDa,"Is a collection of models to analyze genome scale codon
        data using a Bayesian framework. Provides visualization
        routines and checkpointing for model fittings. Currently
        published models to analyze gene data for selection on codon
        usage based on Ribosome Overhead Cost (ROC) are: ROC (Gilchrist
        et al. (2015) <doi:10.1093/gbe/evv087>), and ROC with phi
        (Wallace & Drummond (2013) <doi:10.1093/molbev/mst051>). In
        addition 'AnaCoDa' contains three currently unpublished models.
        The FONSE (First order approximation On NonSense Error) model
        analyzes gene data for selection on codon usage against of
        nonsense error rates. The PA (PAusing time) and PANSE (PAusing
        time + NonSense Error) models use ribosome footprinting data to
        analyze estimate ribosome pausing times with and without
        nonsense error rate from ribosome footprinting data.",2020-09-15,Authors@R,https://github.com/clandere/AnaCoDa,TRUE,https://github.com/clandere/anacoda,21625,1,2021-03-01T10:54:34Z,21625
analogsea,"Provides a set of functions for interacting with the 'Digital
    Ocean' API <https://www.digitalocean.com/>, including
    creating images, destroying them, rebooting, getting details on regions, and
    available images.",2021-06-01,Scott Chamberlain,"https://github.com/sckott/analogsea (devel)
https://sckott.github.io/analogsea/ (docs)",TRUE,https://github.com/sckott/analogsea,89840,138,2021-08-31T02:36:57Z,651.0144927536232
analogue,"Fits Modern Analogue Technique and Weighted Averaging transfer
  	     function models for prediction of environmental data from species
	     data, and related methods used in palaeoecology.",2021-06-20,Gavin L. Simpson,https://github.com/gavinsimpson/analogue,TRUE,https://github.com/gavinsimpson/analogue,58790,11,2021-06-14T11:38:47Z,5344.545454545455
analysisPipelines,"Enables data scientists to compose pipelines of analysis which consist of data manipulation, exploratory analysis & reporting, as well as modeling steps. Data scientists can use tools of their choice through an R interface, and compose interoperable pipelines between R, Spark, and Python.
    Credits to Mu Sigma for supporting the development of the package.
    Note - To enable pipelines involving Spark tasks, the package uses the 'SparkR' package. 
    The SparkR package needs to be installed to use Spark as an engine within a pipeline. SparkR is distributed natively with Apache Spark and is not distributed on CRAN. The SparkR version needs to directly map to the Spark version (hence the native distribution), and care needs to be taken to ensure that this is configured properly.
    To install SparkR from Github, run the following command if you know the Spark version: 'devtools::install_github('apache/spark@v2.x.x', subdir='R/pkg')'.
    The other option is to install SparkR by running the following terminal commands if Spark has already been installed: '$ export SPARK_HOME=/path/to/spark/directory && cd $SPARK_HOME/R/lib/SparkR/ && R -e ""devtools::install('.')""'.",2020-06-12,Mu Sigma,https://github.com/Mu-Sigma/analysis-pipelines,TRUE,https://github.com/mu-sigma/analysis-pipelines,19514,21,2020-11-23T10:42:06Z,929.2380952380952
AnchorRegression,"Performs AnchorRegression proposed by Rothenhäusler et al. 2020. 
    The code is adapted from the original paper repository. (<https://github.com/rothenhaeusler/anchor-regression>)
    The code was developed independently from the authors of the paper. ",2021-01-06,Simon Zimmermann,https://github.com/simzim96/AnchorRegression,TRUE,https://github.com/simzim96/anchorregression,4141,2,2021-01-20T12:52:55Z,2070.5
Andromeda,"Storing very large data objects on a local drive, while still making it possible to manipulate the data in an efficient manner.",2021-07-02,Adam Black,"https://ohdsi.github.io/Andromeda/,
https://github.com/OHDSI/Andromeda",TRUE,https://github.com/ohdsi/andromeda,24536,4,2021-07-02T18:38:30Z,6134
anim.plots,"Simple animated versions of basic R plots, using the 'animation'
    package. Includes animated versions of plot, barplot, persp, contour,
    filled.contour, hist, curve, points, lines, text, symbols, segments, and
    arrows.",2021-04-30,David Hugh-Jones,https://github.com/hughjonesd/anim.plots,TRUE,https://github.com/hughjonesd/anim.plots,23420,8,2021-04-30T14:44:11Z,2927.5
animation,"Provides functions for animations in statistics, covering topics
    in probability theory, mathematical statistics, multivariate statistics,
    non-parametric statistics, sampling survey, linear models, time series,
    computational statistics, data mining and machine learning. These functions
    may be helpful in teaching statistics and data analysis. Also provided in this
    package are a series of functions to save animations to various formats, e.g.
    Flash, 'GIF', HTML pages, 'PDF' and videos. 'PDF' animations can be inserted
    into 'Sweave' / 'knitr' easily.",2018-12-11,Yihui Xie,https://yihui.name/animation,TRUE,https://github.com/yihui/animation,485137,177,2021-07-21T02:43:08Z,2740.8870056497176
animint2,"Functions are provided for defining animated,
 interactive data visualizations in R code, and rendering
 on a web page. The 2018 Journal of Computational and 
 Graphical Statistics paper,
 <doi:10.1080/10618600.2018.1513367>
 describes the concepts implemented.",2020-10-20,Toby Hocking,https://github.com/tdhock/animint2,TRUE,https://github.com/tdhock/animint2,9609,41,2021-08-30T23:03:31Z,234.3658536585366
aniview,Animate Shiny and R Markdown content when it comes into view using 'animate-css' effects thanks to 'jQuery AniView'.,2020-03-31,Félix Luginbuhl,"https://felixluginbuhl.com/aniview,
https://github.com/lgnbhl/aniview",TRUE,https://github.com/lgnbhl/aniview,7822,1,2021-02-27T11:18:45Z,7822
ANN2,"Training of neural networks for classification and regression tasks
    using mini-batch gradient descent. Special features include a function for 
    training autoencoders, which can be used to detect anomalies, and some 
    related plotting functions. Multiple activation functions are supported, 
    including tanh, relu, step and ramp. For the use of the step and ramp 
    activation functions in detecting anomalies using autoencoders, see 
    Hawkins et al. (2002) <doi:10.1007/3-540-46145-0_17>. Furthermore, 
    several loss functions are supported, including robust ones such as Huber 
    and pseudo-Huber loss, as well as L1 and L2 regularization. The possible 
    options for optimization algorithms are RMSprop, Adam and SGD with momentum.
    The package contains a vectorized C++ implementation that facilitates 
    fast training through mini-batch learning.",2020-12-01,Bart Lammers,https://github.com/bflammers/ANN2,TRUE,https://github.com/bflammers/ann2,37528,8,2021-02-05T08:29:15Z,4691
anndata,"A 'reticulate' wrapper for the Python package 'anndata'.
    Provides a scalable way of keeping track of data and learned
    annotations.  Used to read from and write to the h5ad file format.",2021-03-28,Philipp Angerer [ccp (<https://orcid.org/0000-0002-0369-2888>,"https://anndata.dynverse.org, https://github.com/dynverse/anndata",TRUE,https://github.com/dynverse/anndata,10520,11,2021-03-28T12:28:42Z,956.3636363636364
AnnotationBustR,Extraction of subsequences into FASTA files from GenBank annotations where gene names may vary among accessions. Borstein & O'Meara (2018) <doi:10.7717/peerj.5179>.,2020-09-24,Samuel R. Borstein,"https://github.com/sborstein/AnnotationBustR,
https://www.ncbi.nlm.nih.gov/nuccore,
https://peerj.com/articles/5179/",TRUE,https://github.com/sborstein/annotationbustr,22065,4,2020-09-27T01:31:00Z,5516.25
anomalize,"
    The 'anomalize' package enables a ""tidy"" workflow for detecting anomalies in data.
    The main functions are time_decompose(), anomalize(), and time_recompose().
    When combined, it's quite simple to decompose time series, detect anomalies,
    and create bands separating the ""normal"" data from the anomalous data at scale (i.e. for multiple time series). 
    Time series decomposition is used to remove trend and seasonal components via the time_decompose() function
    and methods include seasonal decomposition of time series by Loess (""stl"") and 
    seasonal decomposition by piecewise medians (""twitter""). The anomalize() function implements
    two methods for anomaly detection of residuals including using an inner quartile range (""iqr"")
    and generalized extreme studentized deviation (""gesd""). These methods are based on
    those used in the 'forecast' package and the Twitter 'AnomalyDetection' package. 
    Refer to the associated functions for specific references for these methods. ",2020-10-20,Matt Dancho,https://github.com/business-science/anomalize,TRUE,https://github.com/business-science/anomalize,172688,282,2020-10-20T17:22:20Z,612.3687943262412
anscombiser,"Anscombe's quartet are a set of four two-variable datasets that 
    have several common summary statistics but which have very different joint 
    distributions.  This becomes apparent when the data are plotted, which 
    illustrates the importance of using graphical displays in Statistics.  This
    package enables the creation of datasets that have identical marginal sample
    means and sample variances, sample correlation, least squares regression 
    coefficients and coefficient of determination.  The user supplies an initial 
    dataset, which is shifted, scaled and rotated in order to achieve target 
    summary statistics.  The general shape of the initial dataset is retained. 
    The target statistics can be supplied directly or calculated based on a 
    user-supplied dataset.  The 'datasauRus' package 
    <https://cran.r-project.org/package=datasauRus> provides further examples 
    of datasets that have markedly different scatter plots but share many 
    sample summary statistics.",2020-10-11,Paul J. Northrop,"https://paulnorthrop.github.io/anscombiser/,
https://github.com/paulnorthrop/anscombiser",TRUE,https://github.com/paulnorthrop/anscombiser,4606,10,2020-10-12T13:43:23Z,460.6
antaresProcessing,"
    Process results generated by 'Antares', a powerful open source software developed by
    RTE (Réseau de Transport d’Électricité) to simulate and study electric power systems (more information about
    'Antares' here: <https://github.com/AntaresSimulatorTeam/Antares_Simulator>).
    This package provides functions to create new columns like net load, load factors, upward and
    downward margins or to compute aggregated statistics like economic surpluses
    of consumers, producers and sectors.",2020-02-26,Veronique Bachelier,https://github.com/rte-antares-rpackage/antaresProcessing,TRUE,https://github.com/rte-antares-rpackage/antaresprocessing,28161,7,2021-01-18T13:45:38Z,4023
antaresRead,"Import, manipulate and explore results generated by 'Antares', a 
    powerful open source software developed by RTE (Réseau de Transport d’Électricité) to simulate and study electric power systems
    (more information about 'Antares' here : <https://antares-simulator.org/>).",2021-06-22,Veronique Bachelier,https://github.com/rte-antares-rpackage/antaresRead,TRUE,https://github.com/rte-antares-rpackage/antaresread,37078,9,2021-06-22T14:47:45Z,4119.777777777777
antaresViz,"Visualize results generated by Antares, a powerful open source software
    developed by RTE to simulate and study electric power systems
    (more information about 'Antares' here: <https://github.com/AntaresSimulatorTeam/Antares_Simulator>).
    This package provides functions that create interactive charts to help
    'Antares' users visually explore the results of their simulations.",2021-06-21,Veronique Bachelier,https://github.com/rte-antares-rpackage/antaresViz,TRUE,https://github.com/rte-antares-rpackage/antaresviz,24507,17,2021-06-21T08:40:29Z,1441.5882352941176
anthro,"Provides WHO Child Growth Standards (z-scores) with
             confidence intervals and standard errors around the
             prevalence estimates, taking into account complex sample designs.
             More information on the methods is
             available online:
             <https://www.who.int/childgrowth/standards/en/>.",2020-10-30,Dirk Schumacher,https://github.com/dirkschumacher/anthro,TRUE,https://github.com/dirkschumacher/anthro,21074,15,2020-10-30T21:20:49Z,1404.9333333333334
anticlust,"The method of anticlustering partitions a pool of
    elements into groups (i.e., anticlusters) in such a way that the
    between-group similarity is maximized and -- at the same time -- the
    within-group heterogeneity is maximized. This reverses the logic of
    cluster analysis that strives for high within-group homogeneity and
    low similarity of the different groups. Computationally,
    anticlustering is accomplished by maximizing instead of minimizing a
    clustering objective function, such as the intra-cluster variance
    (used in k-means clustering) or the sum of pairwise distances within
    clusters.  The function anticlustering() implements exact and
    heuristic anticlustering algorithms as described in Papenberg and Klau
    (2020; <doi:10.1037/met0000301>). The exact approach requires that the
    GNU linear programming kit
    (<https://www.gnu.org/software/glpk/glpk.html>) is available and the R
    package 'Rglpk' (<https://cran.R-project.org/package=Rglpk>) is
    installed. Some other functions are available to solve classical
    clustering problems. The function balanced_clustering() applies a
    cluster analysis under size constraints, i.e., creates equal-sized
    clusters. The function matching() can be used for (unrestricted,
    bipartite, or K-partite) matching. The function wce() can be used
    optimally solve the (weighted) cluster editing problem, also known as
    correlation clustering, clique partitioning problem or transitivity
    clustering.",2020-11-24,Martin Papenberg,https://github.com/m-Py/anticlust,TRUE,https://github.com/m-py/anticlust,7239,12,2021-07-22T09:08:10Z,603.25
antitrust,"A collection of tools for antitrust practitioners, including the ability to calibrate different consumer demand systems and simulate the effects of mergers under different competitive regimes.",2021-05-06,Charles Taragin and Michael Sandfort,https://github.com/luciu5/antitrust,TRUE,https://github.com/luciu5/antitrust,24673,0,2021-07-22T02:41:02Z,NA
AntMAN,"Fits finite Bayesian mixture models with a random number of components. The MCMC algorithm implemented is based on point processes as proposed by Argiento and De Iorio (2019) <arXiv:1904.09733> and offers a more computationally efficient alternative to reversible jump. Different mixture kernels can be specified: univariate Gaussian, multivariate Gaussian, univariate Poisson, and multivariate Bernoulli (latent class analysis). For the parameters characterising the mixture kernel, we specify conjugate priors, with possibly user specified hyper-parameters. We allow for different choices for the prior on the number of components: shifted Poisson, negative binomial, and point masses (i.e. mixtures with fixed number of components).",2021-07-23,Bruno Bodin,https://github.com/bbodin/AntMAN,TRUE,https://github.com/bbodin/antman,4288,2,2021-07-23T08:12:45Z,2144
anybadger,"
    You can use this package to create custom pipeline badges in a standard 'svg' format.
    This is useful for a company to use internally, where it may not be possible
    to create badges through external providers.
    This project was inspired by the 'anybadge' library in python.",2021-01-29,Lorenzo Meninato,https://github.com/lmeninato/anybadger,TRUE,https://github.com/lmeninato/anybadger,2994,0,2021-02-03T17:00:48Z,NA
anyflights,"Supplies a set of functions to query air travel data for user-
    specified years and airports. Datasets include on-time flights, airlines,
    airports, planes, and weather.",2020-10-22,Simon P. Couch,https://github.com/simonpcouch/anyflights,TRUE,https://github.com/simonpcouch/anyflights,10231,27,2021-02-21T16:12:43Z,378.9259259259259
anytime,"Convert input in any one of character, integer, numeric, factor,
 or ordered type into 'POSIXct' (or 'Date') objects, using one of a number of
 predefined formats, and relying on Boost facilities for date and time parsing.",2020-08-27,Dirk Eddelbuettel,http://dirk.eddelbuettel.com/code/anytime.html,TRUE,https://github.com/eddelbuettel/anytime,2209516,142,2021-06-10T02:49:52Z,15559.971830985916
aopdata,"Download data from the 'Access to Opportunities Project (AOP)'. The 
             'aopdata' package brings annual estimates of access to employment, 
             health and education services by transport mode, as well as data 
             on the spatial distribution of population, schools and health-care 
             facilities at a fine spatial resolution for all cities included in 
             the study. More info on the 'AOP' website <https://www.ipea.gov.br/acessooportunidades/en/>.",2021-04-30,Rafael H. M. Pereira,https://github.com/ipeaGIT/aopdata,TRUE,https://github.com/ipeagit/aopdata,3273,4,2021-04-30T15:31:42Z,818.25
aos,"Trigger animation effects on scroll on any HTML element 
    of 'shiny' and 'rmarkdown', such as any text or plot, thanks to 
    the 'AOS' Animate On Scroll jQuery library.",2020-04-29,Félix Luginbuhl,"https://felixluginbuhl.com/aos, https://github.com/lgnbhl/aos",TRUE,https://github.com/lgnbhl/aos,6459,2,2021-02-27T11:18:05Z,3229.5
apa,"Formatter functions in the 'apa' package take the return value of a
    statistical test function, e.g. a call to chisq.test() and return a string
    formatted according to the guidelines of the APA (American Psychological
    Association).",2020-04-21,Daniel Gromer,https://github.com/dgromer/apa,TRUE,https://github.com/dgromer/apa,35827,25,2020-11-17T13:51:42Z,1433.08
apaTables,"A common task faced by researchers is the creation of APA style
    (i.e., American Psychological Association style) tables from statistical
    output. In R a large number of function calls are often needed to obtain all of
    the desired information for a single APA style table. As well, the process of
    manually creating APA style tables in a word processor is prone to transcription
    errors. This package creates Word files (.doc files) containing APA style tables
    for several types of analyses. Using this package minimizes transcription errors
    and reduces the number commands needed by the user.",2021-01-04,David Stanley,https://github.com/dstanley4/apaTables,TRUE,https://github.com/dstanley4/apatables,107172,41,2021-06-02T15:27:18Z,2613.951219512195
APAtree,"Maps of the 'area potentially available' (APA) of trees is calculated from
  mapped forest stands using the approach from Gspaltl et al. (2012) <doi:10.1093/forestry/cps052>.
  This is done by computing a rasterized version of
  'weighted voronoi diagrams' using a an approximation of the trees competitive
  ability (e.g., crown radius, leaf area) as weight. The main output are 'Raster*'-
  objects from the 'raster' package that are stored together with the raw data in
  apa_list's, the main class of the 'APAtree' package. Aggregation functions are
  provided to calculate stand characteristics based on APA-maps such as relative
  proportions according to APA-size and the neighborhood diversity index NDiv
  (Glatthorn (2021) <doi:10.1016/j.ecolind.2021.108073>).",2021-08-17,Jonas Glatthorn,https://github.com/JonasGlatthorn/APAtree/,TRUE,https://github.com/jonasglatthorn/apatree,258,0,2021-08-20T13:01:47Z,NA
apcf,"The adapted pair correlation function transfers the concept of the
  pair correlation function from point patterns to patterns of objects of 
  finite size and irregular shape (e.g. lakes within a country). This is a 
  reimplementation of the method suggested by Nuske et al. (2009) 
  <doi:10.1016/j.foreco.2009.09.050> using the libraries 'GEOS' and 'GDAL' 
  directly instead of through 'PostGIS'. ",2021-07-27,Robert Nuske,https://github.com/rnuske/apcf,TRUE,https://github.com/rnuske/apcf,17511,6,2021-07-27T13:18:37Z,2918.5
apex,"Toolkit for the analysis of multiple gene data (Jombart et al. 2017) <doi:10.1111/1755-0998.12567>. 
    Apex implements the new S4 classes 'multidna', 'multiphyDat' and associated methods to handle aligned DNA sequences from multiple genes.",2020-04-11,Klaus Schliep,https://github.com/thibautjombart/apex,TRUE,https://github.com/thibautjombart/apex,35780,4,2020-12-11T17:12:32Z,8945
apexcharter,"Provides an 'htmlwidgets' interface to 'apexcharts.js'. 
  'Apexcharts' is a modern JavaScript charting library to build interactive charts and visualizations with simple API.
  'Apexcharts' examples and documentation are available here: <https://apexcharts.com/>.",2021-05-11,Victor Perrier,"https://github.com/dreamRs/apexcharter,
https://dreamrs.github.io/apexcharter/",TRUE,https://github.com/dreamrs/apexcharter,28076,96,2021-08-20T13:57:57Z,292.4583333333333
apisensr,"API for using 'episensr', Basic sensitivity analysis of the observed relative risks adjusting for unmeasured confounding and misclassification of the exposure/outcome, or both. See <https://cran.r-project.org/package=episensr>.",2021-03-15,Denis Haine,https://github.com/dhaine/apisensr,TRUE,https://github.com/dhaine/apisensr,2473,0,2021-03-14T16:20:14Z,NA
aplot,"For many times, we are not just aligning plots as what 'cowplot' and 'patchwork' did. Users would like to align associated information that requires axes to be exactly matched in subplots, e.g. hierarchical clustering with a heatmap. This package provides utilities to aligns associated subplots to a main plot at different sides (left, right, top and bottom) with axes exactly matched. ",2021-09-01,Guangchuang Yu,https://github.com/YuLab-SMU/aplot,TRUE,https://github.com/yulab-smu/aplot,64458,42,2021-09-03T09:50:37Z,1534.7142857142858
appler,"Using 'Apple App Store' <https://www.apple.com/app-store/> web scraping and 'iTunes' API 
    <https://affiliate.itunes.apple.com/resources/documentation/itunes-store-web-service-search-api/>
    to extract content information, app ratings and reviews.",2021-05-24,Ashley Baldry,"https://github.com/ashbaldry/appler,
https://affiliate.itunes.apple.com/resources/documentation/itunes-store-web-service-search-api/",TRUE,https://github.com/ashbaldry/appler,2818,10,2021-05-20T08:37:45Z,281.8
applicable,"A modeling package compiling applicability domain methods in R.
    It combines different methods to measure the amount of extrapolation new
    samples can have from the training set. See Netzeva et al (2005) 
    <doi:10.1177/026119290503300209> for an overview of applicability domains. ",2020-07-12,Marly Gotti,"https://github.com/tidymodels/applicable,
https://applicable.tidymodels.org",TRUE,https://github.com/tidymodels/applicable,8053,37,2021-04-21T16:03:53Z,217.64864864864865
apsimx,"The functions in this package inspect, read, edit and run files for 'APSIM' ""Next Generation"" ('JSON')
             and 'APSIM' ""Classic"" ('XML'). The files with an 'apsim' extension correspond to
	     'APSIM' Classic (7.x) - Windows only - and the ones with an 'apsimx' extension correspond to 'APSIM' ""Next Generation"".
	     For more information about 'APSIM' see (<https://www.apsim.info/>) and for 'APSIM'
	     next generation (<https://apsimnextgeneration.netlify.app/>). ",2021-08-20,Fernando Miguez,NA,TRUE,https://github.com/femiguez/apsimx,11620,16,2021-08-24T15:11:41Z,726.25
aqp,"The Algorithms for Quantitative Pedology (AQP) project was started in 2009 to organize a loosely-related set of concepts and source code on the topic of soil profile visualization, aggregation, and classification into this package (aqp). Over the past 8 years, the project has grown into a suite of related R packages that enhance and simplify the quantitative analysis of soil profile data. Central to the AQP project is a new vocabulary of specialized functions and data structures that can accommodate the inherent complexity of soil profile information; freeing the scientist to focus on ideas rather than boilerplate data processing tasks <doi:10.1016/j.cageo.2012.10.020>. These functions and data structures have been extensively tested and documented, applied to projects involving hundreds of thousands of soil profiles, and deeply integrated into widely used tools such as SoilWeb <https://casoilresource.lawr.ucdavis.edu/soilweb-apps/>. Components of the AQP project (aqp, soilDB, sharpshootR, soilReports packages) serve an important role in routine data analysis within the USDA-NRCS Soil Science Division. The AQP suite of R packages offer a convenient platform for bridging the gap between pedometric theory and practice.",2021-08-19,Dylan Beaudette,https://github.com/ncss-tech/aqp,TRUE,https://github.com/ncss-tech/aqp,120994,34,2021-09-03T08:05:52Z,3558.6470588235293
aquodom,"The Aquo Standard is the Dutch Standard for the exchange of 
    data in water management. With *aquodom* (short for aquo domaintables) 
    it is easy to exploit the API (<https://www.aquo.nl>) to download domaintables 
    of the Aquo Standard and use them in R.",2021-05-05,Johan van Tent,https://redtent.github.io/aquodom/,TRUE,https://github.com/redtent/aquodom,1712,0,2021-05-05T10:56:53Z,NA
arabic2kansuji,"Simple functions to convert given Arabic numerals to Kansuji  
    numerical figures that represent numbers written in Chinese characters.",2021-02-08,Mao Kobayashi,https://github.com/indenkun/arabic2kansuji,TRUE,https://github.com/indenkun/arabic2kansuji,4798,2,2021-02-08T09:42:38Z,2399
arc,"Implements the Classification-based on
    Association Rules (CBA) (Bing Liu, Wynne Hsu,	Yiming Ma (1999) <https://dl.acm.org/doi/10.5555/3000292.3000305>) algorithm for association rule classification (ARC).
    The package also contains several convenience methods that allow to automatically
    set CBA parameters (minimum confidence, minimum support) and it also natively
    handles numeric attributes by integrating a pre-discretization step.
    The rule generation phase is handled by the 'arules' package. 
    To further decrease the size of the CBA models produced by the 'arc' package, postprocessing by the 
    'qCBA' package is suggested. QCBA package can be obtained from CRAN archive or from <https://github.com/kliegr/QCBA>.",2020-11-07,Tomas Kliegr,https://github.com/kliegr/arc,TRUE,https://github.com/kliegr/arc,22368,5,2020-12-23T13:51:00Z,4473.6
ArchaeoPhases,"Provides a list of functions for the statistical analysis of archaeological dates and groups of dates. It is based on the post-processing of the Markov Chains whose stationary distribution is the posterior distribution of a series of dates. Such output can be simulated by different applications as for instance 'ChronoModel' (see <https://chronomodel.com/>), 'Oxcal' (see <https://c14.arch.ox.ac.uk/oxcal.html>) or 'BCal' (see <https://bcal.shef.ac.uk/>). The only requirement is to have a csv file containing a sample from the posterior distribution.  Note that this package interacts with data available through the 'ArchaeoPhases.dataset' package which is available in a separate repository.  The size of the 'ArchaeoPhases.dataset' package is approximately 4 MB.",2020-12-01,Anne Philippe,NA,TRUE,https://github.com/archaeostat/archaeophases,25266,5,2020-12-01T12:34:49Z,5053.2
archeofrag,"Methods to analyse fragmented objects in archaeology using refitting relationships between fragments scattered in archaeological spatial units (e.g. stratigraphic layers). Graphs and graph theory are used to model archaeological observations. The package is mainly based on the 'igraph' package for graph analysis. Functions can: 1) create, manipulate, and simulate fragmentation graphs, 2) measure the cohesion and admixture of archaeological spatial units, and 3) characterise the topology of a specific set of refitting relationships. An empirical dataset is also provided as an example.",2021-04-26,Sebastien Plutniak,https://github.com/sebastien-plutniak/archeofrag,TRUE,https://github.com/sebastien-plutniak/archeofrag,3998,7,2021-04-26T13:39:26Z,571.1428571428571
archetyper,A project template to support the data science workflow.,2021-03-17,Michael Korvink,https://mkorvink.github.io/archetyper/index.html,TRUE,https://github.com/mkorvink/archetyper,2272,5,2021-03-29T15:32:39Z,454.4
aRchi,"Provides a set of tools to manipulate, visualize and compute metrics from quantitative structural model of trees (i.e the so-called 'QSM') . It can be used in various context of forest ecology (i.e biomass estimation) and tree architecture (i.e architectural metrics), see Martin-Ducup et al. (2020) <doi:10.1111/1365-2435.13678>. The package is based on a new S4 class called 'aRchi'.",2021-04-16,Olivier Martin,https://github.com/umr-amap/aRchi,TRUE,https://github.com/umr-amap/archi,2121,4,2021-06-10T13:36:08Z,530.25
archiDART,"Analysis of complex plant root system architectures (RSA) using the output files created by Data Analysis of Root Tracings (DART), an open-access software dedicated to the study of plant root architecture and development across time series (Le Bot et al (2010) ""DART: a software to analyse root system architecture and development from captured images"", Plant and Soil, <DOI:10.1007/s11104-009-0005-2>), and RSA data encoded with the Root System Markup Language (RSML) (Lobet et al (2015) ""Root System Markup Language: toward a unified root architecture description language"", Plant Physiology, <DOI:10.1104/pp.114.253625>). More information can be found in Delory et al (2016) ""archiDART: an R package for the automated computation of plant root architectural traits"", Plant and Soil, <DOI:10.1007/s11104-015-2673-4>.",2021-02-11,Benjamin M Delory,https://archidart.github.io/,TRUE,https://github.com/archidart/archidart,24957,2,2021-01-24T18:30:08Z,12478.5
archive,"Bindings to 'libarchive' <http://www.libarchive.org> the
  Multi-format archive and compression library. Offers R connections and direct
  extraction for many archive formats including 'tar', 'ZIP', '7-zip', 'RAR',
  'CAB' and compression formats including 'gzip', 'bzip2', 'compress', 'lzma'
  and 'xz'.",2021-08-05,Jim Hester,"http://archive.r-lib.org/, https://github.com/r-lib/archive",TRUE,https://github.com/r-lib/archive,4618,105,2021-07-29T18:55:52Z,43.98095238095238
archiveRetriever,"Scraping content from archived web pages stored in
    the 'Internet Archive' (<https://archive.org>) using a systematic
    workflow.  Get an overview of the mementos available from the
    respective homepage, retrieve the Urls and links of the page and
    finally scrape the content. The final output is stored in tibbles,
    which can be then easily used for further analysis.",2021-05-27,Konstantin Gavras,https://github.com/liserman/archiveRetriever/,TRUE,https://github.com/liserman/archiveretriever,2791,5,2021-05-26T21:32:46Z,558.2
archivist,"Data exploration and modelling is a process in which a lot of data
    artifacts are produced. Artifacts like: subsets, data aggregates, plots,
    statistical models, different versions of data sets and different versions
    of results. The more projects we work with the more artifacts are produced
    and the harder it is to manage these artifacts. Archivist helps to store
    and manage artifacts created in R. Archivist allows you to store selected
    artifacts as a binary files together with their metadata and relations.
    Archivist allows to share artifacts with others, either through shared
    folder or github. Archivist allows to look for already created artifacts by
    using it's class, name, date of the creation or other properties. Makes it
    easy to restore such artifacts. Archivist allows to check if new artifact
    is the exact copy that was produced some time ago. That might be useful
    either for testing or caching.",2021-05-20,Przemyslaw Biecek,https://pbiecek.github.io/archivist/,TRUE,https://github.com/pbiecek/archivist,64391,78,2021-05-20T13:39:49Z,825.525641025641
ARCokrig,"For emulating multifidelity computer models. The major methods include univariate autoregressive cokriging and multivariate autoregressive cokriging. The autoregressive cokriging methods are implemented for both hierarchically nested design and non-nested design. For hierarchically nested design, the model parameters are estimated via standard optimization algorithms; For non-nested design, the model parameters are estimated via Monte Carlo expectation-maximization (MCEM) algorithms. In both cases, the priors are chosen such that the posterior distributions are proper. Notice that the uniform priors on range parameters in the correlation function lead to improper posteriors. This should be avoided when Bayesian analysis is adopted. The development of objective priors for autoregressive cokriging models can be found in Pulong Ma (2019) <arXiv:1910.10225>. The development of the multivariate autoregressive cokriging models with possibly non-nested design can be found in Pulong Ma, Georgios Karagiannis, Bledar A Konomi, Taylor G Asher, Gabriel R Toro, and Andrew T Cox (2019) <arXiv:1909.01836>.",2020-07-08,Pulong Ma,https://github.com/pulongma/ARCokrig/issues,TRUE,https://github.com/pulongma/arcokrig,5272,0,2021-04-11T18:01:11Z,NA
arcos,"A wrapper for the 'ARCOS API' <https://arcos-api.ext.nile.works/__swagger__/>
        that returns raw and summarized data frames from the 
        Drug Enforcement Administration’s Automation of Reports and Consolidated Orders System, 
        a database that monitors controlled substances transactions between manufacturers and 
        distributors which was made public by The Washington Post and The Charleston Gazette-Mail.",2021-02-18,Andrew Ba Tran,https://github.com/wpinvestigative/arcos,TRUE,https://github.com/wpinvestigative/arcos,12061,22,2021-03-01T19:53:36Z,548.2272727272727
ARDL,"Creates complex autoregressive distributed lag (ARDL) models
    providing just the order and automatically constructs the underlying
    unrestricted and restricted error correction model (ECM). It also performs
    the bounds-test for cointegration as described in Pesaran et al. (2001) <doi:10.1002/jae.616> and provides the multipliers and the cointegrating
    equation.",2021-01-10,Kleanthis Natsiopoulos,https://github.com/Natsiopoulos/ARDL,TRUE,https://github.com/natsiopoulos/ardl,13456,6,2021-02-06T13:04:22Z,2242.6666666666665
areal,"A pipeable, transparent implementation of areal weighted interpolation
    with support for interpolating multiple variables in a single function call.
    These tools provide a full-featured workflow for validation and estimation
    that fits into both modern data management (e.g. tidyverse) and spatial 
    data (e.g. sf) frameworks.",2020-07-23,Christopher Prener,https://github.com/slu-openGIS/areal,TRUE,https://github.com/slu-opengis/areal,23008,74,2020-12-18T01:48:57Z,310.9189189189189
arealDB,"Many relevant applications in the environmental and socioeconomic 
    sciences use areal data, such as biodiversity checklists, agricultural statistics, 
    or socioeconomic surveys. For applications that surpass the spatial, temporal or 
    thematic scope of any single data source, data must be integrated from several 
    heterogeneous sources. Inconsistent concepts, definitions, or messy data tables 
    make this a tedious and error-prone process. 'arealDB' tackles those problems and 
    helps the user to integrate a harmonised databases of areal data. Read the pre-print 
    at Ehrmann, Seppelt & Meyer (2020) <arXiv:1909.06610>.",2021-07-01,Steffen Ehrmann,https://github.com/EhrmannS/arealDB,TRUE,https://github.com/ehrmanns/arealdb,5853,1,2021-07-01T16:06:51Z,5853
arenar,"Generates data for challenging machine learning models in 'Arena'
    <https://arena.drwhy.ai> - an interactive web application. You can start
    the server with XAI (Explainable Artificial Intelligence) plots to be
    generated on-demand or precalculate and auto-upload data file beside
    shareable 'Arena' URL.",2020-10-01,Piotr Piątyszek,"https://arenar.drwhy.ai, https://github.com/ModelOriented/ArenaR",TRUE,https://github.com/modeloriented/arenar,6261,24,2020-09-30T23:12:33Z,260.875
argonR,"R wrapper around the argon HTML library.
    More at <https://demos.creative-tim.com/argon-design-system/>.",2019-11-27,David Granjon,https://github.com/RinteRface/argonR,TRUE,https://github.com/rinterface/argonr,83223,46,2021-04-16T21:44:37Z,1809.195652173913
argparse,"A command line parser to
    be used with Rscript to write ""#!"" shebang scripts that gracefully
    accept positional and optional arguments and automatically generate usage.",2021-08-06,Trevor L Davis,https://github.com/trevorld/r-argparse,TRUE,https://github.com/trevorld/r-argparse,569735,59,2021-08-06T21:45:13Z,9656.525423728814
ari,"Create videos from 'R Markdown' documents, or images and audio
    files. These images can come from image files or HTML slides, and the audio
    files can be provided by the user or computer voice narration can be created
    using 'Amazon Polly'. The purpose of this package is to allow users to create
    accessible, translatable, and reproducible lecture videos. See
    <https://aws.amazon.com/polly/> for more information.",2020-02-08,Sean Kross,http://github.com/seankross/ari,TRUE,https://github.com/seankross/ari,22621,124,2020-12-09T02:47:56Z,182.42741935483872
ariExtra,"Leverages the 'ari' package and other tools to create automated 
    courses from slides and a script.  Also, uploads these to 
    'YouTube' and other services using 'tuber' package.",2021-06-23,John Muschelli,https://github.com/jhudsl/ariExtra,TRUE,https://github.com/jhudsl/ariextra,6857,4,2021-06-22T18:13:07Z,1714.25
arkdb,"Flat text files provide a robust, compressible, and portable
  way to store tables from databases.  This package provides convenient
  functions for exporting tables from relational database connections
  into compressed text files and streaming those text files back into
  a database without requiring the whole table to fit in working memory.",2021-04-05,Carl Boettiger,https://github.com/ropensci/arkdb,TRUE,https://github.com/ropensci/arkdb,35285,65,2021-04-12T15:46:41Z,542.8461538461538
arkhe,"A collection of classes that represent
    archaeological data. This package provides a set of S4 classes that
    represent different special types of matrix (absolute/relative
    frequency, presence/absence data, co-occurrence matrix, etc.) upon
    which package developers can build subclasses. It also provides a set
    of generic methods (mutators and coercion mechanisms) and functions
    (e.g. summary statistics, predicates). In addition, a few classes of 
    general interest (e.g. that represent stratigraphic relationships) 
    are implemented.",2021-05-14,Nicolas Frerebeau,"https://arkhe.tesselle.org, https://github.com/tesselle/arkhe",TRUE,https://github.com/tesselle/arkhe,13417,8,2021-09-01T17:50:09Z,1677.125
arm,"Functions to accompany A. Gelman and J. Hill, Data Analysis Using Regression and Multilevel/Hierarchical Models, Cambridge University Press, 2007.",2020-07-27,Yu-Sung Su,https://CRAN.R-project.org/package=arm,TRUE,https://github.com/suyusung/arm,1592442,18,2020-10-06T22:48:25Z,88469
aroma.affymetrix,A cross-platform R framework that facilitates processing of any number of Affymetrix microarray samples regardless of computer system.  The only parameter that limits the number of chips that can be processed is the amount of available disk space.  The Aroma Framework has successfully been used in studies to process tens of thousands of arrays.  This package has actively been used since 2006.,2019-06-23,Henrik Bengtsson,"https://www.aroma-project.org/,
https://github.com/HenrikBengtsson/aroma.affymetrix",TRUE,https://github.com/henrikbengtsson/aroma.affymetrix,53143,7,2021-01-06T04:23:56Z,7591.857142857143
aroma.core,"Core methods and classes used by higher-level 'aroma.*' packages
        part of the Aroma Project, e.g. 'aroma.affymetrix' and 'aroma.cn'.",2021-01-05,Henrik Bengtsson,"https://github.com/HenrikBengtsson/aroma.core,
https://www.aroma-project.org/",TRUE,https://github.com/henrikbengtsson/aroma.core,60007,1,2021-01-05T19:30:02Z,60007
arpr,"Provides convenience functions for programming with
    'magrittr' pipes. Conditional pipes, a string prefixer and a function
    to pipe the given object into a specific argument given by character
    name are currently supported. It is named after the dadaist Hans Arp,
    a friend of Rene Magritte.",2021-08-02,Sébastien Rochette,https://github.com/statnmap/arpr,TRUE,https://github.com/statnmap/arpr,22332,5,2021-08-01T12:10:57Z,4466.4
arrangements,"Fast generators and iterators for permutations, combinations,
    integer partitions and compositions. The arrangements are in
    lexicographical order and generated iteratively in a memory efficient manner. 
    It has been demonstrated that 'arrangements' outperforms most existing
    packages of similar kind. Benchmarks could be found at
    <https://randy3k.github.io/arrangements/articles/benchmark.html>.",2020-09-13,Randy Lai,https://github.com/randy3k/arrangements,TRUE,https://github.com/randy3k/arrangements,86639,34,2021-06-05T08:24:29Z,2548.205882352941
arsenal,"An Arsenal of 'R' functions for large-scale statistical summaries,
  which are streamlined to work within the latest reporting tools in 'R' and
  'RStudio' and which use formulas and versatile summary statistics for summary
  tables and models. The primary functions include tableby(), a Table-1-like
  summary of multiple variable types 'by' the levels of one or more categorical
  variables; paired(), a Table-1-like summary of multiple variable types paired across
  two time points; modelsum(), which performs simple model fits on one or more endpoints
  for many variables (univariate or adjusted for covariates);
  freqlist(), a powerful frequency table across many categorical variables;
  comparedf(), a function for comparing data.frames; and
  write2(), a function to output tables to a document.",2021-06-04,Ethan Heinzen,"https://github.com/mayoverse/arsenal,
https://cran.r-project.org/package=arsenal,
https://mayoverse.github.io/arsenal/",TRUE,https://github.com/mayoverse/arsenal,151521,190,2021-09-01T20:44:53Z,797.478947368421
ARTool,"The Aligned Rank Transform for nonparametric
    factorial ANOVAs as described by J. O. Wobbrock,
    L. Findlater, D. Gergle, & J. J. Higgins, ""The Aligned
    Rank Transform for nonparametric factorial analyses
    using only ANOVA procedures"", CHI 2011 <DOI:10.1145/1978942.1978963>.",2021-02-24,Matthew Kay,https://github.com/mjskay/ARTool/,TRUE,https://github.com/mjskay/artool,39502,32,2021-04-26T23:14:51Z,1234.4375
aRtsy,Provides various algorithms for creating artworks in the 'ggplot2' language that incorporate some form of randomness.,2021-08-16,Koen Derks,"https://koenderks.github.io/aRtsy/,
https://github.com/koenderks/aRtsy,
https://twitter.com/aRtsy_package",TRUE,https://github.com/koenderks/artsy,443,38,2021-09-03T00:49:46Z,11.657894736842104
arules,"Provides the infrastructure for representing,
    manipulating and analyzing transaction data and patterns (frequent
    itemsets and association rules). Also provides
    C implementations of the association mining algorithms Apriori and Eclat. 
    Hahsler, Gruen and Hornik (2005) <doi:10.18637/jss.v014.i15>.",2021-05-17,Michael Hahsler,https://github.com/mhahsler/arules,TRUE,https://github.com/mhahsler/arules,1662320,146,2021-09-02T22:14:04Z,11385.753424657534
arulesCBA,Provides the infrastructure for association rule-based classification including algorithms like Classification Based on Associations (CBA).,2020-04-20,Michael Hahsler,https://github.com/ianjjohnson/arulesCBA,TRUE,https://github.com/ianjjohnson/arulescba,43476,35,2020-12-14T19:41:49Z,1242.1714285714286
arulesViz,Extends package 'arules' with various visualization techniques for association rules and itemsets. The package also includes several interactive visualizations for rule exploration. Michael Hahsler (2017) <doi:10.32614/RJ-2017-047>.,2021-05-21,Michael Hahsler,https://github.com/mhahsler/arulesViz,TRUE,https://github.com/mhahsler/arulesviz,560179,42,2021-08-02T09:19:31Z,13337.595238095239
aRxiv,"An interface to the API for 'arXiv'
    (<https://arxiv.org>), a repository of electronic preprints for
    computer science, mathematics, physics, quantitative biology,
    quantitative finance, and statistics.",2019-08-08,Karthik Ram,https://github.com/ropensci/aRxiv,TRUE,https://github.com/ropensci/arxiv,34403,48,2021-07-17T18:48:30Z,716.7291666666666
ascii,"Coerce R object to 'asciidoc', 'txt2tags',
    'restructuredText', 'org', 'textile' or 'pandoc' syntax.
    Package comes with a set of drivers for 'Sweave'.",2020-09-17,Mark Clements,https://github.com/mclements/ascii,TRUE,https://github.com/mclements/ascii,30070,4,2020-09-23T09:42:59Z,7517.5
asciiSetupReader,"Lets you open a fixed-width ASCII file (.txt or
    .dat) that has an accompanying setup file (.sps or .sas). These file
    combinations are sometimes referred to as .txt+.sps, .txt+.sas,
    .dat+.sps, or .dat+.sas. This will only run in a txt-sps or txt-sas
    pair in which the setup file contains instructions to open that text
    file. It will NOT open other text files, .sav, .sas, or .por data
    files. Fixed-width ASCII files with setup files are common in older
    (pre-2000) government data.",2021-02-03,Jacob Kaplan,https://github.com/jacobkap/asciiSetupReader,TRUE,https://github.com/jacobkap/asciisetupreader,25150,4,2021-04-26T19:46:46Z,6287.5
ashr,"The R package 'ashr' implements an Empirical Bayes
    approach for large-scale hypothesis testing and false discovery
    rate (FDR) estimation based on the methods proposed in
    M. Stephens, 2016, ""False discovery rates: a new deal"",
    <DOI:10.1093/biostatistics/kxw041>. These methods can be applied
    whenever two sets of summary statistics---estimated effects and
    standard errors---are available, just as 'qvalue' can be applied
    to previously computed p-values. Two main interfaces are
    provided: ash(), which is more user-friendly; and ash.workhorse(),
    which has more options and is geared toward advanced users. The
    ash() and ash.workhorse() also provides a flexible modeling
    interface that can accommodate a variety of likelihoods (e.g.,
    normal, Poisson) and mixture priors (e.g., uniform, normal).",2020-02-20,Peter Carbonetto,https://github.com/stephens999/ashr,TRUE,https://github.com/stephens999/ashr,71494,69,2020-11-02T20:01:30Z,1036.144927536232
AsioHeaders,"'Asio' is a cross-platform C++ library for network and low-level
 I/O programming that provides developers with a consistent asynchronous model
 using a modern C++ approach. It is also included in Boost but requires linking
 when used with Boost. Standalone it can be used header-only (provided a recent
 compiler). 'Asio' is written and maintained by Christopher M. Kohlhoff, and
 released under the 'Boost Software License', Version 1.0.",2020-07-07,Dirk Eddelbuettel,NA,TRUE,https://github.com/eddelbuettel/asioheaders,228328,10,2021-03-20T00:58:13Z,22832.8
aslib,"Provides an interface to the algorithm selection benchmark library
    at <http://www.aslib.net> and the 'LLAMA' package
    (<https://cran.r-project.org/package=llama>) for building
    algorithm selection models; see Bischl et al. (2016)
    <doi:10.1016/j.artint.2016.04.003>.",2020-05-24,Bernd Bischl,https://github.com/coseal/aslib-r/,TRUE,https://github.com/coseal/aslib-r,16381,5,2021-04-29T00:24:36Z,3276.2
aSPU,"R codes for the (adaptive) Sum of Powered Score ('SPU' and 'aSPU')
    tests, inverse variance weighted Sum of Powered score ('SPUw' and 'aSPUw') tests
    and gene-based and some pathway based association tests (Pathway based Sum of
    Powered Score tests ('SPUpath'), adaptive 'SPUpath' ('aSPUpath') test, 'GEEaSPU'
    test for multiple traits - single 'SNP' (single nucleotide polymorphism)
    association in generalized estimation equations, 'MTaSPUs' test for multiple
    traits - single 'SNP' association with Genome Wide Association Studies ('GWAS')
    summary statistics, Gene-based Association Test that uses an extended 'Simes'
    procedure ('GATES'), Hybrid Set-based Test ('HYST') and extended version
    of 'GATES' test for pathway-based association testing ('GATES-Simes'). ).
    The tests can be used with genetic and other data sets with covariates. The
    response variable is binary or quantitative. Summary; (1) Single trait-'SNP' set
    association with individual-level data ('aSPU', 'aSPUw', 'aSPUr'), (2) Single trait-'SNP'
    set association with summary statistics ('aSPUs'), (3) Single trait-pathway
    association with individual-level data ('aSPUpath'), (4) Single trait-pathway
    association with summary statistics ('aSPUsPath'), (5) Multiple traits-single
    'SNP' association with individual-level data ('GEEaSPU'), (6) Multiple traits-
    single 'SNP' association with summary statistics ('MTaSPUs'), (7) Multiple traits-'SNP' set association with summary statistics('MTaSPUsSet'), (8) Multiple traits-pathway association with summary statistics('MTaSPUsSetPath').",2021-06-28,Il-Youp Kwak and others,https://github.com/ikwak2/aSPU,TRUE,https://github.com/ikwak2/aspu,25148,7,2021-06-28T16:38:24Z,3592.5714285714284
assemblerr,"Construct pharmacometric nonlinear mixed effect models by combining 
    predefined model components and automatically generate model code for NONMEM. 
    Models are created by combining parameter and observation models, algebraic 
    relationships, compartments, and flows. Pharmacokinetic models can be assembled
    from the higher-order components: absorption, distribution, and elimination. 
    The generated code is optimized for performance by recognizing, for example,
    linear differential equations or differential equations with an analytic 
    solution.",2021-07-28,Sebastian Ueckert,https://github.com/UUPharmacometrics/assemblerr,TRUE,https://github.com/uupharmacometrics/assemblerr,635,4,2021-08-02T08:32:25Z,158.75
assert,"Lightweight validation tool for checking function arguments and 
  validating data analysis scripts. This is an alternative to stopifnot() from 
  the 'base' package  and to assert_that() from the 'assertthat' package. It 
  provides more informative error messages and facilitates debugging.",2020-11-28,Olivier Binette,https://github.com/OlivierBinette/assert,TRUE,https://github.com/olivierbinette/assert,6695,3,2021-04-26T13:23:35Z,2231.6666666666665
assignPOP,"Use Monte-Carlo and K-fold cross-validation coupled with machine-
    learning classification algorithms to perform population assignment, with
    functionalities of evaluating discriminatory power of independent training
    samples, identifying informative loci, reducing data dimensionality for genomic
    data, integrating genetic and non-genetic data, and visualizing results.",2020-11-17,Kuan-Yu (Alex) Chen,https://github.com/alexkychen/assignPOP,TRUE,https://github.com/alexkychen/assignpop,25008,15,2021-08-17T19:55:13Z,1667.2
astsa,"Data sets and scripts to accompany Time Series Analysis and Its Applications: With R Examples (4th ed), by R.H. Shumway and D.S. Stoffer. Springer Texts in Statistics, 2017, <DOI:10.1007/978-3-319-52452-8>, and Time Series:  A Data Analysis Approach Using R. Chapman-Hall, 2019, <DOI:10.1201/9780429273285>.",2021-09-02,David Stoffer,"https://github.com/nickpoison/astsa/,
https://www.stat.pitt.edu/stoffer/tsa4/,
https://www.stat.pitt.edu/stoffer/tsda/",TRUE,https://github.com/nickpoison/astsa,291063,61,2021-09-02T15:17:08Z,4771.524590163935
asymptor,"Estimate the lower and upper bound of asymptomatic cases in an 
    epidemic using the capture/recapture methods from Böhning et al. (2020) 
    <doi:10.1016/j.ijid.2020.06.009> and Rocchetti et al. (2020) 
    <doi:10.1101/2020.07.14.20153445>.",2020-12-14,Hugo Gruson,"https://bisaloo.github.io/asymptor/,
https://github.com/bisaloo/asymptor",TRUE,https://github.com/bisaloo/asymptor,4425,1,2021-02-23T11:32:38Z,4425
atable,"Create Tables for Reporting Clinical Trials.
  Calculates descriptive statistics and hypothesis tests, 
  arranges the results in a table ready for reporting with LaTeX, HTML or Word.",2020-12-15,Armin Ströbel,https://github.com/arminstroebel/atable,TRUE,https://github.com/arminstroebel/atable,22292,8,2020-12-15T16:56:06Z,2786.5
AtmChile,Download air quality and meteorological information of Chile from  the National Air Quality System (S.I.N.C.A.)<https://sinca.mma.gob.cl/> dependent on the Ministry of the Environment and the Meteorological Directorate of Chile (D.M.C.)<http://www.meteochile.gob.cl/> dependent on the Directorate General of Civil Aeronautics.,2021-08-18,Francisco Catalan Meyer,https://github.com/franciscoxaxo/AtmChile,TRUE,https://github.com/franciscoxaxo/atmchile,2072,1,2021-08-27T21:35:16Z,2072
attachment,"Tools to help manage dependencies during package
    development.  This can retrieve all dependencies that are used in R
    files in the ""R"" directory, in Rmd files in ""vignettes"" directory and
    in 'roxygen2' documentation of functions. There is a function to
    update the Description file of your package and a function to create a
    file with the R commands to install all dependencies of your package.
    All functions to retrieve dependencies of R scripts and Rmd files can
    be used independently of a package development.",2021-01-21,Sébastien Rochette,"https://thinkr-open.github.io/attachment/,
https://github.com/Thinkr-open/attachment",TRUE,https://github.com/thinkr-open/attachment,44823,74,2021-07-07T07:10:03Z,605.7162162162163
auctionr,"Estimates a first-price auction model with conditionally independent
  private values as described in MacKay (2020) <doi:10.2139/ssrn.3096534>. The
  model allows for unobserved heterogeneity that is common to all bidders in
  addition to observable heterogeneity.",2020-06-25,Alex MacKay,https://github.com/ajmack/auctionr,TRUE,https://github.com/ajmack/auctionr,5167,0,2020-09-30T13:54:48Z,NA
auditor,"Provides an easy to use unified interface for creating validation plots for any model. 
  The 'auditor' helps to avoid repetitive work consisting of writing code needed to create residual plots. 
  This visualizations allow to asses and compare the goodness of fit, performance, and similarity of models. ",2021-07-26,Alicja Gosiewska,https://github.com/ModelOriented/auditor,TRUE,https://github.com/modeloriented/auditor,27954,55,2021-08-25T10:38:01Z,508.25454545454545
auk,"Extract and process bird sightings records from
    eBird (<http://ebird.org>), an online tool for recording bird
    observations.  Public access to the full eBird database is via the
    eBird Basic Dataset (EBD; see <http://ebird.org/ebird/data/download>
    for access), a downloadable text file. This package is an interface to
    AWK for extracting data from the EBD based on taxonomic, spatial, or
    temporal filters, to produce a manageable file size that can be
    imported into R.",2021-07-21,Matthew Strimas-Mackey,"https://github.com/CornellLabofOrnithology/auk,
https://cornelllabofornithology.github.io/auk/",TRUE,https://github.com/cornelllabofornithology/auk,33682,91,2021-07-21T15:40:21Z,370.13186813186815
autocogs,Automatically calculates cognostic groups for plot objects and list column plot objects.  Results are returned in a nested data frame.,2021-05-29,Barret Schloerke,https://github.com/schloerke/autocogs,TRUE,https://github.com/schloerke/autocogs,34085,4,2021-05-29T17:06:14Z,8521.25
autoCovariateSelection,"Contains functions to implement automated covariate selection using methods described in the
             high-dimensional propensity score (HDPS) algorithm by Schneeweiss et.al. Covariate adjustment in real-world-observational-data (RWD) is important for
             for estimating adjusted outcomes and this can be done by using methods such as, but not limited to, propensity score 
             matching, propensity score weighting and regression analysis. While these methods strive to statistically adjust for 
             confounding, the major challenge is in selecting the potential covariates that can bias the outcomes comparison estimates 
             in observational RWD (Real-World-Data). This is where the utility of automated covariate selection comes in. 
             The functions in this package help to implement the three major steps of automated covariate selection as described by
             Schneeweiss et. al elsewhere. These three functions, in order of the steps required to execute automated covariate 
             selection are, get_candidate_covariates(), get_recurrence_covariates() and get_prioritised_covariates(). 
             In addition to these functions, a sample real-world-data from publicly available de-identified medical claims data is 
             also available for running examples and also for further exploration. The original article where the algorithm is described 
             by Schneeweiss et.al. (2009) <doi:10.1097/EDE.0b013e3181a663cc> .",2020-12-14,Dennis Robert,https://github.com/technOslerphile/autoCovariateSelection,TRUE,https://github.com/technoslerphile/autocovariateselection,3463,0,2021-08-04T20:47:25Z,NA
autoFC,"Forced-choice (FC) response has gained increasing popularity and interest for its resistance to faking when well-designed (Cao & Drasgow, 2019 <doi:10.1037/apl0000414>). To established well-designed FC scales, typically each item within a block should measure different trait and have similar level of social desirability (Zhang et al., 2020 <doi:10.1177/1094428119836486>). Recent study also suggests the importance of high inter-item agreement of social desirability between items within a block (Pavlov et al., 2021 <doi:10.31234/osf.io/hmnrc>). In addition to this, FC developers may also need to maximize factor loading differences (Brown & Maydeu-Olivares, 2011 <doi:10.1177/0013164410375112>) or minimize item location differences (Cao & Drasgow, 2019 <doi:10.1037/apl0000414>) depending on scoring models. Decision of which items should be assigned to the same block, termed item pairing, is thus critical to the quality of an FC test. This pairing process is essentially an optimization process which is currently carried out manually. However, given that we often need to simultaneously meet multiple objectives, manual pairing becomes impractical or even not feasible once the number of latent traits and/or number of items per trait are relatively large. To address these problems, autoFC is developed as a practical tool for facilitating the automatic construction of FC tests, essentially exempting users from the burden of manual item pairing and reducing the computational costs and biases induced by simple ranking methods. Given characteristics of each item (and item responses), FC tests can be automatically constructed based on user-defined pairing criteria and weights as well as customized optimization behavior. Users can also construct parallel forms of the same test following the same pairing rules.",2021-06-07,Mengtong Li,https://github.com/tspsyched/autoFC,TRUE,https://github.com/tspsyched/autofc,1194,0,2021-06-07T16:39:15Z,NA
autoimage,"Functions for displaying multiple images  or scatterplots with a color 
    scale, i.e., heat maps, possibly with projected coordinates.  The
    package relies on the base graphics system, so graphics are
    rendered rapidly.",2021-03-16,Joshua French,NA,TRUE,https://github.com/jfrench/autoimage,31185,6,2021-03-16T02:50:47Z,5197.5
autokeras,"R Interface to 'AutoKeras' <https://autokeras.com/>.
    'AutoKeras' is an open source software library for Automated Machine
    Learning (AutoML). The ultimate goal of AutoML is to provide easily
    accessible deep learning tools to domain experts with limited data science
    or machine learning background. 'AutoKeras' provides functions to
    automatically search for architecture and hyperparameters of deep
    learning models.",2021-01-17,Juan Cruz Rodriguez,https://github.com/r-tensorflow/autokeras,TRUE,https://github.com/r-tensorflow/autokeras,10933,71,2021-01-16T23:58:42Z,153.98591549295776
automagic,Parse R code in a given directory for R packages and attempt to install them from CRAN or GitHub. Optionally use a dependencies file for tighter control over which package versions to install.,2019-03-05,Cole Brokamp,https://github.com/cole-brokamp/automagic,TRUE,https://github.com/cole-brokamp/automagic,22634,49,2021-03-24T19:01:47Z,461.9183673469388
autoMrP,"A tool that improves the prediction performance of multilevel
    regression with post-stratification (MrP) by combining a number of machine
    learning methods. For information on the method, please refer to Broniecki, 
	Wüest, Leemann (2020) ''Improving Multilevel Regression with 
	Post-Stratification Through Machine Learning (autoMrP)'' forthcoming in 
	'Journal of Politics'. Final pre-print version: 
	<https://lucasleemann.files.wordpress.com/2020/07/automrp-r2pa.pdf>.",2021-01-21,Reto Wüest,https://github.com/retowuest/autoMrP,TRUE,https://github.com/retowuest/automrp,3230,16,2021-05-25T18:46:34Z,201.875
autoplotly,"Functionalities to automatically generate interactive visualizations for
             statistical results supported by 'ggfortify', such as time series, PCA,
             clustering and survival analysis, with 'plotly.js' <https://plotly.com/>  and
             'ggplot2' style. The generated visualizations can also be easily extended
             using 'ggplot2' and 'plotly' syntax while staying interactive.",2021-04-18,Yuan Tang,https://github.com/terrytangyuan/autoplotly,TRUE,https://github.com/terrytangyuan/autoplotly,25611,80,2021-05-06T09:52:08Z,320.1375
AutoScore,"A novel interpretable machine learning-based framework to automate the development of a clinical scoring model for predefined outcomes. Our novel framework consists of six modules: variable ranking with machine learning, variable transformation, score derivation, model selection, domain knowledge-based score fine-tuning, and performance evaluation.The details are described in our research paper<doi:10.2196/21798>. Users or clinicians could seamlessly generate parsimonious sparse-score risk models (i.e., risk scores), which can be easily implemented and validated in clinical practice. We hope to see its application in various medical case studies.",2021-06-16,Feng Xie,https://github.com/nliulab/AutoScore,TRUE,https://github.com/nliulab/autoscore,1149,10,2021-07-11T01:28:57Z,114.9
av,"Bindings to 'FFmpeg' <http://www.ffmpeg.org/> AV library for working with 
    audio and video in R. Generates high quality video from images or R graphics with 
    custom audio. Also offers high performance tools for reading raw audio, creating
    'spectrograms', and converting between countless audio / video formats. This package 
    interfaces directly to the C API and does not require any command line utilities.",2021-05-04,Jeroen Ooms,"https://docs.ropensci.org/av/ (website),
https://github.com/ropensci/av (devel)",TRUE,https://github.com/ropensci/av,392008,73,2021-05-04T10:56:44Z,5369.972602739726
available,"Check if a given package name is available to use. It checks the
  name's validity. Checks if it is used on 'GitHub', 'CRAN' and 'Bioconductor'. Checks
  for unintended meanings by querying Urban Dictionary, 'Wiktionary' and Wikipedia.",2019-07-19,Jim Hester,https://github.com/ropenscilabs/available,TRUE,https://github.com/ropenscilabs/available,32768,124,2020-11-30T14:15:34Z,264.258064516129
AvInertia,"
    Tools to compute the center of gravity and moment of inertia tensor of any 
    flying bird. The tools function by modeling a bird as a composite structure 
    of simple geometric objects. This requires detailed morphological 
    measurements of bird specimens although those obtained for the associated 
    paper have been included in the package for use. Refer to the vignettes and 
    supplementary material for detailed information on the package function.",2021-07-22,Christina Harvey,https://github.com/charvey23/AvInertia,TRUE,https://github.com/charvey23/avinertia,698,1,2021-09-03T01:13:56Z,698
AWAPer,"NetCDF files of the Bureau of Meteorology Australian Water Availability Project daily national climate grids are built and used for the efficient extraction of point and catchment area weighted precipitation, minimum temperature, maximum temperature, vapour pressure, solar radiation and various measures of evapotranspiration. For details on the source climate data see <http://www.bom.gov.au/jsp/awap/>.",2021-02-17,Tim Peterson,https://github.com/peterson-tim-j/AWAPer,TRUE,https://github.com/peterson-tim-j/awaper,10440,8,2021-03-29T02:24:39Z,1305
aweek,"Which day a week starts depends heavily on the either the local or
  professional context. This package is designed to be a lightweight solution
  to easily switching between week-based date definitions. ",2021-01-04,Zhian N. Kamvar,https://www.repidemicsconsortium.org/aweek/,TRUE,https://github.com/reconhub/aweek,99502,12,2021-03-21T19:30:40Z,8291.833333333334
AWR,Make the compiled Java modules of the Amazon Web Services ('AWS') 'SDK' available to be used in downstream R packages interacting with 'AWS'. See <https://aws.amazon.com/sdk-for-java> for more information on the 'AWS' 'SDK' for Java.,2021-04-13,Gergely Daroczi,https://github.com/daroczig/AWR,TRUE,https://github.com/daroczig/awr,24766,0,2021-04-13T10:44:44Z,NA
aws.ecx,"
    Providing the functions for communicating with Amazon Web Services(AWS)
    Elastic Compute Cloud(EC2) and Elastic Container Service(ECS).
    The functions will have the prefix 'ecs_' or 'ec2_' depending on the class 
    of the API. The request will be sent via the REST API and the parameters are
    given by the function argument. The credentials can be set via 'aws_set_credentials'.
    The EC2 documentation can be found at <https://docs.aws.amazon.com/AWSEC2/latest/APIReference/Welcome.html>
    and ECS can be found at <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/Welcome.html>.",2021-04-04,Jiefei Wang,https://github.com/Jiefei-Wang/aws.ecx,TRUE,https://github.com/jiefei-wang/aws.ecx,4697,1,2021-04-04T15:15:59Z,4697
AzureAuth,"Provides Azure Active Directory (AAD) authentication functionality for R users of Microsoft's 'Azure' cloud <https://azure.microsoft.com/>. Use this package to obtain 'OAuth' 2.0 tokens for services including Azure Resource Manager, Azure Storage and others. It supports both AAD v1.0 and v2.0, as well as multiple authentication methods, including device code and resource owner grant. Tokens are cached in a user-specific directory obtained using the 'rappdirs' package. The interface is based on the 'OAuth' framework in the 'httr' package, but customised and streamlined for Azure. Part of the 'AzureR' family of packages.",2021-05-19,Hong Ooi,https://github.com/Azure/AzureAuth https://github.com/Azure/AzureR,TRUE,https://github.com/azure/azureauth,598093,27,2021-09-03T04:50:36Z,22151.59259259259
azuremlsdk,"Interface to the 'Azure Machine Learning' Software Development Kit
    ('SDK'). Data scientists can use the 'SDK' to train, deploy, automate, and
    manage machine learning models on the 'Azure Machine Learning' service. To
    learn more about 'Azure Machine Learning' visit the website:
    <https://docs.microsoft.com/en-us/azure/machine-learning/service/overview-what-is-azure-ml>.",2020-09-22,Diondra Peck,https://github.com/azure/azureml-sdk-for-r,TRUE,https://github.com/azure/azureml-sdk-for-r,39267,90,2021-08-24T17:57:23Z,436.3
AzureRMR,"A lightweight but powerful R interface to the 'Azure Resource Manager' REST API. The package exposes a comprehensive class framework and related tools for creating, updating and deleting 'Azure' resource groups, resources and templates. While 'AzureRMR' can be used to manage any 'Azure' service, it can also be extended by other packages to provide extra functionality for specific services. Part of the 'AzureR' family of packages.",2021-06-03,Hong Ooi,https://github.com/Azure/AzureRMR https://github.com/Azure/AzureR,TRUE,https://github.com/azure/azurermr,622495,14,2021-06-02T13:43:36Z,44463.92857142857
AzureStor,"Manage storage in Microsoft's 'Azure' cloud: <https://azure.microsoft.com/services/storage/>. On the admin side, 'AzureStor' includes features to create, modify and delete storage accounts. On the client side, it includes an interface to blob storage, file storage, and 'Azure Data Lake Storage Gen2': upload and download files and blobs; list containers and files/blobs; create containers; and so on. Authenticated access to storage is supported, via either a shared access key or a shared access signature (SAS). Part of the 'AzureR' family of packages.",2021-08-13,Hong Ooi,https://github.com/Azure/AzureStor https://github.com/Azure/AzureR,TRUE,https://github.com/azure/azurestor,590770,37,2021-08-13T12:02:41Z,15966.756756756757
AzureVM,"Functionality for working with virtual machines (VMs) in Microsoft's 'Azure' cloud: <https://azure.microsoft.com/en-us/services/virtual-machines/>. Includes facilities to deploy, startup, shutdown, and cleanly delete VMs and VM clusters. Deployment configurations can be highly customised, and can make use of existing resources as well as creating new ones. A selection of predefined configurations is provided to allow easy deployment of commonly used Linux and Windows images, including Data Science Virtual Machines. With a running VM, execute scripts and install optional extensions. Part of the 'AzureR' family of packages.",2020-10-14,Hong Ooi,https://github.com/Azure/AzureVM https://github.com/Azure/AzureR,TRUE,https://github.com/azure/azurevm,28805,10,2021-01-12T18:45:16Z,2880.5
babelgene,"Genomic analysis of model organisms often requires the use of databases based on human data or making comparisons to patient-derived resources. This requires converting genes between human and non-human analogues. The babelgene R package provides predicted gene orthologs/homologs for frequently studied model organisms in an R-friendly tidy/long format. The package integrates orthology assertion predictions sourced from multiple databases as compiled by the HGNC Comparison of Orthology Predictions (HCOP) (Wright et al. 2005 <doi:10.1007/s00335-005-0103-2>, Eyre et al. 2007 <doi:10.1093/bib/bbl030>, Seal et al. 2011 <doi:10.1093/nar/gkq892>).",2021-04-26,Igor Dolgalev,https://igordot.github.io/babelgene/,TRUE,https://github.com/igordot/babelgene,15469,3,2021-04-27T14:23:38Z,5156.333333333333
babelwhale,"Provides a unified interface to interact with
    'docker' and 'singularity' containers.  You can execute a command
    inside a container, mount a volume or copy a file.",2021-06-25,Robrecht Cannoodt  (<https://orcid.org/0000-0003-3641-729X>,https://github.com/dynverse/babelwhale,TRUE,https://github.com/dynverse/babelwhale,22355,16,2021-06-02T11:33:18Z,1397.1875
babynames,"US baby names provided by the SSA. This package contains all
    names used for at least 5 children of either sex.",2021-04-12,Hadley Wickham,https://github.com/hadley/babynames,TRUE,https://github.com/hadley/babynames,186525,114,2021-04-12T14:20:59Z,1636.1842105263158
BacArena,"Can be used for simulation of organisms living in
    communities (Bauer and Zimmermann (2017) <doi:10.1371/journal.pcbi.1005544>). 
    Each organism is represented individually and genome scale
    metabolic models determine the uptake and release of compounds. Biological
    processes such as movement, diffusion, chemotaxis and kinetics are available
    along with data analysis techniques.",2020-05-20,Johannes Zimmermann,https://BacArena.github.io/,TRUE,https://github.com/euba/bacarena,21204,17,2021-02-25T09:56:59Z,1247.2941176470588
backbone,"Provides methods for extracting from a weighted graph 
    a binary or signed backbone that retains only the significant edges. 
    The user may input a weighted graph, or a bipartite graph 
    from which a weighted graph is first constructed via projection.
    Backbone extraction methods include the stochastic degree sequence model (SDSM; Neal, Z. P. (2014). <doi:10.1016/j.socnet.2014.06.001>), 
    the fixed degree sequence model (FDSM; Zweig, K. A., and Kaufmann, M. (2011). <doi:10.1007/s13278-011-0021-0>),
    the fixed row model (FRM; Neal, Z. P. (2013). <doi:10.1007/s13278-013-0107-y>), 
    the fixed column model (FCM; Neal, Domagalski, and Sagan (2021). <arXiv:2105.13396>),
    the fixed fill model (FFM; Neal, Domagalski, and Sagan (2021). <arXiv:2105.13396>),
    and a universal threshold method. ",2021-06-04,Zachary Neal,"https://www.zacharyneal.com/backbone,
https://github.com/domagal9/backbone",TRUE,https://github.com/domagal9/backbone,15787,23,2021-08-06T18:21:37Z,686.3913043478261
backports,"
    Functions introduced or changed since R v3.0.0 are re-implemented in this
    package. The backports are conditionally exported in order to let R resolve
    the function name to either the implemented backport, or the respective base
    version, if available. Package developers can make use of new functions or
    arguments by selectively importing specific backports to
    support older installations.",2020-12-09,Michel Lang,https://github.com/r-lib/backports,TRUE,https://github.com/r-lib/backports,20723043,52,2020-12-09T11:39:11Z,398520.0576923077
badger,Query information and generate badge for using in README and GitHub Pages.,2021-04-21,Guangchuang Yu,https://github.com/GuangchuangYu/badger,TRUE,https://github.com/guangchuangyu/badger,27365,134,2021-04-21T03:06:02Z,204.21641791044777
bagged.outliertrees,"Bagged OutlierTrees is an explainable unsupervised outlier detection method based on an ensemble implementation of the existing OutlierTree procedure (Cortes, 2020). This implementation takes advantage of bootstrap aggregating (bagging) to improve robustness by reducing the possible masking effect and subsequent high variance (similarly to Isolation Forest), hence the name ""Bagged OutlierTrees"". To learn more about the base procedure OutlierTree (Cortes, 2020), please refer to <arXiv:2001.00636>.",2021-07-06,Rafael Santos,https://github.com/RafaJPSantos/bagged.outliertrees,TRUE,https://github.com/rafajpsantos/bagged.outliertrees,868,4,2021-07-10T14:07:01Z,217
baggr,"Running and comparing meta-analyses of data with hierarchical 
    Bayesian models in Stan, including convenience functions for formatting
    data, plotting and pooling measures specific to meta-analysis. This implements many models
    from Meager (2019) <doi:10.1257/app.20170299>.",2021-09-02,Witold Wiecek,https://github.com/wwiecek/baggr,TRUE,https://github.com/wwiecek/baggr,14291,25,2021-09-03T09:36:55Z,571.64
baguette,"Tree- and rule-based models can be bagged using this package
    and their predictions equations are stored in an efficient format to
    reduce the model objects size and speed.",2021-07-14,Max Kuhn,"https://baguette.tidymodels.org,
https://github.com/tidymodels/baguette",TRUE,https://github.com/tidymodels/baguette,13488,16,2021-07-14T18:05:35Z,843
bain,"Computes approximated adjusted fractional Bayes factors for
    equality, inequality, and about equality constrained hypotheses. S3 methods
    are available for specific types of lm() models, namely ANOVA, ANCOVA, and
    multiple regression, and for the t_test(). The statistical underpinnings are
    described in 
    Gu, Mulder, and Hoijtink, (2018) <DOI:10.1111/bmsp.12110>,
    Hoijtink, Gu, and Mulder, (2018) <DOI:10.1111/bmsp.12145>, and
    Hoijtink, Gu, Mulder, and Rosseel, (2018) <DOI:10.1037/met0000187>.",2021-09-03,Caspar J van Lissa,https://informative-hypotheses.sites.uu.nl/software/bain/,TRUE,https://github.com/cjvanlissa/bain,30274,3,2021-09-03T08:25:36Z,10091.333333333334
Ball,"Hypothesis tests and sure independence screening (SIS) procedure based on ball statistics, including ball divergence <doi:10.1214/17-AOS1579>, ball covariance <doi:10.1080/01621459.2018.1543600>, and ball correlation <doi:10.1080/01621459.2018.1462709>, are developed to analyze complex data in metric spaces, e.g, shape, directional, compositional and symmetric positive definite matrix data. The ball divergence and ball covariance based distribution-free tests are implemented to detecting distribution difference and association in metric spaces <doi:10.18637/jss.v097.i06>. Furthermore, several generic non-parametric feature selection procedures based on ball correlation, BCor-SIS and all of its variants, are implemented to tackle the challenge in the context of ultra high dimensional data.",2021-03-17,Xueqin Wang,https://github.com/Mamba413/Ball,TRUE,https://github.com/mamba413/ball,30410,18,2021-08-26T07:54:40Z,1689.4444444444443
bama,"Perform mediation analysis in the presence of high-dimensional
    mediators based on the potential outcome framework. Bayesian Mediation
    Analysis (BAMA), developed by Song et al (2019) <doi:10.1111/biom.13189> and
    Song et al (2020) <arXiv:2009.11409>,
    relies on two Bayesian sparse linear mixed models to simultaneously analyze
    a relatively large number of mediators for a continuous exposure and outcome
    assuming a small number of mediators are truly active. This sparsity
    assumption also allows the extension of univariate mediator analysis by
    casting the identification of active mediators as a variable selection
    problem and applying Bayesian methods with continuous shrinkage priors on
    the effects.",2021-01-21,Mike Kleinsasser,https://github.com/umich-cphds/bama,TRUE,https://github.com/umich-cphds/bama,12374,0,2021-01-15T17:34:02Z,NA
BAMBI,Fit (using Bayesian methods) and simulate mixtures of univariate and bivariate angular distributions. Chakraborty and Wong (2017) <arXiv:1708.07804>.,2021-08-12,Saptarshi Chakraborty,https://arxiv.org/abs/1708.07804,TRUE,https://github.com/c7rishi/bambi,24111,1,2021-08-12T15:09:15Z,24111
bamp,"Bayesian Age-Period-Cohort Modeling and Prediction using efficient Markov Chain Monte Carlo Methods. This is the R version of the previous BAMP software as described in Volker Schmid and Leonhard Held (2007) <DOI:10.18637/jss.v021.i08> Bayesian Age-Period-Cohort Modeling and Prediction - BAMP, Journal of Statistical Software 21:8. This package includes checks of convergence using Gelman's R.",2021-06-10,Volker Schmid,https://volkerschmid.github.io/bamp/,TRUE,https://github.com/volkerschmid/bamp,17355,4,2021-07-29T10:46:36Z,4338.75
BARIS,"Allows the user to access and import data from the rich French open data portal through the provided free API <https://doc.data.gouv.fr/api/reference/>. 
    The portal is free, and no credential is required for extracting datasets. ",2020-05-25,Mohamed El Fodil Ihaddaden,https://github.com/feddelegrand7/BARIS,TRUE,https://github.com/feddelegrand7/baris,10026,18,2020-12-19T21:57:12Z,557
bartCause,Contains a variety of methods to generate typical causal inference estimates using Bayesian Additive Regression Trees (BART) as the underlying regression model (Hill (2012) <doi:10.1198/jcgs.2010.08162>).,2020-08-10,Vincent Dorie,https://github.com/vdorie/bartCause,TRUE,https://github.com/vdorie/bartcause,9395,44,2021-06-10T17:55:21Z,213.52272727272728
baRulho,"Intended to facilitate acoustic analysis of (animal) sound transmission experiments, which typically aim to quantify changes in signal structure when transmitted in a given habitat by broadcasting and re-recording animal sounds at increasing distances. The package offers a workflow with functions to prepare the data set for analysis as well as to calculate and visualize several degradation metrics, including blur ratio, signal-to-noise ratio, excess attenuation and envelope correlation among others (Dabelsteen et al 1993 <doi:10.1121/1.406682>).",2021-04-21,Marcelo Araya-Salas,https://github.com/maRce10/baRulho,TRUE,https://github.com/marce10/barulho,10844,1,2021-07-07T02:45:23Z,10844
BAS,"Package for Bayesian Variable Selection and  Model Averaging 
    in linear models and generalized linear models using stochastic or
    deterministic sampling without replacement from posterior
    distributions.  Prior distributions on coefficients are
    from Zellner's g-prior or mixtures of g-priors
    corresponding to the Zellner-Siow Cauchy Priors or the
    mixture of g-priors from Liang et al (2008)
    <DOI:10.1198/016214507000001337>
    for linear models or mixtures of g-priors from  Li and Clyde
    (2019) <DOI:10.1080/01621459.2018.1469992> in generalized linear models.
    Other model selection criteria include AIC, BIC and Empirical Bayes 
    estimates of g. Sampling probabilities may be updated based on the sampled
    models using sampling w/out replacement or an efficient MCMC algorithm which
    samples models using a tree structure of the model space 
    as an efficient hash table.  See  Clyde, Ghosh and Littman (2010) 
    <DOI:10.1198/jcgs.2010.09049> for  details on the sampling algorithms.
    Uniform priors over all models or beta-binomial prior distributions on
    model size are allowed, and for large p truncated priors on the model
    space may be used to enforce sampling models that are full rank.  
    The user may force variables to always be included in addition to imposing
    constraints that higher order interactions are included only if their 
    parents are included in the model.
    This material is based upon work supported by the National Science
    Foundation under Division of Mathematical Sciences grant 1106891.
    Any opinions, findings, and
    conclusions or recommendations expressed in this material are those of
    the author(s) and do not necessarily reflect the views of the
    National Science Foundation.",2020-01-24,Merlise Clyde,"https://www.r-project.org, https://github.com/merliseclyde/BAS",TRUE,https://github.com/merliseclyde/bas,73835,32,2020-11-10T05:41:34Z,2307.34375
basecamb,"Provides functions streamlining the data analysis workflow: 
  Outsourcing data import, renaming and type casting to a *.csv.
  Manipulating imputed datasets and fitting models on them. Summarizing models.",2021-05-12,J. Peter Marquardt,"https://CRAN.R-project.org/package=basecamb,
https://github.com/codeblue-team/basecamb",TRUE,https://github.com/codeblue-team/basecamb,1797,3,2021-06-06T11:21:31Z,599
basemaps,"A lightweight package to access spatial basemaps from open sources such as OpenStreetMap, Carto, Mapbox and others in R.",2021-05-19,Jakob Schwalb-Willmann,NA,TRUE,https://github.com/16eagle/basemaps,1746,28,2021-06-21T19:22:00Z,62.357142857142854
BaseSet,"Implements a class and methods to work with sets,
    doing intersection, union, complementary sets, power sets, cartesian
    product and other set operations in a ""tidy"" way. These set operations
    are available for both classical sets and fuzzy sets. Import sets from
    several formats or from other several data structures.",2021-07-05,Lluís Revilla Sancho,"https://github.com/ropensci/BaseSet,
https://docs.ropensci.org/BaseSet/",TRUE,https://github.com/ropensci/baseset,3767,7,2021-07-05T11:22:52Z,538.1428571428571
basf,"Resurrects the standard plot for shapes established by the
 'base' and 'graphics' packages. This is suited to workflows that require
 plotting using the established and traditional idioms of plotting spatially
 coincident data where it belongs. This package depends on 'sf' and only replaces 
 the plot method. ",2020-12-09,Michael Sumner,https://github.com/mdsumner/basf,TRUE,https://github.com/mdsumner/basf,8378,0,2020-12-11T00:59:29Z,NA
basictabler,"Easily create tables from data 
    frames/matrices.  Create/manipulate tables 
    row-by-row, column-by-column or cell-by-cell.  
    Use common formatting/styling to output 
    rich tables as 'HTML', 'HTML widgets' or to 
    'Excel'. ",2021-06-26,Christopher Bailiss,"http://www.basictabler.org.uk/,
https://github.com/cbailiss/basictabler",TRUE,https://github.com/cbailiss/basictabler,28526,30,2021-07-01T20:48:46Z,950.8666666666667
basicTrendline,"Plot, draw regression line and confidence interval, and show regression equation, R-square and P-value,  as simple as possible, by using different models (""line2P"", ""line3P"", ""log2P"", ""exp2P"", ""exp3P"", ""power2P"", ""power3P"") built in the 'trendline()' function.",2020-11-23,Weiping Mei,https://github.com/PhDMeiwp/basicTrendline,TRUE,https://github.com/phdmeiwp/basictrendline,36184,10,2021-07-05T08:15:47Z,3618.4
BasketballAnalyzeR,"Contains data and code to accompany  the book 
             P. Zuccolotto and M. Manisera (2020) Basketball Data Science. Applications with R. CRC Press. ISBN 9781138600799.",2020-06-26,Marco Sandri,https://github.com/sndmrc/BasketballAnalyzeR,TRUE,https://github.com/sndmrc/basketballanalyzer,7028,23,2021-05-17T14:53:41Z,305.5652173913044
batata,"
    Allows the user to manage easily R packages removal and installation. It offers many functions to display installed packages according to
    specific dates and removes them if needed. The user is always prompted when running the removal functions in order to confirm
    the required action. It also provides functions that will install 'Github' starred R packages whether available on 'CRAN' or not. ",2021-03-08,Mohamed El Fodil Ihaddaden,https://github.com/feddelegrand7/batata,TRUE,https://github.com/feddelegrand7/batata,6455,27,2021-03-08T09:55:33Z,239.07407407407408
batchr,"Processes multiple files with a user-supplied
    function. The key design principle is that only files which were 
    last modified before the directory was configured are processed.
    A hidden file stores the configuration time and function etc while 
    successfully processed files are automatically touched to update 
    their modification date. As a result batch processing can be 
    stopped and restarted and any files created (or modified or deleted) 
    during processing are ignored.",2021-02-16,Joe Thorley,https://poissonconsulting.github.io/batchr/,TRUE,https://github.com/poissonconsulting/batchr,1826,3,2021-02-16T15:38:44Z,608.6666666666666
batchtools,"As a successor of the packages 'BatchJobs' and 'BatchExperiments',
    this package provides a parallel implementation of the Map function for high
    performance computing systems managed by schedulers 'IBM Spectrum LSF'
    (<https://www.ibm.com/products/hpc-workload-management>),
    'OpenLava' (<https://www.openlava.org/>), 'Univa Grid Engine'/'Oracle Grid
    Engine' (<https://www.univa.com/>), 'Slurm' (<https://slurm.schedmd.com/>),
    'TORQUE/PBS'
    (<https://adaptivecomputing.com/cherry-services/torque-resource-manager/>),
    or 'Docker Swarm' (<https://docs.docker.com/engine/swarm/>).
    A multicore and socket mode allow the parallelization on a local machines,
    and multiple machines can be hooked up via SSH to create a makeshift
    cluster. Moreover, the package provides an abstraction mechanism to define
    large-scale computer experiments in a well-organized and reproducible way.",2021-01-11,Michel Lang,https://github.com/mllg/batchtools,TRUE,https://github.com/mllg/batchtools,135179,136,2021-08-05T12:09:19Z,993.9632352941177
bayefdr,"
    Implements the Bayesian FDR control described by 
    Newton et al. (2004), <doi:10.1093/biostatistics/5.2.155>.
    Allows optimisation and visualisation of expected error rates based on
    tail posterior probability tests.
    Based on code written by Catalina Vallejos for BASiCS, see
    Beyond comparisons of means: understanding changes in gene expression at the
    single-cell level Vallejos et al. (2016) <doi:10.1186/s13059-016-0930-3>.",2020-07-30,Alan OCallaghan,NA,TRUE,https://github.com/vallejosgroup/bayefdr,5559,0,2021-03-12T10:32:16Z,NA
bayes4psy,Contains several Bayesian models for data analysis of psychological tests. A user friendly interface for these models should enable students and researchers to perform professional level Bayesian data analysis without advanced knowledge in programming and Bayesian statistics. This package is based on the Stan platform (Carpenter et el. 2017 <doi:10.18637/jss.v076.i01>).,2021-04-26,Jure Demšar,https://github.com/bstatcomp/bayes4psy,TRUE,https://github.com/bstatcomp/bayes4psy,15401,7,2021-04-28T08:27:10Z,2200.1428571428573
bayesAB,"A suite of functions that allow the user to analyze A/B test
    data in a Bayesian framework. Intended to be a drop-in replacement for
    common frequentist hypothesis test such as the t-test and chi-sq test.",2021-06-25,Frank Portman,https://github.com/FrankPortman/bayesAB,TRUE,https://github.com/frankportman/bayesab,40319,280,2021-06-25T01:31:23Z,143.99642857142857
bayesbr,"Applies the Beta regression model in the Bayesian statistical view with the possibility of adding a spatial effect in the parameters, the Beta regression is used when the response variable is a proportion variable, that is, it only accepts values between 0 and 1.
    The package 'bayesbr' uses 'rstan' package to build the Bayesian statistical models. The main function of the package receives as a parameter a form informing the independent variable and the co-variables of the model to be made, as output it returns a list with the results of the model. For more details see Ferrari and Cribari-Neto (2004) <doi:10.1080/0266476042000214501> and Hoffman and Gelman (2014) <arXiv:1111.4246>.",2021-07-16,Joao Melo,https://github.com/pjoao266/bayesbr,TRUE,https://github.com/pjoao266/bayesbr,5631,0,2021-07-16T19:24:53Z,NA
bayescopulareg,"Tools for Bayesian copula generalized linear models (GLMs). 
             The sampling scheme is based on Pitt, Chan, and Kohn (2006) <doi:10.1093/biomet/93.3.537>. 
             Regression parameters (including coefficients and dispersion parameters) are
             estimated via the adaptive random walk Metropolis approach developed by
             Haario, Saksman, and Tamminen (1999) <doi:10.1007/s001800050022>.
             The prior for the correlation matrix is based on Hoff (2007) <doi:10.1214/07-AOAS107>.",2020-11-30,Ethan Alt,https://github.com/ethan-alt/bayescopulareg,TRUE,https://github.com/ethan-alt/bayescopulareg,7059,0,2020-10-08T14:44:58Z,NA
bayesDccGarch,"Bayesian estimation of dynamic conditional correlation GARCH model for multivariate time series volatility (Fioruci, J.A., Ehlers, R.S. and Andrade-Filho, M.G., (2014). <doi:10.1080/02664763.2013.839635>.",2021-04-07,Jose Augusto Fiorucci,https://ui.adsabs.harvard.edu/abs/2014arXiv1412.2967F/abstract,TRUE,https://github.com/jafiorucci/bayesdccgarch,15693,0,2021-07-08T20:34:17Z,NA
bayesdfa,"Implements Bayesian dynamic factor analysis with 'Stan'. Dynamic 
    factor analysis is a dimension reduction tool for multivariate time series.
    'bayesdfa' extends conventional dynamic factor models in several ways. 
    First, extreme events may be estimated in the latent trend by modeling
    process error with a student-t distribution. Second, alternative constraints
    (including proportions are allowed). Third, the estimated
    dynamic factors can be analyzed with hidden Markov models to evaluate
    support for latent regimes.",2021-05-28,Eric J. Ward,https://fate-ewi.github.io/bayesdfa/,TRUE,https://github.com/fate-ewi/bayesdfa,23105,18,2021-09-02T21:01:01Z,1283.611111111111
bayesDP,"Functions for data augmentation using the
    Bayesian discount prior method for single arm and two-arm clinical trials,
    as described in Haddad et al. (2017) <doi:10.1080/10543406.2017.1300907>.
    The discount power prior methodology was developed in collaboration with
    the The Medical Device Innovation Consortium (MDIC) Computer Modeling & 
    Simulation Working Group.",2021-01-06,Graeme L. Hickey,https://github.com/graemeleehickey/bayesDP,TRUE,https://github.com/graemeleehickey/bayesdp,21772,0,2021-01-09T16:24:44Z,NA
BayesFactor,"A suite of functions for computing
    various Bayes factors for simple designs, including contingency tables,
    one- and two-sample designs, one-way designs, general ANOVA designs, and
    linear regression.",2018-05-19,Richard D. Morey,https://richarddmorey.github.io/BayesFactor/,TRUE,https://github.com/richarddmorey/bayesfactor,368006,110,2021-08-21T09:35:08Z,3345.509090909091
bayesGARCH,"Provides the bayesGARCH() function which performs the
    Bayesian estimation of the GARCH(1,1) model with Student's t innovations as described in Ardia (2008) <doi:10.1007/978-3-540-78657-3>.",2021-05-16,David Ardia,https://github.com/ArdiaD/bayesGARCH,TRUE,https://github.com/ardiad/bayesgarch,34016,7,2021-05-16T14:06:28Z,4859.428571428572
bayesian,"Fit Bayesian models using 'brms'/'Stan' with 'parsnip'/'tidymodels'
    via 'bayesian' <doi:10.5281/zenodo.5091194>. 'tidymodels' is a collection of
    packages for machine learning; see Kuhn and Wickham (2020) <https://www.tidymodels.org>).
    The technical details of 'brms' and 'Stan' are described in Bürkner (2017)
    <doi:10.18637/jss.v080.i01>, Bürkner (2018) <doi:10.32614/RJ-2018-017>,
    and Carpenter et al. (2017) <doi:10.18637/jss.v076.i01>.",2021-07-12,Hamada S. Badr,"https://hsbadr.github.io/bayesian/,
https://github.com/hsbadr/bayesian",TRUE,https://github.com/hsbadr/bayesian,4202,15,2021-08-29T17:11:19Z,280.1333333333333
BayesianLaterality,"Functional differences between the cerebral hemispheres 
    are a fundamental characteristic of the human brain. Researchers 
    interested in studying these differences often infer underlying 
    hemispheric dominance for a certain function (e.g., language) from 
    laterality indices calculated from observed performance or brain 
    activation measures . However, any inference from observed measures 
    to latent (unobserved) classes has to consider the prior probability 
    of class membership in the population. The provided functions 
    implement a Bayesian model for predicting hemispheric dominance from
    observed laterality indices (Sorensen and Westerhausen, Laterality: 
    Asymmetries of Body, Brain and Cognition, 2020, <doi:10.1080/1357650X.2020.1769124>).",2021-08-08,Oystein Sorensen,https://github.com/LCBC-UiO/BayesianLaterality,TRUE,https://github.com/lcbc-uio/bayesianlaterality,4968,0,2021-08-08T20:06:05Z,NA
BayesianReasoning,"Functions to plot and help understand positive and negative predictive values (PPV and NPV), and their relationship with sensitivity, specificity, and prevalence. See Akobeng, A.K. (2007) <doi:10.1111/j.1651-2227.2006.00180.x> for a theoretical overview of the technical concepts and Navarrete et al. (2015) for a practical explanation about the importance of their understanding <doi:10.3389/fpsyg.2015.01327>.",2021-06-21,Gorka Navarrete,https://github.com/gorkang/BayesianReasoning,TRUE,https://github.com/gorkang/bayesianreasoning,5681,8,2021-06-17T10:18:11Z,710.125
BayesianTools,"General-purpose MCMC and SMC samplers, as well as plot and
    diagnostic functions for Bayesian statistics, with a particular focus on
    calibrating complex system models. Implemented samplers include various
    Metropolis MCMC variants (including adaptive and/or delayed rejection MH), the
    T-walk, two differential evolution MCMCs, two DREAM MCMCs, and a sequential
    Monte Carlo (SMC) particle filter.",2019-12-09,Florian Hartig,https://github.com/florianhartig/BayesianTools,TRUE,https://github.com/florianhartig/bayesiantools,37118,76,2021-07-22T14:40:59Z,488.39473684210526
BayesMallows,"An implementation of the Bayesian version of the Mallows rank model 
    (Vitelli et al., Journal of Machine Learning Research, 2018 <https://jmlr.org/papers/v18/15-481.html>; 
    Crispino et al., Annals of Applied Statistics, 2019 <doi:10.1214/18-AOAS1203>). Both Cayley, footrule, 
    Hamming, Kendall, Spearman, and Ulam distances are supported in the models. The rank data to be 
    analyzed can be in the form of complete rankings, top-k rankings, partially missing rankings, as well 
    as consistent and inconsistent pairwise preferences. Several functions for plotting and studying the 
    posterior distributions of parameters are provided. The package also provides functions for estimating 
    the partition function (normalizing constant) of the Mallows rank model, both with the importance 
    sampling algorithm of Vitelli et al. and asymptotic approximation with the IPFP algorithm 
    (Mukherjee, Annals of Statistics, 2016 <doi:10.1214/15-AOS1389>).",2021-06-04,Oystein Sorensen,https://github.com/ocbe-uio/BayesMallows,TRUE,https://github.com/ocbe-uio/bayesmallows,23458,10,2021-06-25T08:56:38Z,2345.8
BayesMassBal,"Bayesian tools that can be used to reconcile, or mass balance, mass flow rate data collected from chemical or particulate separation processes aided by constraints governed by the conservation of mass.
    Functions included in the package aid the user in organizing and constraining data, using Markov chain Monte Carlo methods to obtain samples from Bayesian models, and in computation of the marginal likelihood of the data, given a particular model, for model selection.  Marginal likelihood is approximated by methods in Chib S (1995) <doi:10.2307/2291521>.",2020-11-09,Scott Koermer,https://github.com/skoermer/BayesMassBal,TRUE,https://github.com/skoermer/bayesmassbal,5763,1,2020-11-09T15:13:13Z,5763
bayesmodels,"Bayesian framework for use with the 'tidymodels' ecosystem. Includes the following models: Sarima, Garch, 
    Random walk (naive), Additive Linear State Space Models, Stochastic Volatility Models from 'bayesforecast' package,
    Adaptive Splines Surfaces from 'BASS' package and ETS from 'Rlgt' package.",2021-06-28,Alberto Almuiña,https://github.com/AlbertoAlmuinha/bayesmodels,TRUE,https://github.com/albertoalmuinha/bayesmodels,3258,38,2021-06-28T21:01:50Z,85.73684210526316
bayesmove,"Methods for assessing animal movement from telemetry and biologging
    data using non-parametric Bayesian methods. This includes features for pre-
    processing and analysis of data, as well as the visualization of results
    from the models. This framework does not rely on standard parametric density
    functions, which provides flexibility during model fitting.",2021-04-26,Joshua Cullen,"https://github.com/joshcullen/bayesmove,
https://joshcullen.github.io/bayesmove/",TRUE,https://github.com/joshcullen/bayesmove,4761,3,2021-04-26T20:16:26Z,1587
BayesMRA,Software for fitting sparse Bayesian multi-resolution spatial models using Markov Chain Monte Carlo.,2020-08-18,John Tipton,https://github.com/jtipton25/BayesMRA,TRUE,https://github.com/jtipton25/bayesmra,5192,2,2021-08-06T15:50:34Z,2596
bayesnec,"Implementation of No-Effect-Concentration estimation that uses 'brms' (see Burkner (2017)<doi:10.18637/jss.v080.i01>; Burkner (2018)<doi:10.32614/RJ-2018-017>; Carpenter 'et al.' (2017)<doi:10.18637/jss.v076.i01> to fit concentration(dose)-response data using Bayesian methods for the purpose of estimating 'ECX' values, but more particularly 'NEC' (see Fox (2010)<doi:10.1016/j.ecoenv.2009.09.012>. This package expands and supersedes an original version implemented in R2jags, see Fisher, Ricardo and Fox (2020)<doi:10.5281/ZENODO.3966864>.  ",2021-07-02,Rebecca Fisher,https://open-aims.github.io/bayesnec/,TRUE,https://github.com/open-aims/bayesnec,832,2,2021-09-03T09:23:34Z,416
BayesNetBP,"Belief propagation methods in Bayesian Networks to propagate evidence through the network. The implementation of these methods are based on the article: Cowell, RG (2005). Local Propagation in Conditional Gaussian Bayesian Networks <https://www.jmlr.org/papers/volume6/cowell05a/>. For details please see Yu et. al. (2020) BayesNetBP: An R Package for Probabilistic Reasoning in Bayesian Networks <doi:10.18637/jss.v094.i03>. The optional 'cyjShiny' package for running the Shiny app is available at <https://github.com/cytoscape/cyjShiny>. Please see the example in the documentation of 'runBayesNetApp' function for installing 'cyjShiny' package from GitHub. ",2021-06-01,Han Yu,NA,TRUE,https://github.com/hyu-ub/bayesnetbp,20966,12,2021-03-12T16:09:38Z,1747.1666666666667
bayesplot,"Plotting functions for posterior analysis, MCMC diagnostics,
    prior and posterior predictive checks, and other visualizations 
    to support the applied Bayesian workflow advocated in
    Gabry, Simpson, Vehtari, Betancourt, and Gelman (2019) <doi:10.1111/rssa.12378>.
    The package is designed not only to provide convenient functionality 
    for users, but also a common set of functions that can be easily used by 
    developers working on a variety of R packages for Bayesian modeling, 
    particularly (but not exclusively) packages interfacing with 'Stan'.",2021-06-14,Jonah Gabry,https://mc-stan.org/bayesplot/,TRUE,https://github.com/stan-dev/bayesplot,731058,305,2021-06-13T21:39:57Z,2396.911475409836
BayesPostEst,"An implementation of functions to generate and plot postestimation quantities after estimating Bayesian regression models using Markov chain Monte Carlo (MCMC). Functionality includes the estimation of the Precision-Recall curves (see Beger, 2016 <doi:10.2139/ssrn.2765419>), the implementation of the observed values method of calculating predicted probabilities by Hanmer and Kalkan (2013) <doi:10.1111/j.1540-5907.2012.00602.x>, the implementation of the average value method of calculating predicted probabilities (see King, Tomz, and Wittenberg, 2000 <doi:10.2307/2669316>), and the generation and plotting of first differences to summarize typical effects across covariates (see Long 1997, ISBN:9780803973749; King, Tomz, and Wittenberg, 2000 <doi:10.2307/2669316>). This package can be used with MCMC output generated by any Bayesian estimation tool including 'JAGS', 'BUGS', 'MCMCpack', and 'Stan'.",2021-01-09,Johannes Karreth,https://github.com/ShanaScogin/BayesPostEst,TRUE,https://github.com/shanascogin/bayespostest,14015,9,2021-02-05T15:28:40Z,1557.2222222222222
Bayesrel,"Functionality for the most common single test reliability estimates:  
    Coefficient alpha, 'Guttman's' lambda-2/-4/-6, the Greatest lower bound and coefficient omega. 
    The Bayesian estimates are provided with credible intervals. 
    The frequentist estimates are provided with bootstrapped confidence intervals
    The method for the Bayesian estimates, except for omega, is sampling from the posterior inverse 'Wishart' 
    for the covariance matrix based measures 
    (see 'Murphy', 2007, <https://www.seas.harvard.edu/courses/cs281/papers/murphy-2007.pdf>. 
    In the case of omega it is 'Gibbs' Sampling from the joint conditional distributions of a single factor model 
    ('Lee', 2007, <doi:10.1002/9780470024737>). 
    The glb method uses adjusted code from the 'Rcsdp' package by 'Hector Corrada Bravo', 
    <https://CRAN.R-project.org/package=Rcsdp>. This process applies a slightly adjusted solving algorithm 
    from the 'CSDP' library by 'Brian Borchers' <https://github.com/coin-or/Csdp/wiki>, 
    <doi:10.1080/10556789908805765>, but is wrapped in 'RcppArmadillo'.
    Guttman's Lambda-4 is from 'Benton' (2015) <doi:10.1007/978-3-319-07503-7_19>.
    The principal factor analysis for a version of frequentist omega is from 'Schlegel' (2017) 
    <https://www.r-bloggers.com/2017/03/iterated-principal-factor-method-of-factor-analysis-with-r/>. 
    The analytic confidence interval of alpha is from 'Bonett' and 'Wright' (2015) <doi:10.1002/job.1960>. ",2021-03-29,Julius M. Pfadt,https://github.com/juliuspf/Bayesrel,TRUE,https://github.com/juliuspf/bayesrel,17086,0,2021-08-25T10:25:18Z,NA
bayesrules,"Provides datasets and functions used for analysis 
  and visualizations in the Bayes Rules! book (<https://www.bayesrulesbook.com>). 
  The package contains a set of functions that summarize and plot Bayesian models from some conjugate families 
  and another set of functions for evaluation of some Bayesian models.",2021-06-24,Mine Dogucu,"https://bayes-rules.github.io/bayesrules/docs/,
https://github.com/bayes-rules/bayesrules/",TRUE,https://github.com/bayes-rules/bayesrules,1566,29,2021-08-27T18:54:42Z,54
BayesSampling,"Allows the user to apply the Bayes Linear approach to finite population with the Simple Random Sampling - BLE_SRS() - and
    the Stratified Simple Random Sampling design - BLE_SSRS() - (both without replacement), to the Ratio estimator (using auxiliary
    information) - BLE_Ratio() - and to categorical data - BLE_Categorical().
    The Bayes linear estimation approach is applied to a general linear regression model for finite population prediction in BLE_Reg()
    and it is also possible to achieve the design based estimators using vague prior distributions.    
    Based on Gonçalves, K.C.M, Moura, F.A.S and  Migon, H.S.(2014) <https://www150.statcan.gc.ca/n1/en/catalogue/12-001-X201400111886>.",2021-05-01,Pedro Soares Figueiredo,"https://www150.statcan.gc.ca/n1/en/catalogue/12-001-X201400111886,
https://github.com/pedrosfig/BayesSampling",TRUE,https://github.com/pedrosfig/bayessampling,6460,1,2021-05-01T18:20:39Z,6460
BayesSenMC,"Generates different posterior distributions of adjusted odds ratio under different priors of sensitivity and specificity, and plots the models for comparison. It also provides estimations for the specifications of the models using diagnostics of exposure status with a non-linear mixed effects model. It implements the methods that are first proposed in <doi:10.1016/j.annepidem.2006.04.001> and <doi:10.1177/0272989X09353452>.",2021-09-02,Jinhui Yang,https://github.com/formidify/BayesSenMC,TRUE,https://github.com/formidify/bayessenmc,11233,0,2021-08-23T20:06:07Z,NA
BayesSPsurv,"Parametric spatial split-population (SP) survival models for clustered 
    event processes. The models account for structural and spatial heterogeneity among 
    “at risk” and “immune” populations, and incorporate time-varying covariates. 
    This package currently implements Weibull, Exponential and Log-logistic forms for 
    the duration component. It also includes functions for a series of diagnostic 
    tests and plots to easily visualize spatial autocorrelation, convergence, and spatial effects. Users can 
    create their own spatial weights matrix based on their units and adjacencies of 
    interest, making the use of these models flexible and broadly applicable to a 
    variety of research areas. Joo et al. (2020) <https://github.com/Nicolas-Schmidt/BayesSPsurv/blob/master/man/figures/SPcure.pdf> describe 
    the estimators included in this package. ",2021-05-14,Nicolas Schmidt,https://nicolas-schmidt.github.io/BayesSPsurv/,TRUE,https://github.com/nicolas-schmidt/bayesspsurv,5154,2,2021-05-22T01:41:48Z,2577
bayestestR,"Provides utilities to describe posterior
    distributions and Bayesian models. It includes point-estimates such as
    Maximum A Posteriori (MAP), measures of dispersion (Highest Density
    Interval - HDI; Kruschke, 2015 <doi:10.1016/C2012-0-00477-2>) and
    indices used for null-hypothesis testing (such as ROPE percentage, pd
    and Bayes factors).",2021-09-03,Dominique Makowski  (<https://orcid.org/0000-0001-5375-9967>,https://easystats.github.io/bayestestR/,TRUE,https://github.com/easystats/bayestestr,790498,393,2021-09-02T07:32:09Z,2011.445292620865
BayesTools,"Provides tools for conducting Bayesian analyses. The package contains 
    functions for creating a wide range of prior distribution objects, mixing posterior 
    samples from 'JAGS' and 'Stan' models, plotting posterior distributions, and etc...
    The tools for working with prior distribution span from visualization, generating 'JAGS' 
    and 'bridgesampling' syntax to basic functions such as rng, quantile, and distribution functions. ",2021-07-13,František Bartoš,https://fbartos.github.io/BayesTools/,TRUE,https://github.com/fbartos/bayestools,1548,0,2021-08-09T13:33:12Z,NA
BayesVarSel,"Conceived to calculate Bayes factors in Linear models and then to provide a formal Bayesian answer to testing and variable selection problems. From a theoretical side, the emphasis in this package is placed on the prior distributions and it allows a wide range of them: Jeffreys (1961); Zellner and Siow(1980)<DOI:10.1007/bf02888369>; Zellner and Siow(1984); Zellner (1986)<DOI:10.2307/2233941>; Fernandez et al. (2001)<DOI:10.1016/s0304-4076(00)00076-2>; Liang et al. (2008)<DOI:10.1198/016214507000001337>  and Bayarri et al. (2012)<DOI:10.1214/12-aos1013>. The interaction with the package is through a friendly interface that syntactically mimics the well-known lm() command of R. The resulting objects can be easily explored providing the user very valuable information (like marginal, joint and conditional inclusion probabilities of potential variables; the highest posterior probability model, HPM; the median probability model, MPM) about the structure of the true -data generating- model. Additionally, this package incorporates abilities to handle problems with a large number of potential explanatory variables through parallel and heuristic versions of the main commands, Garcia-Donato and Martinez-Beneito (2013)<DOI:10.1080/01621459.2012.742443>. It also allows problems with p>n and p>>n and also incorporates routines to handle problems with variable selection with factors.",2020-02-18,Anabel Forte,https://github.com/comodin19/BayesVarSel,TRUE,https://github.com/comodin19/bayesvarsel,27206,5,2021-04-27T08:26:00Z,5441.2
bayesvl,"Provides users with its associated functions for pedagogical purposes in visually learning Bayesian networks and Markov chain Monte Carlo (MCMC) computations. It enables users to: a) Create and examine the (starting) graphical structure of Bayesian networks; b) Create random Bayesian networks using a dataset with customized constraints; c) Generate 'Stan' code for structures of Bayesian networks for sampling the data and learning parameters; d) Plot the network graphs; e) Perform Markov chain Monte Carlo computations and produce graphs for posteriors checks. The package refers to one reference item, which describes the methods and algorithms: Vuong, Quan-Hoang and La, Viet-Phuong (2019) <doi:10.31219/osf.io/w5dx6> The 'bayesvl' R package. Open Science Framework (May 18).",2019-05-24,Viet-Phuong La,https://github.com/sshpa/bayesvl,TRUE,https://github.com/sshpa/bayesvl,12623,14,2021-05-16T09:43:03Z,901.6428571428571
BayLum,"Bayesian analysis of luminescence data and C-14 age estimates. Bayesian models are based on the following publications: Combes, B. & Philippe, A. (2017) <doi:10.1016/j.quageo.2017.02.003> and Combes et al (2015) <doi:10.1016/j.quageo.2015.04.001>. This includes, amongst others, data import, export, application of age models and palaeodose model.",2020-12-06,Anne Philippe,https://CRAN.r-project.org/package=BayLum,TRUE,https://github.com/crp2a/baylum,20631,7,2021-06-02T09:59:19Z,2947.285714285714
baymedr,"BAYesian inference for MEDical designs in R. Functions for the 
    computation of Bayes factors for common biomedical research designs. 
    Implemented are functions to test the equivalence (equiv_bf), 
    non-inferiority (infer_bf), and superiority (super_bf) of an experimental 
    group compared to a control group on a continuous outcome measure. Bayes 
    factors for these three tests can be computed based on raw data (x, y) or 
    summary statistics (n_x, n_y, mean_x, mean_y, sd_x, sd_y [or ci_margin 
    and ci_level]).",2021-03-28,Maximilian Linde,https://github.com/maxlinde/baymedr,TRUE,https://github.com/maxlinde/baymedr,11112,2,2021-03-26T15:01:40Z,5556
baytrends,"Enable users to evaluate long-term trends using a Generalized 
    Additive Modeling (GAM) approach. The model development includes selecting a 
    GAM structure to describe nonlinear seasonally-varying changes over time, 
    incorporation of hydrologic variability via either a river flow or salinity, 
    the use of an intervention to deal with method or laboratory changes 
    suspected to impact data values, and representation of left- and 
    interval-censored data. The approach has been applied to water quality data 
    in the Chesapeake Bay, a major estuary on the east coast of the United 
    States to provide insights to a range of management- and research-focused 
    questions.  Methodology described in Murphy (2019) 
    <doi:10.1016/j.envsoft.2019.03.027>.",2021-05-14,Rebecca Murphy,https://github.com/tetratech/baytrends,TRUE,https://github.com/tetratech/baytrends,18294,5,2021-05-14T20:01:00Z,3658.8
bbmle,Methods and functions for fitting maximum likelihood models in R. This package modifies and extends the 'mle' classes in the 'stats4' package.,2021-08-05,Ben Bolker,https://github.com/bbolker/bbmle,TRUE,https://github.com/bbolker/bbmle,420286,20,2021-08-04T21:36:32Z,21014.3
bbotk,"Provides a common framework for optimization of
    black-box functions for other packages, e.g. 'mlr3tuning' or
    'mlr3fselect'.  It offers various optimization methods e.g. grid
    search, random search and generalized simulated annealing.",2021-03-18,Marc Becker,"https://bbotk.mlr-org.com, https://github.com/mlr-org/bbotk",TRUE,https://github.com/mlr-org/bbotk,31148,6,2021-08-19T07:12:36Z,5191.333333333333
bbricks,"A set of frequently used Bayesian parametric and nonparametric model structures, as well as a set of tools for common analytical tasks. Structures include linear Gaussian systems, Gaussian and Normal-Inverse-Wishart conjugate structure, Gaussian and Normal-Inverse-Gamma conjugate structure, Categorical and Dirichlet conjugate structure, Dirichlet Process on positive integers, Dirichlet Process in general, Hierarchical Dirichlet Process ... Tasks include updating posteriors, sampling from posteriors, calculating marginal likelihood, calculating posterior predictive densities, sampling from posterior predictive distributions, calculating ""Maximum A Posteriori"" (MAP) estimates ... See <https://chenhaotian.github.io/Bayesian-Bricks/> to get started.",2020-05-07,Haotian Chen,https://github.com/chenhaotian/Bayesian-Bricks,TRUE,https://github.com/chenhaotian/bayesian-bricks,9862,6,2021-01-16T05:43:16Z,1643.6666666666667
bbsBayes,"The North American Breeding Bird Survey (BBS) is a long-running
  program that seeks to monitor the status and trends of the breeding birds in
  North America. Since its start in 1966, the BBS has accumulated over 50 years 
  of data for over 500 species of North American Birds. Given the temporal and 
  spatial structure of the data, hierarchical Bayesian models are used to assess 
  the status and trends of these 500+ species of birds. 'bbsBayes' allows you to perform 
  hierarchical Bayesian analysis of BBS data. You can run a full
  model analysis for one or more species that you choose, or you can take
  more control and specify how the data should be stratified, prepared
  for 'JAGS', or modelled. The functions provided here allow you to replicate
  analyses performed by the United State Geological Survey (USGS, see Link 
  and Sauer (2011) <doi:10.1525/auk.2010.09220>) and Canadian Wildlife Service
  (CWS, see Smith and Edwards (2020) <doi:10.1101/2020.03.26.010215>).",2021-08-06,Brandon P.M. Edwards,https://github.com/BrandonEdwards/bbsBayes,TRUE,https://github.com/brandonedwards/bbsbayes,8954,20,2021-08-04T14:32:41Z,447.7
bbw,"The blocked weighted bootstrap (BBW) is an estimation technique 
    for use with data from two-stage cluster sampled surveys in which either 
    prior weighting (e.g. population-proportional sampling or PPS as used in 
    Standardized Monitoring and Assessment of Relief and Transitions or SMART 
    surveys) or posterior weighting (e.g. as used in rapid assessment method or
    RAM and simple spatial sampling method or S3M surveys). The method was 
    developed by Accion Contra la Faim, Brixton Health, Concern Worldwide, 
    Global Alliance for Improved Nutrition, UNICEF Sierra Leone,  UNICEF Sudan
    and Valid International. It has been tested by the Centers for Disease
    Control (CDC) using infant and young child feeding (IYCF) data. See Cameron
    et al (2008) <doi:10.1162/rest.90.3.414> for application of bootstrap
    to cluster samples. See Aaron et al (2016) <doi:10.1371/journal.pone.0163176> 
    and Aaron et al (2016) <doi:10.1371/journal.pone.0162462> for application 
    of the blocked weighted bootstrap to estimate indicators from two-stage 
    cluster sampled surveys.",2018-01-17,Mark Myatt,https://github.com/validmeasures/bbw,TRUE,https://github.com/validmeasures/bbw,16744,2,2020-09-26T18:40:25Z,8372
bcaboot,"Computation of bootstrap confidence intervals in an almost automatic fashion as described in Efron and Narasimhan (2020, <doi:10.1080/10618600.2020.1714633>).",2021-05-09,Balasubramanian Narasimhan,"https://bnaras.github.io/bcaboot/,
https://github.com/bnaras/bcaboot",TRUE,https://github.com/bnaras/bcaboot,17261,11,2021-05-08T17:13:04Z,1569.1818181818182
bcdata,"Search, query, and download tabular and
    'geospatial' data from the British Columbia Data Catalogue
    (<https://catalogue.data.gov.bc.ca/>).  Search catalogue data records
    based on keywords, data licence, sector, data format, and B.C.
    government organization. View metadata directly in R, download many
    data formats, and query 'geospatial' data available via the B.C.
    government Web Feature Service ('WFS') using 'dplyr' syntax.",2021-05-04,Andy Teucher,"https://bcgov.github.io/bcdata/,
https://catalogue.data.gov.bc.ca/,
https://github.com/bcgov/bcdata/",TRUE,https://github.com/bcgov/bcdata,15571,59,2021-09-02T23:56:19Z,263.91525423728814
Bchron,"Enables quick calibration of radiocarbon dates under various 
  calibration curves (including user generated ones); age-depth modelling 
  as per the algorithm of Haslett and Parnell (2008) <DOI:10.1111/j.1467-9876.2008.00623.x>; Relative sea level 
  rate estimation incorporating time uncertainty in polynomial regression 
  models (Parnell and Gehrels 2015) <DOI:10.1002/9781118452547.ch32>; non-parametric phase modelling via 
  Gaussian mixtures as a means to determine the activity of a site 
  (and as an alternative to the Oxcal function SUM; currently 
  unpublished), and reverse calibration of dates from calibrated into 
  un-calibrated years (also unpublished).",2021-06-10,Andrew Parnell,https://andrewcparnell.github.io/Bchron/,TRUE,https://github.com/andrewcparnell/bchron,42579,27,2021-06-10T08:29:14Z,1577
bcmaps,"Provides access to various spatial layers for B.C., such as 
    administrative boundaries, natural resource management boundaries, etc. 
    Most layers are imported from the 'bcdata' package as 'sf' or 'Spatial' objects
    through function calls in this package. ",2021-03-09,Andy Teucher,https://github.com/bcgov/bcmaps,TRUE,https://github.com/bcgov/bcmaps,21572,52,2021-08-11T22:26:59Z,414.84615384615387
bcputility,Provides functions to utilize a command line utility that does bulk inserts and exports from SQL Server databases. ,2021-08-06,Thomas Roh,"https://bcputility.roh.engineering,
https://github.com/tomroh/bcputility",TRUE,https://github.com/tomroh/bcputility,953,4,2021-08-06T01:28:28Z,238.25
bdchecks,Supplies a Shiny app and a set of functions to perform and managing data checks for biodiversity data. ,2019-02-18,Povilas Gibas,https://github.com/bd-R/bdchecks,TRUE,https://github.com/bd-r/bdchecks,14854,1,2021-09-02T18:13:44Z,14854
bdclean,"Provides features to manage the complete workflow for biodiversity data cleaning. Uploading data, gathering input from users (in order to adjust cleaning procedures), cleaning data and finally, generating various reports and several versions of the data. Facilitates user-level data cleaning, designed for the inexperienced R user. T Gueta et al (2018) <doi:10.3897/biss.2.25564>. T Gueta et al (2017) <doi:10.3897/tdwgproceedings.1.20311>.",2019-04-11,Thiloshon Nagarajah,"https://github.com/bd-R/bdclean,
https://bd-r.github.io/The-bdverse/index.html",TRUE,https://github.com/bd-r/bdclean,13768,7,2021-09-02T18:13:49Z,1966.857142857143
bdl,"Interface to Local Data Bank ('Bank Danych Lokalnych' - 'bdl') API 
    <https://api.stat.gov.pl/Home/BdlApi?lang=en> with set of useful tools like 
    quick plotting and map generating using data from bank. ",2021-03-02,Marzena Szpadel,https://github.com/statisticspoland/R_Package_to_API_BDL,TRUE,https://github.com/statisticspoland/r_package_to_api_bdl,10040,14,2021-03-03T08:57:30Z,717.1428571428571
bdots,"Analyze differences among time series curves with p-value
        adjustment for multiple comparisons introduced in Oleson et al
        (2015) <DOI:10.1177/0962280215607411>.",2021-03-27,Collin Nolte,https://github.com/collinn/bdots,TRUE,https://github.com/collinn/bdots,19873,2,2021-07-29T19:35:06Z,9936.5
bdpar,"
        Provide a tool to easily build customized data flows to pre-process large volumes 
    of information from different sources. To this end, 'bdpar' allows to (i) easily use and 
    create new functionalities and (ii) develop new data source extractors according to the 
    user needs. Additionally, the package provides by default a predefined data flow 
    to extract and pre-process the most relevant information (tokens, dates, ... ) from some textual 
    sources (SMS, Email, tweets, YouTube comments).",2021-06-24,Miguel Ferreiro-Díaz,https://github.com/miferreiro/bdpar,TRUE,https://github.com/miferreiro/bdpar,13682,4,2021-06-24T10:52:31Z,3420.5
bdrc,Fits a discharge rating curve based on the power-law and the generalized power-law from data on paired stage and discharge measurements in a given river using a Bayesian hierarchical model as described in Hrafnkelsson et al. (2020) <arXiv:2010.04769>.,2021-07-28,Birgir Hrafnkelsson,NA,TRUE,https://github.com/sor16/bdrc,595,3,2021-08-22T13:30:50Z,198.33333333333334
bdvis,"Provides a set of functions to create basic visualizations to quickly
    preview different aspects of biodiversity information such as inventory 
    completeness, extent of coverage (taxonomic, temporal and geographic), gaps
    and biases.",2021-01-15,Vijay Barve,NA,TRUE,https://github.com/vijaybarve/bdvis,24906,24,2021-01-16T20:38:06Z,1037.75
beakr,"A minimalist web framework for developing application programming 
    interfaces in R that provides a flexible framework for handling common 
    HTTP-requests, errors, logging, and an ability to integrate any R code as 
    server middle-ware.",2021-04-06,Jonathan Callahan,https://github.com/MazamaScience/beakr,TRUE,https://github.com/mazamascience/beakr,10386,70,2021-08-26T21:40:06Z,148.37142857142857
beam,"Fast Bayesian inference of marginal and conditional independence structures from high-dimensional data. Leday and Richardson (2019), Biometrics, <doi:10.1111/biom.13064>.",2020-05-28,Gwenael G.R. Leday,https://github.com/gleday/beam,TRUE,https://github.com/gleday/beam,17687,0,2020-12-07T07:57:55Z,NA
BeastJar,"Provides JAR to perform Markov chain Monte Carlo (MCMC) inference using
    the popular Bayesian Evolutionary Analysis by Sampling Trees 'BEAST' software library
    of Suchard et al (2018) <doi:10.1093/ve/vey016>. 'BEAST' supports auto-tuning
    Metropolis-Hastings, slice, Hamiltonian Monte Carlo and Sequential Monte Carlo
    sampling for a large variety of composable standard and phylogenetic statistical
    models using high performance computing.  By placing the 'BEAST' JAR in this package,
    we offer an efficient distribution system for 'BEAST' use by other R packages using
    CRAN.",2020-10-26,Marc A. Suchard,https://github.com/beast-dev/BeastJar,TRUE,https://github.com/beast-dev/beastjar,4628,1,2020-10-25T12:17:46Z,4628
BED,"An interface for the 'Neo4j' database providing
    mapping between different identifiers of biological entities.
    This Biological Entity Dictionary (BED)
    has been developed to address three main challenges.
    The first one is related to the completeness of identifier mappings.
    Indeed, direct mapping information provided by the different systems
    are not always complete and can be enriched by mappings provided by other
    resources.
    More interestingly, direct mappings not identified by any of these
    resources can be indirectly inferred by using mappings to a third reference.
    For example, many human Ensembl gene ID are not directly mapped to any
    Entrez gene ID but such mappings can be inferred using respective mappings
    to HGNC ID. The second challenge is related to the mapping of deprecated
    identifiers. Indeed, entity identifiers can change from one resource
    release to another. The identifier history is provided by some resources,
    such as Ensembl or the NCBI, but it is generally not used by mapping tools.
    The third challenge is related to the automation of the mapping process
    according to the relationships between the biological entities of interest.
    Indeed, mapping between gene and protein ID scopes should not be done
    the same way than between two scopes regarding gene ID.
    Also, converting identifiers from different organisms should be possible
    using gene orthologs information.
    A ready to use database is provided as
    a 'Docker' image <https://hub.docker.com/r/patzaw/bed-ucb-human/>.
    The method has been published by
    Godard and van Eyll (2018) <doi:10.12688/f1000research.13925.3>.",2021-03-12,Patrice Godard,https://github.com/patzaw/BED,TRUE,https://github.com/patzaw/bed,3421,5,2021-05-17T13:13:34Z,684.2
beeswarm,"The bee swarm plot is a one-dimensional scatter plot like
    ""stripchart"", but with closely-packed, non-overlapping points. ",2021-06-01,Aron Eklund,https://github.com/aroneklund/beeswarm,TRUE,https://github.com/aroneklund/beeswarm,401759,32,2021-05-07T08:54:16Z,12554.96875
beezdemand,"Facilitates many of the analyses performed in studies of
    behavioral economic demand. The package supports commonly-used options for
		modeling operant demand including (1) data screening proposed by Stein,
		Koffarnus, Snider, Quisenberry, & Bickel (2015; <doi:10.1037/pha0000020>),
		(2) fitting models of demand such as linear (Hursh, Raslear, Bauman,
		& Black, 1989, <doi:10.1007/978-94-009-2470-3_22>), exponential	(Hursh & Silberberg, 2008,
		<doi:10.1037/0033-295X.115.1.186>) and modified exponential (Koffarnus,
		Franck, Stein, & Bickel, 2015, <doi:10.1037/pha0000045>), and (3) calculating
		numerous measures	relevant to applied behavioral economists (Intensity,
		Pmax, Omax). Also	supports plotting and comparing data.",2018-07-31,Brent Kaplan,https://github.com/brentkaplan/beezdemand,TRUE,https://github.com/brentkaplan/beezdemand,15728,9,2020-10-22T16:59:44Z,1747.5555555555557
beginr,"Useful functions for R beginners, including hints for the arguments of the 'plot()' function, self-defined functions for error bars, user-customized pair plots and hist plots, enhanced linear regression figures, etc.. This package could be helpful to R experts as well.",2019-05-02,Peng Zhao,https://github.com/pzhaonet/beginr,TRUE,https://github.com/pzhaonet/beginr,25819,15,2021-03-23T08:34:26Z,1721.2666666666667
behavr,Implements an S3 class based on 'data.table' to store and process efficiently ethomics (high-throughput behavioural) data.,2019-01-03,Quentin Geissmann,https://github.com/rethomics/behavr,TRUE,https://github.com/rethomics/behavr,20817,5,2021-06-15T05:03:12Z,4163.4
bench,Tools to accurately benchmark and analyze execution times for R expressions.,2020-01-13,Jim Hester,https://github.com/r-lib/bench,TRUE,https://github.com/r-lib/bench,319639,201,2021-03-06T14:35:37Z,1590.2437810945273
benchmarkme,"Benchmark your CPU and compare against other CPUs.
    Also provides functions for obtaining system specifications, such as
    RAM, CPU type, and R version.",2021-03-21,Colin Gillespie,https://github.com/csgillespie/benchmarkme,TRUE,https://github.com/csgillespie/benchmarkme,80114,33,2021-03-21T18:38:12Z,2427.6969696969695
berryFunctions,"Draw horizontal histograms, color scattered points by 3rd dimension,
    enhance date- and log-axis plots, zoom in X11 graphics, trace errors and warnings, 
    use the unit hydrograph in a linear storage cascade, convert lists to data.frames and arrays, 
    fit multiple functions.",2021-04-01,Berry Boessenkool,https://github.com/brry/berryFunctions,TRUE,https://github.com/brry/berryfunctions,61061,9,2021-08-11T23:47:43Z,6784.555555555556
BEST,"An alternative to t-tests, producing posterior estimates for group means and standard deviations and their differences and effect sizes. It implements the method of Kruschke (2013) Bayesian estimation supersedes the t test. Journal of Experimental Psychology: General, 142(2):573-603 <doi: 10.1037/a0029146>.",2021-05-17,John K. Kruschke and Mike Meredith,NA,TRUE,https://github.com/mikemeredith/best,43806,19,2021-06-21T07:36:52Z,2305.5789473684213
bestNormalize,"Estimate a suite of normalizing transformations, including 
    a new adaptation of a technique based on ranks which can guarantee 
    normally distributed transformed data if there are no ties: ordered 
    quantile normalization (ORQ). ORQ normalization combines a rank-mapping
    approach with a shifted logit approximation that allows
    the transformation to work on data outside the original domain. It is 
    also able to handle new data within the original domain via linear 
    interpolation. The package is built to estimate the best normalizing 
    transformation for a vector consistently and accurately. It implements 
    the Box-Cox transformation, the Yeo-Johnson transformation, three types 
    of Lambert WxF transformations, and the ordered quantile normalization 
    transformation. It estimates the normalization efficacy of other
    commonly used transformations, and it allows users to specify 
    custom transformations or normalization statistics. Finally, functionality
    can be integrated into a machine learning workflow via recipes. ",2021-09-02,Ryan Andrew Peterson,"https://petersonr.github.io/bestNormalize/,
https://github.com/petersonR/bestNormalize",TRUE,https://github.com/petersonr/bestnormalize,129075,23,2021-08-11T19:16:06Z,5611.95652173913
BetaBit,"Three games: proton, frequon and regression. Each one is a console-based data-crunching game for younger and older data scientists.
  Act as a data-hacker and find Slawomir Pietraszko's credentials to the Proton server.
  In proton you have to solve four data-based puzzles to find the login and password.
  There are many ways to solve these puzzles. You may use loops, data filtering, ordering, aggregation or other tools.
  Only basics knowledge of R is required to play the game, yet the more functions you know, the more approaches you can try.
  In frequon you will help to perform statistical cryptanalytic attack on a corpus of ciphered messages.
  This time seven sub-tasks are pushing the bar much higher. Do you accept the challenge?
  In regression you will test your modeling skills in a series of eight sub-tasks.
  Try only if ANOVA is your close friend.
  It's a part of Beta and Bit project.
  You will find more about the Beta and Bit project at <https://betabit.wiki>.",2020-10-03,Przemyslaw Biecek,https://betabit.wiki,TRUE,https://github.com/betaandbit/betabitrgame,20499,0,2020-11-11T20:44:57Z,NA
BETS,"It provides access to and information about the most important
    Brazilian economic time series - from the Getulio Vargas Foundation <http://portal.fgv.br/en>,
    the Central Bank of Brazil <http://www.bcb.gov.br> and the Brazilian Institute of Geography
    and Statistics <http://www.ibge.gov.br>. It also presents tools for managing, analysing (e.g.
    generating dynamic reports with a complete analysis of a series) and exporting
    these time series.",2018-09-28,Talitha Speranza,https://github.com/nmecsys/BETS,TRUE,https://github.com/nmecsys/bets,44695,28,2021-04-03T20:57:07Z,1596.25
bettermc,"Drop-in replacement for 'parallel::mclapply()' adding e.g.
    tracebacks, crash dumps, retries, condition handling, improved seeding,
    progress bars and faster inter process communication. Some of the internal
    functions are also exported for other use: 'etry()' (extended try),
    'copy2shm()/allocate_from_shm()' (copy to and allocate from POSIX shared
    memory), 'char_map/map2char()' (split a character vector into its unique
    elements and a mapping on these) and various semaphore related functions.",2021-08-02,Andreas Kersting,https://github.com/gfkse/bettermc,TRUE,https://github.com/gfkse/bettermc,4442,12,2021-08-20T06:44:35Z,370.1666666666667
bfast,"Decomposition of time series into
    trend, seasonal, and remainder components with methods for detecting and
    characterizing abrupt changes within the trend and seasonal components. 'BFAST'
    can be used to analyze different types of satellite image time series and can
    be applied to other disciplines dealing with seasonal or non-seasonal time
    series, such as hydrology, climatology, and econometrics. The algorithm can be
    extended to label detected changes with information on the parameters of the
    fitted piecewise linear models. 'BFAST' monitoring functionality is described
    in Verbesselt et al. (2010) <doi:10.1016/j.rse.2009.08.014>. 'BFAST monitor'
    provides functionality to detect disturbance in near real-time based on 'BFAST'-
    type models, and is described in Verbesselt et al. (2012) <doi:10.1016/j.rse.2012.02.022>.
    'BFAST Lite' approach is a flexible approach that handles missing data
    without interpolation, and will be described in an upcoming paper.
    Furthermore, different models can now be used to fit the
    time series data and detect structural changes (breaks).",2021-05-10,Dainius Masiliunas,https://bfast2.github.io/,TRUE,https://github.com/bfast2/bfast,42646,11,2021-06-23T14:37:48Z,3876.909090909091
BFpack,"Implementation of various default Bayes factors
    for testing statistical hypotheses. The package is
    intended for applied quantitative researchers in the
    social and behavioral sciences, medical research,
    and related fields. The Bayes factor tests can be
    executed for statistical models such as 
    univariate and multivariate normal linear models,
    generalized linear models, special cases of 
    linear mixed models, survival models, relational
    event models. Parameters that can be tested are
    location parameters (e.g., group means, regression coefficients),
    variances (e.g., group variances), and measures of 
    association (e.g,. bivariate correlations), among others.
    The statistical underpinnings are
    described in 
    Mulder, Hoijtink, and Xin (2019) <arXiv:1904.00679>,
    Mulder and Gelissen (2019) <arXiv:1807.05819>,
    Mulder (2016) <DOI:10.1016/j.jmp.2014.09.004>,
    Mulder and Fox (2019) <DOI:10.1214/18-BA1115>,
    Mulder and Fox (2013) <DOI:10.1007/s11222-011-9295-3>,
    Boeing-Messing, van Assen, Hofman, Hoijtink, and Mulder <DOI:10.1037/met0000116>,
    Hoijtink, Mulder, van Lissa, and Gu, (2018) <DOI:10.31234/osf.io/v3shc>,
    Gu, Mulder, and Hoijtink, (2018) <DOI:10.1111/bmsp.12110>,
    Hoijtink, Gu, and Mulder, (2018) <DOI:10.1111/bmsp.12145>, and
    Hoijtink, Gu, Mulder, and Rosseel, (2018) <DOI:10.1037/met0000187>.",2021-02-02,Joris Mulder,https://github.com/jomulder/BFpack,TRUE,https://github.com/jomulder/bfpack,24905,7,2021-04-25T19:57:16Z,3557.8571428571427
BFS,Search and download data from the Swiss Federal Statistical Office <https://www.bfs.admin.ch/>.,2021-05-10,Felix Luginbuhl,"https://felixluginbuhl.com/BFS/, https://github.com/lgnbhl/BFS/",TRUE,https://github.com/lgnbhl/bfs,12633,5,2021-05-10T12:40:27Z,2526.6
bfsl,"Provides the solution from York (1968) <doi:10.1016/S0012-821X(68)80059-7>
  for fitting a straight line to bivariate data with errors in both coordinates.
  It gives unbiased estimates of the intercept, slope and standard errors of the
  best-fit straight line to independent points with (possibly correlated) 
  normally distributed errors in both x and y. Other commonly used 
  errors-in-variables methods, such as orthogonal distance regression, geometric
  mean regression or Deming regression are special cases of York’s solution.",2018-12-16,Patrick Sturm,https://github.com/pasturm/bfsl,TRUE,https://github.com/pasturm/bfsl,13500,0,2021-04-07T20:33:47Z,NA
bfsMaps,"At the Swiss Federal Statistical Office (SFSO), spatial maps of Switzerland are available free of charge as 'Cartographic bases for small-scale thematic mapping'. This package contains convenience functions to import ESRI (Environmental Systems Research Institute) shape files using the package 'rgdal' and to plot them easily and quickly without having to worry too much about the technical details.
      It contains utilities to combine multiple areas to one single polygon and to find neighbours for single regions. For any point on a map, a special locator can be used to determine to which municipality, district or canton it belongs.",2020-12-17,Andri Signorell,https://github.com/AndriSignorell/bfsMaps/,TRUE,https://github.com/andrisignorell/bfsmaps,8547,0,2020-12-18T06:02:47Z,NA
bfw,"Derived from the work of Kruschke (2015, <ISBN:9780124058880>),
    the present package aims to provide a framework for conducting Bayesian
    analysis using Markov chain Monte Carlo (MCMC) sampling utilizing the
    Just Another Gibbs Sampler ('JAGS', Plummer, 2003, <http://mcmc-jags.sourceforge.net/>).
    The initial version includes several modules for conducting Bayesian
    equivalents of chi-squared tests, analysis of variance (ANOVA),
    multiple (hierarchical) regression, softmax regression, and for fitting data
    (e.g., structural equation modeling).",2019-11-25,Øystein Olav Skaar,https://github.com/oeysan/bfw/,TRUE,https://github.com/oeysan/bfw,18056,10,2021-01-07T08:30:35Z,1805.6
BGData,"An umbrella package providing a phenotype/genotype data structure
    and scalable and efficient computational methods for large genomic datasets
    in combination with several other packages: 'BEDMatrix', 'LinkedMatrix',
    and 'symDMatrix'.",2020-08-02,Alexander Grueneberg,https://github.com/QuantGen/BGData,TRUE,https://github.com/quantgen/bgdata,21681,22,2021-07-09T21:33:29Z,985.5
bggAnalytics,"Tools for analysing board game data. Mainly focused on providing 
    an interface for BoardGameGeek's XML API2 through R6 class system objects.
    More details about the BoardGameGeek's API can be obtained here
    <https://boardgamegeek.com/wiki/page/BGG_XML_API2>.",2020-10-22,Jakub Bujnowicz,https://github.com/JakubBujnowicz/bggAnalytics,TRUE,https://github.com/jakubbujnowicz/bgganalytics,3010,1,2020-10-13T16:49:47Z,3010
BGGM,"Fit Bayesian Gaussian graphical models. The methods are separated into 
    two Bayesian approaches for inference: hypothesis testing and estimation. There are 
    extensions for confirmatory hypothesis testing, comparing Gaussian graphical models, 
    and node wise predictability. These methods were recently introduced in the Gaussian 
    graphical model literature, including 
    Williams (2019) <doi:10.31234/osf.io/x8dpr>, 
    Williams and Mulder (2019) <doi:10.31234/osf.io/ypxd8>,
    Williams, Rast, Pericchi, and Mulder (2019) <doi:10.31234/osf.io/yt386>.",2021-08-20,Donald Williams,NA,TRUE,https://github.com/donaldrwilliams/bggm,23435,38,2021-08-18T13:53:46Z,616.7105263157895
BH,"Boost provides free peer-reviewed portable C++ source 
 libraries.  A large part of Boost is provided as C++ template code
 which is resolved entirely at compile-time without linking.  This 
 package aims to provide the most useful subset of Boost libraries 
 for template use among CRAN packages. By placing these libraries in 
 this package, we offer a more efficient distribution system for CRAN 
 as replication of this code in the sources of other packages is 
 avoided. As of release 1.75.0-0, the following Boost libraries are
 included: 'accumulators' 'algorithm' 'align' 'any' 'atomic' 'beast'
 'bimap' 'bind' 'circular_buffer' 'compute' 'concept' 'config'
 'container' 'date_time' 'detail' 'dynamic_bitset' 'exception'
 'flyweight' 'foreach' 'functional' 'fusion' 'geometry' 'graph' 'heap'
 'icl' 'integer' 'interprocess' 'intrusive' 'io' 'iostreams'
 'iterator' 'math' 'move' 'mp11' 'mpl' 'multiprecision' 'numeric'
 'pending' 'phoenix' 'polygon' 'preprocessor' 'propery_tree' 'random'
 'range' 'scope_exit' 'smart_ptr' 'sort' 'spirit' 'tuple'
 'type_traits' 'typeof' 'unordered' 'utility' 'uuid'.",2021-01-11,Dirk Eddelbuettel,https://github.com/eddelbuettel/bh,TRUE,https://github.com/eddelbuettel/bh,19249313,71,2021-03-20T01:04:33Z,271117.08450704225
BI,"Generate the James Blinding Index, as described in James et al (1996) 
             <https://pubmed.ncbi.nlm.nih.gov/8841652/> and the Bang Blinding Index, 
             as described in Bang et al (2004) <https://pubmed.ncbi.nlm.nih.gov/15020033/>.
             These are measures to assess whether or not satisfactory blinding has been 
             maintained in a randomized, controlled, clinical trial. These can be generated 
             for trial subjects, research coordinators and principal investigators, based 
             upon standardized questionnaires that have been administered, to assess whether
             or not they can correctly guess to which treatment arm (e.g. placebo or treatment) 
             subjects were assigned at randomization. ",2021-05-04,Marc Schwartz,https://github.com/marcschwartz/BI,TRUE,https://github.com/marcschwartz/bi,2104,1,2021-05-03T18:52:08Z,2104
BiasCorrector,"A GUI to correct measurement bias in DNA methylation
    analyses. The 'BiasCorrector' package just wraps the functions
    implemented in the 'R' package 'rBiasCorrection' into a shiny web
    application in order to make them more easily accessible. Publication:
    Kapsner et al. (2021) <doi:10.1002/ijc.33681>.",2021-05-17,Lorenz A. Kapsner,https://github.com/kapsner/BiasCorrector,TRUE,https://github.com/kapsner/biascorrector,5451,1,2021-05-19T07:37:55Z,5451
bibliometrix,"Tool for quantitative research in scientometrics and bibliometrics.
    It provides various routines for importing bibliographic data from 'SCOPUS' (<https://scopus.com>),
    'Clarivate Analytics Web of Science' (<https://www.webofknowledge.com/>), 'Digital Science Dimensions' 
	(<https://www.dimensions.ai/>), 'Cochrane Library' (<https://www.cochranelibrary.com/>), 'Lens' (<https://lens.org>), 
	and 'PubMed' (<https://pubmed.ncbi.nlm.nih.gov/>) databases, performing bibliometric analysis 
    and building networks for co-citation, coupling, scientific collaboration and co-word analysis.",2021-07-05,Massimo Aria,"https://www.bibliometrix.org,
https://github.com/massimoaria/bibliometrix,
https://www.k-synth.com",TRUE,https://github.com/massimoaria/bibliometrix,206855,237,2021-07-05T09:59:17Z,872.8059071729958
biblionetwork,"Functions to find edges for bibliometric networks like bibliographic coupling network, co-citation network and co-authorship network. The weights of network edges can be calculated according to different methods, depending on the type of networks, the type of nodes, and what you want to analyse. These functions are optimized to be be used on large dataset. The package contains functions inspired by: Leydesdorff, Loet and Park, Han Woo (2017) <doi:10.1016/j.joi.2016.11.007>; Perianes-Rodriguez, Antonio, Ludo Waltman, and Nees Jan Van Eck (2016) <doi:10.1016/j.joi.2016.10.006>; Sen, Subir K. and Shymal K. Gan (1983) <http://nopr.niscair.res.in/handle/123456789/28008>; Shen, Si, Zhu, Danhao, Rousseau, Ronald, Su, Xinning and Wang, Dongbo (2019) <doi:10.1016/j.joi.2019.01.012>; Zhao, Dangzhi and Strotmann, Andreas (2008) <doi:10.1002/meet.2008.1450450292>.",2021-04-09,Aurélien Goutsmedt,"https://github.com/agoutsmedt/biblionetwork,
https://agoutsmedt.github.io/biblionetwork/",TRUE,https://github.com/agoutsmedt/biblionetwork,2061,2,2021-04-12T16:21:22Z,1030.5
bibtex,Utility to parse a bibtex file. ,2020-09-19,Romain Francois,https://github.com/romainfrancois/bibtex,TRUE,https://github.com/romainfrancois/bibtex,1324145,28,2020-09-26T02:24:40Z,47290.892857142855
biclustermd,"Biclustering is a statistical learning technique that simultaneously 
    partitions and clusters rows and columns of a data matrix. Since the solution 
    space of biclustering is in infeasible to completely search with current 
    computational mechanisms, this package uses a greedy heuristic. The algorithm 
    featured in this package is, to the best our knowledge, the first biclustering 
    algorithm to work on data with missing values. Li, J., Reisner, J., Pham, H., 
    Olafsson, S., and Vardeman, S. (2020) Biclustering with Missing Data. Information 
    Sciences, 510, 304–316.",2021-06-17,John Reisner,https://github.com/jreisner/biclustermd,TRUE,https://github.com/jreisner/biclustermd,14214,3,2021-06-17T14:10:44Z,4738
BifactorIndicesCalculator,"The calculator computes bifactor indices such as explained common variance (ECV), hierarchical Omega (OmegaH), percentage of uncontaminated correlations (PUC), item explained common variance (I-ECV), and more. This package is an R version of the 'Excel' based 'Bifactor Indices Calculator' (Dueber, 2017)  <doi:10.13023/edp.tool.01> with added convenience features for directly utilizing output from several programs that can fit confirmatory factor analysis or item response models.",2021-05-12,David Dueber,https://github.com/ddueber/BifactorIndicesCalculator,TRUE,https://github.com/ddueber/bifactorindicescalculator,9375,3,2021-03-10T15:58:39Z,3125
bife,"Estimates fixed effects binary choice models (logit and probit) with potentially many
  individual fixed effects and computes average partial effects. Incidental parameter bias can be
  reduced with an asymptotic bias correction proposed by Fernandez-Val (2009) 
  <doi:10.1016/j.jeconom.2009.02.007>.",2020-10-30,Amrei Stammann,https://github.com/amrei-stammann/bife,TRUE,https://github.com/amrei-stammann/bife,60258,6,2020-11-10T09:32:39Z,10043
bigalgebra,"Provides arithmetic functions for R matrix and 'big.matrix' objects as well as functions for QR factorization, Cholesky factorization, General eigenvalue, and Singular value decomposition (SVD). A method matrix multiplication and an arithmetic method -for matrix addition, matrix difference- allows for mixed type operation -a matrix class object and a big.matrix class object- and pure type operation for two big.matrix class objects.",2021-05-12,Frederic Bertrand,"https://fbertran.github.io/bigalgebra/,
https://github.com/fbertran/bigalgebra/",TRUE,https://github.com/fbertran/bigalgebra,16221,3,2021-05-12T16:12:40Z,5407
bigassertr,"
    Enhanced message functions (cat() / message() / warning() / error()) 
    using wrappers around sprintf(). Also, multiple assertion functions 
    (e.g. to check class, length, values, files, arguments, etc.).",2021-07-08,Florian Privé,https://github.com/privefl/bigassertr,TRUE,https://github.com/privefl/bigassertr,67211,2,2021-07-08T11:28:57Z,33605.5
BIGDAWG,"Data sets and functions for chi-squared Hardy-Weinberg and case-control association tests of highly polymorphic genetic data [e.g., human leukocyte antigen (HLA) data]. Performs association tests at multiple levels of polymorphism (haplotype, locus and HLA amino-acids) as described in Pappas DJ, Marin W, Hollenbach JA, Mack SJ (2016) <doi:10.1016/j.humimm.2015.12.006>. Combines rare variants to a common class to account for sparse cells in tables as described by Hollenbach JA, Mack SJ, Thomson G, Gourraud PA (2012) <doi:10.1007/978-1-61779-842-9_14>.",2021-02-11,Derek Pappas,"http://tools.immunogenomics.org/,
https://github.com/IgDAWG/BIGDAWG",TRUE,https://github.com/igdawg/bigdawg,23816,0,2021-06-07T17:23:20Z,NA
bigdist,"Provides utilities to compute, store and access distance matrices on disk as file-backed matrices provided by the 'bigstatsr' package. File-backed distance matrices are stored as a symmetric matrix to facilitate out-of-memory operations on file-backed matrix while the in-memory 'dist' object stores only the lower diagonal elements. 'disto' provides an unified interface to work with in-memory and disk-based distance matrices.",2019-03-16,Komala Sheshachala Srikanth,https://github.com/talegari/bigdist,TRUE,https://github.com/talegari/bigdist,14413,4,2021-04-05T06:24:14Z,3603.25
BIGL,"Response surface methods for drug synergy analysis. Available
    methods include generalized and classical Loewe formulations as well as Highest
    Single Agent methodology. Response surfaces can be plotted in an interactive
    3-D plot and formal statistical tests for presence of synergistic effects are
    available. Implemented methods and tests are described in the article 
    ""BIGL: Biochemically Intuitive Generalized Loewe null model for prediction 
    of the expected combined effect compatible with partial agonism and antagonism""
    by Koen Van der Borght, Annelies Tourny, Rytis Bagdziunas, Olivier Thas, 
    Maxim Nazarov, Heather Turner, Bie Verbist & Hugo Ceulemans (2017) 
    <doi:10.1038/s41598-017-18068-5>.",2021-09-02,Heather Turner,https://github.com/openanalytics/BIGL,TRUE,https://github.com/openanalytics/bigl,30666,5,2021-09-01T11:52:27Z,6133.2
biglasso,"Extend lasso and elastic-net model fitting for ultrahigh-dimensional, 
    multi-gigabyte data sets that cannot be loaded into memory. It's much more 
    memory- and computation-efficient as compared to existing lasso-fitting packages 
    like 'glmnet' and 'ncvreg', thus allowing for very powerful big data analysis 
    even with an ordinary laptop.",2021-01-31,Yaohui Zeng,"https://github.com/YaohuiZeng/biglasso,
https://arxiv.org/abs/1701.05936",TRUE,https://github.com/yaohuizeng/biglasso,64426,90,2021-06-29T01:29:43Z,715.8444444444444
bigmds,"MDS is a statistic tool for reduction of dimensionality, using as input a distance matrix of dimensions n × n. 
  When n is large, classical algorithms suffer from computational problems and MDS configuration can not be obtained.
  With this package, we address these problems by means of three algorithms: 
  - Divide and Conquer MDS developed by Delicado and Pachon-Garcia, (2020) <arXiv:2007.11919>.
  - Fast MDS, which is an implementation of Tynia, Y., L. Jinze, M. Leonard, and W. Wei, (2006).
  - MDS based on Gower interpolation, which uses Gower interpolation formula as described in Gower, J.C. and D.J, Hand (1995, ISBN: 978-0-412-71630-0).
  The main idea of these methods is based on partitioning the dataset into small pieces, where classical methods can work. In order to align all the solutions, it is used Procrustes formula as described in Borg, I. and Groenen, P. (2005, ISBN : 978-0-387-25150-9).",2021-03-29,Cristian Pachón García,https://github.com/pachoning/bigmds,TRUE,https://github.com/pachoning/bigmds,3072,4,2021-08-09T16:18:50Z,768
bigmemory,"Create, store, access, and manipulate massive matrices.
    Matrices are allocated to shared memory and may use memory-mapped
    files.  Packages 'biganalytics', 'bigtabulate', 'synchronicity', and
    'bigalgebra' provide advanced functionality.",2019-12-23,Michael J. Kane,https://github.com/kaneplusplus/bigmemory,TRUE,https://github.com/kaneplusplus/bigmemory,464686,103,2021-06-16T14:46:35Z,4511.514563106796
bignum,"Classes for storing and manipulating arbitrary-precision
    integer vectors and high-precision floating-point vectors. These
    extend the range and precision of the 'integer' and 'double' data
    types found in R. This package utilizes the 'Boost.Multiprecision' C++
    library. It is specifically designed to work well with the 'tidyverse'
    collection of R packages.",2021-06-17,David Hall,"https://davidchall.github.io/bignum/,
https://github.com/davidchall/bignum",TRUE,https://github.com/davidchall/bignum,1785,8,2021-06-22T14:27:06Z,223.125
bigparallelr,"Utility functions for easy parallelism in R. Include some reexports
    from other packages, utility functions for splitting and parallelizing over
    blocks, and choosing and setting the number of cores used.",2021-02-02,Florian Privé,https://github.com/privefl/bigparallelr,TRUE,https://github.com/privefl/bigparallelr,57347,1,2021-02-02T13:18:01Z,57347
bigQueryR,"Interface with 'Google BigQuery',
    see <https://cloud.google.com/bigquery/> for more information.
    This package uses 'googleAuthR' so is compatible with similar packages, 
    including 'Google Cloud Storage' (<https://cloud.google.com/storage/>) for result extracts. ",2019-10-09,Mark Edmondson,http://code.markedmondson.me/bigQueryR/,TRUE,https://github.com/cloudyr/bigqueryr,54906,38,2021-04-26T12:29:35Z,1444.8947368421052
bigreadr,"Read large text files by splitting them in smaller files.
    Package 'bigreadr' also provides some convenient wrappers around fread()
    and fwrite() from package 'data.table'. ",2021-04-10,Florian Privé,https://github.com/privefl/bigreadr,TRUE,https://github.com/privefl/bigreadr,47255,34,2021-04-10T16:09:49Z,1389.8529411764705
bigrquery,Easily talk to Google's 'BigQuery' database from R.,2021-08-05,Hadley Wickham,"https://bigrquery.r-dbi.org, https://github.com/r-dbi/bigrquery",TRUE,https://github.com/r-dbi/bigrquery,520673,451,2021-08-05T12:01:16Z,1154.4855875831486
bigsnpr,"Easy-to-use, efficient, flexible and scalable tools for analyzing 
    massive SNP arrays. Privé et al. (2018) <doi:10.1093/bioinformatics/bty185>.",2021-06-03,Florian Privé,https://privefl.github.io/bigsnpr/,TRUE,https://github.com/privefl/bigsnpr,17067,95,2021-08-09T12:55:46Z,179.65263157894736
bigsparser,"Provides a sparse matrix format with data stored on disk, to be
    used in both R and C++. This is intended for more efficient use of sparse 
    data in C++ and also when parallelizing, since data on disk does not need
    copying. Only a limited number of features will be implemented. For now,
    conversion can be performed from a 'dgCMatrix' or a 'dsCMatrix' from R 
    package 'Matrix'. A new compact format is also now available.",2021-07-09,Florian Privé,https://github.com/privefl/bigsparser,TRUE,https://github.com/privefl/bigsparser,14918,5,2021-07-09T08:46:22Z,2983.6
bigstatsr,"Easy-to-use, efficient, flexible and scalable statistical tools.
  Package bigstatsr provides and uses Filebacked Big Matrices via memory-mapping.
  It provides for instance matrix operations, Principal Component Analysis,
  sparse linear supervised models, utility functions and more
  <doi:10.1093/bioinformatics/bty185>.",2021-04-05,Florian Privé,https://privefl.github.io/bigstatsr/,TRUE,https://github.com/privefl/bigstatsr,54780,131,2021-04-05T07:17:00Z,418.1679389312977
bigtime,"Estimation of large Vector AutoRegressive (VAR), Vector AutoRegressive with Exogenous Variables X (VARX) and Vector AutoRegressive Moving Average (VARMA) Models with Structured Lasso Penalties, see Nicholson, Wilms, Bien and Matteson (2020) <https://jmlr.org/papers/v21/19-777.html> and Wilms, Basu, Bien and Matteson (2021) <doi:10.1080/01621459.2021.1942013>.",2021-08-09,Ines Wilms,https://github.com/ineswilms/bigtime,TRUE,https://github.com/ineswilms/bigtime,22430,20,2021-08-23T14:23:14Z,1121.5
bigutilsr,"Utility functions for large-scale data. For now, package 'bigutilsr'
    mainly includes functions for outlier detection and unbiased PCA projection.",2021-04-13,Florian Privé,https://github.com/privefl/bigutilsr,TRUE,https://github.com/privefl/bigutilsr,47190,8,2021-04-08T09:46:09Z,5898.75
BigVAR,Estimates VAR and VARX models with structured Lasso Penalties.,2019-12-02,Will Nicholson,http://www.github.com/wbnicholson/BigVAR,TRUE,https://github.com/wbnicholson/bigvar,28094,38,2021-08-20T22:18:57Z,739.3157894736842
bikeshare14,"Anonymised Bay Area bike share trip data for the year 2014. 
    Also contains additional metadata on stations and weather.",2021-04-07,Arunkumar Srinivasan,https://github.com/arunsrinivasan/bikeshare14,TRUE,https://github.com/arunsrinivasan/bikeshare14,19055,1,2021-04-07T16:35:33Z,19055
billboarder,"Provides an 'htmlwidgets' interface to 'billboard.js', 
    a re-usable easy interface JavaScript chart library, based on D3 v4+.
    Chart types include line charts, scatterplots, bar/lollipop charts, histogram/density plots, pie/donut charts and gauge charts.
    All charts are interactive, and a proxy method is implemented to smoothly update a chart without rendering it again in 'shiny' apps. ",2021-03-27,Victor Perrier,https://github.com/dreamRs/billboarder,TRUE,https://github.com/dreamrs/billboarder,95641,163,2021-07-05T14:00:05Z,586.7546012269938
binb,"A collection of 'LaTeX' styles using 'Beamer' customization for
 pdf-based presentation slides in 'RMarkdown'. At present it contains
 'RMarkdown' adaptations of the LaTeX themes 'Metropolis' (formerly 'mtheme')
 theme by Matthias Vogelgesang and others (now included in 'TeXLive'), the
 'IQSS' by Ista Zahn (which is included here), and the 'Monash' theme by
 Rob J Hyndman. Additional (free) fonts may be needed: 'Metropolis' prefers
 'Fira', and 'IQSS' requires 'Libertinus'.",2020-06-10,Dirk Eddelbuettel,https://github.com/eddelbuettel/binb,TRUE,https://github.com/eddelbuettel/binb,23954,170,2021-03-21T22:38:30Z,140.90588235294118
binman,"Tools and functions for managing the download of binary files.
    Binary repositories are defined in 'YAML' format. Defining new 
    pre-download, download and post-download templates allow additional 
    repositories to be added.",2020-10-02,John Harrison,"https://docs.ropensci.org/binman/,
https://github.com/ropensci/binman",TRUE,https://github.com/ropensci/binman,151265,13,2020-10-02T21:46:52Z,11635.76923076923
binsegRcpp,"Standard template library 
 containers are used to implement an efficient binary segmentation
 algorithm, which is log-linear on average and quadratic in the
 worst case.",2020-09-14,Toby Dylan Hocking,https://github.com/tdhock/binseg,TRUE,https://github.com/tdhock/binseg,4663,2,2020-09-15T17:21:17Z,2331.5
bioacoustics,"Contains all the necessary tools to process audio recordings of
             various formats (e.g., WAV, WAC, MP3, ZC), filter noisy files, 
             display audio signals, detect and extract automatically acoustic
             features for further analysis such as classification.",2021-01-05,Jean Marchal,https://github.com/wavx/bioacoustics/,TRUE,https://github.com/wavx/bioacoustics,31341,31,2021-06-15T00:17:32Z,1011
bioC.logs,Download stats reported from the BioConductor.org stats website.,2021-05-05,Marcelo Ponce,https://github.com/mponce0/bioC.logs,TRUE,https://github.com/mponce0/bioc.logs,10037,0,2021-05-05T04:24:56Z,NA
BiocManager,A convenient tool to install and update Bioconductor packages.,2021-06-15,Martin Morgan,NA,TRUE,https://github.com/bioconductor/biocmanager,3105670,47,2021-06-15T20:49:08Z,66078.08510638298
biocompute,"Tools to create, validate, and export BioCompute Objects
    described in King et al. (2019) <doi:10.17605/osf.io/h59uh>.
    Users can encode information in data frames, and compose
    BioCompute Objects from the domains defined by the standard.
    A checksum validator and a JSON schema validator are provided.
    This package also supports exporting BioCompute Objects as JSON,
    PDF, HTML, or 'Word' documents, and exporting to cloud-based platforms.",2020-10-16,Soner Koc,"https://sbg.github.io/biocompute/,
https://github.com/sbg/biocompute",TRUE,https://github.com/sbg/biocompute,11420,3,2020-10-15T04:51:44Z,3806.6666666666665
biogram,"Tools for extraction and analysis of various
    n-grams (k-mers) derived from biological sequences (proteins
    or nucleic acids). Contains QuiPT (quick permutation test) for fast
    feature-filtering of the n-gram data.",2020-03-31,Michal Burdukiewicz,https://github.com/michbur/biogram,TRUE,https://github.com/michbur/biogram,25057,6,2021-06-04T18:50:10Z,4176.166666666667
bioimagetools,"Tools for 3D imaging, mostly for biology/microscopy. 
    Read and write TIFF stacks. Functions for segmentation, filtering and analyzing 3D point patterns.",2020-05-29,Volker Schmid,https://bioimaginggroup.github.io/bioimagetools,TRUE,https://github.com/bioimaginggroup/bioimagetools,23329,3,2021-06-21T09:34:27Z,7776.333333333333
biokNN,"The bi-objective k-nearest neighbors method (biokNN) is an imputation method designed to estimate missing values on data with a multilevel structure. The original algorithm is an extension of the k-nearest neighbors method proposed by Bertsimas et al. (2017) (<https://jmlr.org/papers/v18/17-073.html>) using a bi-objective approach.  A brief description of the method can be found in Cubillos (2021) (<https://pure.au.dk/portal/files/214627979/biokNN.pdf>). The 'biokNN' package provides an R implementation of the method for datasets with continuous variables (e.g. employee productivity, student grades) and a categorical class variable (e.g. department, school). Given an incomplete dataset with such structure, this package produces complete datasets using both single and multiple imputation, including visualization tools to better understand the pattern of the missing values. ",2021-04-22,Maximiliano Cubillos,https://github.com/mcubillos3/biokNN,TRUE,https://github.com/mcubillos3/bioknn,1636,0,2021-03-18T17:17:14Z,NA
biomartr,"Perform large scale genomic data retrieval and functional annotation retrieval. This package aims to provide users with a standardized
                way to automate genome, proteome, 'RNA', coding sequence ('CDS'), 'GFF', and metagenome
                retrieval from 'NCBI RefSeq', 'NCBI Genbank', 'ENSEMBL', 'ENSEMBLGENOMES',
                and 'UniProt' databases. Furthermore, an interface to the 'BioMart' database
                (Smedley et al. (2009) <doi:10.1186/1471-2164-10-22>) allows users to retrieve
                functional annotation for genomic loci. In addition, users can download entire databases such
                as 'NCBI RefSeq' (Pruitt et al. (2007) <doi:10.1093/nar/gkl842>), 'NCBI nr',
                'NCBI nt', 'NCBI Genbank' (Benson et al. (2013) <doi:10.1093/nar/gks1195>), etc. as
                well as 'ENSEMBL' and 'ENSEMBLGENOMES' with only one command.",2020-01-10,Hajk-Georg Drost,"https://docs.ropensci.org/biomartr,
https://github.com/ropensci/biomartr",TRUE,https://github.com/ropensci/biomartr,50620,155,2021-03-18T20:11:24Z,326.5806451612903
BIOMASS,"Contains functions to estimate aboveground biomass/carbon and its uncertainty in tropical forests. 
	These functions allow to (1) retrieve and to correct taxonomy, (2) estimate wood density and its uncertainty, 
	(3) construct height-diameter models, (4) manage tree and plot coordinates, 
	(5) estimate the aboveground biomass/carbon at the stand level with associated uncertainty. 
	To cite 'BIOMASS', please use citation(""BIOMASS""). 
	See more in the article of Réjou-Méchain et al. (2017) <doi:10.1111/2041-210X.12753>.",2021-06-25,Maxime Réjou-Méchain,https://github.com/umr-amap/BIOMASS,TRUE,https://github.com/umr-amap/biomass,28327,9,2021-06-22T11:14:03Z,3147.4444444444443
bioRad,"Extract, visualize and summarize aerial movements of birds and
    insects from weather radar data. See <doi:10.1111/ecog.04028>
    for a software paper describing package and methodologies.",2020-05-11,Adriaan M. Dokter,"https://github.com/adokter/bioRad,
https://adokter.github.io/bioRad",TRUE,https://github.com/adokter/biorad,15668,16,2021-07-16T18:47:53Z,979.25
bioseq,"Classes and functions to work with biological sequences (DNA, RNA and amino acid sequences).
    Implements S3 infrastructure to work with biological sequences as described in Keck (2020) <doi:10.1111/2041-210X.13490>.
    Provides a collection of functions to perform biological conversion among classes
    (transcription, translation) and basic operations on sequences
    (detection, selection and replacement based on positions or patterns).
    The package also provides functions to import and export sequences from and to other package formats.",2021-09-03,Francois Keck,https://fkeck.github.io/bioseq/,TRUE,https://github.com/fkeck/bioseq,6371,12,2021-08-31T15:55:24Z,530.9166666666666
BioStatR,"Datasets and functions for the book ""Initiation à la Statistique avec R"", F. Bertrand and M. Maumy-Bertrand (2018, ISBN:978-2100782826 Dunod, 3eme edition).",2021-03-14,Frederic Bertrand,"http://www-irma.u-strasbg.fr/~fbertran/,
https://github.com/fbertran/BioStatR/",TRUE,https://github.com/fbertran/biostatr,34727,3,2021-04-12T16:57:37Z,11575.666666666666
biotools,"Tools designed to perform and evaluate cluster analysis (including Tocher's algorithm), 
	discriminant analysis and path analysis (standard and under collinearity), as well as some 
	useful miscellaneous tools for dealing with sample size and optimum plot size calculations. 
	A test for seed sample heterogeneity is now available. Mantel's permutation test can be found in this package. 
	A new approach for calculating its power is implemented. biotools also contains tests for genetic covariance components.
	Heuristic approaches for performing non-parametric spatial predictions of generic response variables and 
	spatial gene diversity are implemented.",2021-08-07,Anderson Rodrigo da Silva,https://arsilva87.github.io/biotools/,TRUE,https://github.com/arsilva87/biotools,74170,0,2021-08-08T01:41:25Z,NA
bipartite,"Functions to visualise webs and calculate a series of indices commonly used to describe pattern in (ecological) webs. It focuses on webs consisting of only two levels (bipartite), e.g. pollination webs or predator-prey-webs. Visualisation is important to get an idea of what we are actually looking at, while the indices summarise different aspects of the web's topology. ",2021-02-04,Carsten F. Dormann,https://github.com/biometry/bipartite,TRUE,https://github.com/biometry/bipartite,98352,25,2021-08-24T09:26:26Z,3934.08
BiplotML,"Logistic Biplot is a method that allows representing multivariate binary data on a subspace of low dimension, where each individual is represented by a point and each variable as vectors directed through the origin. The orthogonal projection of individuals onto these vectors predicts the expected probability that the characteristic occurs. The package contains new techniques to estimate the model parameters and constructs in each case the 'Logistic-Biplot'. References can be found in the help of each procedure.",2021-06-23,Jose Giovany Babativa-Marquez,https://github.com/jgbabativam/BiplotML,TRUE,https://github.com/jgbabativam/biplotml,630,0,2021-06-19T16:56:25Z,NA
biscale,"Provides a 'ggplot2' centric approach to bivariate mapping. This is a 
    technique that maps two quantities simultaneously rather than the single value 
    that most thematic maps display. The package provides a suite of tools 
    for calculating breaks using multiple different approaches, a selection of 
    palettes appropriate for bivariate mapping and a scale function for 'ggplot2' 
    calls that adds those palettes to maps. A tool for creating bivariate legends 
    is also included.",2020-05-06,Christopher Prener,https://github.com/slu-openGIS/biscale,TRUE,https://github.com/slu-opengis/biscale,15515,70,2020-12-18T01:34:19Z,221.64285714285714
BisqueRNA,"Provides tools to accurately estimate cell type abundances 
    from heterogeneous bulk expression. A reference-based method utilizes
    single-cell information to generate a signature matrix and transformation
    of bulk expression for accurate regression based estimates. A marker-based
    method utilizes known cell-specific marker genes to measure relative
    abundances across samples.
    For more details, see Jew and Alvarez et al (2019) <doi:10.1101/669911>.",2021-05-23,Brandon Jew,https://www.biorxiv.org/content/10.1101/669911v1,TRUE,https://github.com/cozygene/bisque,16839,40,2021-05-23T20:15:38Z,420.975
bitmexr,"A client for cryptocurrency exchange BitMEX
    <https://www.bitmex.com/> including the ability to obtain historic
    trade data and place, edit and cancel orders. BitMEX's Testnet and
    live API are both supported.",2020-06-27,Harry Fisher,"https://github.com/hfshr/bitmexr, https://hfshr.github.io/bitmexr",TRUE,https://github.com/hfshr/bitmexr,6683,2,2020-12-22T21:46:47Z,3341.5
bitops,Functions for bitwise operations on integer vectors.,2021-04-24,"S original by Steve Dutky <sdutky@terpalum.umd.edu> initial R
        port and extensions by Martin Maechler; revised and modified
        by Steve Dutky",https://github.com/mmaechler/R-bitops,TRUE,https://github.com/mmaechler/r-bitops,8831729,3,2021-06-18T07:15:20Z,2943909.6666666665
BivRec,"A collection of models for bivariate alternating recurrent event data analysis. 
             Includes non-parametric and semi-parametric methods.",2021-06-05,Sandra Castro-Pearson,https://github.com/SandraCastroPearson/BivRec,TRUE,https://github.com/sandracastropearson/bivrec,15498,1,2021-06-04T23:17:15Z,15498
biwavelet,"This is a port of the WTC MATLAB package written by Aslak Grinsted
    and the wavelet program written by Christopher Torrence and Gibert P.
    Compo. This package can be used to perform univariate and bivariate
    (cross-wavelet, wavelet coherence, wavelet clustering) analyses.",2021-05-26,Tarik C. Gouhier,https://github.com/tgouhier/biwavelet,TRUE,https://github.com/tgouhier/biwavelet,37485,32,2021-05-24T20:23:11Z,1171.40625
bizdays,"Business days calculations based on a list of holidays and
    nonworking weekdays. Quite useful for fixed income and derivatives pricing.",2021-04-19,Wilson Freitas,https://github.com/wilsonfreitas/R-bizdays,TRUE,https://github.com/wilsonfreitas/r-bizdays,333439,33,2021-05-23T22:43:10Z,10104.212121212122
bkmr,"Implementation of a statistical approach 
  for estimating the joint health effects of multiple 
  concurrent exposures.",2017-03-24,Jennifer F. Bobb,https://github.com/jenfb/bkmr,TRUE,https://github.com/jenfb/bkmr,23571,16,2020-09-29T19:37:50Z,1473.1875
BlanketStatsments,Build and compare nested statistical models with sets of equal and different independent variables. An analysis using this package is Marquardt et al. (2021) <https://github.com/p-mq/Percentile_based_averaging>.,2021-08-02,J. Peter Marquardt,https://github.com/p-mq/BlanketStatsments,TRUE,https://github.com/p-mq/blanketstatsments,422,1,2021-07-29T15:59:09Z,422
blaster,"Implementation of an efficient BLAST-like sequence 
             comparison algorithm, written in C++11 and using
             native R datatypes. Blaster is based on 'nsearch' -
             Schmid et al 2018; <doi:10.1101/399782>.",2021-03-07,Manu Tamminen,https://github.com/manutamminen/blaster,TRUE,https://github.com/manutamminen/blaster,2619,7,2021-03-29T11:15:03Z,374.14285714285717
blastula,"Compose and send out responsive HTML email messages that render
    perfectly across a range of email clients and device sizes. Helper functions
    let the user insert embedded images, web link buttons, and 'ggplot2' plot
    objects into the message body. Messages can be sent through an 'SMTP'
    server, through the 'RStudio Connect' service, or through the 'Mailgun' API
    service <http://mailgun.com/>.",2020-05-19,Richard Iannone,https://github.com/rich-iannone/blastula,TRUE,https://github.com/rich-iannone/blastula,75659,427,2021-04-23T19:04:44Z,177.18735362997657
BLCOP,"An implementation of the Black-Litterman Model and Attilio
    Meucci's copula opinion pooling framework as described in
    Meucci, Attilio (2005) <doi:10.2139/ssrn.848407>,
    Meucci, Attilio (2006) <doi:10.2139/ssrn.872577> and
    Meucci, Attilio (2008) <doi:10.2139/ssrn.1117574>.",2021-01-25,Ava Yang [ctb,https://github.com/mangothecat/BLCOP,TRUE,https://github.com/mangothecat/blcop,25032,5,2021-01-25T15:00:33Z,5006.4
blindrecalc,"Computation of key characteristics and plots for blinded sample size recalculation.
   Continuous as well as binary endpoints are supported in superiority and non-inferiority trials.
   The implemented methods include the approaches by
   Lu, K. (2019) <doi:10.1002/pst.1737>,
   Kieser, M. and Friede, T. (2000) <doi:10.1002/(SICI)1097-0258(20000415)19:7%3C901::AID-SIM405%3E3.0.CO;2-L>,
   Friede, T. and Kieser, M. (2004) <doi:10.1002/pst.140>, 
   Friede, T., Mitchell, C., Mueller-Veltern, G. (2007) <doi:10.1002/bimj.200610373>, and
   Friede, T. and Kieser, M. (2011) <doi:10.3414/ME09-01-0063>.",2021-07-06,Lukas Baumann,https://github.com/imbi-heidelberg/blindrecalc,TRUE,https://github.com/imbi-heidelberg/blindrecalc,5803,3,2021-07-06T11:38:14Z,1934.3333333333333
blme,"Maximum a posteriori estimation for linear and generalized linear mixed-effects models in a Bayesian setting, implementing the methods of Chung, et al. (2013) <doi:10.1007/s11336-013-9328-2>. Extends package 'lme4' (Bates, Maechler, Bolker, and Walker (2015) <doi:10.18637/jss.v067.i01>).",2021-01-05,Vincent Dorie,https://github.com/vdorie/blme,TRUE,https://github.com/vdorie/blme,153139,26,2021-03-07T15:05:55Z,5889.961538461538
blob,"R's raw vector is useful for storing a single
    binary object.  What if you want to put a vector of them in a data
    frame? The 'blob' package provides the blob object, a list of raw
    vectors, suitable for use as a column in data frame.",2021-07-23,Kirill Müller,"https://blob.tidyverse.org, https://github.com/tidyverse/blob",TRUE,https://github.com/tidyverse/blob,7081734,36,2021-07-29T04:14:08Z,196714.83333333334
blockCV,"Creating spatially or environmentally separated folds for cross-validation to provide a robust error estimation in spatially structured environments; Investigating and visualising the effective range of spatial autocorrelation in continuous raster covariates to find an initial realistic distance band to separate training and testing datasets spatially described in Valavi, R. et al. (2019) <doi:10.1111/2041-210X.13107>.",2021-06-17,Roozbeh Valavi,https://github.com/rvalavi/blockCV,TRUE,https://github.com/rvalavi/blockcv,20946,70,2021-06-18T05:20:04Z,299.22857142857146
blocklength,"A set of functions to select the optimal block-length for a 
    dependent bootstrap (block-bootstrap). Includes the Hall, Horowitz, and Jing
    (1995) <doi:10.1093/biomet/82.3.561> cross-validation method and the 
    Politis and White (2004) <doi:10.1081/ETC-120028836> Spectral Density 
    Plug-in method, including the Patton, Politis, and White (2009)
    <doi:10.1080/07474930802459016> correction with a corresponding set of S3
    plot methods.",2021-05-12,Alec Stashevsky,"https://alecstashevsky.com/r/blocklength,
https://github.com/Alec-Stashevsky/blocklength",TRUE,https://github.com/alec-stashevsky/blocklength,3074,1,2021-07-28T01:00:17Z,3074
blogdown,"Write blog posts and web pages in R Markdown. This package supports
    the static site generator 'Hugo' (<https://gohugo.io>) best, and it also
    supports 'Jekyll' (<https://jekyllrb.com>) and 'Hexo' (<https://hexo.io>).",2021-09-02,Yihui Xie,"https://github.com/rstudio/blogdown,
https://pkgs.rstudio.com/blogdown",TRUE,https://github.com/rstudio/blogdown,247384,1394,2021-09-03T08:44:01Z,177.46341463414635
blorr,"Tools designed to make it easier for beginner and intermediate users to build and validate 
    binary logistic regression models. Includes bivariate analysis, comprehensive regression output, 
    model fit statistics, variable selection procedures, model validation techniques and a 'shiny' 
    app for interactive model building.",2020-05-28,Aravind Hebbali,"URL: https://blorr.rsquaredacademy.com/,
https://github.com/rsquaredacademy/blorr",TRUE,https://github.com/rsquaredacademy/blorr,30286,15,2021-07-08T11:25:36Z,2019.0666666666666
bltm,"Fits latent threshold model for simulated data
    and describes how to adjust model using real data. Implements algorithm
    proposed by Nakajima and West (2013) <doi:10.1080/07350015.2012.747847>. 
    This package has a function to generate data, a function to configure 
    priors and a function to fit the model. Examples may be checked inside 
    the demonstration files.",2019-07-18,Julio Trecenti,https://github.com/curso-r/bltm,TRUE,https://github.com/curso-r/bltm,11388,1,2020-09-28T14:26:58Z,11388
BMA,"Package for Bayesian model averaging and variable selection for linear models,
        generalized linear models and survival models (cox
        regression).",2021-05-21,Adrian Raftery,"http://stats.research.att.com/volinsky/bma.html,
https://github.com/hanase/BMA",TRUE,https://github.com/hanase/bma,197480,10,2021-05-21T21:07:53Z,19748
bmabasket,"An implementation of the Bayesian model averaging method 
    of Psioda and others (2019) <doi:10.1093/biostatistics/kxz014> for basket trials. 
    Contains a user-friendly wrapper for simulating basket trials under conditions 
    and analyzing them with a Bayesian model averaging approach.",2021-01-21,Matt Psioda,https://github.com/ethan-alt/bmabasket,TRUE,https://github.com/ethan-alt/bmabasket,3051,0,2021-01-15T15:49:44Z,NA
bmass,"Multivariate tool for analyzing genome-wide association
    study results in the form of univariate summary statistics. The 
    goal of 'bmass' is to comprehensively test all possible multivariate
    models given the phenotypes and datasets provided. Multivariate
    models are determined by assigning each phenotype to being either
    Unassociated (U), Directly associated (D) or Indirectly associated
    (I) with the genetic variant of interest. Test results for each model 
    are presented in the form of Bayes factors, thereby allowing direct
    comparisons between models. The underlying framework implemented
    here is based on the modeling developed in ""A Unified Framework 
    for Association Analysis with Multiple Related Phenotypes"",
    M. Stephens (2013) <doi:10.1371/journal.pone.0065245>.",2019-05-17,Michael Turchin,https://github.com/mturchin20/bmass,TRUE,https://github.com/mturchin20/bmass,11709,9,2020-09-17T18:53:31Z,1301
bmgarch,"Fit Bayesian multivariate GARCH models using 'Stan' for full Bayesian inference. Generate (weighted) forecasts for means, variances (volatility) and correlations. Currently DCC(P,Q), CCC(P,Q), pdBEKK(P,Q), and BEKK(P,Q) parameterizations are implemented, based either on a multivariate gaussian normal or student-t distribution. DCC and CCC models are based on Engle (2002) <doi:10.1198/073500102288618487> and Bollerslev (1990) <doi:10.2307/2109358>. The BEKK parameterization follows Engle and Kroner (1995) <doi:10.1017/S0266466600009063> while the pdBEKK as well as the estimation approach for this package is described in Rast et al. (2020) <doi:10.31234/osf.io/j57pk>. The fitted models contain 'rstan' objects and can be examined with 'rstan' functions.  ",2021-06-14,Philippe Rast,NA,TRUE,https://github.com/ph-rast/bmgarch,4807,8,2021-08-31T20:31:14Z,600.875
bmggum,"Full Bayesian estimation of Multidimensional Generalized Graded Unfolding Model (MGGUM) using 'rstan' (See Stan Development Team (2020) <https://mc-stan.org/>).
    Functions are provided for estimation, result extraction, model fit statistics, and plottings.",2021-04-09,Naidan Tu,https://github.com/Naidantu/bmggum,TRUE,https://github.com/naidantu/bmggum,1774,2,2021-05-01T23:03:40Z,887
BNPMIXcluster,"Model-based approach for clustering of multivariate data, capable of combining different types of variables (continuous, ordinal and nominal) and accommodating for different sampling probabilities in a complex survey design. The model is based on a location mixture model with a Poisson-Dirichlet process prior on the location parameters of the associated latent variables. Details of the underlying model is described in Carmona, C., Nieto-Barajas, L. E., Canale, A. (2016) <arXiv:1612.00083>.",2020-11-30,Christian Carmona,https://github.com/christianu7/BNPMIXcluster,TRUE,https://github.com/christianu7/bnpmixcluster,19487,1,2021-02-22T16:41:43Z,19487
bnpsd,"The Pritchard-Stephens-Donnelly (PSD) admixture model has k intermediate subpopulations from which n individuals draw their alleles dictated by their individual-specific admixture proportions.  The BN-PSD model additionally imposes the Balding-Nichols (BN) allele frequency model to the intermediate populations, which therefore evolved independently from a common ancestral population T with subpopulation-specific FST (Wright's fixation index) parameters.  The BN-PSD model can be used to yield complex population structures.  This simulation approach is now extended to subpopulations related by a tree.  Method described in Ochoa and Storey (2021) <doi:10.1371/journal.pgen.1009241>.",2021-08-25,Alejandro Ochoa,https://github.com/StoreyLab/bnpsd/,TRUE,https://github.com/storeylab/bnpsd,17952,9,2021-08-09T19:43:36Z,1994.6666666666667
bodenmiller,"This data package contains a subset of the Bodenmiller et al, Nat Biotech 2012 dataset for testing single cell, high dimensional analysis and visualization methods.",2021-04-11,Yann Abraham,https://github.com/yannabraham/bodenmiller,TRUE,https://github.com/yannabraham/bodenmiller,18202,2,2021-04-11T14:17:48Z,9101
boilerpipeR,"Generic Extraction of main text content from HTML files; removal
    of ads, sidebars and headers using the boilerpipe 
    <https://github.com/kohlschutter/boilerpipe> Java library. The
    extraction heuristics from boilerpipe show a robust performance for a wide
    range of web site templates.",2021-05-19,See AUTHORS file.,https://github.com/mannau/boilerpipeR,TRUE,https://github.com/mannau/boilerpiper,35236,23,2021-05-19T09:05:01Z,1532
bookdown,Output formats and utilities for authoring books and technical documents with R Markdown.,2021-09-02,Yihui Xie,"https://github.com/rstudio/bookdown,
https://pkgs.rstudio.com/bookdown/",TRUE,https://github.com/rstudio/bookdown,997558,2607,2021-09-03T05:41:51Z,382.6459532029152
bookdownplus,"A collection and selector of R 'bookdown' templates. 'bookdownplus' helps you write academic journal articles, guitar books, chemical equations, mails, calendars, and diaries. R 'bookdownplus' extends the features of 'bookdown', and simplifies the procedure. Users only have to choose a template, clarify the book title and author name, and then focus on writing the text. No need to struggle in 'YAML' and 'LaTeX'.",2020-02-26,Peng Zhao,https://github.com/pzhaonet/bookdownplus,TRUE,https://github.com/pzhaonet/bookdownplus,40980,216,2021-08-06T02:39:27Z,189.72222222222223
boomer,"Provides debugging tools that let you inspect the
    intermediate results of a call. The output looks as if we explode a call
    into its parts hence the package name.",2021-07-20,Antoine Fabri,https://github.com/moodymudskipper/boomer,TRUE,https://github.com/moodymudskipper/boomer,752,105,2021-07-20T12:02:13Z,7.161904761904762
boot.heterogeneity,"Implements a bootstrap-based heterogeneity test for standardized mean differences (d), Fisher-transformed Pearson's correlations (r), and natural-logarithm-transformed odds ratio (or) in meta-analysis studies. Depending on the presence of moderators, this Monte Carlo based test can be implemented in the random- or mixed-effects model. This package uses rma() function from the R package 'metafor' to obtain parameter estimates and likelihoods, so installation of R package 'metafor' is required. This approach refers to the studies of Anscombe (1956) <doi:10.2307/2332926>, Haldane (1940) <doi:10.2307/2332614>, Hedges (1981) <doi:10.3102/10769986006002107>, Hedges & Olkin (1985, ISBN:978-0123363800), Silagy, Lancaster, Stead, Mant, & Fowler (2004) <doi:10.1002/14651858.CD000146.pub2>, Viechtbauer (2010) <doi:10.18637/jss.v036.i03>, and Zuckerman (1994, ISBN:978-0521432009). ",2021-01-07,Ge Jiang,https://github.com/gabriellajg/boot.heterogeneity/,TRUE,https://github.com/gabriellajg/boot.heterogeneity,7327,0,2021-01-06T01:13:45Z,NA
bootES,Calculate robust measures of effect sizes using the bootstrap.,2021-07-04,Daniel Gerlanc,https://github.com/dgerlanc/bootES,TRUE,https://github.com/dgerlanc/bootes,61214,6,2021-07-04T22:43:47Z,10202.333333333334
bootf2,"Compare dissolution profiles with confidence interval of similarity
  factor f2 using bootstrap methodology as described in the literature, such as 
  Efron and Tibshirani (1993, ISBN:9780412042317), Davison and Hinkley (1997,
  ISBN:9780521573917), and Shah et al. (1998) <doi:10.1023/A:1011976615750>. 
  The package can also be used to simulate dissolution profiles based on 
  mathematical modelling and multivariate normal distribution.",2021-08-25,Zhengguo Xu,https://github.com/zhengguoxu/bootf2,TRUE,https://github.com/zhengguoxu/bootf2,234,0,2021-08-18T11:11:20Z,NA
bootGOF,"Bootstrap based goodness-of-fit tests. It allows
    to perform rigorous statistical tests to check if a chosen
    model family is correct based on the marked empirical
    process. The implemented algorithms are described in
    (Dikta and Scheer (2021) <doi:10.1007/978-3-030-73480-0>)
    and can be applied
    to generalized linear models without any further implementation
    effort. As far as certain linearity conditions are fulfilled
    the resampling scheme are also applicable beyond generalized
    linear models. This is reflected in the software architecture
    which allows to reuse the resampling scheme by implementing
    only certain interfaces for models that are not supported
    natively by the package.",2021-06-24,Marsel Scheer,https://github.com/MarselScheer/bootGOF,TRUE,https://github.com/marselscheer/bootgof,971,1,2021-09-02T18:21:24Z,971
bootPLS,"Several implementations of non-parametric stable bootstrap-based techniques to determine the numbers of components for Partial Least Squares linear or generalized linear regression models as well as and sparse Partial Least Squares linear or generalized linear regression models. The package collects techniques that were published in a book chapter (Magnanensi et al. 2016, 'The Multiple Facets of Partial Least Squares and Related Methods', <doi:10.1007/978-3-319-40643-5_18>) and two articles (Magnanensi et al. 2017, 'Statistics and Computing', <doi:10.1007/s11222-016-9651-4>) and (Magnanensi et al. 2021, 'Frontiers in Applied Mathematics and Statistics', accepted.).",2021-07-15,Frederic Bertrand,"https://fbertran.github.io/bootPLS/,
https://github.com/fbertran/bootPLS/",TRUE,https://github.com/fbertran/bootpls,747,0,2021-07-15T09:05:42Z,NA
bootstrapFP,"Finite Population bootstrap algorithms to estimate the variance
    of the Horvitz-Thompson estimator for single-stage sampling. 
    For a survey of bootstrap methods for finite populations, see Mashreghi et Al. (2016) <doi:10.1214/16-SS113>.",2020-10-13,Roberto Sichera,NA,TRUE,https://github.com/rhobis/bootstrapfp,14725,0,2021-05-23T18:05:00Z,NA
BootstrapQTL,"Identifies genome-related molecular traits with significant 
  evidence of genetic regulation and performs a bootstrap procedure to 
  correct estimated effect sizes for over-estimation present in cis-QTL
  mapping studies (The ""Winner's Curse""), described in Huang QQ *et al.* 
  2018 <doi: 10.1093/nar/gky780>. ",2021-05-12,Scott Ritchie,NA,TRUE,https://github.com/sritchie73/bootstrapqtl,15418,6,2021-05-11T15:55:03Z,2569.6666666666665
bootUR,"Set of functions to perform various bootstrap unit root tests for both individual time series
  (including augmented Dickey-Fuller test and union tests), multiple time series and panel data; see
  Palm, Smeekes and Urbain (2008) <doi:10.1111/j.1467-9892.2007.00565.x>,
  Palm, Smeekes and Urbain (2011) <doi:10.1016/j.jeconom.2010.11.010>, 
  Moon and Perron (2012) <doi:10.1016/j.jeconom.2012.01.008>, 
  Smeekes and Taylor (2012) <doi:10.1017/S0266466611000387> and 
  Smeekes (2015) <doi:10.1111/jtsa.12110> for key references. ",2020-07-24,Stephan Smeekes,https://github.com/smeekes/bootUR,TRUE,https://github.com/smeekes/bootur,7333,2,2021-03-18T17:06:02Z,3666.5
BOSSreg,"Best Orthogonalized Subset Selection (BOSS) is a least-squares (LS) based subset selection method, that performs best subset selection upon an orthogonalized basis of ordered predictors, with the computational effort of a single ordinary LS fit. This package provides a highly optimized implementation of BOSS and estimates a heuristic degrees of freedom for BOSS, which can be plugged into an information criterion (IC) such as AICc in order to select the subset from candidates. It provides various choices of IC, including AIC, BIC, AICc, Cp and GCV. It also implements the forward stepwise selection (FS) with no additional computational cost, where the subset of FS is selected via cross-validation (CV). CV is also an option for BOSS. For details see: Tian, Hurvich and Simonoff (2021), ""On the Use of Information Criteria for Subset Selection in Least Squares Regression"", <arXiv:1911.10191>.",2021-03-06,Sen Tian,https://github.com/sentian/BOSSreg,TRUE,https://github.com/sentian/bossreg,10220,1,2021-03-06T23:17:19Z,10220
box,"A modern module system for R. Organise code into hierarchical,
    composable, reusable modules, and use it effortlessly across projects via a
    flexible, declarative dependency loading syntax.",2021-04-22,Konrad Rudolph,"https://klmr.me/box/, https://github.com/klmr/box",TRUE,https://github.com/klmr/box,4063,485,2021-08-24T08:58:07Z,8.377319587628866
boxr,"An R interface for the remote file hosting service 'Box'
    (<https://www.box.com/>). In addition to uploading and downloading files,
    this package includes functions which mirror base R operations for local
    files, (e.g. box_load(), box_save(), box_read(), box_setwd(), etc.), as well
    as 'git' style functions for entire directories (e.g. box_fetch(),
    box_push()).",2021-01-19,Ian Lyttle,https://github.com/r-box/boxr/,TRUE,https://github.com/r-box/boxr,42088,53,2021-05-21T12:34:49Z,794.11320754716985
bp,A comprehensive package to aid in the analysis of blood pressure data of all forms by providing both descriptive and visualization tools for researchers.,2021-03-05,John Schwenck,https://github.com/johnschwenck/bp,TRUE,https://github.com/johnschwenck/bp,2943,6,2021-08-27T21:43:46Z,490.5
bpbounds,"Implementation of the nonparametric bounds for the average causal 
    effect under an instrumental variable model by Balke and Pearl (Bounds on 
    Treatment Effects from Studies with Imperfect Compliance, JASA, 1997, 92, 
    439, 1171-1176). The package can calculate bounds for a binary outcome, a 
    binary treatment/phenotype, and an instrument with either 2 or 3 
    categories. The package implements bounds for situations where these 3 
    variables are measured in the same dataset (trivariate data) or where the 
    outcome and instrument are measured in one study and the 
    treatment/phenotype and instrument are measured in another study 
    (bivariate data).",2020-01-21,Tom Palmer,https://github.com/remlapmot/bpbounds,TRUE,https://github.com/remlapmot/bpbounds,15578,0,2021-08-13T17:11:24Z,NA
bpcs,"Models for the analysis of paired comparison data using Stan. The models include Bayesian versions of the Bradley-Terry model, including random effects (1 level), generalized model for predictors, order effect (home advantage) and the  variations for the Davidson (1970) model to handle ties. Additionally, we provide a number of functions to facilitate inference and obtaining results with these models. References: Bradley and Terry (1952) <doi:10.2307/2334029>; Davidson (1970) <doi:10.1080/01621459.1970.10481082>; Carpenter et al. (2017) <doi:10.18637/jss.v076.i01>. ",2020-12-09,David Issa Mattos,"https://github.com/davidissamattos/bpcs,
https://davidissamattos.github.io/bpcs/",TRUE,https://github.com/davidissamattos/bpcs,2634,1,2021-06-08T07:13:28Z,2634
bpnreg,"Fitting Bayesian multiple and mixed-effect regression models for 
    circular data based on the projected normal distribution. Both continuous 
    and categorical predictors can be included. Sampling from the posterior is 
    performed via an MCMC algorithm. Posterior descriptives of all parameters, 
    model fit statistics and Bayes factors for hypothesis tests for inequality 
    constrained hypotheses are provided. See Cremers, Mulder & Klugkist (2018) 
    <doi:10.1111/bmsp.12108> and Nuñez-Antonio & Guttiérez-Peña (2014) 
    <doi:10.1016/j.csda.2012.07.025>.",2021-08-06,Jolien Cremers,https://github.com/joliencremers/bpnreg,TRUE,https://github.com/joliencremers/bpnreg,17552,6,2021-08-10T12:47:12Z,2925.3333333333335
bRacatus,"Automated assessment of accuracy and geographical status of georeferenced biological data. The methods rely on reference regions, namely checklists and range maps. Includes functions to obtain data from the Global Biodiversity Information Facility <https://www.gbif.org/> and from the Global Inventory of Floras and Traits <https://gift.uni-goettingen.de/home>. Alternatively, the user can input their own data. Furthermore, provides easy visualisation of the data and the results through the plotting functions. Especially suited for large datasets. The reference for the methodology is: Arlé et al. (under review).",2021-05-28,Eduardo Arlé,https://github.com/EduardoArle/bRacatus,TRUE,https://github.com/eduardoarle/bracatus,5362,5,2021-07-19T12:08:47Z,1072.4
bracer,"Performs brace expansions on strings.  Made popular by Unix shells, brace expansion allows users to concisely generate certain character vectors by taking a single string and (recursively) expanding the comma-separated lists and double-period-separated integer and character sequences enclosed within braces in that string.  The double-period-separated numeric integer expansion also supports padding the resulting numbers with zeros.",2021-05-19,Trevor Davis,"https://trevorldavis.com/R/bracer/,
https://github.com/trevorld/bracer",TRUE,https://github.com/trevorld/bracer,12003,1,2021-05-18T15:27:39Z,12003
brainGraph,"A set of tools for performing graph theory analysis of brain MRI
    data. It works with data from a Freesurfer analysis (cortical thickness,
    volumes, local gyrification index, surface area), diffusion tensor
    tractography data (e.g., from FSL) and resting-state fMRI data (e.g., from
    DPABI). It contains a graphical user interface for graph visualization and
    data exploration, along with several functions for generating useful
    figures.",2020-09-29,Christopher G. Watson,https://github.com/cwatson/brainGraph,TRUE,https://github.com/cwatson/braingraph,27300,107,2021-02-03T15:09:07Z,255.14018691588785
BrazilMet,"A compilation of functions to download and processing AWS data of INMET-Brazil, with the purpose of reference evapotranspiration (ETo) estimation. The package aims to make meteorological and agricultural data analysis more parsimonious.",2021-03-02,Roberto Filgueiras,NA,TRUE,https://github.com/filgueirasr/brazilmet,2395,1,2021-03-01T18:54:14Z,2395
BRDT,"This is an implementation of design methods for binomial reliability demonstration tests (BRDTs) with failure count data. 
    The acceptance decision uncertainty of BRDT has been quantified and the impacts of the uncertainty on related reliability assurance activities such as reliability growth (RG) and warranty services (WS) are evaluated.
    This package is associated with the work from the published paper ""Optimal Binomial Reliability Demonstration Tests Design under Acceptance Decision Uncertainty"" by Suiyao Chen et al. (2020) <doi:10.1080/08982112.2020.1757703>.",2020-06-09,Suiyao Chen,https://github.com/ericchen12377/BRDT,TRUE,https://github.com/ericchen12377/brdt,5037,2,2020-11-01T22:23:11Z,2518.5
breathtestcore,"Reads several formats of 13C data (IRIS/Wagner,
    BreathID) and CSV.  Creates artificial sample data for testing.  Fits
    Maes/Ghoos, Bluck-Coward self-correcting formula using 'nls', 'nlme'.
    Methods to fit breath test curves with Bayesian Stan methods are
    refactored to package 'breathteststan'. For a Shiny GUI, see package
    'dmenne/breathtestshiny' on github.",2021-04-22,Dieter Menne,https://github.com/dmenne/breathtestcore,TRUE,https://github.com/dmenne/breathtestcore,19688,2,2021-05-25T08:51:10Z,9844
breathteststan,"Stan-based curve-fitting function
  for use with package 'breathtestcore' by the same author.
  Stan functions are refactored here for easier testing.",2020-07-14,Dieter Menne,https://github.com/dmenne/breathteststan,TRUE,https://github.com/dmenne/breathteststan,20927,3,2021-05-25T09:06:27Z,6975.666666666667
brglm,"Fit generalized linear models with binomial responses using either an adjusted-score approach to bias reduction or maximum penalized likelihood where penalization is by Jeffreys invariant prior. These procedures return estimates with improved frequentist properties (bias, mean squared error) that are always finite even in cases where the maximum likelihood estimates are infinite (data separation). Fitting takes place by fitting generalized linear models on iteratively updated pseudo-data. The interface is essentially the same as 'glm'.  More flexibility is provided by the fact that custom pseudo-data representations can be specified and used for model fitting. Functions are provided for the construction of confidence intervals for the reduced-bias estimates.",2021-04-22,Ioannis Kosmidis,https://github.com/ikosmidis/brglm,TRUE,https://github.com/ikosmidis/brglm,752974,6,2021-04-22T15:05:47Z,125495.66666666667
brglm2,"Estimation and inference from generalized linear models based on various methods for bias reduction and maximum penalized likelihood with powers of the Jeffreys prior as penalty. The 'brglmFit' fitting method can achieve reduction of estimation bias by solving either the mean bias-reducing adjusted score equations in Firth (1993) <doi:10.1093/biomet/80.1.27> and Kosmidis and Firth (2009) <doi:10.1093/biomet/asp055>, or the median bias-reduction adjusted score equations in Kenne et al. (2017) <doi:10.1093/biomet/asx046>, or through the direct subtraction of an estimate of the bias of the maximum likelihood estimator from the maximum likelihood estimates as in Cordeiro and McCullagh (1991) <https://www.jstor.org/stable/2345592>. See Kosmidis et al (2020) <doi:10.1007/s11222-019-09860-6> for more details. Estimation in all cases takes place via a quasi Fisher scoring algorithm, and S3 methods for the construction of of confidence intervals for the reduced-bias estimates are provided. In the special case of generalized linear models for binomial and multinomial responses (both ordinal and nominal), the adjusted score approaches to mean and media bias reduction have been found to return estimates with improved frequentist properties, that are also always finite, even in cases where the maximum likelihood estimates are infinite (e.g. complete and quasi-complete separation; see Kosmidis and Firth, 2020 <doi:10.1093/biomet/asaa052>, for a proof for mean bias reduction in logistic regression). ",2021-07-17,Ioannis Kosmidis,https://github.com/ikosmidis/brglm2,TRUE,https://github.com/ikosmidis/brglm2,61913,15,2021-07-18T09:11:54Z,4127.533333333334
bridger,"Produce bridge hands, allowing parameters for hands to offer specific for bidding sequences.",2021-08-24,Jason Kaplan,https://github.com/CommoditiesAI/bridger,TRUE,https://github.com/commoditiesai/bridger,139,1,2021-09-01T15:24:21Z,139
bridgesampling,"Provides functions for estimating marginal likelihoods, Bayes
    factors, posterior model probabilities, and normalizing constants in general,
    via different versions of bridge sampling (Meng & Wong, 1996, 
    <http://www3.stat.sinica.edu.tw/statistica/j6n4/j6n43/j6n43.htm>).
    Gronau, Singmann, & Wagenmakers (2020) <doi:10.18637/jss.v092.i10>.",2021-04-16,Quentin F. Gronau,https://github.com/quentingronau/bridgesampling,TRUE,https://github.com/quentingronau/bridgesampling,368191,23,2021-04-15T18:55:09Z,16008.304347826086
brinton,"An automated graphical exploratory data analysis (EDA) tool that introduces: 
  a.) wideplot() graphics for exploring the structure of a dataset through a grid of variables 
      and graphic types. 
  b.) longplot() graphics, which present the entire catalog of available graphics for representing 
      a particular variable using a grid of graphic types and variations on these types.
  c.) plotup() function, which presents a particular graphic for a specific variable of a dataset. 
      The plotup() function also makes it possible to obtain the code used to generate the graphic, 
      meaning that the user can adjust its properties as needed.",2021-04-02,Pere Millán-Martínez,"https://sciencegraph.github.io/brinton/,
https://github.com/sciencegraph/brinton",TRUE,https://github.com/sciencegraph/brinton,18058,11,2021-04-02T07:38:02Z,1641.6363636363637
brio,"Functions to handle basic input output, these functions always
  read and write UTF-8 (8-bit Unicode Transformation Format) files and provide
  more explicit control over line endings.",2021-04-23,Jim Hester,https://github.com/r-lib/brio,TRUE,https://github.com/r-lib/brio,5348312,40,2021-06-29T16:45:08Z,133707.8
BRISC,Fits bootstrap with univariate spatial regression models using Bootstrap for Rapid Inference on Spatial Covariances (BRISC) for large datasets using nearest neighbor Gaussian processes detailed in Saha and Datta (2018) <doi:10.1002/sta4.184>.,2021-03-09,Arkajyoti Saha,https://github.com/ArkajyotiSaha/BRISC,TRUE,https://github.com/arkajyotisaha/brisc,16879,4,2021-03-09T17:37:07Z,4219.75
brms,"Fit Bayesian generalized (non-)linear multivariate multilevel models
    using 'Stan' for full Bayesian inference. A wide range of distributions 
    and link functions are supported, allowing users to fit -- among others -- 
    linear, robust linear, count data, survival, response times, ordinal, 
    zero-inflated, hurdle, and even self-defined mixture models all in a 
    multilevel context. Further modeling options include non-linear and 
    smooth terms, auto-correlation structures, censored data, meta-analytic 
    standard errors, and quite a few more. In addition, all parameters of the 
    response distribution can be predicted in order to perform distributional 
    regression. Prior specifications are flexible and explicitly encourage 
    users to apply prior distributions that actually reflect their beliefs.
    Model fit can easily be assessed and compared with posterior predictive 
    checks and leave-one-out cross-validation. References: Bürkner (2017)
    <doi:10.18637/jss.v080.i01>; Bürkner (2018) <doi:10.32614/RJ-2018-017>;
    Carpenter et al. (2017) <doi:10.18637/jss.v076.i01>.",2021-08-23,Paul-Christian Bürkner,"https://github.com/paul-buerkner/brms,
https://discourse.mc-stan.org/",TRUE,https://github.com/paul-buerkner/brms,606457,910,2021-08-31T17:14:41Z,666.4362637362638
Brobdingnag,"Handles very large numbers in R.  Real numbers are held
        using their natural logarithms, plus a logical flag indicating
        sign.  The package includes a vignette that gives a
        step-by-step introduction to using S4 methods.",2018-08-13,Robin K. S. Hankin,https://github.com/RobinHankin/Brobdingnag.git,TRUE,https://github.com/robinhankin/brobdingnag,337399,1,2021-08-28T00:59:02Z,337399
brokenstick,"The broken stick model describes a set of individual curves by a
    linear mixed model using a second-order linear B-spline. The main use of the model
    is to align irregularly observed data to a user-specified grid of break ages.
    All fitting can done in the Z-score scale, so non-linearity and irregular data
    can be treated as separate problems. This package contains functions for fitting
    a broken stick model to data, for predicting broken stick curves in new data,
    and for plotting the broken stick estimates. For additional documentation on 
    background, methodology and applications of the broken stick model see
    <https://stefvanbuuren.name/publications/2020_Brokenstick_JSS_manuscript.pdf>.",2020-11-02,Stef van Buuren,"https://github.com/growthcharts/brokenstick,
https://growthcharts.org/brokenstick/",TRUE,https://github.com/growthcharts/brokenstick,4133,4,2021-03-08T17:38:27Z,1033.25
brolgar,"Provides a framework of tools to summarise, visualise, and explore 
  longitudinal data. It builds upon the tidy time series data frames used in the
  'tsibble' package, and is designed to integrate within the 'tidyverse', and
  'tidyverts' (for time series) ecosystems. The methods implemented include 
  calculating features for understanding longitudinal data, including 
  calculating summary statistics such as quantiles, medians, and numeric ranges,
  sampling individual series, identifying individual series representative of a 
  group, and extending the facet system  in 'ggplot2' to facilitate exploration of samples of data. These methods are
  fully described in the paper ""brolgar: An R package to Browse Over 
  Longitudinal Data Graphically and Analytically in R"", Nicholas Tierney, 
  Dianne Cook, Tania Prvan (2020) <arXiv:2012.01619>.",2021-08-25,Nicholas Tierney,https://github.com/njtierney/brolgar,TRUE,https://github.com/njtierney/brolgar,3060,99,2021-08-11T01:55:56Z,30.90909090909091
broman,"Miscellaneous R functions, including functions related to
    graphics (mostly for base graphics), permutation tests, running
    mean/median, and general utilities.",2021-02-05,Karl W Broman,https://github.com/kbroman/broman,TRUE,https://github.com/kbroman/broman,38398,167,2021-08-05T20:58:07Z,229.92814371257484
broom,"Summarizes key information about statistical
    objects in tidy tibbles. This makes it easy to report results, create
    plots and consistently work with large numbers of models at once.
    Broom provides three verbs that each provide different types of
    information about a model. tidy() summarizes information about model
    components such as coefficients of a regression. glance() reports
    information about an entire model, such as goodness of fit measures
    like AIC and BIC. augment() adds information about individual
    observations to a dataset, such as fitted values or influence
    measures.",2021-07-27,Alex Hayes,"https://broom.tidymodels.org/, https://github.com/tidymodels/broom",TRUE,https://github.com/tidymodels/broom,19676957,1171,2021-09-01T19:49:22Z,16803.549957301453
broom.helpers,"Provides suite of functions to work with regression
    model 'broom::tidy()' tibbles.  The suite includes functions to group
    regression model terms by variable, insert reference and header rows
    for categorical variables, add variable labels, and more.",2021-04-10,Joseph Larmarange,https://larmarange.github.io/broom.helpers/,TRUE,https://github.com/larmarange/broom.helpers,71646,9,2021-08-09T12:55:06Z,7960.666666666667
broom.mixed,"Convert fitted objects from various R mixed-model packages
    into tidy data frames along the lines of the 'broom' package.
    The package provides three
    S3 generics for each model: tidy(), which summarizes a model's statistical findings such as
    coefficients of a regression; augment(), which adds columns to the original
    data such as predictions, residuals and cluster assignments; and glance(), which
    provides a one-row summary of model-level statistics.",2021-07-07,Ben Bolker,https://github.com/bbolker/broom.mixed,TRUE,https://github.com/bbolker/broom.mixed,256758,184,2021-08-13T21:48:10Z,1395.4239130434783
broomExtra,"Provides helper functions that assist in data
    analysis workflows involving regression analyses. The goal is to
    combine the functionality offered by different set of packages
    ('broom', 'broom.mixed', 'parameters', and 'performance') through a
    common syntax to return tidy dataframes containing model parameters
    and performance measure summaries. The 'grouped_' variants of the
    generics provides a convenient way to execute functions across a
    combination of grouping variable(s) in a dataframe.",2021-05-12,Indrajeet Patil,"https://indrajeetpatil.github.io/broomExtra/,
https://github.com/IndrajeetPatil/broomExtra",TRUE,https://github.com/indrajeetpatil/broomextra,114526,43,2021-06-20T12:45:24Z,2663.3953488372094
bruceR,"
  Broadly useful convenient and efficient R functions
  that bring users concise and elegant R data analyses.
  This package includes easy-to-use functions for
  (1) basic R programming
  (e.g., set working directory to where the current file is,
  print strings with rich formats and colors);
  (2) multivariate computation
  (e.g., compute scale sums/means/... with reverse scoring);
  (3) reliability and factor analyses;
  (4) descriptive statistics and correlation analyses;
  (5) multi-factor analysis of variance (ANOVA),
  simple-effect analysis, and post-hoc multiple comparison;
  (6) tidy report of regression models and other results
  (to R Console and MS Word);
  (7) mediation and moderation analyses (PROCESS);
  and (8) additional toolbox for statistics and graphics.",2021-06-21,Han-Wu-Shuang Bao,https://github.com/psychbruce/bruceR,TRUE,https://github.com/psychbruce/brucer,4945,66,2021-07-12T08:03:11Z,74.92424242424242
bs4cards,"Allows the user to generate bootstrap cards within
    R markdown documents. Intended for use in conjunction with
    R markdown HTML outputs and other formats that support the 
    bootstrap 4 library.",2021-09-03,Danielle Navarro,https://github.com/djnavarro/bs4cards,TRUE,https://github.com/djnavarro/bs4cards,0,21,2021-09-01T11:12:50Z,0
bs4Dash,"Make 'Bootstrap 4' Shiny dashboards. Use the full power
    of 'AdminLTE3', a dashboard template built on top of 'Bootstrap 4' 
    <https://github.com/ColorlibHQ/AdminLTE>.",2021-07-23,David Granjon,"https://rinterface.github.io/bs4Dash/index.html,
https://github.com/RinteRface/bs4Dash",TRUE,https://github.com/rinterface/bs4dash,113583,293,2021-07-22T15:46:59Z,387.6552901023891
bsem,"Flexible routines to allow structural equation modeling particular cases using 'rstan' integration. 'bsem' includes Bayesian semi Confirmatory Factor Analysis, Confirmatory Factor Analysis, and Structural Equation Model.  VD Mayrink (2013) <doi:10.1214/12-AOAS607>.",2020-08-14,Renato Panaro  (<https://orcid.org/0000-0002-1903-2091>,https://github.com/rvpanaro/bsem,TRUE,https://github.com/rvpanaro/bsem,5237,4,2020-11-24T02:22:40Z,1309.25
bSims,"A highly scientific and utterly addictive 
  bird point count simulator 
  to test statistical assumptions, aid survey design,
  and have fun while doing it.
  The simulations follow time-removal and distance sampling models 
  based on Matsuoka et al. (2012) <doi:10.1525/auk.2012.11190>,
  Solymos et al. (2013) <doi:10.1111/2041-210X.12106>,
  and Solymos et al. (2018) <doi:10.1650/CONDOR-18-32.1>,
  and sound attenuation experiments by 
  Yip et al. (2017) <doi:10.1650/CONDOR-16-93.1>.",2019-12-20,Peter Solymos,https://github.com/psolymos/bSims,TRUE,https://github.com/psolymos/bsims,9725,3,2021-07-21T00:03:29Z,3241.6666666666665
bslib,Simplifies custom 'CSS' styling of both 'shiny' and 'rmarkdown' via 'Bootstrap' 'Sass'. Supports both 'Bootstrap' 3 and 4 as well as their various 'Bootswatch' themes. An interactive widget is also provided for previewing themes in real time.,2021-09-02,Carson Sievert,"https://rstudio.github.io/bslib/, https://github.com/rstudio/bslib",TRUE,https://github.com/rstudio/bslib,2076789,226,2021-07-20T17:58:53Z,9189.33185840708
bspm,"Enables binary package installations on Linux distributions.
    Provides functions to manage packages via the distribution's package
    manager. Also provides transparent integration with R's install.packages()
    and a fallback mechanism. When installed as a system package, interacts
    with the system's package manager without requiring administrative
    privileges via an integrated D-Bus service; otherwise, uses sudo.
    Currently, the following backends are supported: DNF, APT.",2021-06-25,Iñaki Ucar,https://github.com/Enchufa2/bspm,TRUE,https://github.com/enchufa2/bspm,6674,34,2021-07-12T09:17:39Z,196.2941176470588
bssm,"Efficient methods for Bayesian inference of state space models 
    via particle Markov chain Monte Carlo (MCMC) and MCMC based on parallel 
    importance sampling type weighted estimators 
    (Vihola, Helske, and Franks, 2020, <doi:10.1111/sjos.12492>). 
    Gaussian, Poisson, binomial, negative binomial, and Gamma
    observation densities and basic stochastic volatility models 
    with linear-Gaussian state dynamics, 
    as well as general non-linear Gaussian models and discretised 
    diffusion models are supported.",2021-07-10,Jouni Helske,NA,TRUE,https://github.com/helske/bssm,56670,25,2021-09-03T13:01:43Z,2266.8
bsub,"It submits R code/R scripts/shell commands to 'LSF cluster' 
  (<https://en.wikipedia.org/wiki/Platform_LSF>, the 'bsub' system) without 
  leaving R. There is also an interactive 'shiny' app for monitoring the job status.",2021-07-01,Zuguang Gu,https://github.com/jokergoo/bsub,TRUE,https://github.com/jokergoo/bsub,5774,11,2021-07-02T12:01:08Z,524.9090909090909
BSW,Implements a modified Newton-type algorithm (BSW algorithm) for solving the maximum likelihood estimation problem in fitting a log-binomial model under linear inequality constraints.,2021-03-22,Adam Bekhit,https://github.com/adam-bec/BSW,TRUE,https://github.com/adam-bec/bsw,7435,0,2021-05-18T09:52:16Z,NA
btergm,"Temporal Exponential Random Graph Models (TERGM) estimated by maximum pseudolikelihood with bootstrapped confidence intervals or Markov Chain Monte Carlo maximum likelihood. Goodness of fit assessment for ERGMs, TERGMs, and SAOMs. Micro-level interpretation of ERGMs and TERGMs. As described in Leifeld, Cranmer and Desmarais (2018), JStatSoft <doi:10.18637/jss.v083.i06>.",2021-06-25,Philip Leifeld,https://github.com/leifeld/btergm,TRUE,https://github.com/leifeld/btergm,110585,9,2021-06-24T20:53:44Z,12287.222222222223
BTM,"Biterm Topic Models find topics in collections of short texts. 
    It is a word co-occurrence based topic model that learns topics by modeling word-word co-occurrences patterns which are called biterms.
    This in contrast to traditional topic models like Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis 
    which are word-document co-occurrence topic models.
    A biterm consists of two words co-occurring in the same short text window.  
    This context window can for example be a twitter message, a short answer on a survey, a sentence of a text or a document identifier. 
    The techniques are explained in detail in the paper 'A Biterm Topic Model For Short Text' by Xiaohui Yan, Jiafeng Guo, Yanyan Lan, Xueqi Cheng (2013) <https://github.com/xiaohuiyan/xiaohuiyan.github.io/blob/master/paper/BTM-WWW13.pdf>.",2021-07-02,Jan Wijffels,https://github.com/bnosac/BTM,TRUE,https://github.com/bnosac/btm,23778,67,2021-07-02T08:43:12Z,354.8955223880597
BTSPAS,"Provides advanced Bayesian methods to estimate
	     abundance and run-timing from temporally-stratified
	     Petersen mark-recapture experiments. Methods include
	     hierarchical modelling of the capture probabilities
  	     and spline smoothing of the daily run size.",2020-12-16,Carl J Schwarz,https://github.com/cschwarz-stat-sfu-ca/BTSPAS,TRUE,https://github.com/cschwarz-stat-sfu-ca/btspas,23872,0,2020-12-16T19:52:05Z,NA
bubblyr,Creates bubbles within 'shiny' and 'rmarkdown' backgrounds using the 'bubbly-bg' 'JavaScript' library.,2020-07-29,Mohamed El Fodil Ihaddaden,https://github.com/feddelegrand7/bubblyr,TRUE,https://github.com/feddelegrand7/bubblyr,6357,17,2020-12-12T22:28:19Z,373.94117647058823
buffeRs,Generates non-circular simple feature geometries e.g. for the use as buffers in model-building.,2021-08-22,Tilman Leo Hohenberger,https://github.com/tlhenvironment/buffeRs,TRUE,https://github.com/tlhenvironment/buffers,2831,0,2021-08-22T06:23:21Z,NA
buildmer,"Finds the largest possible regression model that will still converge
    for various types of regression analyses (including mixed models and generalized
    additive models) and then optionally performs stepwise elimination similar to the
    forward and backward effect-selection methods in SAS, based on the change in
    log-likelihood or its significance, Akaike's Information Criterion, the Bayesian
    Information Criterion, the explained deviance, or the F-test of the change in R².",2021-08-19,Cesko C. Voeten,NA,TRUE,https://github.com/cvoeten/buildmer,43697,4,2021-08-30T07:56:26Z,10924.25
buildr,"Working with reproducible reports or any other
    similar projects often require to run the script that builds the
    output file in a specified way. 'buildr' can help you organize, modify
    and comfortably run those scripts. The package provides a set of
    functions that interactively guides you through the process and that
    are available as 'RStudio' Addin, meaning you can set up the keyboard
    shortcuts, enabling you to choose and run the desired build script
    with one keystroke anywhere anytime.",2021-01-10,Jan Netik,https://netique.github.io/buildr/,TRUE,https://github.com/netique/buildr,8603,2,2021-08-26T18:34:54Z,4301.5
BuildSys,"A build system based on 'GNU make' that creates and
    maintains (simply) make files in an R session and provides
    GUI debugging support through 'Microsoft Visual Code'.",2021-06-16,Paavo Jumppanen,https://github.com/pjumppanen/BuildSys,TRUE,https://github.com/pjumppanen/buildsys,1535,1,2021-07-02T01:50:45Z,1535
bumbl,"Bumblebee colonies grow during worker production, then decline after switching to production of reproductive individuals (drones and gynes).  This package provides tools for modeling and visualizing this pattern by identifying a switchpoint with a growth rate before and a decline rate after the switchpoint. The mathematical models fit by bumbl are described in Crone and Williams (2016) <doi:10.1111/ele.12581>.",2021-08-25,Eric R. Scott,https://github.com/Aariq/bumbl,TRUE,https://github.com/aariq/bumbl,1728,0,2021-08-25T15:23:25Z,NA
bumblebee,"A simple tool to quantify the amount of transmission
   of an infectious disease of interest occurring within and between 
   population groups. 'bumblebee' uses counts of observed directed 
   transmission pairs, identified phylogenetically from deep-sequence data or 
   from epidemiological contacts, to quantify transmission flows within and 
   between population groups accounting for sampling heterogeneity. Population 
   groups might include: geographical areas (e.g. communities, regions), 
   demographic groups (e.g. age, gender) or arms of a randomized clinical 
   trial. See the 'bumblebee' website for statistical theory, documentation 
   and examples <https://magosil86.github.io/bumblebee/>.",2021-05-11,Lerato E Magosi,https://magosil86.github.io/bumblebee/,TRUE,https://github.com/magosil86/bumblebee,1515,0,2021-05-10T11:15:43Z,NA
bunching,"Implementation of the bunching estimator for kinks and notches. 
        Allows for flexible estimation of counterfactual (e.g. controlling for round number bunching, accounting for other bunching masses within bunching window, fixing bunching point to be minimum, maximum or median value in its bin, etc.). 
        It produces publication-ready plots in the style followed since Chetty et al. (2011) <DOI:10.1093/qje/qjr013>, with lots of functionality to set plot options.",2019-09-23,Panos Mavrokonstantis,http://github.com/mavpanos/bunching,TRUE,https://github.com/mavpanos/bunching,11037,1,2021-04-25T10:05:42Z,11037
burnr,"Tools to read, write, parse, and analyze forest fire history data (e.g. FHX). Described in Malevich et al. (2018) <doi:10.1016/j.dendro.2018.02.005>.",2021-03-10,Steven Malevich,https://github.com/ltrr-arizona-edu/burnr/,TRUE,https://github.com/ltrr-arizona-edu/burnr,25056,11,2021-03-22T18:23:55Z,2277.818181818182
butcher,"Provides a set of five S3 generics to axe
    components of fitted model objects and help reduce the size of model
    objects saved to disk.",2021-06-28,Joyce Cahoon,https://butcher.tidymodels.org,TRUE,https://github.com/tidymodels/butcher,39846,94,2021-06-28T12:57:23Z,423.8936170212766
BuyseTest,"Implementation of the Generalized Pairwise Comparisons (GPC)
             as defined in Buyse (2010) <doi:10.1002/sim.3923> for complete observations,
             and extended in Peron (2018) <doi:10.1177/0962280216658320> to deal with right-censoring.        
             GPC compare two groups of observations (intervention vs. control group)
			 regarding several prioritized endpoints to estimate the probability that a random observation drawn from
			 one group performs better than a random observation drawn from the other group (Mann-Whitney parameter).
			 The net benefit and win ratio statistics,
			 i.e. the difference and ratio between the probabilities relative to the intervention and control groups,
			 can then also be estimated. Confidence intervals and p-values are obtained using permutations, a non-parametric bootstrap, or the asymptotic theory.
			 The software enables the use of thresholds of minimal importance difference,
			 stratification, non-prioritized endpoints (O'Brien test), and can handle right-censoring and competing-risks.",2021-04-23,Brice Ozenne,https://github.com/bozenne/BuyseTest,TRUE,https://github.com/bozenne/buysetest,26948,1,2021-08-23T17:07:20Z,26948
BVAR,"Estimation of hierarchical Bayesian vector autoregressive models.
    Implements hierarchical prior selection for conjugate priors in the fashion
    of Giannone, Lenza & Primiceri (2015) <doi:10.1162/REST_a_00483>. Functions
    to compute and identify impulse responses, calculate forecasts,
    forecast error variance decompositions and scenarios are available.
    Several methods to print, plot and summarise results facilitate analysis.",2020-09-27,Nikolas Kuschnig,https://github.com/nk027/bvar,TRUE,https://github.com/nk027/bvar,21474,23,2020-09-26T11:59:02Z,933.6521739130435
bvartools,"Assists in the set-up of algorithms for Bayesian inference of vector autoregressive (VAR) models. Functions for posterior simulation, forecasting, impulse response analysis and forecast error variance decomposition are largely based on the introductory texts of Chan, Koop, Poirier and Tobias (2019, ISBN: 9781108437493), Koop and Korobilis (2010) <doi:10.1561/0800000013> and Luetkepohl (2006, ISBN: 9783540262398). ",2021-04-25,Franz X. Mohr,https://github.com/franzmohr/bvartools,TRUE,https://github.com/franzmohr/bvartools,18903,15,2021-05-01T18:26:34Z,1260.2
BVARverse,"Functions to prepare tidy objects from estimated models via 'BVAR'
    (see Kuschnig & Vashold, 2019 <doi:10.13140/RG.2.2.25541.60643>) and
    visualisation thereof. Bridges the gap between estimating models with 'BVAR'
    and plotting the results in a more sophisticated way with 'ggplot2' as well
    as passing them on in a tidy format. ",2020-09-22,Lukas Vashold,https://github.com/nk027/bvarverse,TRUE,https://github.com/nk027/bvarverse,4518,1,2020-09-14T08:06:52Z,4518
BWGS,"Package for Breed Wheat Genomic Selection Pipeline. 
    The R package 'BWGS' is developed by Gilles Charmet <gilles.charmet@inra.fr>.
    This repository is forked from original repository <https://forgemia.inra.fr/umr-gdec/bwgs>
    and modified as a R package.",2020-07-30,Bangyou Zheng,https://github.com/byzheng/BWGS,TRUE,https://github.com/byzheng/bwgs,5577,4,2021-03-19T00:30:31Z,1394.25
BWStest,"Performs the 'Baumgartner-Weiss-Schindler' two-sample test of equal
   probability distributions, <doi:10.2307/2533862>. Also performs
   similar rank-based tests for equal probability distributions due to
   Neuhauser <doi:10.1080/10485250108832874> and
   Murakami <doi:10.1080/00949655.2010.551516>.",2018-10-18,Steven E. Pav,https://github.com/shabbychef/BWStest,TRUE,https://github.com/shabbychef/bwstest,119810,0,2021-04-03T23:10:21Z,NA
bwsTools,"Tools to design best-worst scaling designs (i.e., balanced incomplete block designs) and
    to analyze data from these designs, using aggregate and individual methods such as: difference 
    scores, Louviere, Lings, Islam, Gudergan, & Flynn (2013) <doi:10.1016/j.ijresmar.2012.10.002>; 
    analytical estimation, Lipovetsky & Conklin (2014) <doi:10.1016/j.jocm.2014.02.001>; empirical 
    Bayes, Lipovetsky & Conklin (2015) <doi:10.1142/S1793536915500028>; Elo, Hollis (2018) 
    <doi:10.3758/s13428-017-0898-2>; and network-based measures.",2020-08-26,Mark White,https://github.com/markhwhiteii/bwsTools,TRUE,https://github.com/markhwhiteii/bwstools,11626,7,2021-07-04T18:09:18Z,1660.857142857143
c060,"The c060 package provides additional functions to perform stability selection, model validation and parameter tuning for glmnet models.",2021-03-16,Martin Sill,"https://github.com/fbertran/c060/,
https://fbertran.github.io/c060/",TRUE,https://github.com/fbertran/c060,21830,1,2021-03-16T00:46:28Z,21830
c2d4u.tools,"The 'c2d4u' project provides precompiled CRAN packages for 'Ubuntu' users.
    They can be installed into user libraries without requiring root permissions.",2021-06-17,Neal Fultz,https://github.com/nfultz/c2d4u.tools,TRUE,https://github.com/nfultz/c2d4u.tools,905,2,2021-06-16T19:24:26Z,452.5
c3,"Create interactive charts with the 'C3.js' <http://c3js.org/> charting library. All plot 
    types in 'C3.js' are available and include line, bar, scatter, and mixed geometry plots. Plot 
    annotations, labels and axis are highly adjustable. Interactive web based charts can be embedded 
    in R Markdown documents or Shiny web applications. ",2020-03-16,Matt Johnson,https://github.com/mrjoh3/c3,TRUE,https://github.com/mrjoh3/c3,18247,39,2021-07-04T04:37:46Z,467.87179487179486
C50,"C5.0 decision trees and rule-based models for pattern recognition that extend the work of Quinlan (1993, ISBN:1-55860-238-0).",2021-06-01,Max Kuhn,https://topepo.github.io/C5.0/,TRUE,https://github.com/topepo/c5.0,445246,45,2021-05-28T17:10:47Z,9894.355555555556
cabinets,"Creates project specific directory and file templates that are 
 written to a .Rprofile file. Upon starting a new R session, these templates 
 can be used to streamline the creation of new directories that are 
 standardized to the user's preferences and can include the initiation of a 
 git repository, an RStudio R project, and project-local dependency management 
 with the 'renv' package.  ",2020-11-07,Nick Williams,https://github.com/nt-williams/cabinets,TRUE,https://github.com/nt-williams/cabinets,13491,15,2020-11-07T03:31:31Z,899.4
cache,"Easily cache and retrieve computation results. The package works seamlessly across interactive R sessions, R scripts and Rmarkdown documents.",2021-07-09,Olivier Binette,https://github.com/OlivierBinette/cache,TRUE,https://github.com/olivierbinette/cache,1107,1,2021-07-08T23:40:04Z,1107
cachem,"Key-value stores with automatic pruning. Caches can limit
    either their total size or the age of the oldest object (or both),
    automatically pruning objects to maintain the constraints.",2021-08-19,Winston Chang,"https://cachem.r-lib.org/, https://github.com/r-lib/cachem",TRUE,https://github.com/r-lib/cachem,3266235,37,2021-07-06T16:04:30Z,88276.62162162163
caesar,"Encrypts and decrypts strings using either the Caesar cipher or a
    pseudorandom number generation (using set.seed()) method.",2020-09-03,Jacob Kaplan,https://github.com/jacobkap/caesar,TRUE,https://github.com/jacobkap/caesar,38917,1,2020-09-09T21:57:26Z,38917
CALANGO,"A first-principle, phylogeny-aware comparative genomics tool for 
             investigating associations between terms used to annotate genomic
             components (e.g., Pfam IDs, Gene Ontology terms,) with quantitative 
             or rank variables such as number of cell types, genome size, or 
             density of specific genomic elements. See the project website for 
             more information, documentation and examples.",2021-09-02,Felipe Campelo,https://fcampelo.github.io/CALANGO/,TRUE,https://github.com/fcampelo/calango,857,1,2021-09-01T08:54:39Z,857
calculus,"Efficient C++ optimized functions for numerical and symbolic calculus as described in Guidotti (2020) <arXiv:2101.00086>. It includes basic arithmetic, tensor calculus, Einstein summing convention, fast computation of the Levi-Civita symbol and generalized Kronecker delta, Taylor series expansion, multivariate Hermite polynomials, high-order derivatives, ordinary differential equations, differential operators (Gradient, Jacobian, Hessian, Divergence, Curl, Laplacian) and numerical integration in arbitrary orthogonal coordinate systems: cartesian, polar, spherical, cylindrical, parabolic or user defined by custom scale factors. ",2021-05-26,Emanuele Guidotti,https://calculus.guidotti.dev,TRUE,https://github.com/eguidotti/calculus,19032,27,2021-05-26T09:46:12Z,704.8888888888889
calibrar,"Automated parameter estimation for complex (ecological) models in R. 
  This package allows the parameter estimation or calibration of complex models, 
  including stochastic ones. It is a generic tool that can be used for fitting 
  any type of models, especially those with non-differentiable objective functions. 
  It supports multiple phases and constrained optimization. 
  It implements maximum likelihood estimation methods and automated construction 
  of the objective function from simulated model outputs. 
  See <http://roliveros-ramos.github.io/calibrar> for more details.",2016-02-17,Ricardo Oliveros-Ramos,http://roliveros-ramos.github.io/calibrar,TRUE,https://github.com/roliveros-ramos/calibrar,17873,6,2021-08-25T05:24:21Z,2978.8333333333335
calibrator,"Performs Bayesian calibration of computer models as per
 Kennedy and O'Hagan 2001.  The package includes routines to find the
 hyperparameters and parameters; see the help page for stage1() for a
 worked example using the toy dataset.  A tutorial is provided in the
 calex.Rnw vignette; and a suite of especially simple one dimensional
 examples appears in inst/doc/one.dim/.",2019-03-07,Robin K. S. Hankin,https://github.com/RobinHankin/calibrator.git,TRUE,https://github.com/robinhankin/calibrator,34351,1,2021-04-25T06:04:53Z,34351
caliver,"Utility functions for the post-processing, calibration and
  validation of grid model outputs. Initial test cases include the outputs of
  the following forest fire models: GEFF and RISICO. The package is
  described in Vitolo et al. (2018) ""Caliver: An R package for CALIbration and
  VERification of forest fire gridded model outputs""
  <doi:10.1371/journal.pone.0189419>.",2021-02-19,Claudia Vitolo,"https://ecmwf.github.io/caliver/, https://github.com/ecmwf/caliver",TRUE,https://github.com/ecmwf/caliver,2343,15,2021-06-15T20:34:06Z,156.2
calmate,A multi-array post-processing method of allele-specific copy-number estimates (ASCNs).,2015-10-27,Henrik Bengtsson,https://github.com/HenrikBengtsson/calmate/,TRUE,https://github.com/henrikbengtsson/calmate,21424,1,2020-12-13T17:32:19Z,21424
CamelUp,Implements the board game 'CamelUp' for use in introductory statistics classes using a Shiny app. ,2021-02-20,Michael Czekanski,NA,TRUE,https://github.com/mczekanski1/camel-up,13192,0,2021-02-20T18:32:10Z,NA
campfin,"Explore and normalize American campaign finance
    data. Created by the Investigative Reporting Workshop to facilitate
    work on The Accountability Project, an effort to collect public data
    into a central, standard database that is more easily searched:
    <https://publicaccountability.org/>.",2021-09-02,Kiernan Nicholls,"https://github.com/irworkshop/campfin,
https://irworkshop.github.io/campfin/",TRUE,https://github.com/irworkshop/campfin,4699,12,2021-09-02T17:14:25Z,391.5833333333333
camtrapR,"Management of and data extraction from camera trap data in wildlife studies. The package provides a workflow for storing and sorting camera trap photos (and videos), tabulates records of species and individuals, and creates detection/non-detection matrices for occupancy and spatial capture-recapture analyses with great flexibility. In addition, it can visualise species activity data and provides simple mapping functions with GIS export.",2020-07-29,Juergen Niedballa,"https://github.com/jniedballa/camtrapR,
https://jniedballa.github.io/camtrapR,
https://groups.google.com/forum/#!forum/camtrapr",TRUE,https://github.com/jniedballa/camtrapr,46537,6,2021-07-12T10:41:10Z,7756.166666666667
cancensus,"Integrated, convenient, and uniform access to Canadian
    Census data and geography retrieved using the 'CensusMapper' API. This package produces analysis-ready 
    tidy data frames and spatial data in multiple formats, as well as convenience functions
    for working with Census variables, variable hierarchies, and region selection. API
    keys are freely available with free registration at <https://censusmapper.ca/api>.
    Census data and boundary geometries are reproduced and distributed on an ""as
    is"" basis with the permission of Statistics Canada (Statistics Canada 2001; 2006;
    2011; 2016).",2021-06-08,Jens von Bergmann,"https://github.com/mountainMath/cancensus,
https://mountainmath.github.io/cancensus/,
https://censusmapper.ca/api",TRUE,https://github.com/mountainmath/cancensus,29217,61,2021-06-09T04:59:11Z,478.9672131147541
CancerGram,"Predicts anticancer peptides using random forests trained on the
    n-gram encoded peptides. The implemented algorithm can be accessed from
    both the command line and shiny-based GUI. The CancerGram model is too large 
    for CRAN and it has to be downloaded separately from the repository:
    <https://github.com/BioGenies/CancerGramModel>. For more information see: 
    Burdukiewicz et al. (2020) <doi:10.3390/pharmaceutics12111045>. ",2020-11-19,Michal Burdukiewicz,https://github.com/BioGenies/CancerGram,TRUE,https://github.com/biogenies/cancergram,3667,3,2020-11-19T20:29:36Z,1222.3333333333333
candisc,"Functions for computing and visualizing 
	generalized canonical discriminant analyses and canonical correlation analysis
	for a multivariate linear model.
	Traditional canonical discriminant analysis is restricted to a one-way 'MANOVA'
	design and is equivalent to canonical correlation analysis between a set of quantitative
	response variables and a set of dummy variables coded from the factor variable.
	The 'candisc' package generalizes this to higher-way 'MANOVA' designs
	for all factors in a multivariate linear model,
	computing canonical scores and vectors for each term. The graphic functions provide low-rank (1D, 2D, 3D) 
	visualizations of terms in an 'mlm' via the 'plot.candisc' and 'heplot.candisc' methods. Related plots are
	now provided for canonical correlation analysis when all predictors are quantitative.",2021-01-22,Michael Friendly,NA,TRUE,https://github.com/friendly/candisc,148058,3,2021-01-21T17:40:24Z,49352.666666666664
Canopy,"A statistical framework and computational procedure for identifying
  the sub-populations within a tumor, determining the mutation profiles of each 
  subpopulation, and inferring the tumor's phylogenetic history. The input are 
  variant allele frequencies (VAFs) of somatic single nucleotide alterations 
  (SNAs) along with allele-specific coverage ratios between the tumor and matched
  normal sample for somatic copy number alterations (CNAs). These quantities can
  be directly taken from the output of existing software. Canopy provides a 
  general mathematical framework for pooling data across samples and sites to 
  infer the underlying parameters. For SNAs that fall within CNA regions, Canopy
  infers their temporal ordering and resolves their phase.  When there are 
  multiple evolutionary configurations consistent with the data, Canopy outputs 
  all configurations along with their confidence assessment.",2017-12-18,Yuchao Jiang,https://github.com/yuchaojiang/Canopy,TRUE,https://github.com/yuchaojiang/canopy,18627,55,2021-08-22T14:14:55Z,338.6727272727273
canprot,"Compositional analysis of differentially expressed proteins in
  cancer and cell culture proteomics experiments. The data include lists of up-
  and down-regulated proteins in different cancer types (breast, colorectal,
  liver, lung, pancreatic, prostate) and laboratory conditions (hypoxia,
  hyperosmotic stress, high glucose, 3D cell culture, and proteins secreted in
  hypoxia), together with amino acid compositions computed for protein sequences
  obtained from UniProt. Functions are provided to calculate compositional metrics
  including protein length, carbon oxidation state, and stoichiometric hydration
  state. In addition, phylostrata (evolutionary ages) of protein-coding genes are
  compiled using data from Liebeskind et al. (2016) <doi:10.1093/gbe/evw113> or
  Trigos et al. (2017) <doi:10.1073/pnas.1617743114>. The vignettes contain
  plots of compositional differences, phylostrata for human proteins, and
  references for all datasets.",2020-10-19,Jeffrey Dick,https://github.com/jedick/canprot,TRUE,https://github.com/jedick/canprot,19474,2,2021-07-14T01:14:27Z,9737
cansim,"Searches for, accesses, and retrieves new-format and old-format Statistics Canada data 
    tables, as well as individual vectors, as tidy data frames. This package deals with encoding issues, allows for 
    bilingual English or French language data retrieval, and bundles convenience functions 
    to make it easier to work with retrieved table data. Optional caching features are provided.",2021-07-29,Jens von Bergmann,"https://github.com/mountainMath/cansim,
https://mountainmath.github.io/cansim/,
https://www.statcan.gc.ca/",TRUE,https://github.com/mountainmath/cansim,23918,30,2021-07-28T21:10:24Z,797.2666666666667
canvasXpress,"Enables creation of visualizations using the CanvasXpress framework
    in R. CanvasXpress is a standalone JavaScript library for reproducible research
    with complete tracking of data and end-user modifications stored in a single
    PNG image that can be played back. See <https://www.canvasxpress.org> for more
    information.",2021-06-25,Connie Brett,https://github.com/neuhausi/canvasXpress,TRUE,https://github.com/neuhausi/canvasxpress,41376,251,2021-09-01T22:31:53Z,164.84462151394422
canvasXpress.data,"Contains the prepared data that is needed for the 'shiny' application examples in the 
    'canvasXpress' package.  This package also includes datasets used for automated 'testthat' tests.
    Scotto L, Narayan G, Nandula SV, Arias-Pulido H et al. (2008) <doi:10.1002/gcc.20577>.
    Davis S, Meltzer PS (2007) <doi:10.1093/bioinformatics/btm254>.",2021-06-29,Connie Brett,https://github.com/neuhausi/canvasXpress.data,TRUE,https://github.com/neuhausi/canvasxpress.data,12532,0,2021-06-29T19:28:40Z,NA
capl,"A toolkit for computing and visualizing CAPL-2
    (Canadian Assessment of Physical Literacy, Second Edition;
    <https://www.capl-eclp.ca>) scores and interpretations from raw data.",2021-03-16,Joel Barnes,https://github.com/barnzilla/capl,TRUE,https://github.com/barnzilla/capl,2018,1,2021-03-13T19:04:38Z,2018
caracas,"Computer algebra via the 'SymPy' library (<https://www.sympy.org/>). 
  This makes it possible to solve equations symbolically, 
  find symbolic integrals, symbolic sums and other important quantities. ",2021-07-05,Mikkel Meyer Andersen,https://github.com/r-cas/caracas,TRUE,https://github.com/r-cas/caracas,10598,11,2021-08-19T19:59:49Z,963.4545454545455
caRamel,"Multi-objective optimizer initially developed for the calibration of hydrological models.
     The algorithm is a hybrid of the MEAS algorithm (Efstratiadis and Koutsoyiannis (2005) <doi:10.13140/RG.2.2.32963.81446>) by using the directional search method based on the simplexes of the objective space
     and the epsilon-NGSA-II algorithm with the method of classification of the parameter vectors archiving management by epsilon-dominance (Reed and Devireddy <doi:10.1142/9789812567796_0004>).",2020-09-17,Fabrice Zaoui,https://github.com/fzao/caRamel,TRUE,https://github.com/fzao/caramel,18809,6,2021-07-27T13:44:22Z,3134.8333333333335
CARBayes,"Implements a class of univariate and multivariate spatial generalised linear mixed models for areal unit data, with inference in a Bayesian setting using Markov chain Monte Carlo (MCMC) simulation. The response variable can be binomial, Gaussian, multinomial, Poisson or zero-inflated Poisson (ZIP), and spatial autocorrelation is modelled by a set of random effects that are assigned a conditional autoregressive (CAR) prior distribution. A number of different models are available for univariate spatial data, including models with no random effects as well as random effects modelled by different types of CAR prior, including the BYM model (Besag et al. (1991) <doi:10.1007/BF00116466>), the Leroux model (Leroux et al. (2000) <doi:10.1007/978-1-4612-1284-3_4>) and the localised model (Lee et al. (2015) <doi:10.1002/env.2348>). Additionally,  a multivariate CAR (MCAR) model for multivariate spatial data is available, as is a two-level hierarchical model for modelling data relating to individuals within areas. Full details are given in the vignette accompanying this package. The initial creation of this package was supported by the Economic and Social Research Council (ESRC) grant RES-000-22-4256, and on-going development has been supported by the Engineering and Physical Science Research Council (EPSRC) grant EP/J017442/1, ESRC grant ES/K006460/1, Innovate UK / Natural Environment Research Council (NERC) grant NE/N007352/1 and the TB Alliance. ",2021-05-31,Duncan Lee,https://github.com/duncanplee/CARBayes,TRUE,https://github.com/duncanplee/carbayes,56739,4,2021-05-28T04:21:49Z,14184.75
CARBayesST,"Implements a class of univariate and multivariate spatio-temporal generalised linear mixed models for areal unit data, with inference in a Bayesian setting using Markov chain Monte Carlo (MCMC) simulation. The response variable can be binomial, Gaussian, or Poisson, but for some models only the binomial and Poisson data likelihoods are available. The spatio-temporal autocorrelation is modelled by  random effects, which are assigned conditional autoregressive (CAR) style prior distributions. A number of different random effects structures are available, including models similar to Bernardinelli et al. (1995) <doi:10.1002/sim.4780142112> and Rushworth et al. (2014) <doi:10.1016/j.sste.2014.05.001>. Full details are given in the vignette accompanying this package. The creation and development of this package was supported by the Engineering and Physical Sciences Research Council  (EPSRC) grants EP/J017442/1 and EP/T004878/1 and the Medical Research Council (MRC) grant MR/L022184/1.",2021-05-31,Duncan Lee,https://github.com/duncanplee/CARBayesST,TRUE,https://github.com/duncanplee/carbayesst,33185,7,2021-05-27T08:18:35Z,4740.714285714285
card,"Tools that can aid in the assessment of the autonomic regulation 
		of cardiovascular physiology. The aims of this package are to: 1) study 
		electrocardiography (both intervals and morphology) as extensions 
		of signal processing, 2) study circadian rhythms and how it effects 
		autonomic physiology, 3) assess clinical risk of autonomic dysfunction on 
		cardiovascular health through the perspective of epidemiology and causality.
		The analysis of circadian rhythms through cosinor analysis are built upon 
		the methods by Cornelissen (2014) <doi:10.1186/1742-4682-11-16> and 
		Refinetti, Cornelissen, Halberg (2014) <doi:10.1080/09291010600903692>.",2020-09-03,Anish S. Shah,https://github.com/asshah4/card,TRUE,https://github.com/asshah4/card,5251,2,2021-08-18T16:26:34Z,2625.5
caRecall,"Provides API access to the Government of Canada Vehicle Recalls 
    Database <https://tc.api.canada.ca/en/detail?api=VRDB> used by the Defect 
    Investigations and Recalls Division for vehicles, tires, and child car 
    seats. The API wrapper provides access to recall summary information 
    searched using make, model, and year range, as well as detailed recall 
    information searched using recall number.",2021-02-16,Nathan Smith,https://github.com/WraySmith/caRecall,TRUE,https://github.com/wraysmith/carecall,2484,3,2021-02-16T22:45:59Z,828
careless,"When taking online surveys, participants sometimes respond to items without regard to their content.
    These types of responses, referred to as careless or insufficient effort responding, constitute significant problems for data quality, leading to distortions in data analysis and hypothesis testing, such as spurious correlations. The 'R' package 'careless' provides solutions designed to detect such careless / insufficient effort responses by allowing easy calculation of indices proposed in the literature. It currently supports the calculation of longstring, even-odd consistency, psychometric synonyms/antonyms, Mahalanobis distance, and intra-individual response variability (also termed inter-item standard deviation). For a review of these methods, see Curran (2016) <doi:10.1016/j.jesp.2015.07.006>.",2021-03-15,Richard Yentes,https://github.com/ryentes/careless/,TRUE,https://github.com/ryentes/careless,20653,12,2021-04-25T17:13:56Z,1721.0833333333333
caret,"Misc functions for training and plotting classification and
    regression models.",2021-05-15,Max Kuhn,https://github.com/topepo/caret/,TRUE,https://github.com/topepo/caret,5973072,1355,2021-05-15T13:40:38Z,4408.171217712177
cargo,"A framework is provided to transparently develop R packages using 'Rust' <https://www.rust-lang.org/> with
 minimal overhead, and more wrappers are easily added. Help is provided to run 'Cargo' <https://doc.rust-lang.org/cargo/> in a manner
 consistent with CRAN policies. Rust code can also be embedded directly in an R script.",2021-08-22,David B. Dahl,"https://github.com/dbdahl/cargo-framework (repository),
https://arxiv.org/pdf/2108.07179.pdf (paper)",TRUE,https://github.com/dbdahl/cargo-framework,3785,3,2021-09-02T16:23:50Z,1261.6666666666667
CARlasso,Algorithms to fit Bayesian Conditional Autoregressive LASSO with automatic and adaptive shrinkage described in Shen and Solis-Lemus (2020) <arXiv:2012.08397>.,2021-08-11,Yunyi Shen,https://github.com/YunyiShen/CAR-LASSO,TRUE,https://github.com/yunyishen/car-lasso,857,9,2021-08-17T16:28:11Z,95.22222222222223
Carlson,"Evaluation of the Carlson elliptic integrals and the incomplete elliptic integrals with complex arguments. The implementations use Carlson's algorithms <doi:10.1007/BF02198293>. Applications of elliptic integrals include probability distributions, geometry, physics, mechanics, electrodynamics, statistical mechanics, astronomy, geodesy, geodesics on conics, and magnetic field calculations.",2021-01-16,Stéphane Laurent,https://github.com/stla/Carlson,TRUE,https://github.com/stla/carlson,9590,0,2021-01-16T09:07:44Z,NA
cartogram,Construct continuous and non-contiguous area cartograms.,2020-08-26,Sebastian Jeworutzki,https://github.com/sjewo/cartogram,TRUE,https://github.com/sjewo/cartogram,146956,124,2021-05-26T08:59:11Z,1185.1290322580646
cartography,"Create and integrate maps in your R workflow. This package helps 
    to design cartographic representations such as proportional symbols, 
    choropleth, typology, flows or discontinuities maps. It also offers several 
    features that improve the graphic presentation of maps, for instance, map 
    palettes, layout elements (scale, north arrow, title...), labels or legends. 
    See Giraud and Lambert (2017) <doi:10.1007/978-3-319-57336-6_13>.",2021-03-18,Timothée Giraud,https://github.com/riatelab/cartography/,TRUE,https://github.com/riatelab/cartography,111097,365,2021-03-24T10:32:01Z,304.3753424657534
Cascade,"A modeling tool allowing gene selection, reverse engineering, and prediction in cascade networks. Jung, N., Bertrand, F., Bahram, S., Vallat, L., and Maumy-Bertrand, M. (2014) <doi:10.1093/bioinformatics/btt705>.",2021-03-19,Frederic Bertrand,"http://www-irma.u-strasbg.fr/~fbertran/,
https://github.com/fbertran/Cascade",TRUE,https://github.com/fbertran/cascade,15647,1,2021-04-12T16:58:41Z,15647
CascadeData,"These experimental expression data (5 leukemic 'CLL' B-lymphocyte of aggressive form from 'GSE39411', <doi:10.1073/pnas.1211130110>), after B-cell receptor stimulation, are used as examples by packages such as the 'Cascade' one, a modeling tool allowing gene selection, reverse engineering, and prediction in cascade networks. Jung, N., Bertrand, F., Bahram, S., Vallat, L., and Maumy-Bertrand, M. (2014) <doi:10.1093/bioinformatics/btt705>.",2021-03-19,Frederic Bertrand,"https://fbertran.github.io/CascadeData/,
https://github.com/fbertran/CascadeData/",TRUE,https://github.com/fbertran/cascadedata,17152,1,2021-03-19T10:24:30Z,17152
cascadess,"Apply styles to tag elements directly or with the
  .style pronoun. Using the pronoun, styles are created within
  the context of a tag element. Change borders, background colors,
  margins, layouts, and more.",2020-11-30,Nathan Teetor,https://github.com/nteetor/cascadess,TRUE,https://github.com/nteetor/cascadess,3420,17,2020-12-13T10:04:49Z,201.1764705882353
casebase,"Fit flexible and fully parametric hazard regression models to survival data with single event type or multiple 
    competing causes via logistic and multinomial regression. Our formulation allows for arbitrary functional forms 
    of time and its interactions with other predictors for time-dependent hazards and hazard ratios. From the 
    fitted hazard model, we provide functions to readily calculate and plot cumulative incidence and survival 
    curves for a given covariate profile. This approach accommodates any log-linear hazard function of 
    prognostic time, treatment, and covariates, and readily allows for non-proportionality. We also provide 
    a plot method for visualizing incidence density via population time plots. Based on the case-base sampling 
    approach of Hanley and Miettinen (2009) <DOI:10.2202/1557-4679.1125>, Saarela and Arjas (2015) <DOI:10.1111/sjos.12125>, 
    and Saarela (2015) <DOI:10.1007/s10985-015-9352-x>.",2021-02-07,Sahir Bhatnagar,http://sahirbhatnagar.com/casebase/,TRUE,https://github.com/sahirbhatnagar/casebase,30624,5,2021-08-23T13:47:46Z,6124.8
casen,"Funciones para realizar estadistica descriptiva e inferencia con el
 disenio complejo de la Encuesta CASEN (Encuesta de Caracterizacion 
 Socio-Economica). Incluye datasets que permiten armonizar los codigos de 
 comunas que cambian entre anios y permite convertir a los codigos oficiales de 
 SUBDERE.
 (Functions to compute descriptive and inferential statistics with CASEN
 Survey [Socio-Economic Characterization Survey] complex design. Includes 
 datasets to harmonize commune codes that change across years and allows to 
 convert to official SUBDERE codes.)",2020-04-08,Mauricio Vargas,https://pachamaltese.github.io/casen/,TRUE,https://github.com/pachamaltese/casen,10326,6,2020-09-06T15:49:13Z,1721
CAST,"Supporting functionality to run 'caret' with spatial or spatial-temporal data. 'caret' is a frequently used package for model training and prediction using machine learning. This package includes functions to improve spatial-temporal modelling tasks using 'caret'. It prepares data for Leave-Location-Out and Leave-Time-Out cross-validation which are target-oriented validation strategies for spatial-temporal models. To decrease overfitting and improve model performances, the package implements a forward feature selection that selects suitable predictor variables in view to their contribution to the target-oriented performance. CAST further includes functionality to estimate the (spatial) area of applicability of prediction models by analysing the similarity between new data and training data.",2021-04-07,Hanna Meyer,https://github.com/HannaMeyer/CAST,TRUE,https://github.com/hannameyer/cast,30992,52,2021-07-02T13:02:04Z,596
cat.dt,"Implements the Merged Tree-CAT method (Javier Rodriguez-Cuadrado et al., 2020, <doi:10.1016/j.eswa.2019.113066>) to generate Computerized Adaptive Tests (CATs) based on a decision tree. The tree growth is controlled by merging branches with similar trait distributions and estimations. This package has the necessary tools for creating CATs and estimate the subject's ability level. ",2021-03-31,Javier Rodriguez-Cuadrado,https://github.com/jlaria/cat.dt,TRUE,https://github.com/jlaria/cat.dt,14696,1,2021-03-31T10:35:56Z,14696
cat2cat,"
  There are offered automatic methods to map a categorical variable according to a specific encoding across different time points. 
  The main rule is to replicate the observation if it could be assign to a few categories.
  Then using simple frequencies or statistical methods to approximate probabilities of being assign to each of them.
  This algorithm was invented and implemented in the paper by (Nasinski, Majchrowska and Broniatowska (2020) <doi:10.24425/cejeme.2020.134747>).",2021-03-27,Maciej Nasinski,https://github.com/Polkas/cat2cat,TRUE,https://github.com/polkas/cat2cat,7668,0,2021-03-24T18:59:30Z,NA
catalog,"Gain access to the 'Spark Catalog' API making use of the 'sparklyr' API. 'Catalog' <https://spark.apache.org/docs/2.4.3/api/java/org/apache/spark/sql/catalog/Catalog.html> is the interface for managing a metastore (aka metadata catalog) of relational entities (e.g. database(s), tables, functions, table columns and temporary views).",2021-03-20,Nathan Eastwood,"https://nathaneastwood.github.io/catalog/,
https://github.com/nathaneastwood/catalog",TRUE,https://github.com/nathaneastwood/catalog,9508,2,2021-03-20T14:47:02Z,4754
CatDataAnalysis,"Datasets used in the book ""Categorical Data Analysis""
   by Agresti (2012, ISBN:978-0-470-46363-5) but not printed in the book.
   Datasets and help pages were automatically produced from the source
   <http://www.stat.ufl.edu/~aa/cda/data.html> by the R script foo.R,
   which can be found in the GitHub repository.",2020-11-18,Alan Agesti,https://github.com/cjgeyer/CatDataAnalysis,TRUE,https://github.com/cjgeyer/catdataanalysis,5215,1,2020-11-19T13:58:51Z,5215
catmaply,"Methods and plotting functions for displaying categorical data on an 
             interactive heatmap using 'plotly'. Provides functionality for strictly 
             categorical heatmaps, heatmaps illustrating categorized continuous data 
             and annotated heatmaps. Also, there are various options to interact with the x-axis
             to prevent overlapping axis labels, e.g. via simple sliders or range sliders. 
             Besides the viewer pane, resulting plots can be saved as a standalone HTML file, 
             embedded in 'R Markdown' documents or in a 'Shiny' app.",2020-09-07,Yves Mauron,https://github.com/VerkehrsbetriebeZuerich/catmaply,TRUE,https://github.com/verkehrsbetriebezuerich/catmaply,5330,14,2021-08-16T07:15:24Z,380.7142857142857
catsim,"Computes a structural similarity metric (after the style of
    MS-SSIM for images) for binary and categorical 2D and 3D images. Can be
    based on accuracy (simple matching), Cohen's kappa, Rand index, adjusted
    Rand index, Jaccard index, Dice index, normalized mutual information, or
    adjusted mutual information. In addition, has fast computation
    of Cohen's kappa, the Rand indices, and the two mutual informations.
    Implements the methods of Thompson and Maitra (2020) <arXiv:2004.09073>.",2020-12-16,Geoffrey Thompson,"https://github.com/gzt/catsim, https://gzt.github.io/catsim/",TRUE,https://github.com/gzt/catsim,7058,2,2020-12-08T16:51:59Z,3529
catSurv,"Provides methods of computerized adaptive testing for survey researchers.  See Montgomery and Rossiter (2019) <doi:10.1093/jssam/smz027>. Includes functionality for data fit with the classic item response methods including the latent trait model, Birnbaum`s three parameter model, the graded response, and the generalized partial credit model.  Additionally, includes several ability parameter estimation and item selection routines.  During item selection, all calculations are done in compiled C++ code.",2020-10-06,Erin Rossiter,NA,TRUE,https://github.com/erossiter/catsurv,17447,7,2021-05-27T22:28:22Z,2492.4285714285716
causact,"Accelerate Bayesian analytics workflows in 'R' through interactive modelling,
    visualization, and inference. Define probabilistic graphical models using directed
    acyclic graphs (DAGs) as a unifying language for business stakeholders, statisticians, 
    and programmers. This package relies on the sleek and elegant 'greta' package for 
    Bayesian inference. 'greta', in turn, is an interface into 'TensorFlow' from 'R'. 
    Install 'greta' using instructions available here: <https://www.causact.com/install-tensorflow-greta-and-causact.html>.
    See <https://github.com/flyaflya/causact> or <https://www.causact.com/> for more documentation.",2021-01-25,Adam Fleischhacker,"https://github.com/flyaflya/causact, https://www.causact.com/",TRUE,https://github.com/flyaflya/causact,7876,9,2021-05-27T15:54:56Z,875.1111111111111
causalCmprsk,Estimation of average treatment effects (ATE) of two static treatment regimes on time-to-event outcomes with K competing events (K can be 1). The method uses propensity scores weighting for emulation of baseline randomization. ,2021-08-09,Bella Vakulenko-Lagun,https://github.com/Bella2001/causalCmprsk,TRUE,https://github.com/bella2001/causalcmprsk,4203,1,2021-01-31T07:32:34Z,4203
causaleffect,"Functions for identification and transportation of causal effects.
 Provides a conditional causal effect identification algorithm (IDC) by
 Shpitser, I. and Pearl, J. (2006)
 <http://ftp.cs.ucla.edu/pub/stat_ser/r329-uai.pdf>, an algorithm for
 transportability from multiple domains with limited experiments by
 Bareinboim, E. and Pearl, J. (2014)
 <http://ftp.cs.ucla.edu/pub/stat_ser/r443.pdf> and a selection bias recovery
 algorithm by Bareinboim, E. and Tian, J. (2015)
 <http://ftp.cs.ucla.edu/pub/stat_ser/r445.pdf>. All of the previously mentioned
 algorithms are based on a causal effect identification algorithm by
 Tian , J. (2002) <http://ftp.cs.ucla.edu/pub/stat_ser/r309.pdf>.",2021-06-14,Santtu Tikka,https://github.com/santikka/causaleffect/,TRUE,https://github.com/santikka/causaleffect,31146,14,2021-06-13T14:55:18Z,2224.714285714286
CausalMBSTS,"Infers the causal effect of an intervention on a multivariate response through the use of Multivariate 
    Bayesian Structural Time Series models (MBSTS) as described in Menchetti & Bojinov (2020) <arXiv:2006.12269>. 
    The package also includes functions for model building and forecasting.  ",2020-11-03,Fiammetta Menchetti,NA,TRUE,https://github.com/fmenchetti/causalmbsts,3812,8,2020-11-10T14:16:31Z,476.5
causaloptim,"When causal quantities are not identifiable from the observed data, it still may be possible 
            to bound these quantities using the observed data. We outline a class of problems for which the 
            derivation of tight bounds is always a linear programming problem and can therefore, at least 
            theoretically, be solved using a symbolic linear optimizer. We extend and generalize the 
            approach of Balke and Pearl (1994) <doi:10.1016/B978-1-55860-332-5.50011-0> and we provide 
            a user friendly graphical interface for setting up such problems via directed acyclic 
            graphs (DAG), which only allow for problems within this class to be depicted. The user can 
            then define linear constraints to further refine their assumptions to meet their specific 
            problem, and then specify a causal query using a text interface. The program converts this 
            user defined DAG, query, and constraints, and returns tight bounds. The bounds can be 
            converted to R functions to evaluate them for specific datasets, and to latex code for 
            publication. The methods and proofs of tightness and validity of the bounds are described in
            a preprint by Sachs, Gabriel, and Sjölander (2020) 
            <https://sachsmc.github.io/causaloptim/articles/CausalBoundsMethods.pdf>.",2021-06-09,Michael C Sachs,https://github.com/sachsmc/causaloptim,TRUE,https://github.com/sachsmc/causaloptim,9599,11,2021-06-09T09:12:28Z,872.6363636363636
CBDA,"Classification performed on Big Data. It uses concepts from compressive sensing, and implements ensemble predictor (i.e., 'SuperLearner') and knockoff filtering as the main machine learning and feature mining engines.",2018-04-16,Simeone Marino,https://github.com/SOCR/CBDA,TRUE,https://github.com/socr/cbda,14828,14,2021-01-11T16:02:42Z,1059.142857142857
cbsodataR,"The data and meta data from Statistics
    Netherlands (<https://www.cbs.nl>) can be browsed and downloaded. The client uses
    the open data API of Statistics Netherlands.",2020-09-14,Edwin de Jonge,https://github.com/edwindj/cbsodataR,TRUE,https://github.com/edwindj/cbsodatar,31438,23,2021-05-31T22:00:39Z,1366.8695652173913
cchsflow,"Supporting the use of the Canadian Community Health Survey 
             (CCHS) by transforming variables from each cycle into harmonized, 
             consistent versions that span survey cycles (currently, 2001 to 
             2014). CCHS data used in this library is accessed and adapted in 
             accordance to the Statistics Canada Open Licence Agreement. This 
             package uses rec_with_table(), which was developed from 'sjmisc' 
             rec(). Lüdecke D (2018). ""sjmisc: Data and Variable Transformation 
             Functions"". Journal of Open Source Software, 3(26), 754. 
             <doi:10.21105/joss.00754>.",2021-06-23,Doug Manuel,https://github.com/Big-Life-Lab/cchsflow,TRUE,https://github.com/big-life-lab/cchsflow,10684,10,2021-08-20T12:32:28Z,1068.4
ccid,"Provides efficient implementation of the Cross-Covariance
    Isolate Detect (CCID) methodology for the estimation of the number
    and location of multiple change-points in the second-order
    (cross-covariance or network) structure of multivariate, possibly
    high-dimensional time series. The method is motivated by the detection
    of change points in functional connectivity networks for functional
    magnetic resonance imaging (fMRI), electroencephalography (EEG),
    magentoencephalography (MEG) and electrocorticography (ECoG) data. The
    main routines in the package have been extensively tested on fMRI data. 
    For details on the CCID methodology, please see Anastasiou et
    al (2020) <doi:10.1101/2020.12.20.423696>.",2021-01-07,Andreas Anastasiou,https://github.com/Anastasiou-Andreas/ccid,TRUE,https://github.com/anastasiou-andreas/ccid,2918,0,2020-12-22T06:51:36Z,NA
ccmReportR,Provides a set of functions to perform queries against the 'CCM' API <https://mohcontacttracing.my.salesforce.com>.,2021-02-11,James Lane,https://github.com/DurhamRegionHARP/ccmReportR,TRUE,https://github.com/durhamregionharp/ccmreportr,3241,1,2021-02-16T17:31:23Z,3241
ccrtm,"A set of radiative transfer models to quantitatively describe the absorption, reflectance and transmission of solar energy in vegetation, and model remotely sensed spectral signatures of vegetation at distinct spatial scales (leaf,canopy and stand). The main principle behind ccrtm is that many radiative transfer models can form a coupled chain, basically models that feed into each other in a linked chain (from leaf, to canopy, to stand, to atmosphere). It allows the simulation of spectral datasets in the solar spectrum (400-2500nm) using leaf models as PROSPECT5, 5b, and D which can be coupled with canopy models as 'FLIM', 'SAIL' and 'SAIL2'. Currently, only a simple atmospheric model ('skyl') is implemented. Jacquemoud et al 2008 provide the most comprehensive overview of these models <doi:10.1016/j.rse.2008.01.026>. ",2021-02-26,Marco D. Visser,https://github.com/MarcoDVisser/ccrtm,TRUE,https://github.com/marcodvisser/ccrtm,2239,3,2021-07-21T07:29:13Z,746.3333333333334
ccTensor,"CUR/CX decomposition factorizes a matrix into two factor matrices and Multidimensional CX Decomposition factorizes a tensor into a core tensor and some factor matrices. See the reference section of GitHub README.md <https://github.com/rikenbit/ccTensor>, for details of the methods.",2021-08-12,Koki Tsuyuzaki,https://github.com/rikenbit/ccTensor,TRUE,https://github.com/rikenbit/cctensor,613,0,2021-08-02T04:59:36Z,NA
cdata,"Supplies higher-order coordinatized data specification and fluid transform operators that include pivot and anti-pivot as special cases. 
    The methodology is describe in 'Zumel', 2018, ""Fluid data reshaping with 'cdata'"", <https://winvector.github.io/FluidData/FluidDataReshapingWithCdata.html> , <DOI:10.5281/zenodo.1173299> .
    This package introduces the idea of explicit control table specification of data transforms.
    Works on in-memory data or on remote data using 'rquery' and 'SQL' database interfaces.",2021-06-12,John Mount,"https://github.com/WinVector/cdata/,
https://winvector.github.io/cdata/",TRUE,https://github.com/winvector/cdata,74653,43,2021-06-12T00:09:56Z,1736.1162790697674
CDatanet,"Likelihood-based estimation and data generation from a class of models used to estimate peer effects on count data by controlling for the network endogeneity. This class includes count data models with social interactions (Houndetoungan 2020; <doi:10.2139/ssrn.3721250>), spatial tobit models (Xu and Lee 2015; <doi:10.1016/j.jeconom.2015.05.004>), and spatial linear-in-means models (Lee 2004; <doi:10.1111/j.1468-0262.2004.00558.x>).  ",2021-02-17,Elysée Aristide Houndetoungan,https://github.com/ahoundetoungan/CDatanet,TRUE,https://github.com/ahoundetoungan/cdatanet,2389,0,2021-03-02T02:36:46Z,NA
cdcatR,"Provides a set of functions for conducting cognitive diagnostic computerized adaptive testing applications (Chen (2009) <doi:10.1007/s11336-009-9123-2>). It includes different item selection rules such us the global discrimination index (Kaplan, de la Torre, and Barrada (2015) <doi:10.1177/0146621614554650>) and the nonparametric selection method (Chang, Chiu, and Tsai (2019) <doi:10.1177/0146621618813113>), as well as several stopping rules. Functions for generating item banks and responses are also provided. To guide item bank calibration, model comparison at the item level can be conducted using the two-step likelihood ratio test statistic by Sorrel, de la Torre, Abad and Olea (2017) <doi:10.1027/1614-2241/a000131>.",2021-07-06,Miguel A. Sorrel,https://github.com/miguel-sorrel/cdcatR,TRUE,https://github.com/miguel-sorrel/cdcatr,6610,3,2021-07-06T08:52:07Z,2203.3333333333335
cdcfluview,"The 'U.S.' Centers for Disease Control and Prevention (CDC) maintain
    a portal <https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html> for
    accessing state, regional and national influenza statistics as well as
    mortality surveillance data. The web interface makes it difficult and
    time-consuming to select and retrieve influenza data. Tools are provided 
    to access the data provided by the portal's underlying 'API'.",2021-05-22,Bob Rudis,https://github.com/hrbrmstr/cdcfluview,TRUE,https://github.com/hrbrmstr/cdcfluview,26293,50,2021-05-22T12:01:03Z,525.86
cder,"Connect to the California Data Exchange Center (CDEC) 
    Web Service <http://cdec.water.ca.gov/>. 'CDEC' provides a centralized 
    database to store, process, and exchange real-time hydrologic information 
    gathered by various cooperators throughout California. The 'CDEC' Web Service 
    <http://cdec.water.ca.gov/dynamicapp/wsSensorData> provides a data download 
    service for accessing historical records. ",2020-01-24,Michael Koohafkan,https://github.com/mkoohafkan/cder,TRUE,https://github.com/mkoohafkan/cder,12047,1,2020-10-28T16:26:03Z,12047
CDM,"
    Functions for cognitive diagnosis modeling and multidimensional item response modeling 
    for dichotomous and polytomous item responses. This package enables the estimation of 
    the DINA and DINO model (Junker & Sijtsma, 2001, <doi:10.1177/01466210122032064>),
    the multiple group (polytomous) GDINA model (de la Torre, 2011, 
    <doi:10.1007/s11336-011-9207-7>), the multiple choice DINA model (de la Torre, 2009, 
    <doi:10.1177/0146621608320523>), the general diagnostic model (GDM; von Davier, 2008, 
    <doi:10.1348/000711007X193957>), the structured latent class model (SLCA; Formann, 1992, 
    <doi:10.1080/01621459.1992.10475229>) and regularized latent class analysis 
    (Chen, Li, Liu, & Ying, 2017, <doi:10.1007/s11336-016-9545-6>). 
    See George, Robitzsch, Kiefer, Gross, and Uenlue (2017) <doi:10.18637/jss.v074.i02> 
    or Robitzsch and George (2019, <doi:10.1007/978-3-030-05584-4_26>)     
    for further details on estimation and the package structure.
    For tutorials on how to use the CDM package see 
    George and Robitzsch (2015, <doi:10.20982/tqmp.11.3.p189>) as well as
    Ravand and Robitzsch (2015).",2020-03-10,Alexander Robitzsch,"https://github.com/alexanderrobitzsch/CDM,
https://sites.google.com/site/alexanderrobitzsch2/software",TRUE,https://github.com/alexanderrobitzsch/cdm,159794,13,2021-08-11T13:55:11Z,12291.846153846154
cdmTools,"Provides useful tools for cognitive diagnosis modeling (CDM). The packages includes the discrete factor loading method for Q-matrix estimation (Wang, Song, & Ding, 2018, <doi:10.1007/978-3-319-77249-3_29>) and the Hull method for Q-matrix validation (Nájera, Sorrel, de la Torre, & Abad, 2021, <doi:10.1111/bmsp.12228>). It also provides dimensionality assessment procedures for determining the number of attributes underlying CDM data, including parallel analysis and automated CDM fit comparison as explored in Nájera, Abad, and Sorrel (2021, <doi:10.3389/fpsyg.2021.614470>). Lastly, the package provides some useful functions for CDM simulation studies, such as random Q-matrix generation and detection of complete/identified Q-matrices.",2021-05-13,Pablo Nájera,https://github.com/pablo-najera/cdmTools,TRUE,https://github.com/pablo-najera/cdmtools,1105,1,2021-05-12T08:22:14Z,1105
CDSeq,Estimate cell-type-specific gene expression profiles and sample-specific cell-type proportions simultaneously using bulk sequencing data. Kang et al. (2019) <doi:10.1371/journal.pcbi.1007510>. ,2021-02-10,Kai Kang,"https://github.com/kkang7/CDSeq_R_Package,
https://doi.org/10.1371/journal.pcbi.1007510",TRUE,https://github.com/kkang7/cdseq_r_package,2891,6,2021-05-03T04:41:00Z,481.8333333333333
cecs,"Goal of this package is to provide access to benchmark functions defined for
    the Special Session and Competition on Real-Parameter Single Objective Optimization in one place.
    The package contains functions from following years: 2013, 2014, 2015,
    2017, 2019, 2021 (<https://github.com/P-N-Suganthan>). 
    Implementations of CEC-2013 (Y. Gonzalez-Fernandez & M. Zambrano-Bigiarini) and CEC2017 (D. Jagodziński) are taken from existed R packages. Also, 
    the original C source code has been cleaned and reorganized for better readability.",2021-08-13,Eryk Warchulski,https://github.com/ewarchul/cecs,TRUE,https://github.com/ewarchul/cecs,1423,1,2021-08-23T13:53:50Z,1423
CEDARS,"Streamlined annotation pipeline for collection and aggregation of time-to-event data in retrospective clinical studies. 'CEDARS' aims to systematize and accelerate the review of electronic health record (EHR) corpora. It accomplishes those goals by deploying natural language processing as a tool to assist detection and characterization of clinical events by human abstractors. The online user manual presents the necessary steps to install 'CEDARS', process EHR corpora and obtain clinical event dates: <https://cedars.io>.",2021-02-07,Simon Mantha,"https://cedars.io (main) https://github.com/simon-hans/CEDARS
(devel)",TRUE,https://github.com/simon-hans/cedars,3206,3,2021-08-14T17:06:40Z,1068.6666666666667
censable,"Creates a common framework for organizing, naming, and gathering population, age, race, and ethnicity data from the Census Bureau. Accesses the API <https://www.census.gov/data/developers/data-sets.html> via the package tidycensus. Provides tools for adding information to existing data to line up with Census data.",2021-07-29,Christopher T. Kenny,"https://christopherkenny.github.io/censable/,
https://github.com/christopherkenny/censable",TRUE,https://github.com/christopherkenny/censable,608,4,2021-07-28T21:26:05Z,152
censo2017,"Provee un acceso conveniente a mas de 17 millones de registros
    de la base de datos del Censo 2017. Los datos fueron importados desde
    el DVD oficial del INE usando el Convertidor REDATAM creado por Pablo De
    Grande. Esta paquete esta documentado intencionalmente en castellano
    asciificado para que funcione sin problema en diferentes plataformas.
    (Provides convenient access to more than 17 million records from the
    Chilean Census 2017 database. The datasets were imported from the official
    DVD provided by the Chilean National Bureau of Statistics by using the
    REDATAM converter created by Pablo De Grande and in addition it includes the
    maps accompanying these datasets.)",2021-08-16,Mauricio Vargas,https://docs.ropensci.org/censo2017/,TRUE,https://github.com/ropensci/censo2017,4021,16,2021-08-15T21:35:32Z,251.3125
censusapi,"A wrapper for the U.S. Census Bureau APIs that returns data frames of 
	Census data and metadata. Available datasets include the 
	Decennial Census, American Community Survey, Small Area Health Insurance Estimates,
	Small Area Income and Poverty Estimates, Population Estimates and Projections, and more.",2020-10-14,Hannah Recht,https://github.com/hrecht/censusapi,TRUE,https://github.com/hrecht/censusapi,82222,121,2021-06-04T23:44:30Z,679.5206611570248
censusxy,"Provides access to the U.S. Census Bureau's A.P.I for matching American
    street addresses with their longitude and latitude. This includes both single address matching
    as well as batch functionality for multiple addresses. Census geographies can be appended to 
    addresses if desired, and reverse geocoding of point locations to census geographies is also 
    supported. ",2021-02-18,Christopher Prener,https://github.com/slu-openGIS/censusxy,TRUE,https://github.com/slu-opengis/censusxy,14751,9,2021-03-23T14:59:13Z,1639
CePa,Use pathway topology information to assign weight to pathway nodes.,2020-02-25,Zuguang Gu,https://github.com/jokergoo/CePa,TRUE,https://github.com/jokergoo/cepa,26542,0,2021-07-01T13:03:59Z,NA
cepiigeodist,"Provides data on countries and their main city or agglomeration and
    the different distance measures and dummy variables indicating whether two
    countries are contiguous, share a common language or a colonial
    relationship. The reference article for these datasets is Mayer and Zignago
    (2011) <http://www.cepii.fr/CEPII/en/publications/wp/abstract.asp?NoDoc=3877>.",2020-09-18,Mauricio Vargas,https://pacha.dev/cepiigeodist/,TRUE,https://github.com/pachamaltese/cepiigeodist,4474,2,2020-09-04T14:55:03Z,2237
cepR,"
    Retorna detalhes de dados de CEPs brasileiros, bairros, logradouros 
    e tal. (Returns info of Brazilian postal codes, city names, addresses 
    and so on.)",2020-09-19,Robert Myles McDonnell,https://github.com/RobertMyles/cepR,TRUE,https://github.com/robertmyles/cepr,21548,18,2020-09-19T10:18:31Z,1197.111111111111
cepreader,"Read Condensed Cornell Ecology Program ('CEP') and legacy
    'CANOCO' files into R data frames.",2021-01-05,Jari Oksanen,"https://cran.r-project.org/,
https://github.com/vegandevs/cepreader/",TRUE,https://github.com/vegandevs/cepreader,18216,0,2021-01-10T18:13:27Z,NA
ceramic,"Download imagery tiles to a standard cache and load the data into raster objects. 
 Facilities for 'AWS' terrain <https://aws.amazon.com/public-datasets/terrain/> terrain and 'Mapbox' 
 <https://www.mapbox.com/> servers are provided. ",2019-07-20,Michael Sumner,https://github.com/hypertidy/ceramic,TRUE,https://github.com/hypertidy/ceramic,12622,79,2020-12-13T22:18:24Z,159.77215189873417
ceser,Implementation of the Cluster Estimated Standard Errors (CESE) proposed in Jackson (2020) <DOI:10.1017/pan.2019.38> to compute clustered standard errors of linear coefficients in regression models with grouped data.,2020-11-09,Diogo Ferrari,https://github.com/DiogoFerrari/ceser,TRUE,https://github.com/diogoferrari/ceser,3726,2,2020-11-11T05:51:13Z,1863
CFAcoop,"Cellular cooperation compromises the plating efficiency-based 
    analysis of clonogenic survival data. This tool provides functions that 
    enable a robust analysis of colony formation assay (CFA) data in presence 
    or absence of cellular cooperation. 
    The implemented method has been described 
    in Brix et al. (2020). (Brix, N., Samaga, D., Hennel, R. et al. 
    ""The clonogenic assay: robustness of plating efficiency-based analysis is 
    strongly compromised by cellular cooperation."" Radiat Oncol 15, 248 (2020).
    <doi:10.1186/s13014-020-01697-y>)
        Power regression for parameter estimation, calculation of survival
    fractions, uncertainty analysis and plotting functions are provided.",2021-06-11,Daniel Samaga,https://github.com/ZytoHMGU/CFAcoop,TRUE,https://github.com/zytohmgu/cfacoop,3744,0,2021-06-11T08:20:35Z,NA
cfda,"Package for the analysis of categorical functional data. 
  The main purpose is to compute an encoding (real functional variable) for each state <https://hal.inria.fr/hal-02973094>. 
  It also provides functions to perform basic statistical analysis on categorical functional data. ",2021-02-12,Quentin Grimonprez,NA,TRUE,https://github.com/modal-inria/cfda,2615,1,2021-06-08T17:15:23Z,2615
cfid,"Facilitates the identification of counterfactual queries in
 structural causal models via the ID* and IDC* algorithms
 by Shpitser, I. and Pearl, J. (2008)
 <https://jmlr.org/papers/v9/shpitser08a.html>. Provides a simple interface
 for defining causal graphs and counterfactual conjunctions. Construction
 of parallel worlds graphs and counterfactual graphs is done automatically
 based on the counterfactual query and the causal graph.",2021-06-10,Santtu Tikka,https://github.com/santikka/cfid,TRUE,https://github.com/santikka/cfid,732,0,2021-08-18T15:12:05Z,NA
cforward,"Performs forward model selection, using the C-index/concordance
  in survival analysis models. ",2021-03-29,John Muschelli,https://github.com/muschellij2/cforward,TRUE,https://github.com/muschellij2/cforward,4392,0,2021-03-25T16:00:10Z,NA
cgdsr,"Provides a basic set of R functions for querying the Cancer
  Genomics Data Server (CGDS), hosted by the Computational Biology Center at
  Memorial-Sloan-Kettering Cancer Center (MSKCC) at <www.cbioportal.org>.",2019-06-26,Anders Jacobsen,https://github.com/cBioPortal/cgdsr,TRUE,https://github.com/cbioportal/cgdsr,44544,20,2020-12-15T19:16:47Z,2227.2
CGGP,"Run computer experiments using the adaptive composite grid
    algorithm with a Gaussian process model.
    The algorithm works best when running an experiment that can evaluate thousands
    of points from a deterministic computer simulation.
    This package is an implementation of a forthcoming paper by Plumlee,
    Erickson, Ankenman, et al. For a preprint of the paper,
    contact the maintainer of this package.",2021-05-08,Collin Erickson,https://github.com/CollinErickson/CGGP,TRUE,https://github.com/collinerickson/cggp,12570,2,2021-05-09T15:00:53Z,6285
CGPfunctions,Miscellaneous functions useful for teaching statistics as well as actually practicing the art. They typically are not new methods but rather wrappers around either base R or other packages.,2020-11-12,Chuck Powell,https://github.com/ibecav/CGPfunctions,TRUE,https://github.com/ibecav/cgpfunctions,36191,16,2020-11-12T14:31:17Z,2261.9375
ch,"The solution to some common problems is proposed, 
     as well as a summary of some small functions. In particular, 
     it provides a useful function for some problems in chemistry.
     For example, monoa(), monob() and mono() function can be used 
     to calculate The pH of weak acid/base. 
     The ggpng() function can save the PNG format with transparent background. 
     The period_table() function will show the periodic table. Also the
     show_ruler() function will show the ruler.
     The show_color() function is funny and easier to show colors.
      I also provide the symb() function to generate multiple symbols at once.
     The csv2vcf() function provides an easy method to generate a file.
     The sym2poly() and sym2coef() function can extract coefficients from 
     polynomials.",2021-07-09,Hailong Chai [aut],https://github.com/tsiamut/ch,TRUE,https://github.com/tsiamut/ch,909,1,2021-07-17T04:51:48Z,909
ChainLadder,"Various statistical methods and models which are
    typically used for the estimation of outstanding claims reserves
    in general insurance, including those to estimate the claims
    development result as required under Solvency II.",2021-01-05,Markus Gesmann,https://github.com/mages/ChainLadder,TRUE,https://github.com/mages/chainladder,99688,54,2021-01-05T12:34:46Z,1846.0740740740741
chandwich,"Performs adjustments of a user-supplied independence loglikelihood 
    function using a robust sandwich estimator of the parameter covariance 
    matrix, based on the methodology in Chandler and Bate (2007) 
    <doi:10.1093/biomet/asm015>.  This can be used for cluster correlated data 
    when interest lies in the parameters of the marginal distributions or for 
    performing inferences that are robust to certain types of model 
    misspecification.  Functions for profiling the adjusted loglikelihoods are 
    also provided, as are functions for calculating and plotting confidence 
    intervals, for single model parameters, and confidence regions, for pairs 
    of model parameters.  Nested models can be compared using an adjusted 
    likelihood ratio test.",2021-07-02,Paul J. Northrop,https://github.com/paulnorthrop/chandwich,TRUE,https://github.com/paulnorthrop/chandwich,18385,2,2021-07-03T09:47:55Z,9192.5
changepoint,"Implements various mainstream and specialised changepoint methods for finding single and multiple changepoints within data.  Many popular non-parametric and frequentist methods are included.  The cpt.mean(), cpt.var(), cpt.meanvar() functions should be your first point of call.",2016-10-04,Rebecca Killick,https://github.com/rkillick/changepoint/,TRUE,https://github.com/rkillick/changepoint,269317,94,2021-08-12T21:01:05Z,2865.074468085106
changepoint.geo,Implements the high-dimensional changepoint detection method GeomCP and the related mappings used for changepoint detection. These methods view the changepoint problem from a geometrical viewpoint and aim to extract relevant geometrical features in order to detect changepoints. The geomcp() function should be your first point of call. References: Grundy et al. (2020) <doi:10.1007/s11222-020-09940-y>. ,2020-03-31,Thomas Grundy,https://github.com/grundy95/changepoint.geo/,TRUE,https://github.com/grundy95/changepoint.geo,7791,3,2021-03-25T16:16:19Z,2597
changepoint.influence,"Allows users to input their data, segmentation and function used for the segmentation (and additional arguments) and the package calculates the influence of the data on the changepoint locations, see Wilms et al. (2021) <arXiv:2107.10572>.  Currently this can only be used with the changepoint package functions to identify changes, but we plan to extend this.  There are options for different types of graphics to assess the influence.",2021-08-04,Rebecca Killick,https://github.com/rkillick/changepoint.influence/,TRUE,https://github.com/rkillick/changepoint.influence,420,0,2021-08-03T14:52:13Z,NA
cheatsheet,A simple package to grab cheat sheets and save them to your local computer.,2021-04-13,Brad Lindblad,https://bradlindblad.github.io/cheatsheet/,TRUE,https://github.com/bradlindblad/cheatsheet,4397,4,2021-07-14T01:54:03Z,1099.25
checkdown,"Creates auto checking check-fields and check-boxes for 'rmarkdown' html. It could be used in class, when teacher share materials and tasks, so student can solve some problems and check themselves. In contrast with the 'learnr' package the 'checkdown' package works without 'shiny'.",2020-11-01,George Moroz,https://agricolamz.github.io/checkdown/,TRUE,https://github.com/agricolamz/checkdown,11037,25,2020-11-01T09:37:51Z,441.48
checkmate,"Tests and assertions to perform frequent argument checks. A
    substantial part of the package was written in C to minimize any worries
    about execution time overhead.",2020-02-06,Michel Lang,https://github.com/mllg/checkmate,TRUE,https://github.com/mllg/checkmate,8840954,188,2021-08-25T07:35:36Z,47026.35106382979
checkpoint,"The goal of checkpoint is to solve the problem of package
    reproducibility in R. Specifically, checkpoint allows you to install packages
    as they existed on CRAN on a specific snapshot date as if you had a CRAN time
    machine. To achieve reproducibility, the checkpoint() function installs the
    packages required or called by your project and scripts to a local library
    exactly as they existed at the specified point in time. Only those packages
    are available to your project, thereby avoiding any package updates that came
    later and may have altered your results. In this way, anyone using checkpoint's
    checkpoint() can ensure the reproducibility of your scripts or projects at any
    time. To create the snapshot archives, once a day (at midnight UTC) Microsoft
    refreshes the Austria CRAN mirror on the ""Microsoft R Archived Network""
    server (<https://mran.microsoft.com/>). Immediately after completion
    of the rsync mirror process, the process takes a snapshot, thus creating the
    archive. Snapshot archives exist starting from 2014-09-17.",2021-06-14,James Rowland-Jones,https://github.com/RevolutionAnalytics/checkpoint,TRUE,https://github.com/revolutionanalytics/checkpoint,95808,145,2021-05-25T23:55:34Z,660.744827586207
checkr,"Expressive, assertive, pipe-friendly functions 
  to check the properties of common R objects.
  In the case of failure the functions issue informative error messages.",2019-04-25,Joe Thorley,https://github.com/poissonconsulting/checkr,TRUE,https://github.com/poissonconsulting/checkr,33549,11,2021-07-06T15:48:41Z,3049.909090909091
cheddar,"Provides a flexible, extendable representation of an ecological community and a range of functions for analysis and visualisation, focusing on food web, body mass and numerical abundance data. Allows inter-web comparisons such as examining changes in community structure over environmental, temporal or spatial gradients.",2020-02-13,Lawrence Hudson with contributions from Dan Reuman and Rob Emerson,https://github.com/quicklizard99/cheddar/,TRUE,https://github.com/quicklizard99/cheddar,30953,13,2021-09-02T15:22:31Z,2381
cheese,"Contains tools for working with data during statistical analysis, promoting flexible, intuitive, and reproducible workflows. There are functions designated for specific statistical tasks such building a custom univariate descriptive table, computing pairwise association statistics, etc. These are built on a collection of data manipulation tools designed for general use that are motivated by the functional programming concept.",2020-10-19,Alex Zajichek,"https://zajichek.github.io/cheese,
https://github.com/zajichek/cheese",TRUE,https://github.com/zajichek/cheese,15238,0,2021-06-05T14:36:07Z,NA
chemCal,"Simple functions for plotting linear
	calibration functions and estimating standard errors for measurements
	according to the Handbook of Chemometrics and Qualimetrics: Part A
	by Massart et al. (1997) There are also functions estimating the limit
	of detection (LOD) and limit of quantification (LOQ).
	The functions work on model objects from - optionally weighted - linear
	regression (lm) or robust linear regression ('rlm' from the 'MASS' package).",2021-04-17,Johannes Ranke,"https://pkgdown.jrwb.de/chemCal/,
https://cgit.jrwb.de/chemCal/about",TRUE,https://github.com/jranke/chemcal,29694,3,2021-04-17T05:19:08Z,9898
chemmodlab,"Contains a set of methods for fitting models and methods for
    validating the resulting models. The statistical methodologies comprise
    a comprehensive collection of approaches whose validity and utility have
    been accepted by experts in the Cheminformatics field. As promising new
    methodologies emerge from the statistical and data-mining communities, they
    will be incorporated into the laboratory. These methods are aimed at discovering
    quantitative structure-activity relationships (QSARs). However, the user can
    directly input their own choices of descriptors and responses, so the capability
    for comparing models is effectively unlimited.",2020-09-07,Jeremy Ash,https://github.com/jrash/ChemModLab,TRUE,https://github.com/jrash/chemmodlab,17631,10,2021-08-31T02:47:15Z,1763.1
ChemoSpec,"A collection of functions for top-down exploratory data analysis
    of spectral data including nuclear magnetic resonance (NMR), infrared (IR),
    Raman, X-ray fluorescence (XRF) and other similar types of spectroscopy.
    Includes functions for plotting and inspecting spectra, peak alignment,
    hierarchical cluster analysis (HCA), principal components analysis (PCA) and
    model-based clustering. Robust methods appropriate for this type of
    high-dimensional data are available. ChemoSpec is designed for structured
    experiments, such as metabolomics investigations, where the samples fall into
    treatment and control groups. Graphical output is formatted consistently for
    publication quality plots. ChemoSpec is intended to be very user friendly and
    to help you get usable results quickly. A vignette covering typical operations
    is available.",2021-07-06,Bryan A. Hanson,https://bryanhanson.github.io/ChemoSpec/,TRUE,https://github.com/bryanhanson/chemospec,50510,37,2021-09-01T18:40:05Z,1365.1351351351352
ChemoSpec2D,"A collection of functions for exploratory chemometrics of 2D spectroscopic data sets such as COSY (correlated spectroscopy) and HSQC (heteronuclear single quantum coherence) 2D NMR (nuclear magnetic resonance) spectra. 'ChemoSpec2D' deploys methods aimed primarily at classification of samples and the identification of spectral features which are important in distinguishing samples from each other. Each 2D spectrum (a matrix) is treated as the unit of observation, and thus the physical sample in the spectrometer corresponds to the  sample from a statistical perspective.  In addition to chemometric tools, a few tools are provided for plotting 2D spectra, but these are not intended to replace the functionality typically available on the spectrometer. 'ChemoSpec2D' takes many of its cues from 'ChemoSpec' and tries to create consistent graphical output and to be very user friendly.",2021-03-16,Bryan A. Hanson,https://github.com/bryanhanson/ChemoSpec2D,TRUE,https://github.com/bryanhanson/chemospec2d,17224,2,2021-03-17T03:02:17Z,8612
ChemoSpecUtils,Functions supporting the common needs of packages 'ChemoSpec' and 'ChemoSpec2D'.,2021-03-15,Bryan A. Hanson,https://github.com/bryanhanson/ChemoSpecUtils,TRUE,https://github.com/bryanhanson/chemospecutils,32032,2,2021-09-01T17:34:13Z,16016
cherryblossom,"Race results of the Cherry Blossom Run, which is an annual road race that takes place in Washington, DC.",2020-06-25,Mine Çetinkaya-Rundel,https://github.com/OpenIntroStat/cherryblossom,TRUE,https://github.com/openintrostat/cherryblossom,73899,4,2021-09-02T13:17:34Z,18474.75
chess,"This is an opinionated wrapper around the
    python-chess package. It allows users to read and write PGN files as
    well as create and explore game trees such as the ones seen in chess
    books.",2020-12-04,C. Lente,https://github.com/curso-r/chess,TRUE,https://github.com/curso-r/chess,3928,51,2021-03-26T19:59:05Z,77.01960784313725
childdevdata,"Measuring child development starts by collecting responses to 
    developmental milestones, such as ""able to sit"" or ""says two words"". 
    There are many ways to combine such responses into summaries. 
    The package bundles publicly available datasets with individual milestone 
    data for children aged 0-5 years, with the aim of supporting the construction, 
    evaluation, validation and interpretation of methodologies that aggregate 
    milestone data into informative measures of child development.",2021-04-15,Stef van Buuren,"https://github.com/d-score/childdevdata,
https://d-score.org/childdevdata/",TRUE,https://github.com/d-score/childdevdata,1673,0,2021-04-19T10:13:02Z,NA
chilemapas,"Mapas terrestres con topologias simplificadas. Estos mapas no 
  tienen precision geodesica, por lo que aplica el DFL-83 de 1979 de la Republica
  de Chile y se consideran referenciales sin validez legal.
  No se incluyen los territorios antarticos y bajo ningun evento estos mapas
  significan que exista una cesion u ocupacion de territorios soberanos en
  contra del Derecho Internacional por parte de Chile. Esta paquete esta 
  documentado intencionalmente en castellano asciificado para que funcione sin 
  problema en diferentes plataformas.
  (Terrestrial maps with simplified toplogies. These maps lack geodesic
  precision, therefore DFL-83 1979 of the Republic of Chile applies and are
  considered to have no legal validity.
  Antartic territories are excluded and under no event these maps mean
  there is a cession or occupation of sovereign territories against International
  Laws from Chile. This package was intentionally documented in asciified
  spanish to make it work without problem on different platforms.)",2020-03-28,Mauricio Vargas,https://pachamaltese.github.io/chilemapas/,TRUE,https://github.com/pachamaltese/chilemapas,13044,20,2020-10-30T17:54:28Z,652.2
ChineseNames,"
  A database of Chinese surnames and Chinese given names (1930-2008).
  This database contains nationwide frequency statistics of
  1,806 Chinese surnames and 2,614 Chinese characters used in given names,
  covering about 1.2 billion Han Chinese population
  (96.8% of the Han Chinese household-registered population
  born from 1930 to 2008 and still alive in 2008).
  This package also contains a function for computing multiple features of
  Chinese surnames and Chinese given names for scientific research (e.g.,
  name uniqueness, name gender, name valence, and name warmth/competence).",2021-06-21,Han-Wu-Shuang Bao,https://github.com/psychbruce/ChineseNames,TRUE,https://github.com/psychbruce/chinesenames,1934,74,2021-06-21T16:05:59Z,26.135135135135137
chipPCR,"A collection of functions to pre-process amplification curve data from polymerase chain reaction (PCR) or isothermal amplification reactions. Contains functions to normalize and baseline amplification curves, to detect both the start and end of an amplification reaction, several smoothers (e.g., LOWESS, moving average, cubic splines, Savitzky-Golay), a function to detect false positive amplification reactions and a function to determine the amplification efficiency. Quantification point (Cq) methods include the first (FDM) and second approximate derivative maximum (SDM) methods (calculated by a 5-point-stencil) and the cycle threshold method. Data sets of experimental nucleic acid amplification systems ('VideoScan HCU', capillary convective PCR (ccPCR)) and commercial systems are included. Amplification curves were generated by helicase dependent amplification (HDA), ccPCR or PCR. As detection system intercalating dyes (EvaGreen, SYBR Green) and hydrolysis probes (TaqMan) were used. For more information see: Roediger et al. (2015) <doi:10.1093/bioinformatics/btv205>. ",2021-03-05,Stefan Roediger,https://github.com/PCRuniversum/chipPCR,TRUE,https://github.com/pcruniversum/chippcr,28145,4,2021-02-27T20:04:47Z,7036.25
chirps,"API Client for the Climate Hazards Group InfraRed Precipitation
  with Station Data 'CHIRPS'. The 'CHIRPS' data is a 35+ year quasi-global
  rainfall data set, which incorporates 0.05 arc-degrees resolution satellite
  imagery, and in-situ station data to create gridded rainfall time series for
  trend analysis and seasonal drought monitoring. For more details on 'CHIRPS'
  data please visit its official home page <https://www.chc.ucsb.edu/data/chirps>.
  Requests from large time series (> 10 years) and large geographic coverage 
  (global scale) may take several minutes.",2020-07-13,Kauê de Sousa,https://docs.ropensci.org/chirps/,TRUE,https://github.com/ropensci/chirps,7055,14,2021-08-29T13:13:14Z,503.92857142857144
chk,"For developers to check user-supplied function arguments.  It
    is designed to be simple, fast and customizable.  Error messages
    follow the tidyverse style guide.",2021-07-04,Joe Thorley,https://github.com/poissonconsulting/chk,TRUE,https://github.com/poissonconsulting/chk,55426,31,2021-09-02T13:58:47Z,1787.9354838709678
chlorpromazineR,"As different antipsychotic medications have different potencies,
    the doses of different medications cannot be directly compared. Various
    strategies are used to convert doses into a common reference so that
    comparison is meaningful. Chlorpromazine (CPZ) has historically been used
    as a reference medication into which other antipsychotic doses can be
    converted, as ""chlorpromazine-equivalent doses"". Using conversion keys
    generated from widely-cited scientific papers, e.g. Gardner et. al 2010
    <doi:10.1176/appi.ajp.2009.09060802> and Leucht et al. 2016
    <doi:10.1093/schbul/sbv167>, antipsychotic doses are converted
    to CPZ (or any specified antipsychotic) equivalents. The use of the package
    is described in the included vignette. Not for clinical use.",2021-03-14,Eric Brown,"https://docs.ropensci.org/chlorpromazineR/,
https://github.com/ropensci/chlorpromazineR",TRUE,https://github.com/ropensci/chlorpromaziner,10675,6,2021-03-13T22:16:48Z,1779.1666666666667
CHNCapitalStock,"Compute Chinese capital stocks in provinces level, based on Zhang (2008) <DOI:10.1080/14765280802028302>. ",2020-10-26,Pu Chen,https://github.com/common2016/CapitalStock,TRUE,https://github.com/common2016/capitalstock,5677,1,2021-02-14T11:00:32Z,5677
CHOIRBM,"Collection of utility functions for visualizing
    body map data collected with the Collaborative Health Outcomes
    Information Registry.",2021-02-15,Eric Cramer,https://github.com/emcramer/CHOIRBM,TRUE,https://github.com/emcramer/choirbm,2392,2,2021-08-27T23:40:11Z,1196
cholera,"Amends errors, augments data and aids analysis of John Snow's map
  of the 1854 London cholera outbreak.",2021-04-22,Peter Li,https://github.com/lindbrook/cholera,TRUE,https://github.com/lindbrook/cholera,21316,113,2021-09-02T23:16:18Z,188.63716814159292
chorrrds,"Extracts music chords from the 'CifraClub' website <https://www.cifraclub.com.br/>.
	The package also has functions for cleaning the extracted data and 
	feature extraction.",2020-06-30,Bruna Wundervald,https://github.com/r-music/chorrrds,TRUE,https://github.com/r-music/chorrrds,21527,94,2020-09-22T10:33:02Z,229.01063829787233
chromer,"A programmatic interface to the Chromosome Counts Database
    (http://ccdb.tau.ac.il/). This package is part of the rOpenSci suite
    (http://ropensci.org)",2015-01-13,Matthew Pennell,http://www.github.com/ropensci/chromer,TRUE,https://github.com/ropensci/chromer,18525,7,2020-10-28T10:16:13Z,2646.4285714285716
chronosphere,"The purpose of the 'chronosphere' project is to facilitate spatially explicit analyses of (paleo)environmental/ecological research. The package serves as a gateway to plate tectonic reconstructions, deep time global climate model results as well as fossil occurrence datasets such as the Paleobiology Database <https://paleobiodb.org/> and the PaleoReefs Database <https://www.paleo-reefs.pal.uni-erlangen.de/>. Environmental data stored on a remote server can be downloaded and imported directly to the R environment. Query functions to the GPlates <https://www.gplates.org/> desktop application or the GPlates Web Service <https://gws.gplates.org/> allow users to reconstruct coordinates, static plates, and Spatial objects. A wrapper class 'RasterArray' is implemented around the 'RasterLayer' class, allowing the organization of spatially explicit raster data in n-dimensional arrays. The project is developed under the umbrella of the DFG (Deutsche Forschungsgemeinschaft) Research Unit TERSANE2 (For 2332, TEmperature Related Stressors in ANcient Extinctions).",2021-04-18,Adam T. Kocsis,NA,TRUE,https://github.com/chronosphere-portal/r_package,12519,3,2021-04-28T09:21:00Z,4173
chunked,"Data stored in text file can be processed chunkwise using 'dplyr' commands. These
    are recorded and executed per data chunk, so large files can be processed with
    limited memory using the 'LaF' package.",2020-11-03,Edwin de Jonge,https://github.com/edwindj/chunked,TRUE,https://github.com/edwindj/chunked,23137,154,2021-06-09T12:15:44Z,150.24025974025975
CICA,Clustering multi-subject resting state functional Magnetic Resonance Imaging data. This methods enables the clustering of subjects based on multi-subject resting state functional Magnetic Resonance Imaging data. Objects are clustered based on similarities and differences in cluster-specific estimated components obtained by Independent Component Analysis.,2021-07-01,Jeffrey Durieux,"https://hdl.handle.net/1887/35077,
https://github.com/jeffreydurieux/CICA",TRUE,https://github.com/jeffreydurieux/cica,924,1,2021-08-18T14:33:30Z,924
ciccr,"Estimation and inference methods for causal relative and attributable risk in case-control and case-population studies.
    Semiparametrically efficient estimation of the aggregated (log) odds ratio and causal inference procedures for relative and attributable risk. 
    For more details, see the paper by Jun and Lee (2020), ""Causal Inference in Case-Control Studies,"" <arXiv:2004.08318 [econ.EM]>.",2020-10-29,Sokbae Lee,https://github.com/sokbae/ciccr/,TRUE,https://github.com/sokbae/ciccr,4553,1,2021-08-07T14:10:30Z,4553
cicerone,Provide step by step guided tours of 'Shiny' applications.,2021-01-10,John Coene,https://cicerone.john-coene.com/,TRUE,https://github.com/johncoene/cicerone,11666,143,2021-07-16T12:37:08Z,81.58041958041959
ciftiTools,"CIFTI files contain brain imaging data in ""grayordinates,"" which 
    represent the gray matter as cortical surface vertices (left and right) and
    subcortical voxels (cerebellum, basal ganglia, and other deep gray matter). 
    'ciftiTools' provides a unified environment for reading, writing, 
    visualizing and manipulating CIFTI-format data. It supports the ""dscalar,"" 
    ""dlabel,"" and ""dtseries"" intents. Greyordinate data is read in as a ""xifti"" 
    object, which is structured for convenient access to the data and metadata,
    and includes support for surface geometry files to enable
    spatially-dependent functionality such as static or interactive 
    visualizations and smoothing.",2021-08-19,Amanda Mejia,https://github.com/mandymejia/ciftiTools,TRUE,https://github.com/mandymejia/ciftitools,4292,8,2021-08-19T14:19:49Z,536.5
cimir,"Connect to the California Irrigation Management 
    Information System (CIMIS) Web API. See the CIMIS main page 
    <https://cimis.water.ca.gov> and web API documentation
    <https://et.water.ca.gov> for more information.",2021-02-17,Michael Koohafkan,https://github.com/mkoohafkan/cimir,TRUE,https://github.com/mkoohafkan/cimir,15235,5,2021-02-17T21:49:06Z,3047
cinaR,"Differential analyses and Enrichment pipeline for bulk 'ATAC-seq' data
  analyses. This package combines different packages to have an ultimate package
  for both data analyses and visualization of 'ATAC-seq' data. Methods are described in 
  Karakaslar et al. (2021) <doi:10.1101/2021.03.05.434143>.",2021-04-02,Onur Karakaslar,https://github.com/eonurk/cinaR/,TRUE,https://github.com/eonurk/cinar,3880,8,2021-04-07T20:12:12Z,485
cinaRgenesets,Immune related gene sets provided along with the 'cinaR' package.,2021-01-22,Onur Karakaslar,https://github.com/eonurk/cinaR-genesets,TRUE,https://github.com/eonurk/cinar-genesets,3652,2,2021-01-28T15:13:21Z,1826
circglmbayes,"Perform a Bayesian analysis of a circular outcome General Linear
    Model (GLM), which allows regressing a circular outcome on linear and
    categorical predictors. Posterior samples are obtained by means of an MCMC
    algorithm written in 'C++' through 'Rcpp'. Estimation and credible intervals
    are provided, as well as hypothesis testing through Bayes Factors.
    See Mulder and Klugkist (2017) <doi:10.1016/j.jmp.2017.07.001>.",2021-01-22,Kees Mulder,https://github.com/keesmulder/circglmbayes,TRUE,https://github.com/keesmulder/circglmbayes,8885,3,2021-01-22T11:20:22Z,2961.6666666666665
circle,"Tools for interacting with the 'Circle CI' API
    (<https://circleci.com/docs/api/v2/>). Besides executing common tasks
    such as querying build logs and restarting builds, this package also
    helps setting up permissions to deploy from builds.",2021-04-21,Patrick Schratz,"https://docs.ropensci.org/circle/,
https://github.com/ropensci/circle",TRUE,https://github.com/ropensci/circle,7761,13,2021-04-21T07:56:34Z,597
circletyper,Enables curving text elements in 'Shiny' apps.,2021-07-17,Etienne Bacher,https://github.com/etiennebacher/circletyper,TRUE,https://github.com/etiennebacher/circletyper,2981,3,2021-07-20T20:12:46Z,993.6666666666666
circlize,"Circular layout is an efficient way for the visualization of huge 
    amounts of information. Here this package provides an implementation 
    of circular layout generation in R as well as an enhancement of available 
    software. The flexibility of the package is based on the usage of low-level 
    graphics functions such that self-defined high-level graphics can be easily 
    implemented by users for specific purposes. Together with the seamless 
    connection between the powerful computational and visual environment in R, 
    it gives users more convenience and freedom to design figures for 
    better understanding complex patterns behind multiple dimensional data. 
    The package is described in Gu et al. 2014 <doi:10.1093/bioinformatics/btu393>.",2021-06-09,Zuguang Gu,"https://github.com/jokergoo/circlize,
https://jokergoo.github.io/circlize_book/book/",TRUE,https://github.com/jokergoo/circlize,653012,676,2021-07-03T08:16:59Z,965.9940828402367
circumplex,"Circumplex models, which organize constructs in a circle around two 
    underlying dimensions, are popular for studying interpersonal functioning, 
    mood/affect, and vocational preferences/environments. This package provides 
    tools for analyzing and visualizing circular data, including scoring 
    functions for relevant instruments and a generalization of the bootstrapped 
    structural summary method from Zimmermann & Wright (2017) 
    <doi:10.1177/1073191115621795> and functions for creating publication-ready 
    tables and figures from the results.",2021-05-28,Jeffrey Girard,https://github.com/jmgirard/circumplex,TRUE,https://github.com/jmgirard/circumplex,24665,7,2021-07-12T16:13:30Z,3523.5714285714284
cit,"A likelihood-based hypothesis testing approach is implemented for
  assessing causal mediation. Described in Millstein, Chen, and Breton (2016),
  <DOI:10.1093/bioinformatics/btw135>, it could be used to test for mediation
  of a known causal association between a DNA variant, the 'instrumental variable',
  and a clinical outcome or phenotype by gene expression or DNA methylation, the
  potential mediator. Another example would be testing mediation of the effect
  of a drug on a clinical outcome by the molecular target. The hypothesis test
  generates a p-value or permutation-based FDR value with confidence intervals
  to quantify uncertainty in the causal inference. The outcome can be represented
  by either a continuous or binary variable, the potential mediator is continuous,
  and the instrumental variable can be continuous or binary and is not limited to
  a single variable but may be a design matrix representing multiple variables.",2021-05-25,Joshua Millstein,https://github.com/USCbiostats/cit,TRUE,https://github.com/uscbiostats/cit,28122,1,2021-05-21T18:34:40Z,28122
citation,"A collection of functions to extract citation information from 'R' packages and to deal with files in 'citation file format' (<https://citation-file-format.github.io/>), extending the functionality already provided by the citation() function in the 'utils' package.",2020-06-30,Jan Philipp Dietrich,"https://github.com/pik-piam/citation,
https://doi.org/10.5281/zenodo.3813429",TRUE,https://github.com/pik-piam/citation,6097,1,2021-08-27T07:48:24Z,6097
citecorp,"Client for the Open Citations Corpus (<http://opencitations.net/>).
    Includes a set of functions for getting one identifier type from another,
    as well as getting references and citations for a given identifier.",2020-04-16,Scott Chamberlain,"https://github.com/ropenscilabs/citecorp (devel),
https://docs.ropensci.org/citecorp/ (docs)",TRUE,https://github.com/ropenscilabs/citecorp,13840,11,2020-12-30T17:39:21Z,1258.1818181818182
ciTools,"Functions to append confidence intervals, prediction intervals,
    and other quantities of interest to data frames. All appended quantities
    are for the response variable, after conditioning on the model and covariates.
    This package has a data frame first syntax that allows for easy piping.
    Currently supported models include (log-) linear, (log-) linear mixed,
    generalized linear models, generalized linear mixed models, and
    accelerated failure time models.",2020-10-25,John Haman,https://github.com/jthaman/ciTools,TRUE,https://github.com/jthaman/citools,41177,101,2021-05-20T02:11:10Z,407.6930693069307
cjbart,"A tool for analyzing conjoint experiments using Bayesian Additive Regression Trees ('BART'), a machine learning method developed by Chipman, George and McCulloch (2010) <doi:10.1214/09-AOAS285>. This tool focuses specifically on estimating and visualizing the heterogeneity within marginal component effects, at the observation- and individual-level.",2021-05-25,Thomas Robinson,https://github.com/tsrobinson/cjbart,TRUE,https://github.com/tsrobinson/cjbart,1216,4,2021-05-24T09:54:42Z,304
classifierplots,"
  Generates a visualization of binary classifier performance as a grid of
  diagnostic plots with just one function call. Includes ROC curves,
  prediction density, accuracy, precision, recall and calibration plots, all using
  ggplot2 for easy modification.
  Debug your binary classifiers faster and easier!",2020-10-13,Aaron Defazio,https://github.com/adefazio/classifierplots,TRUE,https://github.com/adefazio/classifierplots,22686,46,2020-10-12T15:25:29Z,493.17391304347825
classInt,Selected commonly used methods for choosing univariate class intervals for mapping or other graphics purposes.,2020-04-07,Roger Bivand,"https://r-spatial.github.io/classInt/,
https://github.com/r-spatial/classInt/",TRUE,https://github.com/r-spatial/classint,4117355,23,2020-11-23T20:05:01Z,179015.4347826087
cld3,"Google's Compact Language Detector 3 is a neural network model for language 
    identification and the successor of 'cld2' (available from CRAN). The algorithm is still
    experimental and takes a novel approach to language detection with different properties
    and outcomes. It can be useful to combine this with the Bayesian classifier results 
    from 'cld2'. See <https://github.com/google/cld3#readme> for more information.",2021-07-28,Jeroen Ooms,"https://docs.ropensci.org/cld3/, https://github.com/ropensci/cld3
(devel) https://github.com/google/cld3 (upstream)",TRUE,https://github.com/ropensci/cld3,33608,35,2021-07-26T08:05:54Z,960.2285714285714
clean,"A wrapper around the new 'cleaner' package, that allows
  data cleaning functions for classes 'logical', 'factor', 'numeric', 
  'character', 'currency' and 'Date' to make data cleaning fast and
  easy. Relying on very few dependencies, it provides smart guessing,
  but with user options to override anything if needed.",2020-06-01,Matthijs S. Berends,https://github.com/msberends/cleaner,TRUE,https://github.com/msberends/cleaner,15451,23,2021-06-22T11:19:35Z,671.7826086956521
cleaner,"Data cleaning functions for classes logical,
  factor, numeric, character, currency and Date to make
  data cleaning fast and easy. Relying on very few dependencies, it 
  provides smart guessing, but with user options to override 
  anything if needed.",2021-06-13,Matthijs S. Berends,https://github.com/msberends/cleaner,TRUE,https://github.com/msberends/cleaner,25275,23,2021-06-22T11:19:35Z,1098.9130434782608
cleangeo,"
  Provides a set of utility tools to inspect spatial objects, facilitate
  handling and reporting of topology errors and geometry validity issues.
  Finally, it provides a geometry cleaner that will fix all geometry problems,
  and eliminate (at least reduce) the likelihood of having issues when doing
  spatial data processing.",2021-04-17,Emmanuel Blondel,https://github.com/eblondel/cleangeo,TRUE,https://github.com/eblondel/cleangeo,38688,41,2021-04-17T12:47:27Z,943.609756097561
cleanNLP,"Provides a set of fast tools for converting a textual corpus into
  a set of normalized tables. Users may make use of the 'udpipe' back end with
  no external dependencies, or two Python back ends with 'spaCy'
  <https://spacy.io> or 'CoreNLP' <https://stanfordnlp.github.io/CoreNLP/>.
  Exposed annotation tasks include tokenization, part of speech tagging, named
  entity recognition, and dependency parsing.",2020-10-13,Taylor B. Arnold,https://statsmaths.github.io/cleanNLP/,TRUE,https://github.com/statsmaths/cleannlp,33760,181,2020-10-20T00:01:46Z,186.51933701657458
cleanTS,"A reliable and efficient tool for cleaning univariate time 
    series data. It implements reliable and efficient procedures for 
    automating the process of cleaning univariate time series data. 
    The package provides integration with already developed and deployed 
    tools for missing value imputation and outlier detection. It also 
    provides a way of visualizing large time-series data in different 
    resolutions.",2021-08-26,Mayur Shende,https://github.com/Mayur1009/cleanTS,TRUE,https://github.com/mayur1009/cleants,96,2,2021-08-27T07:09:03Z,48
clevr,"Tools for evaluating link prediction and clustering algorithms 
    with respect to ground truth. Includes efficient implementations of 
    common performance measures such as pairwise precision/recall, 
    cluster homogeneity/completeness, variation of information, 
    Rand index etc.",2020-12-04,Neil Marchant,https://github.com/cleanzr/clevr,TRUE,https://github.com/cleanzr/clevr,3259,3,2021-02-09T03:38:54Z,1086.3333333333333
clhs,"Conditioned Latin hypercube sampling, as published by Minasny and McBratney (2006) <DOI:10.1016/j.cageo.2005.12.009>. This method proposes to stratify sampling in presence of ancillary data. An extension of this method, which propose to associate a cost to each individual and take it into account during the optimisation process, is also proposed (Roudier et al., 2012, <DOI:10.1201/b12728>).",2021-05-10,Pierre Roudier,https://github.com/pierreroudier/clhs/,TRUE,https://github.com/pierreroudier/clhs,29130,10,2021-08-26T01:38:51Z,2913
clifford,"A suite of routines for Clifford algebras, using the
   'Map' class of the Standard Template Library.  Canonical
   reference: Hestenes (1987, ISBN 90-277-1673-0, ""Clifford algebra
   to geometric calculus"").  Special cases including Lorentz transforms,
   quaternion multiplication, and Grassman algebra, are discussed.
   Conformal geometric algebra theory is implemented.",2020-03-08,Robin K. S. Hankin,https://github.com/RobinHankin/clifford.git,TRUE,https://github.com/robinhankin/clifford,10194,2,2021-09-02T10:06:40Z,5097
clifro,"CliFlo is a web portal to the New Zealand National Climate
    Database and provides public access (via subscription) to around 6,500
    various climate stations (see <https://cliflo.niwa.co.nz/> for more
    information). Collating and manipulating data from CliFlo
    (hence clifro) and importing into R for further analysis, exploration and
    visualisation is now straightforward and coherent. The user is required to
    have an internet connection, and a current CliFlo subscription (free) if
    data from stations, other than the public Reefton electronic weather
    station, is sought.",2021-05-24,Blake Seers,"https://docs.ropensci.org/clifro/,
https://github.com/ropensci/clifro",TRUE,https://github.com/ropensci/clifro,28686,21,2021-06-26T10:58:46Z,1366
climaemet,"Tools to download the climatic data of the Spanish Meteorological Agency (AEMET) directly from R using their API <https://opendata.aemet.es/> and create scientific graphs (climate charts, trend analysis of climate time series, temperature and precipitation anomalies maps, warming stripes graphics, climatograms, etc.).",2020-07-17,Manuel Pizarro,"https://mpizarrotig.github.io/climaemet,
https://github.com/mpizarrotig/climaemet",TRUE,https://github.com/mpizarrotig/climaemet,6125,19,2021-08-11T17:53:51Z,322.36842105263156
climate,"Automatize downloading of meteorological and hydrological data from publicly available repositories:
    OGIMET (<http://ogimet.com/index.phtml.en>), 
    University of Wyoming - atmospheric vertical profiling data (<http://weather.uwyo.edu/upperair/>),
    Polish Institute of Meterology and Water Management - National Research Institute (<https://danepubliczne.imgw.pl>),
    and National Oceanic & Atmospheric Administration (NOAA).
    This package also allows for adding geographical coordinates for each observation.",2021-05-02,Bartosz Czernecki,https://github.com/bczernecki/climate,TRUE,https://github.com/bczernecki/climate,21247,48,2021-07-21T22:15:33Z,442.6458333333333
climatrends,"Supports analysis of trends in climate change, ecological and crop modelling.",2020-05-22,Kauê de Sousa,https://agrobioinfoservices.github.io/climatrends,TRUE,https://github.com/agrobioinfoservices/climatrends,10248,1,2021-03-15T18:24:07Z,10248
ClimDown,"A suite of routines for downscaling coarse scale global
    climate model (GCM) output to a fine spatial resolution. Includes
    Bias-Corrected Spatial Downscaling (BCDS), Constructed Analogues
    (CA), Climate Imprint (CI), and Bias Correction/Constructed
    Analogues with Quantile mapping reordering (BCCAQ). Developed by
    the the Pacific Climate Impacts Consortium (PCIC), Victoria,
    British Columbia, Canada.",2021-06-25,Lee Zeman,https://www.r-project.org,TRUE,https://github.com/pacificclimate/climdown,19346,36,2021-04-09T21:57:42Z,537.3888888888889
ClimMobTools,"API client for 'ClimMob', an open source software for experimental 
    crowdsourcing citizen science under the 'tricot' approach <https://climmob.net/>. 
    Developed by van Etten et al. (2019) <doi:10.1017/S0014479716000739>, it turns the 
    research paradigm on its head; instead of a few researchers designing complicated 
    trials to compare several technologies in search of the best solutions for the 
    target environment, it enables many participants to carry out reasonably simple 
    experiments that taken together can offer even more information. 
    'ClimMobTools' enables project managers to deep explore and analyse their 
    'ClimMob' data in R.",2021-04-13,Kauê de Sousa,https://agrdatasci.github.io/ClimMobTools/,TRUE,https://github.com/agrdatasci/climmobtools,12979,2,2021-04-13T08:24:10Z,6489.5
clinDataReview,"Creation of interactive tables, listings and figures ('TLFs') 
  and associated report for exploratory analysis of data in a clinical trial, 
  e.g. for clinical oversight activities.
  Interactive figures include sunburst, treemap, scatterplot, line plot and
  barplot of counts data. 
  Interactive tables include table of summary statistics
  (as counts of adverse events, enrollment table) and listings.
  Possibility to compare data (summary table or listing) across two data batches/sets.
  A clinical data review report is created via study-specific configuration 
  files and template 'R Markdown' reports contained in the package.",2021-07-14,Laure Cougnaud,https://github.com/openanalytics/clinDataReview,TRUE,https://github.com/openanalytics/clindatareview,959,5,2021-07-14T07:44:19Z,191.8
clinmon,"
   Every research team have their own script for calculation of hemodynamic indexes. 
   This package makes it possible  to insert a long-format dataframe, and add both 
   periods of interest (trigger-periods), and delete artifacts with deleter-files.",2021-02-04,Markus Harboe Olsen,https://github.com/lilleoel/clinmon,TRUE,https://github.com/lilleoel/clinmon,5094,1,2021-08-06T06:47:14Z,5094
clinspacy,"Performs biomedical named entity recognition,
    Unified Medical Language System (UMLS) concept mapping, and negation
    detection using the Python 'spaCy', 'scispaCy', and 'medspaCy' packages, and 
    transforms extracted data into a wide format for inclusion in machine
    learning models. The development of the 'scispaCy' package is described by
    Neumann (2019) <doi:10.18653/v1/W19-5034>. The 'medspacy' package uses
    'ConText', an algorithm for determining the context of clinical statements
    described by Harkema (2009) <doi:10.1016/j.jbi.2009.05.002>. Clinspacy
    also supports entity embeddings from 'scispaCy' and UMLS 'cui2vec' concept
    embeddings developed by Beam (2018) <arXiv:1804.01486>.",2021-03-20,Karandeep Singh,https://github.com/ML4LHS/clinspacy,TRUE,https://github.com/ml4lhs/clinspacy,1989,66,2021-08-21T19:15:52Z,30.136363636363637
clintools,"
   Every research team have their own script for data management, statistics and 
   most importantly hemodynamic indices. The purpose is to standardize scripts 
   utilized in clinical research. The hemodynamic indices can be used in a long-format dataframe, 
   and add both periods of interest (trigger-periods), and delete artifacts with deleter-files. 
   Transfer function analysis (Claassen et al. (2016) <doi:10.1177/0271678X15626425>) and
   Mx (Czosnyka et al. (1996) <doi:10.1161/01.str.27.10.1829>) can be calculated using this package.",2021-08-06,Markus Harboe Olsen,https://github.com/lilleoel/clintools,TRUE,https://github.com/lilleoel/clintools,1753,1,2021-08-06T06:47:14Z,1753
clinUtils,"
 Utility functions to facilitate the import, 
 the reporting and analysis of clinical data.
 Example datasets in 'SDTM' and 'ADaM' format, containing a subset of patients/domains
 from the 'CDISC Pilot 01 study' are also available as R datasets to demonstrate
 the package functionalities.",2021-07-21,Laure Cougnaud,https://github.com/openanalytics/clinUtils,TRUE,https://github.com/openanalytics/clinutils,1262,1,2021-07-20T11:16:21Z,1262
clipr,"Simple utility functions to read from and write to
    the Windows, OS X, and X11 clipboards.",2020-10-08,Matthew Lincoln,https://github.com/mdlincoln/clipr,TRUE,https://github.com/mdlincoln/clipr,14933858,114,2020-10-08T15:28:27Z,130998.75438596492
clock,"Provides a comprehensive library for date-time manipulations
    using a new family of orthogonal date-time classes (durations, time
    points, zoned-times, and calendars) that partition responsibilities so
    that the complexities of time zones are only considered when they are
    really needed. Capabilities include: date-time parsing, formatting,
    arithmetic, extraction and updating of components, and rounding.",2021-07-22,Davis Vaughan,"https://clock.r-lib.org, https://github.com/r-lib/clock",TRUE,https://github.com/r-lib/clock,11283,59,2021-07-22T19:18:14Z,191.23728813559322
clogitLasso,"Fit a sequence of conditional logistic regression models with lasso, for small to large sized samples. Avalos, M., Pouyes, H., Grandvalet, Y., Orriols, L., & Lagarde, E. (2015) <doi:10.1186/1471-2105-16-S6-S1>.",2018-06-27,Marta Avalos,NA,TRUE,https://github.com/mavalosf/clogitlasso,17843,1,2021-01-05T13:24:46Z,17843
cloudos,"The 'CloudOS' client library for R makes it easy to interact with 
    CloudOS <https://cloudos.lifebit.ai/> in the R environment for analysis.",2021-08-09,Sangram Keshari Sahu,https://github.com/lifebit-ai/cloudos,TRUE,https://github.com/lifebit-ai/cloudos,315,1,2021-08-09T11:14:40Z,315
clubSandwich,"Provides several cluster-robust variance estimators (i.e.,
    sandwich estimators) for ordinary and weighted least squares linear regression
    models, including the bias-reduced linearization estimator introduced by Bell
    and McCaffrey (2002) 
    <https://www150.statcan.gc.ca/n1/pub/12-001-x/2002002/article/9058-eng.pdf> and 
    developed further by Pustejovsky and Tipton (2017) 
    <DOI:10.1080/07350015.2016.1247004>. The package includes functions for estimating
    the variance- covariance matrix and for testing single- and multiple-
    contrast hypotheses based on Wald test statistics. Tests of single regression
    coefficients use Satterthwaite or saddle-point corrections. Tests of multiple-
    contrast hypotheses use an approximation to Hotelling's T-squared distribution.
    Methods are provided for a variety of fitted models, including lm() and mlm
    objects, glm(), ivreg() (from package 'AER'), plm() (from package 'plm'), gls()
    and lme() (from 'nlme'), lmer() (from `lme4`), robu() (from 'robumeta'), and 
    rma.uni() and rma.mv() (from 'metafor').",2021-01-24,James Pustejovsky,https://github.com/jepusto/clubSandwich,TRUE,https://github.com/jepusto/clubsandwich,140471,39,2021-07-01T21:56:01Z,3601.8205128205127
clusrank,"Non-parametric tests (Wilcoxon rank sum test and Wilcoxon signed rank test)
       for clustered data documented in
       Jiang et. al (2020) <doi:10.18637/jss.v096.i06>.",2021-07-26,Wenjie Wang,https://github.com/wenjie2wang/clusrank,TRUE,https://github.com/wenjie2wang/clusrank,24602,0,2021-07-23T12:47:04Z,NA
ClustAssess,"A set of tools for evaluating clustering similarity across methods 
    and method stability using element-centric clustering comparison (Gates et 
    al. (2019) <doi:10.1038/s41598-019-44892-y>). Additionally, this package 
    enables data-wide assessment of clustering robustness using proportion of
    ambiguously clustered pairs (Senbabaoglu et al. (2014) 
    <doi:10.1038/srep06207>), which can be used to infer the optimal number of 
    clusters in the data.",2021-03-31,Arash Shahsavari,https://github.com/Core-Bioinformatics/ClustAssess,TRUE,https://github.com/core-bioinformatics/clustassess,1661,2,2021-05-13T16:32:04Z,830.5
clustcurv,"A method for determining groups in multiple 
    curves with an automatic selection of their number based on k-means or 
    k-medians algorithms. The selection of the optimal number is provided by 
    bootstrap methods. The methodology can be applied both in regression and survival framework.
     Implemented methods are:
    Grouping multiple survival curves described by Villanueva et al. (2018) <doi:10.1002/sim.8016>.",2021-01-10,Nora M. Villanueva,https://github.com/noramvillanueva/clustcurv,TRUE,https://github.com/noramvillanueva/clustcurv,13531,2,2020-12-28T12:15:36Z,6765.5
Cluster.OBeu,"Estimate and return the needed parameters for visualisations designed for 'OpenBudgets' <http://openbudgets.eu/> data. Calculate cluster analysis measures in Budget data of municipalities across Europe, according to the 'OpenBudgets' data model. It involves a set of techniques and algorithms used to find and divide the data into groups of similar observations. Also, can be used generally to extract visualisation parameters convert them to 'JSON' format and use them as input in a different graphical interface.",2019-12-17,Kleanthis Koupidis,https://github.com/okgreece/Cluster.OBeu,TRUE,https://github.com/okgreece/cluster.obeu,16581,2,2021-09-02T13:47:10Z,8290.5
ClusterBootstrap,Provides functionality for the analysis of clustered data using the cluster bootstrap. ,2021-02-16,Mathijs Deen,https://github.com/mathijsdeen/ClusterBootstrap,TRUE,https://github.com/mathijsdeen/clusterbootstrap,21251,1,2021-02-16T11:17:32Z,21251
clustermole,Assignment of cell type labels to single-cell RNA sequencing (scRNA-seq) clusters is often a time-consuming process that involves manual inspection of the cluster marker genes complemented with a detailed literature search. This is especially challenging when unexpected or poorly described populations are present. The clustermole R package provides methods to query thousands of human and mouse cell identity markers sourced from a variety of databases.,2021-01-26,Igor Dolgalev,"https://igordot.github.io/clustermole/,
https://github.com/igordot/clustermole",TRUE,https://github.com/igordot/clustermole,10802,6,2021-01-29T21:07:32Z,1800.3333333333333
clustermq,"Evaluate arbitrary function calls using workers on HPC schedulers
    in single line of code. All processing is done on the network without
    accessing the file system. Remote schedulers are supported via SSH.",2020-07-13,Michael Schubert,https://mschubert.github.io/clustermq/,TRUE,https://github.com/mschubert/clustermq,52684,121,2021-06-24T08:48:32Z,435.40495867768595
ClusterR,"Gaussian mixture models, k-means, mini-batch-kmeans, k-medoids and affinity propagation clustering with the option to plot, validate, predict (new data) and estimate the optimal number of clusters. The package takes advantage of 'RcppArmadillo' to speed up the computationally intensive parts of the functions. For more information, see (i) ""Clustering in an Object-Oriented Environment"" by Anja Struyf, Mia Hubert, Peter Rousseeuw (1997), Journal of Statistical Software, <doi:10.18637/jss.v001.i04>; (ii) ""Web-scale k-means clustering"" by D. Sculley (2010), ACM Digital Library, <doi:10.1145/1772690.1772862>; (iii) ""Armadillo: a template-based C++ library for linear algebra"" by Sanderson et al (2016), The Journal of Open Source Software, <doi:10.21105/joss.00026>; (iv) ""Clustering by Passing Messages Between Data Points"" by Brendan J. Frey and Delbert Dueck, Science 16 Feb 2007: Vol. 315, Issue 5814, pp. 972-976, <doi:10.1126/science.1136800>.",2021-05-21,Lampros Mouselimis,https://github.com/mlampros/ClusterR,TRUE,https://github.com/mlampros/clusterr,178146,60,2021-05-21T10:43:54Z,2969.1
ClusTorus,"Provides various tools of for clustering multivariate angular 
  data on the torus. The package provides angular 
  adaptations of usual clustering methods such as the k-means 
  clustering, pairwise angular distances, which can be used as an 
  input for distance-based clustering algorithms, and implements
  clustering based on the conformal prediction framework. Options 
  for the conformal scores include scores based on a kernel density 
  estimate, multivariate von Mises mixtures, and naive k-means clusters. 
  Moreover, the package provides some basic data handling tools for 
  angular data.",2021-07-26,Seungki Hong,https://github.com/sungkyujung/ClusTorus,TRUE,https://github.com/sungkyujung/clustorus,3013,1,2021-07-30T02:20:37Z,3013
clustree,"Deciding what resolution to use can be a difficult question when
    approaching a clustering analysis. One way to approach this problem is to
    look at how samples move as the number of clusters increases. This package
    allows you to produce clustering trees, a visualisation for interrogating
    clusterings as resolution increases.",2020-06-14,Luke Zappia,https://github.com/lazappi/clustree,TRUE,https://github.com/lazappi/clustree,100158,107,2020-12-29T15:02:18Z,936.0560747663551
CLVTools,"
    A set of state-of-the-art probabilistic modeling approaches to derive estimates of individual customer lifetime values (CLV).
    Commonly, probabilistic approaches focus on modelling 3 processes, i.e. individuals' attrition, transaction, and spending process. 
    Latent customer attrition models, which are also known as ""buy-'til-you-die models"", model the attrition as well as the transaction process. 
    They are used to make inferences and predictions about transactional patterns of individual customers such as their future purchase behavior. 
    Moreover, these models have also been used to predict individuals’ long-term engagement in activities such as playing an online game or 
    posting to a social media platform. The spending process is usually modelled by a separate probabilistic model. Combining these results yields in 
    lifetime values estimates for individual customers.
    This package includes fast and accurate implementations of various probabilistic models for non-contractual settings 
    (e.g., grocery purchases or hotel visits). All implementations support time-invariant covariates, which can be used to control for e.g., 
    socio-demographics. If such an extension has been proposed in literature, we further provide the possibility to control for time-varying 
    covariates to control for e.g., seasonal patterns. 
    Currently, the package includes the following latent attrition models to model individuals' attrition and transaction process: 
    [1] Pareto/NBD model (Pareto/Negative-Binomial-Distribution), 
    [2] the Extended Pareto/NBD model (Pareto/Negative-Binomial-Distribution with time-varying covariates), 
    [3] the BG/NBD model (Beta-Gamma/Negative-Binomial-Distribution) and the 
    [4] GGom/NBD (Gamma-Gompertz/Negative-Binomial-Distribution). 
    Further, we provide an implementation of the Gamma/Gamma model to model the spending process of individuals.",2021-03-23,Patrick Bachmann,https://github.com/bachmannpatrick/CLVTools,TRUE,https://github.com/bachmannpatrick/clvtools,9697,23,2021-04-02T13:23:23Z,421.60869565217394
cmaRs,"An implementation of 'Conic Multivariate Adaptive Regression Splines (CMARS)' in R.
    See Weber et al. (2011) CMARS: a new contribution to nonparametric regression with 
    multivariate adaptive regression splines supported by continuous optimization, 
    <DOI:10.1080/17415977.2011.624770>. It constructs models by using the terms
    obtained from the forward step of MARS and then estimates parameters by using 
    'Tikhonov' regularization and conic quadratic optimization. It is possible to 
    construct models for prediction and binary classification. It provides performance 
    measures for the model developed. The package needs the optimisation software 'MOSEK' 
    <https://www.mosek.com/> to construct the models. Please follow the instructions in 
    'Rmosek' for the installation. ",2021-02-22,Ceyda Yazici,NA,TRUE,https://github.com/yaziciceyda/cmars,2216,0,2021-02-15T19:59:40Z,NA
cmdfun,"Writing interfaces to command line software is cumbersome. 
    'cmdfun' provides a framework for building function calls to seamlessly 
    interface with shell commands by allowing lazy evaluation of command line arguments. 
    'cmdfun' also provides methods for handling user-specific paths to tool installs or secrets like API keys. 
    Its focus is to equally serve package builders who wish to wrap command line software, and to help analysts stay inside 
    R when they might usually leave to execute non-R software.",2020-10-10,Spencer Nystrom,"https://snystrom.github.io/cmdfun/,
https://github.com/snystrom/cmdfun",TRUE,https://github.com/snystrom/cmdfun,5200,9,2020-10-12T21:53:10Z,577.7777777777778
cmfrec,"Collective matrix factorization (a.k.a. multi-view or multi-way factorization,
	Singh, Gordon, (2008) <doi:10.1145/1401890.1401969>) tries to approximate a matrix 'X' as the
	product of two low-dimensional matrices aided with secondary information matrices about rows
	and/or columns of 'X' which are also factorized using the same latent components.
	The intended usage is for recommender systems, dimensionality reduction, and missing value imputation.
	Implements extensions of the original model (Cortes, (2018) <arXiv:1809.00366>) and can produce
	different factorizations such as the weighted 'implicit-feedback' model (Hu, Koren, Volinsky,
	(2008) <doi:10.1109/ICDM.2008.22>), the 'weighted-lambda-regularization' model,
	(Zhou, Wilkinson, Schreiber, Pan, (2008) <doi:10.1007/978-3-540-68880-8_32>),
	or the enhanced model with 'implicit features' (Rendle, Zhang,
	Koren, (2019) <arXiv:1905.01395>), with or without side information. Can use gradient-based
	procedures or alternating-least squares procedures (Koren, Bell, Volinsky, (2009)
	<doi:10.1109/MC.2009.263>), with either a Cholesky solver, a faster conjugate gradient solver
	(Takacs, Pilaszy, Tikk, (2011) <doi:10.1145/2043932.2043987>), or a non-negative
	coordinate descent solver (Franc, Hlavac, Navara, (2005) <doi:10.1007/11556121_50>),
	providing efficient methods for sparse and dense data, and mixtures thereof.
	Supports L1 and L2 regularization in the main models,
	offers alternative most-popular and content-based models, and implements functionality
	for cold-start recommendations and imputation of 2D data.",2021-07-30,David Cortes,https://github.com/david-cortes/cmfrec,TRUE,https://github.com/david-cortes/cmfrec,4889,76,2021-08-28T00:28:16Z,64.32894736842105
cmmr,"CEU (CEU San Pablo University) Mass Mediator is an on-line tool for aiding researchers in 
  performing metabolite annotation. 'cmmr' (CEU Mass Mediator RESTful API) allows 
  for programmatic access in R: batch search, batch advanced search, MS/MS (tandem mass spectrometry) search, etc.
  For more information about the API Endpoint please go to <https://github.com/lzyacht/cmmr>.",2019-04-16,Yaoxiang Li,https://github.com/lzyacht/cmmr,TRUE,https://github.com/lzyacht/cmmr,12202,9,2021-06-28T20:04:58Z,1355.7777777777778
cmna,"Provides the source and examples for James P. Howard, II, 
             ""Computational Methods for Numerical Analysis with R,"" 
             <https://jameshoward.us/cmna/>, a book on numerical
             methods in R.",2021-07-14,James Howard,https://jameshoward.us/cmna/,TRUE,https://github.com/k3jph/cmna-pkg,23131,12,2021-07-14T13:18:39Z,1927.5833333333333
cmocean,"Perceptually uniform palettes for commonly used
	variables in oceanography as functions taking an integer
	and producing character vectors of colours.
	See Thyng, K.M., Greene, C.A., Hetland, R.D., Zimmerle, H.M.
	and S.F. DiMarco (2016) <doi:10.5670/oceanog.2016.66> for
	the guidelines adhered to when creating the palettes.",2020-11-18,Ivan Krylov,https://matplotlib.org/cmocean/,TRUE,https://github.com/aitap/cmocean,17794,1,2021-02-25T17:17:10Z,17794
CMplot,"Manhattan plot, a type of scatter plot, was widely used to display the association results. However, it is usually time-consuming and laborious for a non-specialist user to write scripts and adjust parameters of an elaborate plot. Moreover, the ever-growing traits measured have necessitated the integration of results from different Genome-wide association study researches. Circle Manhattan Plot is the first open R package that can lay out. Genome-wide association study P-value results in both traditional rectangular patterns, QQ-plot and novel circular ones. United in only one bull's eye style plot, association results from multiple traits can be compared interactively, thereby to reveal both similarities and differences between signals. Additional functions include: highlight signals, a group of SNPs, chromosome visualization and candidate genes around SNPs.",2020-06-30,LiLin-Yin,https://github.com/YinLiLin/CMplot,TRUE,https://github.com/yinlilin/cmplot,49980,257,2021-09-01T01:14:55Z,194.47470817120623
cmR,"Computes maximum response from Cardiac Magnetic Resonance Images using spatial and voxel wise spline based Bayesian model. This is an implementation of the methods described in Schmid (2011) <doi:10.1109/TMI.2011.2109733> ""Voxel-Based Adaptive Spatio-Temporal Modelling of Perfusion Cardiovascular MRI"". IEEE TMI 30(7) p. 1305 - 1313.",2021-06-14,Volker Schmid,https://bioimaginggroup.github.io/cmr/,TRUE,https://github.com/bioimaginggroup/cmr,3200,2,2021-06-10T09:07:47Z,1600
cms,"Uses the 'CMS' application programming interface <https://dnav.cms.gov/api/healthdata>
    to provide users databases containing yearly
    Medicare reimbursement rates in the United States. Data can be acquired
    for the entire United States or only for specific localities. Currently,
    support is only provided for the Medicare Physician Fee Schedule,
    but support will be expanded for other 'CMS' databases in future versions.",2020-09-10,Vigneshwar Subramanian,https://github.com/subramv/cms,TRUE,https://github.com/subramv/cms,4604,2,2020-11-07T05:12:26Z,2302
cmstatr,"An implementation of the statistical methods commonly
  used for advanced composite materials in aerospace applications.
  This package focuses on calculating basis values (lower tolerance
  bounds) for material strength properties, as well as performing the
  associated diagnostic tests. This package provides functions for
  calculating basis values assuming several different distributions,
  as well as providing functions for non-parametric methods of computing
  basis values. Functions are also provided for testing the hypothesis
  that there is no difference between strength and modulus data from an
  alternate sample and that from a ""qualification"" or ""baseline"" sample.
  For a discussion of these statistical methods and their use, see the
  Composite Materials Handbook, Volume 1 (2012, ISBN: 978-0-7680-7811-4).
  Additional details about this package are available in the paper by
  Kloppenborg (2020, <doi:10.21105/joss.02265>).",2021-07-01,Stefan Kloppenborg,"https://www.cmstatr.net/, https://github.com/cmstatr/cmstatr",TRUE,https://github.com/cmstatr/cmstatr,6213,1,2021-07-01T15:02:04Z,6213
cmvnorm,Various utilities for the complex multivariate Gaussian distribution.,2019-05-20,Robin K. S. Hankin,https://github.com/RobinHankin/cmvnorm.git,TRUE,https://github.com/robinhankin/cmvnorm,22267,2,2021-04-26T09:31:42Z,11133.5
CNAIM,"Implementation of the CNAIM standard in R. Contains a series of
    algorithms which determine the probability of failure, consequences of
    failure and monetary risk associated with electricity distribution
    companies' assets such as transformers and cables. Results are visualized
    in an easy-to-understand risk matrix.",2020-10-29,Emil Larsen,https://www.cnaim.io/,TRUE,https://github.com/utiligize/cnaim,4233,3,2021-03-12T15:52:54Z,1411
cNORM,"Conventional methods for producing standard scores in psychometrics or biometrics 
    are often plagued with ""jumps"" or ""gaps"" (i.e., discontinuities) in norm tables and low 
    confidence for assessing extreme scores. The continuous norming method introduced by A. 
    Lenhard et al. (2016, <doi:10.1177/1073191116656437>; 2019, <doi:10.1371/journal.pone.0222279>) and generates continuous test norm 
    scores on the basis of the raw data from standardization samples, without requiring 
    assumptions about the distribution of the raw data: Norm scores are directly established 
    from raw data by modeling the latter ones as a function of both percentile scores and an 
    explanatory variable (e.g., age). The method minimizes bias arising from sampling and 
    measurement error, while handling marked deviations from normality, addressing bottom 
    or ceiling effects and capturing almost all of the variance in the original norm data 
    sample. An online demonstration is available via <https://cnorm.shinyapps.io/cNORM/>.",2021-08-12,Wolfgang Lenhard,"https://www.psychometrica.de/cNorm_en.html,
https://github.com/WLenhard/cNORM",TRUE,https://github.com/wlenhard/cnorm,47752,1,2021-08-26T07:44:20Z,47752
cnum,"Chinese numerals processing in R, such as conversion between 
    Chinese numerals and Arabic numerals as well as detection and extraction of
    Chinese numerals in character objects and string. This package supports 
    the casual scale naming system and the respective SI prefix systems used 
    in mainland China and Taiwan: 
    ""China Statutory Measurement Units""
    State Administration for Market Regulation (2019) <http://gkml.samr.gov.cn/nsjg/jls/201902/t20190225_291134.html>
    ""Names, Definitions and Symbols of the Legal Units of Measurement and the Decimal Multiples and Submultiples"" 
    Ministry of Economic Affairs (2019) <https://gazette.nat.gov.tw/egFront/detail.do?metaid=108965>.",2021-01-11,Elgar Teo,https://github.com/elgarteo/cnum/,TRUE,https://github.com/elgarteo/cnum,10766,7,2021-01-11T16:19:06Z,1538
CNVScope,"Provides the ability to create interaction maps, discover CNV map domains (edges), gene annotate interactions, and create interactive visualizations of these CNV interaction maps.",2021-05-24,James Dalgeish,https://github.com/jamesdalg/CNVScope/,TRUE,https://github.com/jamesdalg/cnvscope,15661,4,2021-06-27T02:56:27Z,3915.25
coalitions,"An implementation of a Bayesian framework for the opinion poll
    based estimation of event probabilities in multi-party electoral systems
    (Bender and Bauer (2018) <doi:10.21105/joss.00606>).",2021-06-30,Andreas Bender,https://adibender.github.io/coalitions/,TRUE,https://github.com/adibender/coalitions,20652,17,2021-08-31T17:07:51Z,1214.8235294117646
cobalt,"Generate balance tables and plots for covariates of groups preprocessed through matching, weighting or subclassification, for example, using propensity scores. Includes integration with 'MatchIt', 'twang', 'Matching', 'optmatch', 'CBPS', 'ebal', 'WeightIt', 'cem', 'sbw', and 'designmatch' for assessing balance on the output of their preprocessing functions. Users can also specify data for balance assessment not generated through the above packages. Also included are methods for assessing balance in clustered or multiply imputed data sets or data sets with longitudinal treatments.",2021-03-30,Noah Greifer,"https://ngreifer.github.io/cobalt/,
https://github.com/ngreifer/cobalt",TRUE,https://github.com/ngreifer/cobalt,109778,42,2021-06-15T08:24:51Z,2613.7619047619046
cocktailApp,"A 'shiny' app to discover cocktails. The
    app allows one to search for cocktails by ingredient,
    filter on rating, and number of ingredients. The
    package also contains data with the ingredients of
    nearly 26 thousand cocktails scraped from the web.",2021-04-01,Steven E. Pav,https://github.com/shabbychef/cocktailApp,TRUE,https://github.com/shabbychef/cocktailapp,16356,37,2021-04-03T23:08:20Z,442.05405405405406
cocorresp,"Fits predictive and symmetric co-correspondence analysis (CoCA) models to relate one data matrix to another data matrix. More specifically, CoCA maximises the weighted covariance between the weighted averaged species scores of one community and the weighted averaged species scores of another community. CoCA attempts to find patterns that are common to both communities.",2021-05-07,"Original Matlab routines by C.J.F. ter Braak and A.P. Schaffers. R port by Gavin L. Simpson.
  Function simpls based on simpls.fit (package pls) by Ron Wehrens and Bjorn-Helge Mevik.",https://gavinsimpson.github.io/cocorresp/,TRUE,https://github.com/gavinsimpson/cocorresp,32361,3,2021-05-07T08:27:09Z,10787
codalm,"Implements the expectation-maximization (EM) algorithm as described in Fiksel et al. (2021) <doi:10.1111/biom.13465>
    for transformation-free linear regression for compositional outcomes and predictors.",2021-07-26,Jacob Fiksel,https://github.com/jfiksel/codalm,TRUE,https://github.com/jfiksel/codalm,7903,1,2020-09-24T16:40:04Z,7903
codebook,"Easily automate the following tasks to describe data frames:
		Summarise the distributions, and labelled missings of variables graphically
		and using descriptive statistics.
		For surveys, compute and summarise reliabilities (internal consistencies, 
		retest, multilevel) for psychological scales.
		Combine this information with metadata (such as item labels and labelled 
		values) that is derived from R attributes.
		To do so, the package relies on 'rmarkdown' partials, so you can generate 
		HTML, PDF, and Word documents. 
		Codebooks are also available as tables (CSV, Excel, etc.) and in JSON-LD, so
		that search engines can find your data and index the metadata.
		The metadata are also available at your fingertips via RStudio Addins.",2020-06-06,Ruben Arslan,https://github.com/rubenarslan/codebook,TRUE,https://github.com/rubenarslan/codebook,31095,112,2021-04-27T14:36:05Z,277.63392857142856
codebreaker,"Logic game in the style of the early 1980s home computers 
    that can be played in the R console. This game is inspired by 
    Mastermind, a game that became popular in the 1970s.
    Can you break the code?",2020-11-12,Roland Krasser,https://github.com/rolkra/codebreaker,TRUE,https://github.com/rolkra/codebreaker,3546,4,2020-12-25T21:44:18Z,886.5
codemeta,"The 'Codemeta' Project defines a 'JSON-LD' format
    for describing software metadata, as detailed at
    <https://codemeta.github.io>. This package provides core utilities
    to generate this metadata with a minimum of dependencies.",2021-07-22,Carl Boettiger,https://github.com/cboettig/codemeta,TRUE,https://github.com/cboettig/codemeta,648,3,2021-08-30T15:48:46Z,216
codename,"This creates code names that a user can consider for their organizations, their projects, themselves, people
    in their organizations or projects, or whatever else. The user can also supply a numeric seed (and even a character seed)
    for maximum reproducibility. Use is simple and the code names produced come in various types too, contingent on what the
    user may be desiring as a code name or nickname.",2021-06-24,Steve Miller,https://github.com/svmiller/codename,TRUE,https://github.com/svmiller/codename,2167,2,2021-08-25T15:54:23Z,1083.5
coder,"
  Fast categorization of items based on external code data identified by 
  regular expressions. A typical use case considers patient with medically coded 
  data, such as codes from the International Classification of Diseases ('ICD') or 
  the Anatomic Therapeutic Chemical ('ATC') classification system. 
  Functions of the package relies on a triad of objects: (1) case data with unit 
  id:s and possible dates of interest; (2) external code data for corresponding 
  units in (1) and with optional dates of interest and; (3) a classification 
  scheme ('classcodes' object) with regular expressions to identify and 
  categorize relevant codes from (2). 
  It is easy to introduce new classification schemes ('classcodes' objects) or  
  to use default schemes included in the package. Use cases includes patient 
  categorization based on 'comorbidity indices' such as 'Charlson', 'Elixhauser', 
  'RxRisk V', or the 'comorbidity-polypharmacy' score (CPS), as well as adverse 
  events after hip and knee replacement surgery.",2021-01-18,Erik Bulow,https://docs.ropensci.org/coder/,TRUE,https://github.com/ropensci/coder,2846,13,2021-02-02T05:46:57Z,218.92307692307693
codified,"Augment clinical data with metadata to create
    output used in conventional publications and reports.",2018-09-30,Will Beasley,"https://ouhscbbmc.github.io/codified/,
https://github.com/OuhscBbmc/codified,
https://github.com/higgi13425/nih_enrollment_table",TRUE,https://github.com/ouhscbbmc/codified,14361,2,2020-10-20T23:27:29Z,7180.5
codyn,"Univariate and multivariate temporal and spatial diversity indices, 
    rank abundance curves, and community stability measures. The functions 
    implement measures that are either explicitly temporal and include the 
    option to calculate them over multiple replicates, or spatial and include 
    the option to calculate them over multiple time points. Functions fall into 
    five categories: static diversity indices, temporal diversity indices, 
    spatial diversity indices, rank abundance curves, and community stability 
    measures. The diversity indices are temporal and spatial analogs to 
    traditional diversity indices. Specifically, the package includes functions 
    to calculate community richness, evenness and diversity at a given point in 
    space and time. In addition, it contains functions to calculate species 
    turnover, mean rank shifts, and lags in community similarity between two 
    time points. Details of the methods are available in
    Hallett et al. (2016) <doi:10.1111/2041-210X.12569> and Avolio 
    et al. (2019) <doi:10.1002/ecs2.2881>.",2020-12-01,A. Andrew M. MacDonald,https://github.com/NCEAS/codyn/,TRUE,https://github.com/nceas/codyn,25000,21,2020-12-01T02:49:39Z,1190.4761904761904
coenocliner,"Simulate species occurrence and abundances (counts) along
    gradients.",2021-02-14,Gavin L. Simpson,https://github.com/gavinsimpson/coenocliner/,TRUE,https://github.com/gavinsimpson/coenocliner,20769,9,2021-02-08T19:34:54Z,2307.6666666666665
coga,"Evaluation for density and distribution function of convolution of gamma
    distributions in R. Two related exact methods and one approximate method are
    implemented with efficient algorithm and C++ code. A quick guide for choosing
    correct method and usage of this package is given in package vignette. For the
    detail of methods used in this package, we refer the user to
    Mathai(1982)<doi:10.1007/BF02481056>,
    Moschopoulos(1984)<doi:10.1007/BF02481123>,
    Hu et al.(2019)<doi:10.1007/s00180-019-00924-9>.",2019-10-08,Chaoran Hu,https://github.com/ChaoranHu/coga,TRUE,https://github.com/chaoranhu/coga,21321,4,2021-07-11T17:08:13Z,5330.25
cognitoR,Provides authentication for Shiny applications using 'Amazon Cognito' ( <https://aws.amazon.com/es/cognito/>).,2020-10-06,Pablo Pagnone,NA,TRUE,https://github.com/chi2labs/cognitor,9739,13,2020-10-06T00:59:12Z,749.1538461538462
cohorts,Functions to simplify the process of preparing event and transaction for cohort analysis.,2021-07-08,Peer Christensen,https://github.com/PeerChristensen/cohorts,TRUE,https://github.com/peerchristensen/cohorts,785,2,2021-07-11T21:13:00Z,392.5
coinmarketcapr,Extract and monitor price and market cap of 'Cryptocurrencies' from 'Coin Market Cap' <https://coinmarketcap.com/api/>. ,2020-03-25,AbdulMajedRaja RS,http://github.com/amrrs/coinmarketcapr,TRUE,https://github.com/amrrs/coinmarketcapr,23833,61,2021-01-07T13:38:16Z,390.7049180327869
collapse,"A C/C++ based package for advanced data transformation and 
    statistical computing in R that is extremely fast, flexible and 
    parsimonious to code with, class-agnostic and programmer friendly. 
    It is well integrated with base R, 'dplyr' / (grouped) 'tibble', 
    'data.table', 'plm' (panel-series and data frames), 'sf' data frames, and 
    non-destructively handles other matrix or data frame based classes (such as 
    'ts', 'xts' / 'zoo', 'timeSeries', 'tsibble', 'tibbletime' etc.)
    --- Key Features: ---
    (1) Advanced statistical programming: A full set of fast statistical functions 
        supporting grouped and weighted computations on vectors, matrices and 
        data frames. Fast and programmable grouping, ordering, unique values / rows, 
        factor generation and interactions. Fast and flexible functions for data 
        manipulation and data object conversions.
    (2) Advanced aggregation: Fast and easy multi-data-type, multi-function, 
        weighted, parallelized and fully customized data aggregation.
    (3) Advanced transformations: Fast row / column arithmetic, (grouped) replacing 
        and sweeping out of statistics, (grouped, weighted) scaling / standardizing, 
        between (averaging) and (quasi-)within (centering / demeaning) transformations, 
        higher-dimensional centering (i.e. multiple fixed effects transformations), 
        linear prediction / partialling-out, linear model fitting and testing.
    (4) Advanced time-computations: Fast (sequences of) lags / leads, and 
        (lagged / leaded, iterated, quasi-, log-) differences, (compounded) 
        growth rates, and cumulative sums on (unordered, irregular) time series and 
        panel data. Multivariate auto-, partial- and cross-correlation functions for panel data. 
        Panel data to (ts-)array conversions.
    (5) List processing: (Recursive) list search / identification, splitting, 
        extraction / subsetting, data-apply, and generalized recursive 
        row-binding / unlisting in 2D.
    (6) Advanced data exploration: Fast (grouped, weighted, panel-decomposed) 
        summary statistics for complex multilevel / panel data.",2021-07-24,Sebastian Krantz,"https://sebkrantz.github.io/collapse/,
https://github.com/SebKrantz/collapse,
https://twitter.com/collapse_R",TRUE,https://github.com/sebkrantz/collapse,41805,257,2021-08-09T14:34:14Z,162.66536964980546
collapsibleTree,"
    Interactive Reingold-Tilford tree diagrams created using 'D3.js', where every node can be expanded and collapsed by clicking on it.
    Tooltips and color gradients can be mapped to nodes using a numeric column in the source data frame.
    See 'collapsibleTree' website for more information and examples.",2018-08-22,Adeel Khan,"https://github.com/AdeelK93/collapsibleTree,
https://AdeelK93.github.io/collapsibleTree/",TRUE,https://github.com/adeelk93/collapsibletree,51130,139,2021-01-27T05:01:10Z,367.841726618705
collections,"Provides high performance container data types such
    as queues, stacks, deques, dicts and ordered dicts. Benchmarks
    <https://randy3k.github.io/collections/articles/benchmark.html> have
    shown that these containers are asymptotically more efficient than
    those offered by other packages.",2020-08-10,Randy Lai,https://github.com/randy3k/collections,TRUE,https://github.com/randy3k/collections,106016,79,2021-06-05T08:24:57Z,1341.9746835443038
coloc,"Performs the colocalisation tests described in
    Giambartolomei et al (2013) <doi:10.1371/journal.pgen.1004383>,
    Wallace (2020) <doi:10.1371/journal.pgen.1008720>,
    Wallace (2021) <doi:10.1101/2021.02.23.432421>.",2021-06-14,Chris Wallace,https://github.com/chr1swallace/coloc,TRUE,https://github.com/chr1swallace/coloc,31539,46,2021-08-04T10:00:11Z,685.6304347826087
colorednoise,"Temporally autocorrelated populations are correlated in their vital rates (growth, death, etc.) from year to year. It is very common for populations, whether they be bacteria, plants, or humans, to be temporally autocorrelated. This poses a challenge for stochastic population modeling, because a temporally correlated population will behave differently from an uncorrelated one.
    This package provides tools for simulating populations with white noise (no temporal autocorrelation), red noise (positive temporal autocorrelation), and blue noise (negative temporal autocorrelation).  The algebraic formulation for autocorrelated noise comes from Ruokolainen et al. (2009) <doi:10.1016/j.tree.2009.04.009>. Models for unstructured populations and for structured populations (matrix models) are available.",2020-08-05,Julia Pilowsky,NA,TRUE,https://github.com/japilo/colorednoise,21421,6,2020-10-08T00:16:36Z,3570.1666666666665
colorfindr,"Extracts colors from various image types, returns customized reports and plots treemaps 
    and 3D scatterplots of image compositions. Color palettes can also be created. ",2019-02-01,David Zumbach,NA,TRUE,https://github.com/zumbov2/colorfindr,24399,114,2020-09-25T19:02:10Z,214.02631578947367
colorhex,"The website <https:www.color-hex.com> is a great resource of hex colour
  codes and palettes. This package allows you to retrieve palettes
  and colour information from the website directly from R. There are 
  also custom scale-functions for 'ggplot2'. ",2021-05-25,Athanasia Mo Mowinckel,https://github.com/Athanasiamo/colorhex,TRUE,https://github.com/athanasiamo/colorhex,1210,8,2021-05-21T18:42:40Z,151.25
colorist,"Color and visualize wildlife distributions in
    space-time using raster data. In addition to enabling display of
    sequential change in distributions through the use of small multiples,
    'colorist' provides functions for extracting several features of
    interest from a sequence of distributions and for visualizing those
    features using HCL (hue-chroma-luminance) color palettes. Resulting
    maps allow for ""fair"" visual comparison of intensity values (e.g.,
    occurrence, abundance, or density) across space and time and can be
    used to address questions about where, when, and how consistently a
    species, group, or individual is likely to be found.",2020-11-23,Justin Schuetz,https://github.com/mstrimas/colorist,TRUE,https://github.com/mstrimas/colorist,8356,9,2021-01-20T02:50:45Z,928.4444444444445
colorizer,"Call the 'DeOldify' <https://github.com/jantic/DeOldify> image
    colorization API on 'DeepAI'<https://deepai.org/machine-learning-model/colorizer>
    to colorize black and white images.",2020-11-06,David Zumbach,https://github.com/zumbov2/colorizer,TRUE,https://github.com/zumbov2/colorizer,5473,15,2020-11-09T22:29:03Z,364.8666666666667
ColorNameR,"A tool for transforming coordinates in a color space to common
    color names using data from the Royal Horticultural Society and the
    International Union for the Protection of New Varieties of Plants.",2021-07-08,Marco Sánchez Beeckman,https://github.com/msanchez-beeckman/ColorNameR,TRUE,https://github.com/msanchez-beeckman/colornamer,784,2,2021-07-11T12:17:34Z,392
colourlovers,"Provides access to the COLOURlovers <https://www.colourlovers.com/>
    API, which offers color inspiration and color palettes.",2020-12-09,Andrew Heiss,https://github.com/andrewheiss/colourlovers,TRUE,https://github.com/andrewheiss/colourlovers,21030,95,2021-08-31T16:40:49Z,221.3684210526316
colourpicker,"A colour picker that can be used as an input in 'Shiny' apps
    or Rmarkdown documents. The colour picker supports alpha opacity, custom
    colour palettes, and many more options. A Plot Colour Helper tool is
    available as an 'RStudio' Addin, which helps you pick colours to use in your
    plots. A more generic Colour Picker 'RStudio' Addin is also provided to let 
    you select colours to use in your R code.",2020-09-14,Dean Attali,https://github.com/daattali/colourpicker,TRUE,https://github.com/daattali/colourpicker,989342,149,2021-01-17T09:10:32Z,6639.879194630873
colourvalues,"Maps one of the viridis colour palettes, or a user-specified palette to values. 
    Viridis colour maps are created by Stéfan van der Walt and Nathaniel Smith, 
    and were set as the default palette for the 'Python' 'Matplotlib' library <https://matplotlib.org/>. 
    Other palettes available in this library have been derived from 
    'RColorBrewer' <https://CRAN.R-project.org/package=RColorBrewer> and 
    'colorspace' <https://CRAN.R-project.org/package=colorspace> packages.",2020-12-07,David Cooley,https://symbolixau.github.io/colourvalues/,TRUE,https://github.com/symbolixau/colourvalues,183573,35,2021-03-29T22:33:57Z,5244.942857142857
comat,"Builds co-occurrence matrices based on spatial raster data.
    It includes creation of weighted co-occurrence matrices (wecoma) and 
    integrated co-occurrence matrices 
    (incoma; Vadivel et al. (2007) <doi:10.1016/j.patrec.2007.01.004>).",2021-04-19,Jakub Nowosad,https://nowosad.github.io/comat/,TRUE,https://github.com/nowosad/comat,14024,3,2021-05-21T10:50:08Z,4674.666666666667
combinedevents,"Includes functions to calculate scores and marks for track and
    field combined events competitions. The functions are based on the scoring
    tables for combined events set forth by the International Association of 
    Athletics Federation (2001).",2021-02-03,Katie Frank,"https://katie-frank.github.io/combinedevents/,
https://github.com/katie-frank/combinedevents",TRUE,https://github.com/katie-frank/combinedevents,4566,0,2021-02-06T20:29:07Z,NA
combiroc,"Provides functions and a workflow to easily and powerfully calculating specificity, sensitivity and ROC curves of biomarkers combinations. Allows to rank and select multi-markers signatures as well as to find the best performing sub-signatures. The method used was first published as a Shiny app and described in Mazzara et al. (2017) <doi:10.1038/srep45477> and further described in Bombaci & Rossi (2019) <doi:10.1007/978-1-4939-9164-8_16>.",2021-08-17,Ivan Ferrari,https://github.com/ingmbioinfo/combiroc,TRUE,https://github.com/ingmbioinfo/combiroc,229,2,2021-08-17T13:57:59Z,114.5
cometr,"A convenient 'R' wrapper to the 'Comet' API, which is a cloud
    platform allowing you to track, compare, explain and optimize machine
    learning experiments and models. Experiments can be viewed on the 'Comet'
    online dashboard at <https://www.comet.ml>.",2020-08-13,Doug Blank,https://github.com/comet-ml/cometr,TRUE,https://github.com/comet-ml/cometr,8190,6,2021-08-12T22:42:39Z,1365
comFuncs,"A set of common functions to be used for displaying messages, checking variables, 
  finding absolute paths, starting applications, etc. More functions will be added later.",2020-12-09,Hanming Tu,https://github.com/TuCai/comFuncs,TRUE,https://github.com/tucai/comfuncs,4941,1,2020-12-08T04:00:59Z,4941
commonmark,"The CommonMark specification defines a rationalized version of markdown
    syntax. This package uses the 'cmark' reference implementation for converting
    markdown text into various formats including html, latex and groff man. In
    addition it exposes the markdown parse tree in xml format. Also includes opt-in
    support for GFM extensions including tables, autolinks, and strikethrough text.",2018-12-01,Jeroen Ooms,"http://github.com/jeroen/commonmark (devel)
https://github.github.com/gfm/ (spec)",TRUE,https://github.com/jeroen/commonmark,6909654,72,2021-03-22T15:30:31Z,95967.41666666667
comorbidity,"Computing comorbidity scores such as the weighted Charlson score
  (Charlson, 1987 <doi:10.1016/0021-9681(87)90171-8>) and the Elixhauser
  comorbidity score (Elixhauser, 1998 <doi:10.1097/00005650-199801000-00004>)
  using ICD-9-CM or ICD-10 codes (Quan, 2005 <doi:10.1097/01.mlr.0000182534.19832.83>).",2020-01-09,Alessandro Gasparini,https://ellessenne.github.io/comorbidity,TRUE,https://github.com/ellessenne/comorbidity,28925,37,2021-07-28T07:23:29Z,781.7567567567568
comparator,"Implements functions for comparing strings, sequences and 
    numeric vectors for clustering and record linkage applications. 
    Supported comparison functions include: generalized edit distances 
    for comparing sequences/strings, Monge-Elkan similarity for fuzzy 
    comparison of token sets, and L-p distances for comparing numeric 
    vectors. Where possible, comparison functions are implemented in 
    C/C++ to ensure good performance.",2020-12-10,Neil Marchant,https://github.com/ngmarchant/comparator,TRUE,https://github.com/ngmarchant/comparator,3163,8,2021-04-26T03:20:48Z,395.375
comparer,"Quickly run experiments to compare the run time and output of
    code blocks. The function mbc() can make fast comparisons of code,
    and will calculate statistics comparing the resulting outputs.
    It can be used to compare model fits to the same data or
    see which function runs faster.
    The R6 class ffexp$new() runs a function using all possible combinations
    of selected inputs. This is useful for comparing the effect of
    different parameter values. It can also run in parallel and
    automatically save intermediate results, which is very useful
    for long computations.",2021-03-29,Collin Erickson,https://github.com/CollinErickson/comparer,TRUE,https://github.com/collinerickson/comparer,16002,2,2021-06-10T03:01:29Z,8001
comperes,"Tools for storing and managing competition results.
    Competition is understood as a set of games in which players gain some
    abstract scores.  There are two ways for storing results: in long (one
    row per game-player) and wide (one row per game with fixed amount of
    players) formats. This package provides functions for creation and
    conversion between them. Also there are functions for computing their
    summary and Head-to-Head values for players. They leverage grammar of
    data manipulation from 'dplyr'.",2020-11-23,Evgeni Chasnovski,https://github.com/echasnovski/comperes,TRUE,https://github.com/echasnovski/comperes,18623,6,2020-11-24T19:15:21Z,3103.8333333333335
competitiontoolbox,"A graphical user interface for simulating the effects of mergers, tariffs, and quotas under an
             assortment of different economic models. The interface is powered by the 'Shiny' web application framework from 
             'RStudio'.",2021-05-21,Charles Taragin,https://github.com/luciu5/competitiontoolbox,TRUE,https://github.com/luciu5/competitiontoolbox,9416,1,2021-05-21T18:35:21Z,9416
complexNet,"Providing a set of functions to easily generate and iterate complex networks.
 The functions can be used to generate realistic networks with a wide range of different clustering, density, and average path length.
 For more information consult research articles by Amiyaal Ilany and Erol Akcay (2016) <doi:10.1093/icb/icw068> and Ilany and Erol Akcay (2016) <doi:10.1101/026120>, which have inspired many methods in this package.",2021-03-31,Marco Smolla,"https://marcosmolla.github.io/complexNet/,
https://github.com/marcosmolla/complexNet",TRUE,https://github.com/marcosmolla/complexnet,1801,1,2021-05-06T17:28:40Z,1801
ComplexUpset,"UpSet plots are an improvement over Venn Diagram for set overlap visualizations.
    Striving to bring the best of the 'UpSetR' and 'ggplot2', this package offers a way to create
    complex overlap visualisations, using simple and familiar tools, i.e. geoms of 'ggplot2'.
    For introduction to UpSet concept, see Lex et al. (2014) <doi:10.1109/TVCG.2014.2346248>.",2021-08-05,Michał Krassowski,"https://github.com/krassowski/complex-upset,
https://krassowski.github.io/complex-upset/",TRUE,https://github.com/krassowski/complex-upset,8156,206,2021-08-10T16:28:34Z,39.592233009708735
comprehenr,"Provides 'Python'-style list comprehensions.
    List comprehension expressions use usual loops (for(), while() and repeat()) and
    usual if() as list producers. In many cases it gives more concise notation than
    standard ""*apply + filter"" strategy.",2021-01-31,Gregory Demin,https://github.com/gdemin/comprehenr,TRUE,https://github.com/gdemin/comprehenr,30344,13,2021-01-31T00:27:57Z,2334.153846153846
compstatr,"Provides a set of tools for creating yearly data sets of St. Louis 
    Metropolitan Police Department (SLMPD) crime data, which are available from
    January 2008 onward as monthly CSV releases on their website 
    (<http:www.slmpd.org/Crimereports.shtml>). Once data are validated and created 
    (monthly data releases have varying numbers of columns 
    as well as different column names and formats), 'compstatr' also provides 
    functions for categorizing and mapping crimes in St. Louis. The categorization 
    tools that are provided will also work with any police department that uses 5 
    and 6 digit numeric codes to identify specific crimes. These data provide researchers
    and policy makers detailed data for St. Louis, which in the last several years 
    has had some of the highest or the highest violent crime rates in the United States.",2020-05-14,Christopher Prener,https://github.com/slu-openGIS/compstatr,TRUE,https://github.com/slu-opengis/compstatr,12482,6,2021-03-09T22:46:11Z,2080.3333333333335
comtradr,"Interface with and extract data from the United Nations Comtrade 
  API <https://comtrade.un.org/data/>. Comtrade provides country level shipping 
  data for a variety of commodities, these functions allow for easy API query 
  and data returned as a tidy data frame.",2018-10-05,Chris Muir,https://github.com/ropensci/comtradr,TRUE,https://github.com/ropensci/comtradr,24191,33,2021-07-18T14:56:13Z,733.060606060606
concatipede,"Concatenation of multiple sequence alignments based on a
    correspondence table that can be edited in Excel <doi:10.5281/zenodo.5130603>.",2021-08-06,Matteo Vecchi,"https://github.com/tardipede/concatipede,
https://tardipede.github.io/concatipede/",TRUE,https://github.com/tardipede/concatipede,379,0,2021-08-06T07:51:15Z,NA
concordance,"A set of utilities for matching products in different
	     classification codes used in international trade
	     research. It supports concordance between the Harmonized
	     System (HS0, HS1, HS2, HS3, HS4, HS5, HS combined), the Standard 
	     International Trade Classification (SITC1, SITC2, SITC3, SITC4), 
	     the North American Industry Classification System (NAICS combined),
	     as well as the Broad Economic Categories (BEC), the International 
	     Standard of Industrial Classification (ISIC), and the Standard Industrial 
	     Classification (SIC). It also provides code nomenclature/descriptions 
	     look-up, Rauch classification look-up (via concordance to SITC2), and
	     trade elasticity look-up (via concordance to HS0 or SITC3
	     codes).",2020-04-24,Steven Liao,NA,TRUE,https://github.com/insongkim/concordance,20754,8,2021-01-29T03:12:51Z,2594.25
concorR,"Contains the CONCOR (CONvergence of iterated CORrelations) 
    algorithm and a series of supplemental functions for easy running, 
    plotting, and blockmodeling. The CONCOR algorithm is used on social network
    data to identify network positions based off a definition of structural 
    equivalence; see Breiger, Boorman, and Arabie (1975) 
    <doi:10.1016/0022-2496(75)90028-0> and Wasserman and Faust's book Social 
    Network Analysis: Methods and Applications (1994). This version allows 
    multiple relationships for the same set of nodes and uses both incoming and
    outgoing ties to find positions.",2020-11-25,Adrienne Traxler,https://github.com/ATraxLab/concorR,TRUE,https://github.com/atraxlab/concorr,5960,0,2020-12-18T19:07:17Z,NA
concreg,"Implements concordance regression which can be used to estimate generalized odds of concordance.
	Can be used for non- and semi-parametric survival analysis with non-proportional hazards, for binary and 
    for continuous outcome data. The method was introduced by Dunkler, Schemper and Heinze (2010) <doi:10.1093/bioinformatics/btq035>.",2020-10-02,Georg Heinze,https://cemsiis.meduniwien.ac.at/kb/wf/software/statistische-software/concreg/,TRUE,https://github.com/georgheinze/concreg,21428,0,2020-10-02T08:46:13Z,NA
concurve,"Computes compatibility (confidence) distributions along with their corresponding P-values,
    S-values, and likelihoods. The intervals can be plotted to form the distributions themselves.  
    Functions can be compared to one another to see how much they overlap. Results can be
    exported to Microsoft Word, Powerpoint, and TeX documents. The package currently supports 
    resampling methods, computing differences, generalized linear models, mixed-effects models, 
    survival analysis, and meta-analysis. These methods are discussed 
    by Schweder T, Hjort NL. (2016, ISBN:9781316445051) and 
    Rafi Z, Greenland S. (2020) <doi:10.1186/s12874-020-01105-9>.",2020-10-12,Zad Rafi,"https://data.lesslikely.com/concurve/,
https://github.com/zadrafi/concurve",TRUE,https://github.com/zadrafi/concurve,17757,15,2021-07-22T03:14:14Z,1183.8
condir,Set of functions for the easy analyses of conditioning data.,2020-12-01,Angelos-Miltiadis Krypotos,https://github.com/AngelosPsy/condir,TRUE,https://github.com/angelospsy/condir,17012,1,2020-12-01T11:47:29Z,17012
conditionz,"Provides ability to control how many times in function
    calls conditions are thrown (shown to the user). Includes control of
    warnings and messages.",2019-04-24,Scott Chamberlain,https://github.com/ropenscilabs/conditionz,TRUE,https://github.com/ropenscilabs/conditionz,74791,1,2020-11-02T19:44:00Z,74791
condTruncMVN,"Computes the density and probability for the conditional truncated multivariate normal (Horrace (2005) p. 4, <doi:10.1016/j.jmva.2004.10.007>). Also draws random samples from this distribution.",2020-09-17,Paul M. Hargarten,NA,TRUE,https://github.com/phargarten2/condtruncmvn,4297,0,2020-09-04T15:41:14Z,NA
condvis2,"Constructs a shiny app function with interactive displays for conditional visualization of models, 
     data and density functions. An extended version of package 'condvis'. 
     Mark O'Connell, Catherine B. Hurley, Katarina Domijan (2017) <doi:10.18637/jss.v081.i05>.",2020-09-25,Catherine Hurley,https://github.com/cbhurley/condvis2,TRUE,https://github.com/cbhurley/condvis2,12807,5,2021-07-01T18:04:30Z,2561.4
config,"Manage configuration values across multiple environments (e.g.
  development, test, production). Read values using a function that determines
  the current environment and returns the appropriate value.",2020-12-17,Andrie de Vries,https://github.com/rstudio/config,TRUE,https://github.com/rstudio/config,3046115,181,2021-05-14T07:13:10Z,16829.364640883978
configural,"R functions for criterion profile analysis, Davison and Davenport (2002) <doi:10.1037/1082-989X.7.4.468> and meta-analytic criterion profile analysis, Wiernik, Wilmot, Davison, and Ones (2020) <doi:10.1037/met0000305>. Sensitivity analyses to aid in interpreting criterion profile analysis results are also included.",2021-01-18,Brenton M. Wiernik,NA,TRUE,https://github.com/bwiernik/configural,13347,2,2021-06-07T16:35:31Z,6673.5
confintr,"Calculates classic and/or bootstrap confidence
    intervals for many parameters such as the population mean, variance,
    interquartile range (IQR), median absolute deviation (MAD), skewness,
    kurtosis, Cramer's V, odds ratio, R-squared, quantiles (incl. median),
    proportions, different types of correlation measures, difference in
    means, quantiles and medians. Many of the classic confidence intervals
    are described in Smithson, M. (2003, ISBN: 978-0761924999). Bootstrap
    confidence intervals are calculated with the R package 'boot'. Both
    one- and two-sided intervals are supported.",2020-06-29,Michael Mayer,https://github.com/mayer79/confintr,TRUE,https://github.com/mayer79/confintr,5709,2,2021-05-08T08:10:01Z,2854.5
conflicted,"R's default conflict management system gives the most recently
    loaded package precedence. This can make it hard to detect conflicts, 
    particularly when they arise because a package update creates ambiguity
    that did not previously exist. 'conflicted' takes a different approach, 
    making every conflict an error and forcing you to choose which function 
    to use.",2019-06-21,Hadley Wickham,https://github.com/r-lib/conflicted,TRUE,https://github.com/r-lib/conflicted,281366,199,2020-10-23T17:50:10Z,1413.8994974874372
conflr,"Provides utilities for working with various 'Confluence' API 
    <https://docs.atlassian.com/ConfluenceServer/rest/latest/>, including a
    functionality to convert an R Markdown document to 'Confluence' format and
    upload it to 'Confluence' automatically.",2020-04-08,Hiroaki Yutani,"https://line.github.io/conflr/, https://github.com/line/conflr",TRUE,https://github.com/line/conflr,11497,107,2020-12-28T12:06:10Z,107.44859813084112
ConformalSmallest,"An implementation of efficiency first conformal prediction (EFCP) and validity first conformal prediction (VFCP) that demonstrates both validity (coverage guarantee) and efficiency (width guarantee). To learn how to use it, check the vignettes for a quick tutorial. The package is based on the work by Yang Y., Kuchibhotla A.,(2021) <arxiv:2104.13871>.",2021-08-09,Yachong Yang,https://github.com/Elsa-Yang98/ConformalSmallest,TRUE,https://github.com/elsa-yang98/conformalsmallest,305,0,2021-08-10T11:51:00Z,NA
confoundr,"Implements three covariate-balance diagnostics for time-varying confounding and selection-bias in complex longitudinal data, as described in Jackson (2016) <doi:10.1097/EDE.0000000000000547> and Jackson (2019) <doi:10.1093/aje/kwz136>. Diagnostic 1 assesses measured confounding/selection-bias, diagnostic 2 assesses exposure-covariate feedback, and diagnostic 3 assesses residual confounding/selection-bias after inverse probability weighting or propensity score stratification. All diagnostics appropriately account for exposure history, can be adapted to assess a particular depth of covariate history, and can be implemented in right-censored data. Balance assessments can be obtained for all times, selected-times, or averaged across person-time. The balance measures are reported as tables or plots. These diagnostics can be applied to the study of multivariate exposures including time-varying exposures, direct effects, interaction, and censoring.",2019-09-20,John W. Jackson,NA,TRUE,https://github.com/jwjackson/confoundr,10729,9,2021-01-15T04:15:05Z,1192.111111111111
conjurer,Builds synthetic data applicable across multiple domains. This package also provides flexibility to control data distribution to make it relevant to many industry examples.,2020-09-08,Sidharth Macherla,https://www.foyi.co.nz/posts/documentation/documentationconjurer/,TRUE,https://github.com/sidharthmacherla/conjurer,11107,5,2021-05-19T09:32:59Z,2221.4
connections,"Enables 'DBI' compliant packages to integrate with the 'RStudio' connections 
  pane, and the 'pins' package. It automates the display of schemata, tables, views, as well 
  as the preview of the table's top 1000 records. ",2020-02-07,Javier Luraschi,https://github.com/edgararuiz/connections,TRUE,https://github.com/edgararuiz/connections,10386,44,2020-12-17T18:35:43Z,236.04545454545453
connectwidgets,"A collection of helper functions and 'htmlwidgets' to help publishers
    curate content collections on 'RStudio Connect'. The components,
    Card, Grid, Table, Search, and Filter can be used to produce a
    showcase page or gallery contained within a static or interactive
    R Markdown page.",2021-07-23,Brian Smith,"https://rstudio.github.io/connectwidgets/,
https://github.com/rstudio/connectwidgets",TRUE,https://github.com/rstudio/connectwidgets,655,7,2021-07-28T16:04:21Z,93.57142857142857
conos,"Wires together large collections of single-cell RNA-seq datasets, which allows for both the identification of recurrent cell clusters and the propagation of information between datasets in multi-sample or atlas-scale collections. 'Conos' focuses on the uniform mapping of homologous cell types across heterogeneous sample collections. For instance, users could investigate a collection of dozens of peripheral blood samples from cancer patients combined with dozens of controls, which perhaps includes samples of a related tissue such as lymph nodes. This package interacts with data available through the 'conosPanel' package, which is available in a 'drat' repository. To access this data package, see the instructions at <https://github.com/kharchenkolab/conos>. The size of the 'conosPanel' package is approximately 12 MB.",2021-08-07,Evan Biederstedt,https://github.com/kharchenkolab/conos,TRUE,https://github.com/kharchenkolab/conos,3979,123,2021-08-10T16:28:21Z,32.34959349593496
conquer,Fast and accurate convolution-type smoothed quantile regression. Implemented using Barzilai-Borwein gradient descent with a Huber regression warm start. Construct confidence intervals for regression coefficients using multiplier bootstrap.,2020-08-27,Xiaoou Pan,https://github.com/XiaoouPan/conquer,TRUE,https://github.com/xiaooupan/conquer,3591716,5,2021-08-01T01:57:09Z,718343.2
ConR,"Multi-species estimation of geographical range parameters
	for preliminary assessment of conservation status following Criterion B of the 
	International Union for Conservation of Nature (IUCN, 
	see <http://www.iucnredlist.org>).",2020-05-18,Gilles Dauby,https://gdauby.github.io/ConR/,TRUE,https://github.com/gdauby/conr,21983,6,2021-08-25T10:48:00Z,3663.8333333333335
conserveR,"Helping biologists to choose the most suitable approach to link their research to conservation. After answering few questions on the data available, geographic and taxonomic scope, 'conserveR' ranks existing methods for conservation prioritization and systematic conservation planning by suitability. The methods data base of 'conserveR' contains 133 methods for conservation prioritization based on a systematic review of > 12,000 scientific publications from the fields of spatial conservation prioritization, systematic conservation planning, biogeography and ecology.",2021-08-02,Alexander Zizka,https://github.com/azizka/conserveR,TRUE,https://github.com/azizka/conserver,459,2,2021-08-17T11:57:07Z,229.5
consort,"To make it easy to create CONSORT diagrams for the transparent reporting of participant allocation in randomized, controlled clinical trials. This is done by creating a standardized disposition data, and using this data as the source for the creation a standard CONSORT diagram. Human effort by supplying text labels on the node can also be achieved.",2021-08-10,Alim Dayim,https://github.com/adayim/consort/,TRUE,https://github.com/adayim/consort,297,0,2021-08-12T07:15:17Z,NA
constants,"CODATA internationally recommended values of the fundamental
    physical constants, provided as symbols for direct use within the R language.
    Optionally, the values with uncertainties and/or units are also provided if
    the 'errors', 'units' and/or 'quantities' packages are installed.
    The Committee on Data for Science and Technology (CODATA) is an
    interdisciplinary committee of the International Council for Science which
    periodically provides the internationally accepted set of values of the
    fundamental physical constants. This package contains the ""2018 CODATA""
    version, published on May 2019:
    Eite Tiesinga, Peter J. Mohr, David B. Newell, and Barry N. Taylor (2020)
    <https://physics.nist.gov/cuu/Constants/>.",2021-02-25,Iñaki Ucar,https://github.com/r-quantities/constants,TRUE,https://github.com/r-quantities/constants,18792,13,2021-02-22T14:02:18Z,1445.5384615384614
contact,"Process spatially- and temporally-discrete data into contact and 
   social networks, and facilitate network analysis by randomizing 
   individuals' movement paths and/or related categorical variables. To use 
   this package, users need only have a dataset containing spatial data 
   (i.e., latitude/longitude, or planar x & y coordinates), individual IDs 
   relating spatial data to specific individuals, and date/time information 
   relating spatial locations to temporal locations. The functionality of this 
   package ranges from data ""cleaning"" via multiple filtration functions, to 
   spatial and temporal data interpolation, and network creation and analysis. 
   Functions within this package are not limited to describing interpersonal 
   contacts. Package functions can also identify and quantify ""contacts"" 
   between individuals and fixed areas (e.g., home ranges, water bodies, 
   buildings, etc.). As such, this package is an incredibly useful resource 
   for facilitating epidemiological, ecological, ethological and sociological 
   research.",2021-05-17,Trevor Farthing,NA,TRUE,https://github.com/lanzaslab/contact,10959,1,2021-08-29T14:57:21Z,10959
contactdata,"Data package for the supplementary data in Prem et al. (2017)
    <doi:10.1371/journal.pcbi.1005697>.
    Provides easy access to contact data for 152 countries, for use in
    epidemiological, demographic or social sciences research.",2021-02-19,Hugo Gruson,"https://bisaloo.github.io/contactdata/,
https://github.com/bisaloo/contactdata",TRUE,https://github.com/bisaloo/contactdata,4557,3,2021-02-23T11:21:35Z,1519
contentid,"An interface for creating, registering, and resolving content-based
             identifiers for data management. Content-based identifiers rely on
             the 'cryptographic' hashes to refer to the files they identify, thus,
             anyone possessing the file can compute the identifier using a 
             well-known standard algorithm, such as 'SHA256'.  By registering
             a URL at which the content is accessible to a public archive (such as 
             Hash Archive) or depositing data in a scientific repository such 'Zenodo',
             'DataONE' or 'SoftwareHeritage', the content identifier can serve 
             many functions typically associated with A Digital Object Identifier
             ('DOI').  Unlike location-based identifiers like 'DOIs', content-based
             identifiers permit the same content to be registered in many locations.",2021-08-08,Carl Boettiger,https://github.com/cboettig/contentid,TRUE,https://github.com/cboettig/contentid,8269,33,2021-08-06T22:20:25Z,250.57575757575756
contribution,"Contribution table for credit assignment based on 'ggplot2'.
    This can improve the author contribution information in academic journals and personal CV.  ",2020-12-03,Shixiang Wang,https://github.com/openbiox/contribution,TRUE,https://github.com/openbiox/contribution,14767,5,2020-12-03T10:01:36Z,2953.4
convdistr,"Convolute probabilistic distributions using the random generator 
 function of each distribution. A new random number generator function is created that 
 perform the mathematical operation on the individual random samples from the 
 random generator function of each distribution. See the documentation for examples.",2021-04-20,Aponte John,https://github.com/johnaponte/convdistr,TRUE,https://github.com/johnaponte/convdistr,1619,1,2021-04-19T12:54:19Z,1619
ConvergenceClubs,"Functions for clustering regions that form convergence clubs, according to the definition of Phillips and Sul (2009) <doi:10.1002/jae.1080>. A package description is available in Sichera and Pizzuto (2019).",2019-11-21,Roberto Sichera,https://CRAN.R-project.org/package=ConvergenceClubs,TRUE,https://github.com/rhobis/convergenceclubs,19442,1,2021-08-25T06:15:56Z,19442
convey,"Variance estimation on indicators of income concentration and
    poverty using complex sample survey designs. Wrapper around the
    'survey' package.",2021-07-29,Anthony Damico,https://guilhermejacob.github.io/context/,TRUE,https://github.com/ajdamico/convey,29083,13,2021-07-19T19:46:44Z,2237.153846153846
CoordinateCleaner,"Automated flagging of common spatial and temporal errors in biological and paleontological collection data, for the use in conservation, ecology and paleontology. Includes automated tests to easily flag (and exclude) records assigned to country or province centroid, the open ocean, the headquarters of the Global Biodiversity Information Facility, urban areas or the location of biodiversity institutions (museums, zoos, botanical gardens, universities). Furthermore identifies per species outlier coordinates, zero coordinates, identical latitude/longitude and invalid coordinates. Also implements an algorithm to identify data sets with a significant proportion of rounded coordinates. Especially suited for large data sets. The reference for the methodology is: Zizka et al. (2019) <doi:10.1111/2041-210X.13152>.",2020-10-14,Alexander Zizka,https://ropensci.github.io/CoordinateCleaner/,TRUE,https://github.com/ropensci/coordinatecleaner,37591,53,2020-10-29T10:52:25Z,709.2641509433962
copent,"The nonparametric methods for estimating copula entropy and transfer entropy are implemented. The method for estimating copula entropy composes of two simple steps: estimating empirical copula by rank statistic and estimating copula entropy with k-Nearest-Neighbour method. The method for estimating transfer entropy composes of two steps: estimating three copula entropy terms and then calculate transfer entropy from the estimated copula entropy terms. Copula Entropy is a mathematical concept for multivariate statistical independence measuring and testing, and proved to be equivalent to mutual information. Estimating copula entropy can be applied to many cases, including but not limited to variable selection and causal discovery (by estimating transfer entropy). Please refer to Ma and Sun (2011) <doi:10.1016/S1007-0214(11)70008-6> and Ma (2019) <arXiv:1910.04375> for more information.",2021-03-21,MA Jian,https://github.com/majianthu/copent,TRUE,https://github.com/majianthu/copent,7887,12,2021-03-20T20:13:28Z,657.25
CopernicusDEM,"Copernicus Digital Elevation Model datasets (DEM) of 90 and 30 meters resolution using the 'awscli' command line tool. The Copernicus (DEM) is included in the Registry of Open Data on 'AWS (Amazon Web Services)' and represents the surface of the Earth including buildings, infrastructure and vegetation.",2021-05-19,Lampros Mouselimis,https://github.com/mlampros/CopernicusDEM,TRUE,https://github.com/mlampros/copernicusdem,1663,15,2021-07-03T05:21:36Z,110.86666666666666
copulaedas,"Provides a platform where EDAs (estimation of
    distribution algorithms) based on copulas can be implemented and
    studied. The package offers complete implementations of various
    EDAs based on copulas and vines, a group of well-known
    optimization problems, and utility functions to study the
    performance of the algorithms. Newly developed EDAs can be easily
    integrated into the package by extending an S4 class with generic
    functions for their main components.",2018-07-29,Yasser Gonzalez-Fernandez,https://github.com/yasserglez/copulaedas,TRUE,https://github.com/yasserglez/copulaedas,24031,1,2021-06-09T03:40:25Z,24031
coRanking,"Calculates the co-ranking matrix to assess the
    quality of a dimensionality reduction.",2020-02-12,Guido Kraemer,https://github.com/gdkrmr/coRanking,TRUE,https://github.com/gdkrmr/coranking,53155,6,2020-11-11T14:22:52Z,8859.166666666666
corazon,"Allows the user to apply nice color gradients to 'shiny' elements.
    The gradients are extracted from the 'colorffy' website. See <https://www.colorffy.com/gradients/catalog>.",2020-06-28,Mohamed El Fodil Ihaddaden,https://github.com/feddelegrand7/corazon,TRUE,https://github.com/feddelegrand7/corazon,4616,3,2020-12-10T21:05:53Z,1538.6666666666667
coreCT,"Computed tomography (CT) imaging is a powerful tool for understanding the composition of sediment cores. This package streamlines and accelerates the analysis of CT data generated in the context of environmental science. Included are tools for processing raw DICOM images to characterize sediment composition (sand, peat, etc.). Root analyses are also enabled, including measures of external surface area and volumes for user-defined root size classes. For a detailed description of the application of computed tomography imaging for sediment characterization, see: Davey, E., C. Wigand, R. Johnson, K. Sundberg, J. Morris, and C. Roman. (2011) <DOI: 10.1890/10-2037.1>.",2021-02-05,Troy D. Hill,https://github.com/troyhill/coreCT,TRUE,https://github.com/troyhill/corect,19154,1,2021-02-05T13:32:06Z,19154
corncob,"Statistical modeling for correlated count data using the beta-binomial distribution, described in Martin et al. (2020) <doi:10.1214/19-AOAS1283>. It allows for both mean and overdispersion covariates.",2021-03-11,Bryan D Martin,https://github.com/bryandmartin/corncob,TRUE,https://github.com/bryandmartin/corncob,2722,68,2021-03-11T19:41:13Z,40.029411764705884
cornet,Implements lasso and ridge regression for dichotomised outcomes (Rauschenberger et al. 2021). Such outcomes are not naturally but artificially binary. They indicate whether an underlying measurement is greater than a threshold.,2021-04-19,Armin Rauschenberger,https://github.com/rauschenberger/cornet,TRUE,https://github.com/rauschenberger/cornet,16286,1,2021-04-19T07:52:26Z,16286
coro,"Provides 'coroutines' for R, a family of functions that
    can be suspended and resumed later on. This includes 'async'
    functions (which await) and generators (which yield). 'Async'
    functions are based on the concurrency framework of the 'promises'
    package. Generators are based on a dependency free iteration
    protocol defined in 'coro' and are compatible with iterators from
    the 'reticulate' package.",2020-12-17,Lionel Henry,https://github.com/r-lib/coro,TRUE,https://github.com/r-lib/coro,24988,98,2020-12-17T21:00:29Z,254.9795918367347
coronavirus,Provides a daily summary of the Coronavirus (COVID-19) cases by state/province. Data source: Johns Hopkins University Center for Systems Science and Engineering (JHU CCSE) Coronavirus <https://systems.jhu.edu/research/public-health/ncov/>.,2021-05-29,Rami Krispin,https://github.com/RamiKrispin/coronavirus,TRUE,https://github.com/ramikrispin/coronavirus,46941,395,2021-09-03T06:17:11Z,118.8379746835443
corporaexplorer,"Facilitates dynamic exploration of text collections through an
    intuitive graphical user interface and the power of regular expressions.
    The package contains 1) a helper function to convert a data frame to a
    'corporaexplorerobject', 2) a 'Shiny' app for fast and flexible exploration
    of a 'corporaexplorerobject', and 3) a 'Shiny' app for simple
    retrieval/extraction of documents from a 'corporaexplorerobject' in a
    reading-friendly format. The package also includes demo apps with which
    one can explore Jane Austen's novels and the State of the Union Addresses
    (data from the 'janeaustenr' and 'sotu' packages respectively).",2021-03-18,Kristian Lundby Gjerde,"https://kgjerde.github.io/corporaexplorer/,
https://github.com/kgjerde/corporaexplorer",TRUE,https://github.com/kgjerde/corporaexplorer,14643,48,2021-07-23T13:50:52Z,305.0625
CoRpower,"Calculates power for assessment of intermediate biomarker responses as correlates of risk in the active treatment group in clinical efficacy trials, as described in Gilbert, Janes, and Huang, Power/Sample Size Calculations for Assessing Correlates of Risk in Clinical Efficacy Trials (2016, Statistics in Medicine). The methods differ from past approaches by accounting for the level of clinical treatment efficacy overall and in biomarker response subgroups, which enables the correlates of risk results to be interpreted in terms of potential correlates of efficacy/protection. The methods also account for inter-individual variability of the observed biomarker response that is not biologically relevant (e.g., due to technical measurement error of the laboratory assay used to measure the biomarker response), which is important because power to detect a specified correlate of risk effect size is heavily affected by the biomarker's measurement error. The methods can be used for a general binary clinical endpoint model with a univariate dichotomous, trichotomous, or continuous biomarker response measured in active treatment recipients at a fixed timepoint after randomization, with either case-cohort Bernoulli sampling or case-control without-replacement sampling of the biomarker (a baseline biomarker is handled as a trivial special case). In a specified two-group trial design, the computeN() function can initially be used for calculating additional requisite design parameters pertaining to the target population of active treatment recipients observed to be at risk at the biomarker sampling timepoint. Subsequently, the power calculation employs an inverse probability weighted logistic regression model fitted by the tps() function in the 'osDesign' package. Power results as well as the relationship between the correlate of risk effect size and treatment efficacy can be visualized using various plotting functions. To link power calculations for detecting a correlate of risk and a correlate of treatment efficacy, a baseline immunogenicity predictor (BIP) can be simulated according to a specified classification rule (for dichotomous or trichotomous BIPs) or correlation with the biomarker response (for continuous BIPs), then outputted along with biomarker response data under assignment to treatment, and clinical endpoint data for both treatment and placebo groups.",2020-11-17,Michal Juraska,https://github.com/mjuraska/CoRpower,TRUE,https://github.com/mjuraska/corpower,15391,0,2020-11-17T01:51:35Z,NA
corpus,"Text corpus data analysis, with full support for international text (Unicode).  Functions for reading data from newline-delimited 'JSON' files, for normalizing and tokenizing text, for searching for term occurrences, and for computing term occurrence frequencies, including n-grams.",2021-05-02,Leslie Huang,"https://leslie-huang.github.io/r-corpus/,
https://github.com/leslie-huang/r-corpus",TRUE,https://github.com/leslie-huang/r-corpus,69595,1,2021-05-01T12:23:12Z,69595
corpustools,"Provides text analysis in R, focusing on the use of a tokenized text format. In this format, the positions of tokens are maintained, and each token can be annotated (e.g., part-of-speech tags, dependency relations).
    Prominent features include advanced Lucene-like querying for specific tokens or contexts (e.g., documents, sentences),
    similarity statistics for words and documents, exporting to DTM for compatibility with many text analysis packages,
    and the possibility to reconstruct original text from tokens to facilitate interpretation.",2021-06-25,Kasper Welbers and Wouter van Atteveldt,https://github.com/kasperwelbers/corpustools,TRUE,https://github.com/kasperwelbers/corpustools,28084,23,2021-05-25T10:41:30Z,1221.0434782608695
correlation,"Lightweight package for computing different kinds
    of correlations, such as partial correlations, Bayesian correlations,
    multilevel correlations, polychoric correlations, biweight
    correlations, distance correlations and more. Part of the 'easystats'
    ecosystem.",2021-04-09,Dominique Makowski,https://easystats.github.io/correlation/,TRUE,https://github.com/easystats/correlation,130952,252,2021-08-21T19:31:10Z,519.6507936507936
corrgram,"Calculates correlation of variables and displays the results
    graphically. Included panel functions can display points, shading, ellipses, and
    correlation values with confidence intervals. See Friendly (2002) <doi:10.1198/000313002533>.",2021-04-29,Kevin Wright,https://kwstat.github.io/corrgram/,TRUE,https://github.com/kwstat/corrgram,335213,12,2021-04-29T16:54:10Z,27934.416666666668
corrgrapher,"When exploring data or models we often examine variables one by one. 
  This analysis is incomplete if the relationship between these variables is 
  not taken into account. The 'corrgrapher' package facilitates simultaneous 
  exploration of the Partial Dependence Profiles and the correlation between 
  variables in the model.
  The package 'corrgrapher' is a part of the 'DrWhy.AI' universe.",2020-10-13,Pawel Morgen,"https://modeloriented.github.io/corrgrapher/,
https://github.com/ModelOriented/corrgrapher",TRUE,https://github.com/modeloriented/corrgrapher,6142,10,2020-10-10T16:03:29Z,614.2
corrplot,Provides a visual exploratory tool on correlation matrix that supports automatic variable reordering to help detect hidden patterns among variables.,2021-06-30,Taiyun Wei,https://github.com/taiyun/corrplot,TRUE,https://github.com/taiyun/corrplot,5872320,222,2021-07-12T13:12:57Z,26451.891891891893
corrr,"A tool for exploring correlations.
    It makes it possible to easily perform routine tasks when
    exploring correlation matrices such as ignoring the diagonal,
    focusing on the correlations of certain variables against others,
    or rearranging and visualizing the matrix in terms of the
    strength of the correlations.",2020-11-24,Max Kuhn,"https://github.com/tidymodels/corrr, https://corrr.tidymodels.org",TRUE,https://github.com/tidymodels/corrr,185717,515,2021-07-06T19:31:53Z,360.61553398058254
cort,"Provides S4 classes and methods to fit several copula models: The classic empirical checkerboard copula and the empirical checkerboard copula with known margins, see Cuberos, Masiello and Maume-Deschamps (2019) <doi:10.1080/03610926.2019.1586936> are proposed. These two models allow to fit copulas in high dimension with a small number of observations, and they are always proper copulas. Some flexibility is added via a possibility to differentiate the checkerboard parameter by dimension. The last model consist of the implementation of the Copula Recursive Tree algorithm proposed by Laverny, Maume-Deschamps, Masiello and Rullière (2020) <arXiv:2005.02912>, including the localised dimension reduction, which fits a copula by recursive splitting of the copula domain. We also provide an efficient way of mixing copulas, allowing to bag the algorithm into a forest, and a generic way of measuring d-dimensional boxes with a copula.",2020-12-01,Oskar Laverny,https://github.com/lrnv/cort,TRUE,https://github.com/lrnv/cort,8558,2,2021-01-21T15:21:20Z,4279
CoSMoS,"Makes univariate, multivariate, or random fields simulations precise and simple. Just select the desired time series or random fields’ properties and it will do the rest. CoSMoS is based on the framework described in Papalexiou (2018, <doi:10.1016/j.advwatres.2018.02.013>), extended for random fields in Papalexiou and Serinaldi (2020, <doi:10.1029/2019WR026331>), and further advanced in Papalexiou et al. (2021, <doi:10.1029/2020WR029466>) to allow fine-scale space-time simulation of storms (or even cyclone-mimicking fields).",2021-05-29,Simon Michael Papalexiou,https://github.com/TycheLab/CoSMoS,TRUE,https://github.com/tychelab/cosmos,14343,5,2021-05-31T17:47:57Z,2868.6
costsensitive,"Reduction-based techniques for cost-sensitive multi-class classification, in which each observation has a different cost for classifying it into one class, and the goal is to predict the class with the minimum expected cost for each new observation.
	Implements Weighted All-Pairs (Beygelzimer, A., Langford, J., & Zadrozny, B., 2008, <doi:10.1007/978-0-387-79361-0_1>), Weighted One-Vs-Rest (Beygelzimer, A., Dani, V., Hayes, T., Langford, J., & Zadrozny, B., 2005, <https://dl.acm.org/citation.cfm?id=1102358>) and Regression One-Vs-Rest.
	Works with arbitrary classifiers taking observation weights, or with regressors. Also implements cost-proportionate rejection sampling for working with classifiers
	that don't accept observation weights.",2019-07-28,David Cortes,https://github.com/david-cortes/costsensitive,TRUE,https://github.com/david-cortes/costsensitive,18202,31,2021-07-25T01:09:35Z,587.1612903225806
CoTiMA,"The 'CoTiMA' package performs meta-analyses of correlation matrices of repeatedly measured variables taken from 
   studies that used different time intervals. Different time intervals between measurement occasions impose problems for 
   meta-analyses because the effects (e.g. cross-lagged effects) cannot be simply aggregated, for example, by means of common 
   fixed or random effects analysis. However, continuous time math, which is applied in 'CoTiMA', can be used to extrapolate or 
   intrapolate the results from all studies to any desired time lag. By this, effects obtained in studies that used different 
   time intervals can be meta-analyzed. 'CoTiMA' fits models to empirical data using the structural equation model (SEM) package 
   'ctsem', the effects specified in a SEM are related to parameters that are not directly included in the model (i.e., 
   continuous time parameters; together, they represent the continuous time structural equation model, CTSEM). Statistical 
   model comparisons and significance tests are then performed on the continuous time parameter estimates. 'CoTiMA' also allows 
   analysis of publication bias (Egger's test, PET-PEESE estimates, zcurve analysis etc.) and analysis of statistical power 
   (post hoc power, required sample sizes). See Dormann, C., Guthier, C., & Cortina, J. M. (2019) <doi:10.1177/1094428119847277>.
   and Guthier, C., Dormann, C., & Voelkle, M. C. (2020) <doi:10.1037/bul0000304>.",2021-09-02,Christian Dormann,https://github.com/CoTiMA/CoTiMA,TRUE,https://github.com/cotima/cotima,2030,0,2021-08-24T06:52:50Z,NA
countfitteR,"A large number of measurements generate count data. This is a statistical data type 
    that only assumes non-negative integer values and is generated by counting. Typically, counting 
    data can be found in biomedical applications, such as the analysis of DNA double-strand breaks. 
    The number of DNA double-strand breaks can be counted in individual cells using various 
    bioanalytical methods. For diagnostic applications, it is relevant to record the distribution of 
    the number data in order to determine their biomedical significance (Roediger, S. et al., 2018. 
    Journal of Laboratory and Precision Medicine. <doi:10.21037/jlpm.2018.04.10>). The software 
    offers functions for a comprehensive automated evaluation of distribution models of count 
    data. In addition to programmatic interaction, a graphical user interface (web server) 
    is included, which enables fast and interactive data-scientific analyses. The user is supported 
    in selecting the most suitable counting distribution for his own data set.",2020-09-30,Jaroslaw Chilimoniuk,https://github.com/BioGenies/countfitteR,TRUE,https://github.com/biogenies/countfitter,14608,4,2021-03-05T16:29:15Z,3652
countrycode,"Standardize country names, convert them into one of 40
    different coding schemes, convert between coding schemes, and assign
    region descriptors.",2021-07-15,Vincent Arel-Bundock,https://vincentarelbundock.github.io/countrycode/,TRUE,https://github.com/vincentarelbundock/countrycode,364614,261,2021-07-15T02:16:38Z,1396.9885057471265
countToFPKM,"Implements the algorithm described in Trapnell,C. et al. (2010) <doi: 10.1038/nbt.1621>. This function takes read counts matrix of RNA-Seq data, feature lengths which can be retrieved using 'biomaRt' package, and the mean fragment lengths which can be calculated using the 'CollectInsertSizeMetrics(Picard)' tool. It then returns a matrix of FPKM normalised data by library size and feature effective length. It also provides the user with a quick and reliable function to generate FPKM heatmap plot of the highly variable features in RNA-Seq dataset.",2019-04-07,Ahmed Alhendi,https://github.com/AAlhendi1707/countToFPKM,TRUE,https://github.com/aalhendi1707/counttofpkm,22310,32,2021-04-12T16:58:54Z,697.1875
coveffectsplot,"Produce forest plots to visualize covariate effects using either
    the command line or an interactive 'Shiny' application.",2020-12-10,Samer Mouksassi,https://github.com/smouksassi/coveffectsplot,TRUE,https://github.com/smouksassi/coveffectsplot,17711,16,2021-02-12T14:57:14Z,1106.9375
COVID19,"Download COVID-19 data across governmental sources at national, regional, and city level, as described in Guidotti and Ardia (2020) <doi:10.21105/joss.02376>. Includes the time series of vaccines, tests, cases, deaths, recovered, hospitalizations, intensive therapy, and policy measures by 'Oxford COVID-19 Government Response Tracker' <https://www.bsg.ox.ac.uk/research/research-projects/coronavirus-government-response-tracker>. Provides a seamless integration with 'World Bank Open Data' <https://data.worldbank.org/>, 'Google Mobility Reports' <https://www.google.com/covid19/mobility/>, 'Apple Mobility Reports' <https://covid19.apple.com/mobility>.",2021-01-06,Emanuele Guidotti,https://covid19datahub.io,TRUE,https://github.com/covid19datahub/covid19,37066,193,2021-09-01T15:47:12Z,192.05181347150258
covid19.analytics,"Load and analyze updated time series worldwide data of reported cases for the Novel CoronaVirus Disease (CoViD-19) from different sources, including the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE) data repository <https://github.com/CSSEGISandData/COVID-19>, ""Our World in Data"" <https://github.com/owid/> among several others. The datasets reporting the CoViD19 cases are available in two main modalities, as a time series sequences and aggregated data for the last day with greater spatial resolution. Several analysis, visualization and modelling functions are available in the package that will allow the user to compute and visualize total number of cases, total number of changes and growth rate globally or for an specific geographical location, while at the same time generating models using these trends; generate interactive visualizations and generate Susceptible-Infected-Recovered (SIR) model for the disease spread.",2021-03-23,Marcelo Ponce,https://mponce0.github.io/covid19.analytics/,TRUE,https://github.com/mponce0/covid19.analytics,24930,28,2021-08-10T17:02:50Z,890.3571428571429
covid19br,"Set of functions to import COVID-19 pandemic data into R. The Brazilian COVID-19 data, obtained from the official Brazilian repository at <https://covid.saude.gov.br/>, is available at country, region, state, and city-levels. The package also downloads the world-level COVID-19 data from the John Hopkins University's repository.",2020-11-24,Fabio Demarqui,https://github.com/fndemarqui/covid19br,TRUE,https://github.com/fndemarqui/covid19br,5404,2,2020-11-24T11:36:12Z,2702
covid19india,"Pull raw and pre-cleaned versions of national and state-level 
    COVID-19 time-series data from covid19india.org <https://www.covid19india.org>. 
    Easily obtain and merge case count data, testing data, and vaccine data. 
    Also assists in calculating the time-varying effective reproduction number 
    with sensible parameters for COVID-19.",2021-07-28,Max Salvatore,https://github.com/maxsal/covid19india,TRUE,https://github.com/maxsal/covid19india,708,0,2021-08-27T16:13:28Z,NA
covid19italy,"Provides a daily summary of the Coronavirus (COVID-19) cases in Italy by country, region and province level. Data source: Presidenza del Consiglio dei Ministri - Dipartimento della Protezione Civile <https://www.protezionecivile.it/>.",2021-07-28,Rami Krispin,https://github.com/RamiKrispin/covid19italy,TRUE,https://github.com/ramikrispin/covid19italy,11766,38,2021-09-02T16:24:39Z,309.63157894736844
covid19sf,"Provides a verity of summary tables of the Covid19 cases in San Francisco. Data source: San Francisco, Department of Public Health - Population Health Division <https://datasf.org/opendata/>.",2021-07-03,Rami Krispin,https://github.com/RamiKrispin/covid19sf,TRUE,https://github.com/ramikrispin/covid19sf,3848,7,2021-09-03T12:58:30Z,549.7142857142857
covid19swiss,Provides a daily summary of the Coronavirus (COVID-19) cases in Switzerland cantons and Principality of Liechtenstein. Data source: Specialist Unit for Open Government Data Canton of Zurich <https://www.zh.ch/de/politik-staat/opendata.html>.,2020-09-18,Rami Krispin,https://github.com/Covid19R/covid19swiss,TRUE,https://github.com/covid19r/covid19swiss,4437,0,2021-01-05T13:10:20Z,NA
covidcast,"Tools for Delphi's 'COVIDcast Epidata' API: data access, maps and
    time series plotting, and basic signal processing. The API includes a
    collection of numerous indicators relevant to the COVID-19 pandemic in the
    United States, including official reports, de-identified aggregated medical
    claims data, large-scale surveys of symptoms and public behavior, and
    mobility data, typically updated daily and at the county level. All data
    sources are documented at
    <https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html>.",2021-05-04,Alex Reinhart,"https://cmu-delphi.github.io/covidcast/covidcastR/,
https://github.com/cmu-delphi/covidcast",TRUE,https://github.com/cmu-delphi/covidcast,1966,21,2021-06-16T18:58:55Z,93.61904761904762
covidprobability,"We propose a method to estimate the probability of an 
    undetected case of COVID-19 in a defined setting, when a given number of 
    people have been exposed, with a given pretest probability of having 
    COVID-19 as a result of that exposure. Since we are interested in undetected
    COVID-19, we assume no person has developed symptoms (which would warrant 
    further investigation) and that everyone was tested on a given day, and all 
    tested negative.",2021-02-11,Eric Brown,https://github.com/eebrown/covidprobability,TRUE,https://github.com/eebrown/covidprobability,2499,0,2021-02-12T18:49:29Z,NA
covidregionaldata,"An interface to subnational and national level COVID-19 data
    sourced from both official sources, such as Public Health England in
    the UK, and from other COVID-19 data collections, including the World
    Health Organisation (WHO), European Centre for Disease Prevention and
    Control (ECDC), John Hopkins University (JHU), Google Open Data and
    others. Designed to streamline COVID-19 data extraction, cleaning, and
    processing from a range of data sources in an open and transparent
    way. This allows users to inspect and scrutinise the data, and tools
    used to process it, at every step. For all countries supported, data
    includes a daily time-series of cases. Wherever available data is also
    provided for deaths, hospitalisations, and tests. National level data
    are also supported using a range of sources as well as line list data
    and links to intervention data sets.",2021-07-05,Sam Abbott,"https://epiforecasts.io/covidregionaldata/,
https://github.com/epiforecasts/covidregionaldata/",TRUE,https://github.com/epiforecasts/covidregionaldata,5558,29,2021-08-09T09:11:59Z,191.6551724137931
covidsymptom,"The COVID Symptom Study is a non-commercial project that uses a free mobile app to facilitate real-time data collection of symptoms, exposures, and risk factors related to COVID19. The package allows easy access to summary statistics data from COVID Symptom Study Sweden.",2021-03-31,Hugo Fitipaldi,https://github.com/csss-resultat/covidsymptom,TRUE,https://github.com/csss-resultat/covidsymptom,2549,2,2021-09-01T16:07:04Z,1274.5
covr,"Track and report code coverage for your package and (optionally)
    upload the results to a coverage service like 'Codecov' <https://codecov.io> or
    'Coveralls' <https://coveralls.io>. Code coverage is a measure of the amount of
    code being exercised by a set of tests. It is an indirect measure of test
    quality and completeness. This package is compatible with any testing
    methodology or framework and tracks coverage of both R code and compiled
    C/C++/FORTRAN code.",2020-09-16,Jim Hester,"https://covr.r-lib.org, https://github.com/r-lib/covr",TRUE,https://github.com/r-lib/covr,8385465,298,2021-08-19T15:42:47Z,28139.144295302012
CovTools,"Covariance is of universal prevalence across various disciplines within statistics.
    We provide a rich collection of geometric and inferential tools for convenient analysis of
    covariance structures, topics including distance measures, mean covariance estimator,
    covariance hypothesis test for one-sample and two-sample cases, and covariance estimation.
    For an introduction to covariance in multivariate statistical analysis,
    see Schervish (1987) <doi:10.1214/ss/1177013111>.",2021-08-13,Kisung You,https://github.com/kisungyou/CovTools,TRUE,https://github.com/kisungyou/covtools,27247,7,2021-08-13T16:34:51Z,3892.4285714285716
cowplot,"
    Provides various features that help with creating publication-quality figures
    with 'ggplot2', such as a set of themes, functions to align plots and arrange
    them into complex compound figures, and functions that make it easy to annotate
    plots and or mix plots with images. The package was originally written for
    internal use in the Wilke lab, hence the name (Claus O. Wilke's plot package).
    It has also been used extensively in the book Fundamentals of Data
    Visualization.",2020-12-30,Claus O. Wilke,https://wilkelab.org/cowplot/,TRUE,https://github.com/wilkelab/cowplot,6796216,600,2020-12-15T18:00:53Z,11327.026666666667
cowsay,"Allows printing of character strings as messages/warnings/etc.
    with ASCII animals, including cats, cows, frogs, chickens, ghosts,
    and more.",2020-02-06,Scott Chamberlain,https://github.com/sckott/cowsay,TRUE,https://github.com/sckott/cowsay,53558,217,2020-10-26T16:04:43Z,246.8110599078341
coxed,"Functions for generating, simulating, and visualizing expected durations and marginal changes in duration from the Cox proportional hazards model as described in Kropko and Harden (2017) <doi:10.1017/S000712341700045X> and Harden and Kropko (2018) <doi:10.1017/psrm.2018.19>.",2020-08-02,Kropko,https://github.com/jkropko/coxed,TRUE,https://github.com/jkropko/coxed,24253,10,2021-03-25T23:21:02Z,2425.3
coxphf,"Implements Firth's penalized maximum likelihood bias reduction method  for Cox regression
  which has been shown to provide a solution in case of monotone likelihood (nonconvergence of likelihood function), see 
  Heinze and Schemper (2001) and Heinze and Dunkler (2008).
  The program fits profile penalized likelihood confidence intervals which were proved to outperform
  Wald confidence intervals.",2020-09-28,Georg Heinze,https://cemsiis.meduniwien.ac.at/kb/wf/software/statistische-software/fccoxphf/,TRUE,https://github.com/georgheinze/coxphf,29600,0,2020-09-29T11:33:48Z,NA
CPBayes,"A Bayesian meta-analysis method for studying cross-phenotype
    genetic associations. It uses summary-level data across multiple phenotypes to
    simultaneously measure the evidence of aggregate-level pleiotropic association
    and estimate an optimal subset of traits associated with the risk locus. CPBayes
    is based on a spike and slab prior. The methodology is available from: A Majumdar, T Haldar, S Bhattacharya, JS Witte (2018) <doi:10.1371/journal.pgen.1007139>.",2020-12-02,Arunabha Majumdar,https://github.com/ArunabhaCodes/CPBayes,TRUE,https://github.com/arunabhacodes/cpbayes,19904,1,2020-12-01T21:31:30Z,19904
cplm,"Likelihood-based and Bayesian methods for various compound Poisson linear models based on Zhang, Yanwei (2013) <https://link.springer.com/article/10.1007/s11222-012-9343-7>.",2020-10-21,Yanwei (Wayne) Zhang,https://github.com/actuaryzhang/cplm,TRUE,https://github.com/actuaryzhang/cplm,161792,9,2020-11-06T06:49:04Z,17976.88888888889
cpp11,"Provides a header only, C++11 interface to R's C
    interface.  Compared to other approaches 'cpp11' strives to be safe
    against long jumps from the C API as well as C++ exceptions, conform
    to normal R function semantics and supports interaction with 'ALTREP'
    vectors.",2021-06-25,Jim Hester,https://github.com/r-lib/cpp11,TRUE,https://github.com/r-lib/cpp11,10990061,148,2021-08-23T13:41:26Z,74257.16891891892
cpr,"Implementation of the Control Polygon Reduction and Control Net
    Reduction methods for finding parsimonious B-spline regression models.",2017-03-07,Peter DeWitt,https://github.com/dewittpe/cpr/,TRUE,https://github.com/dewittpe/cpr,17176,2,2021-02-18T19:58:20Z,8588
cpsvote,"Provides automated methods for downloading, recoding, and merging 
    selected years of the Current Population Survey's Voting and Registration 
    Supplement, a large N national survey about registration, voting, and 
    non-voting in United States federal elections. Provides documentation for 
    appropriate use of sample weights to generate statistical estimates, 
    drawing from Hur & Achen (2013) <doi:10.1093/poq/nft042> and McDonald (2018) 
    <http://www.electproject.org/home/voter-turnout/voter-turnout-data>.",2020-11-05,Jay Lee,https://github.com/Reed-EVIC/cpsvote,TRUE,https://github.com/reed-evic/cpsvote,3634,2,2021-04-30T22:46:19Z,1817
cptcity,Incorporates colour gradients from the 'cpt-city' web archive available at <http://soliton.vm.bytemark.co.uk/pub/cpt-city/>. ,2020-10-02,Sergio Ibarra-Espinosa,https://github.com/ibarraespinosa/cptcity,TRUE,https://github.com/ibarraespinosa/cptcity,21887,11,2020-11-30T23:10:21Z,1989.7272727272727
cqcr,"Access data from the 'Care Quality Commission', the health 
    and adult social care regulator for England. The 'Care Quality Commission' 
    operates an API 
    <https://www.cqc.org.uk/about-us/transparency/using-cqc-data#api>, with data
    available under the Open Government License. Data includes information on 
    service providers, locations such as hospitals, care homes and 
    medical clinics, and ratings and inspection reports.",2019-10-07,Evan Odell,"https://github.com/evanodell/cqcr, https://docs.evanodell.com/cqcr",TRUE,https://github.com/evanodell/cqcr,10340,2,2020-11-22T13:29:24Z,5170
crandep,"The dependencies of CRAN packages can be analysed in a network fashion. For each package we can obtain the packages that it depends, imports, suggests, etc. By iterating this procedure over a number of packages, we can build, visualise, and analyse the dependency network, enabling us to have a bird's-eye view of the CRAN ecosystem. One aspect of interest is the number of reverse dependencies of the packages, or equivalently the in-degree distribution of the dependency network. This can be fitted by the power law and/or an extreme value mixture distribution <arXiv:2008.03073>, of which functions are provided.",2021-06-10,Clement Lee,https://github.com/clement-lee/crandep,TRUE,https://github.com/clement-lee/crandep,8245,6,2021-06-10T10:28:43Z,1374.1666666666667
cranlogs,"'API' to the database of 'CRAN' package downloads from the
    'RStudio' 'CRAN mirror'. The database itself is at <http://cranlogs.r-pkg.org>,
    see <https://github.com/r-hub/cranlogs.app> for the raw 'API'.",2019-04-29,Gábor Csárdi,"https://github.com/r-hub/cranlogs,
https://r-hub.github.io/cranlogs",TRUE,https://github.com/r-hub/cranlogs,42646,68,2020-11-24T08:57:11Z,627.1470588235294
CREAM,"Provides a new method for identification of clusters of genomic
 regions within chromosomes. Primarily, it is used for calling clusters of 
 cis-regulatory elements (COREs). 'CREAM' uses genome-wide maps of genomic regions
 in the tissue or cell type of interest, such as those generated from chromatin-based 
 assays including DNaseI, ATAC or ChIP-Seq. 'CREAM' considers proximity of the elements 
 within chromosomes of a given sample to identify COREs in the following steps:
 1) It identifies window size or the maximum allowed distance between the elements 
 within each CORE, 2) It identifies number of elements which should be clustered 
 as a CORE, 3) It calls COREs, 4) It filters the COREs with lowest order which 
 does not pass the threshold considered in the approach.",2018-06-06,Benjamin Haibe-Kains,https://github.com/bhklab/CREAM,TRUE,https://github.com/bhklab/cream,14897,11,2021-02-11T01:54:32Z,1354.2727272727273
crfsuite,"Wraps the 'CRFsuite' library <https://github.com/chokkan/crfsuite> allowing users 
    to fit a Conditional Random Field model and to apply it on existing data.
    The focus of the implementation is in the area of Natural Language Processing where this R package allows you to easily build and apply models 
    for named entity recognition, text chunking, part of speech tagging, intent recognition or classification of any category you have in mind. Next to training, a small web application
    is included in the package to allow you to easily construct training data.",2020-10-10,Jan Wijffels,https://github.com/bnosac/crfsuite,TRUE,https://github.com/bnosac/crfsuite,21818,52,2020-11-03T20:58:26Z,419.5769230769231
cricketr,"Tools for analyzing performances of cricketers based on stats in
    ESPN Cricinfo Statsguru. The toolset can  be used for analysis of Tests,ODIs 
    and Twenty20 matches of both batsmen and bowlers. The package can also be used to
    analyze team performances.",2021-03-23,Tinniam V Ganesh,https://github.com/tvganesh/cricketr,TRUE,https://github.com/tvganesh/cricketr,24048,56,2021-03-23T03:54:59Z,429.42857142857144
crimeutils,"A collection of functions that make it easier to understand crime (or other)
    data, and assist others in understanding it. The package helps you read data 
    from various sources, clean it, fix column names, and graph the data. ",2021-02-03,Jacob Kaplan,https://github.com/jacobkap/crimeutils/,TRUE,https://github.com/jacobkap/crimeutils,5001,0,2021-08-11T19:23:53Z,NA
crispRdesignR,"Designs guide sequences for CRISPR/Cas9 genome editing and 
    provides information on sequence features pertinent to guide 
    efficiency. Sequence features include annotated off-target 
    predictions in a user-selected genome and a predicted efficiency 
    score based on the model described in Doench et al. (2016) 
    <doi:10.1038/nbt.3437>. Users are able to import additional genomes 
    and genome annotation files to use when searching and annotating 
    off-target hits. All guide sequences and off-target data can be 
    generated through the 'R' console with sgRNA_Design() or through 
    'crispRdesignR's' user interface with crispRdesignRUI(). CRISPR 
    (Clustered Regularly Interspaced Short Palindromic Repeats) and the 
    associated protein Cas9 refer to a technique used in genome editing.",2021-01-11,Dylan Beeber,<https://github.com/dylanbeeber/crispRdesignR>,TRUE,https://github.com/dylanbeeber/crisprdesignr,6259,8,2021-02-06T18:40:18Z,782.375
criticalpath,"
    An R6 object oriented implementation of the Critical Path Method (CPM).
    CPM is a method used to estimate the minimum project duration and determine 
    the amount of scheduling flexibility on the logical network paths within the 
    schedule model. The flexibility is in terms of early start, early finish, 
    late start, late finish, total float and free float. Beside, it permits 
    to quantify the complexity of network diagram through the analysis of 
    topological indicators. Finally, it permits to change the activities duration 
    to perform what-if scenario analysis. The package was built based on following 
    references: To make topological sorting and other graph operation, we use 
    Csardi, G. & Nepusz, T. (2005) 
    <https://www.researchgate.net/publication/221995787_The_Igraph_Software_Package_for_Complex_Network_Research>;
    For schedule concept, the reference was Project Management Institute (2017) 
    <https://www.pmi.org/pmbok-guide-standards/foundational/pmbok>;
    For standards terms, we use Project Management Institute (2017) 
    <https://www.pmi.org/pmbok-guide-standards/lexicon>;
    For algorithms on Critical Path Method development, we use 
    Vanhoucke, M. (2013) <doi:10.1007/978-3-642-40438-2> and 
    Vanhoucke, M. (2014) <doi:10.1007/978-3-319-04331-9>; 
    And, finally, for topological definitions, we use
    Vanhoucke, M. (2009) <doi:10.1007/978-1-4419-1014-1>.",2021-02-05,Rubens Jose Rosa,"https://rubensjoserosa.com/criticalpath,
https://github.com/rubens2005/criticalpath",TRUE,https://github.com/rubens2005/criticalpath,2245,1,2021-02-05T13:15:18Z,2245
crmPack,"Implements a wide range of model-based dose
    escalation designs, ranging from classical and modern continual
    reassessment methods (CRMs) based on dose-limiting toxicity endpoints to
    dual-endpoint designs taking into account a biomarker/efficacy outcome. The
    focus is on Bayesian inference, making it very easy to setup a new design
    with its own JAGS code. However, it is also possible to implement 3+3
    designs for comparison or models with non-Bayesian estimation. The whole
    package is written in a modular form in the S4 class system, making it very
    flexible for adaptation to new models, escalation or stopping rules.",2019-06-13,Giuseppe Palermo,https://github.com/roche/crmPack,TRUE,https://github.com/roche/crmpack,23032,3,2021-01-15T14:05:05Z,7677.333333333333
cronologia,"Creates an HTML vertical timeline from a data frame as an input for
    'rmarkdown' documents and 'shiny' applications.",2021-04-22,Mohamed El Fodil Ihaddaden,https://github.com/feddelegrand7/cronologia,TRUE,https://github.com/feddelegrand7/cronologia,2794,41,2021-04-23T00:20:33Z,68.14634146341463
cropDemand,"Estimation of crop water demand can be processed via this package. As example, the data  from 'TerraClimate' dataset (<http://www.climatologylab.org/terraclimate.html>) calibrated with automatic weather stations of National Meteorological Institute of Brazil is available in a coarse spatial resolution to do the crop water demand. However, the user have also the option to download the variables directly from 'TerraClimate' repository with the download.terraclimate function  and access the original 'TerraClimate' products. If the user believes that is necessary calibrate the variables, there is another function to do it. Lastly, the estimation of the crop water demand present in this package can be run for all the Brazilian territory with 'TerraClimate' dataset. ",2021-01-26,Roberto Filgueiras,NA,TRUE,https://github.com/filgueirasr/cropdemand,2954,0,2021-01-26T04:20:59Z,NA
cropZoning,"Climate crop zoning based in minimum and maximum air temperature. The data used in the package are from 'TerraClimate' dataset (<http://www.climatologylab.org/terraclimate.html>), but, it have been calibrated with automatic weather stations  of National Meteorological Institute of Brazil.  The climate crop zoning of this package can be run for all the Brazilian territory.",2021-03-20,Roberto Filgueiras,NA,TRUE,https://github.com/filgueirasr/cropzoning,2868,0,2021-01-30T19:26:03Z,NA
crossmap,"Provides an extension to the 'purrr' family of mapping
    functions to apply a function to each combination of elements in a
    list of inputs.  Also includes functions for automatically detecting
    output type in mapping functions, finding every combination of
    elements of lists or rows of data frames, and applying multiple models
    to multiple subsets of a dataset.",2021-04-02,Alexander Rossell Hayes,"https://crossmap.rossellhayes.com,
https://github.com/rossellhayes/crossmap",TRUE,https://github.com/rossellhayes/crossmap,5040,13,2021-07-15T22:35:55Z,387.6923076923077
crosstable,"Create descriptive tables for continuous and categorical variables. 
    Apply summary statistics and counting function, with or without a grouping variable, and create beautiful reports using 'rmarkdown' or 'officer'.
    You can also compute statistical tests and effect sizes if needed.",2021-03-08,Dan Chaltiel,"https://danchaltiel.github.io/crosstable/,
https://github.com/DanChaltiel/crosstable/",TRUE,https://github.com/danchaltiel/crosstable,4389,23,2021-08-10T08:32:46Z,190.82608695652175
crosstalk,"Provides building blocks for allowing HTML widgets to communicate
    with each other, with Shiny or without (i.e. static .html files). Currently
    supports linked brushing and filtering.",2021-01-12,Carson Sievert,https://rstudio.github.io/crosstalk/,TRUE,https://github.com/rstudio/crosstalk,9358227,233,2021-06-29T21:57:02Z,40164.0643776824
crosswalkr,"A pair of functions for renaming and encoding data frames
	     using external crosswalk files. It is especially useful when
	     constructing master data sets from multiple smaller data
	     sets that do not name or encode variables consistently
	     across files. Based on similar commands in 'Stata'.",2020-01-08,Benjamin Skinner,https://github.com/btskinner/crosswalkr,TRUE,https://github.com/btskinner/crosswalkr,20526,8,2020-12-06T18:51:12Z,2565.75
crplyr,"In order to facilitate analysis of datasets hosted on the Crunch
    data platform <https://crunch.io/>, the 'crplyr' package implements 'dplyr'
    methods on top of the Crunch backend. The usual methods 'select', 'filter',
    'group_by', 'summarize', and 'collect' are implemented in such a way as to
    perform as much computation on the server and pull as little data locally
    as possible.",2021-02-02,Greg Freedman Ellis,"https://crunch.io/r/crplyr/, https://github.com/Crunch-io/crplyr",TRUE,https://github.com/crunch-io/crplyr,20592,5,2021-02-03T16:33:29Z,4118.4
crs,"Regression splines that handle a mix of continuous and categorical (discrete) data often encountered in applied settings. I would like to gratefully acknowledge support from the Natural Sciences and Engineering Research Council of Canada (NSERC, <https://www.nserc-crsng.gc.ca>), the Social Sciences and Humanities Research Council of Canada (SSHRC, <https://www.sshrc-crsh.gc.ca>), and the Shared Hierarchical Academic Research Computing Network (SHARCNET, <https://www.sharcnet.ca>).",2021-02-02,Jeffrey S. Racine,https://github.com/JeffreyRacine/R-Package-crs,TRUE,https://github.com/jeffreyracine/r-package-crs,93861,11,2021-02-01T23:37:36Z,8532.818181818182
crseEventStudy,"Based on Dutta et al. (2018) <doi:10.1016/j.jempfin.2018.02.004>, this package provides their standardized test for abnormal returns in long-horizon event studies. The methods used improve the major weaknesses of size, power, and robustness of long-run statistical tests described in Kothari/Warner (2007) <doi:10.1016/B978-0-444-53265-7.50015-9>. Abnormal returns are weighted by their statistical precision (i.e., standard deviation), resulting in abnormal standardized returns. This procedure efficiently captures the heteroskedasticity problem. Clustering techniques following Cameron et al. (2011) <doi:10.1198/jbes.2010.07136> are adopted for computing cross-sectional correlation robust standard errors. The statistical tests in this package therefore accounts for potential biases arising from returns' cross-sectional correlation, autocorrelation, and volatility clustering without power loss.",2021-01-11,Siegfried Köstlmeier,https://github.com/skoestlmeier/crseEventStudy,TRUE,https://github.com/skoestlmeier/crseeventstudy,17698,1,2021-01-11T11:45:53Z,17698
crsra,"Tidies and performs preliminary analysis of 'Coursera' research
    export data. These export data can be downloaded by anyone who has classes
    on Coursera and wants to analyze the data. Coursera is one of the leading 
    providers of MOOCs and was launched in January 2012. With over 25 million 
    learners, Coursera is the most popular provider in the world being followed 
    by EdX, the MOOC provider that was a result of a collaboration between 
    Harvard University and MIT, with over 10 million users. Coursera has over 
    150 university partners from 29 countries and offers a total of 2000+ 
    courses from computer science to philosophy. Besides, Coursera offers 180+ 
    specialization, Coursera's credential system, and four fully online Masters 
    degrees. For more information about Coursera check Coursera's
    About page on <https://blog.coursera.org/about/>.",2018-05-05,Aboozar Hadavand,NA,TRUE,https://github.com/jhudsl/crsra,13852,2,2021-03-23T15:33:11Z,6926
crunch,"The Crunch.io service <https://crunch.io/> provides a cloud-based
    data store and analytic engine, as well as an intuitive web interface.
    Using this package, analysts can interact with and manipulate Crunch
    datasets from within R. Importantly, this allows technical researchers to
    collaborate naturally with team members, managers, and clients who prefer a
    point-and-click interface.",2021-08-13,Greg Freedman Ellis,"https://crunch.io/r/crunch/, https://github.com/Crunch-io/rcrunch",TRUE,https://github.com/crunch-io/rcrunch,47271,8,2021-08-24T14:47:53Z,5908.875
crunchy,"To facilitate building custom dashboards on the Crunch data
    platform <https://crunch.io/>, the 'crunchy' package provides tools for
    working with 'shiny'. These tools include utilities to manage authentication
    and authorization automatically and custom stylesheets to help match the
    look and feel of the Crunch web application. The package also includes
    several gadgets for use in 'RStudio'.",2021-01-13,Greg Freedman Ellis,"https://crunch.io/r/crunchy/, https://github.com/Crunch-io/crunchy",TRUE,https://github.com/crunch-io/crunchy,24044,3,2021-01-13T21:25:55Z,8014.666666666667
CruzPlot,"A utility program oriented to create maps, plot data, and do basic data summaries
    of 'DAS' data <https://swfsc-publications.fisheries.noaa.gov/publications/TM/SWFSC/NOAA-TM-NMFS-SWFSC-305.PDF> 
    produced by 'WinCruz' from the Southwest Fisheries Science Center.
    <https://www.fisheries.noaa.gov/west-coast/science-data/california-current-marine-mammal-assessment-program>.",2021-03-01,Sam Woodman,"https://smwoodman.github.io/CruzPlot/,
https://github.com/smwoodman/CruzPlot/",TRUE,https://github.com/smwoodman/cruzplot,5316,0,2021-09-02T19:14:11Z,NA
crypto2,"Retrieves crypto currency information and historical prices as well as information on the exchanges they are listed on. Historical data contains daily open, high, low and close values for all crypto currencies. All data is scraped from <https://coinmarketcap.com> via their 'web-api'.",2021-06-24,Sebastian Stoeckl  (<https://orcid.org/0000-0002-4196-6093>,https://github.com/sstoeckl/crypto2,TRUE,https://github.com/sstoeckl/crypto2,970,4,2021-06-24T21:41:26Z,242.5
cryptowatchR,"An API wrapper for 'Cryptowatch' to get prices and other information (e.g., volume, trades, order books, bid and ask prices, live quotes, and more) about cryptocurrencies and crypto exchanges. See <https://docs.cryptowat.ch/rest-api> for a detailed documentation.",2021-06-09,Lorenz Brachtendorf,https://github.com/lorenzbr/cryptowatchR,TRUE,https://github.com/lorenzbr/cryptowatchr,1507,4,2021-06-14T19:04:01Z,376.75
cSEM,"Estimate, assess, test, and study linear, nonlinear, hierarchical 
  and multigroup structural equation models using composite-based approaches 
  and procedures, including estimation techniques such as partial least squares 
  path modeling (PLS-PM) and its derivatives (PLSc, ordPLSc, robustPLSc), 
  generalized structured component analysis (GSCA), generalized structured 
  component analysis with uniqueness terms (GSCAm), generalized canonical 
  correlation analysis (GCCA), principal component analysis (PCA), 
  factor score regression (FSR) using sum score, regression or 
  bartlett scores (including bias correction using Croon’s approach), 
  as well as several tests and typical postestimation procedures 
  (e.g., verify admissibility of the estimates, assess the model fit, 
  test the model fit etc.).",2021-04-19,Manuel E. Rademaker,"https://github.com/M-E-Rademaker/cSEM,
https://m-e-rademaker.github.io/cSEM/",TRUE,https://github.com/m-e-rademaker/csem,12417,13,2021-08-27T13:20:41Z,955.1538461538462
CSGo,An implementation of calls designed to collect and organize in an easy way the data from the Steam API specifically for the Counter-Strike Global Offensive Game (CS Go) <https://developer.valvesoftware.com/wiki/Steam_Web_API>.,2021-05-07,Adson Costanzi,https://github.com/adsoncostanzifilho/CSGo,TRUE,https://github.com/adsoncostanzifilho/csgo,2383,5,2021-05-10T13:29:08Z,476.6
CSUV,"Implementation of CSUV from C. Yuen and P. Fryzlewicz (2020) <arXiv:2003.02791> ""Exploiting disagreement between high-dimensional variable selectors for uncertainty visualization"". CSUV aims to perform variable selection and illustrate variable selection uncertainties by combining variable selection results from various methods.",2020-09-20,Christine Yuen,NA,TRUE,https://github.com/christineyuen/csuv,4992,0,2021-07-08T11:20:09Z,NA
csvwr,"Provide functions for reading and writing CSVW - i.e. CSV tables and JSON metadata.
    The metadata helps interpret CSV by setting the types and variable names.",2021-06-14,Robin Gower,"https://robsteranium.github.io/csvwr/,
https://github.com/Robsteranium/csvwr",TRUE,https://github.com/robsteranium/csvwr,1127,5,2021-07-10T20:53:47Z,225.4
ctf,"
    Column Text Format (CTF) is a new tabular data format designed for simplicity and performance.
    CTF is the simplest column store you can imagine: plain text files for each column in a table, and a metadata file.
    The underlying plain text means the data is human readable and familiar to programmers, unlike specialized binary formats.
    CTF is faster than row oriented formats like CSV when loading a subset of the columns in a table.
    This package provides functions to read and write CTF data from R.",2021-07-07,Clark Fitzgerald,https://github.com/julianofhernandez/ctf,TRUE,https://github.com/julianofhernandez/ctf,714,1,2021-07-23T17:46:07Z,714
ctmm,"Functions for identifying, fitting, and applying continuous-space, continuous-time stochastic movement models to animal tracking data.
  The package is described in Calabrese et al (2016) <doi:10.1111/2041-210X.12559>, with models and methods based on those introduced in
  Fleming & Calabrese et al (2014) <doi:10.1086/675504>,
  Fleming et al (2014) <doi:10.1111/2041-210X.12176>,
  Fleming et al (2015) <doi:10.1103/PhysRevE.91.032107>,
  Fleming et al (2015) <doi:10.1890/14-2010.1>,
  Fleming et al (2016) <doi:10.1890/15-1607>,
  Péron & Fleming et al (2016) <doi:10.1186/s40462-016-0084-7>,
  Fleming & Calabrese (2017) <doi:10.1111/2041-210X.12673>,
  Péron et al (2017) <doi:10.1002/ecm.1260>,
  Fleming et al (2017) <doi:10.1016/j.ecoinf.2017.04.008>,
  Fleming et al (2018) <doi:10.1002/eap.1704>,
  Winner & Noonan et al (2018) <doi:10.1111/2041-210X.13027>,
  Fleming et al (2019) <doi:10.1111/2041-210X.13270>,
  Noonan & Fleming et al (2019) <doi:10.1186/s40462-019-0177-1>,
  Fleming et al (2020) <doi:10.1101/2020.06.12.130195>,
  and
  Noonan et al (2021) <doi:10.1111/2041-210X.13597>.",2021-07-28,Christen H. Fleming,"https://github.com/ctmm-initiative/ctmm,
https://groups.google.com/g/ctmm-user",TRUE,https://github.com/ctmm-initiative/ctmm,46841,17,2021-08-28T00:55:22Z,2755.3529411764707
ctrdata,"Provides functions for querying, retrieving and analyzing
        protocol- and results-related information on clinical trials from
        two public registers, the 'European Union Clinical Trials Register'
        ('EUCTR', <https://www.clinicaltrialsregister.eu/>), 
        'ClinicalTrials.gov' ('CTGOV', <https://clinicaltrials.gov/>) and
        the 'ISRCTN' (<http://www.isrctn.com/>). 
        Trial information is downloaded, converted and stored in a database 
        ('SQLite' or 'MongoDB', via 'nodbi'). 
        Functions are provided to identify de-duplicated records, 
        to easily find and extract variables (fields) of interest even 
        from complex nesting as used by the registers, and
        to update previous queries that users retrieved in a database. 
        The package can be used for meta analysis and trend-analysis of
        the design and conduct as well as results of clinical trials.",2021-08-22,Ralf Herold,https://cran.r-project.org/package=ctrdata,TRUE,https://github.com/rfhb/ctrdata,19597,19,2021-08-23T06:36:15Z,1031.421052631579
ctsem,"Hierarchical continuous (and discrete) time state space modelling, for linear
    and nonlinear systems measured by  continuous variables, with limited support for 
    binary data. The subject specific dynamic system is modelled as a stochastic 
    differential equation (SDE) or difference equation, measurement models are typically multivariate normal factor models. 
    Linear mixed effects SDE's estimated via maximum likelihood and optimization are the default.
    Nonlinearities,  (state dependent parameters) and random effects on all parameters
    are possible, using either max likelihood / max a posteriori optimization 
    (with optional importance sampling) or Stan's Hamiltonian Monte Carlo sampling. 
    See  <https://github.com/cdriveraus/ctsem/raw/master/vignettes/hierarchicalmanual.pdf>
    for details. Priors may be used. For the conceptual overview of the hierarchical Bayesian 
    linear SDE approach, 
    see <https://www.researchgate.net/publication/324093594_Hierarchical_Bayesian_Continuous_Time_Dynamic_Modeling>.
    Exogenous inputs may also be included, for an overview of such possibilities see <https://www.researchgate.net/publication/328221807_Understanding_the_Time_Course_of_Interventions_with_Continuous_Time_Dynamic_Models> .
    Stan based functions are not available on 32 bit Windows systems at present. 
    <https://cdriver.netlify.app/> contains some tutorial blog posts.",2021-07-23,Charles Driver,https://github.com/cdriveraus/ctsem,TRUE,https://github.com/cdriveraus/ctsem,38982,19,2021-07-15T12:09:03Z,2051.684210526316
ctsemOMX,"Original 'ctsem' (continuous time structural equation modelling)
    functionality, based on the 'OpenMx' software, as described in 
    Driver, Oud, Voelkle (2017) <doi:10.18637/jss.v077.i05>, with updated details in vignette. 
    Combines stochastic differential equations representing latent processes with 
    structural equation measurement models. These functions were split off from
    the main package of 'ctsem', as the main package uses the 'rstan' package as a backend now --
    offering estimation options from max likelihood to Bayesian.
    There are nevertheless use cases for the wide format SEM style approach as offered here, 
    particularly when there are no individual differences in observation timing and the
    number of individuals is large. For the main 'ctsem' package, see <https://cran.r-project.org/package=ctsem>.",2021-06-02,Charles Driver,https://github.com/cdriveraus/ctsemOMX,TRUE,https://github.com/cdriveraus/ctsemomx,5985,0,2021-06-01T20:23:44Z,NA
cubature,"R wrappers around the cubature C library of Steven
    G. Johnson for adaptive multivariate integration over hypercubes
    and the Cuba C library of Thomas Hahn for deterministic and
    Monte Carlo integration. Scalar and vector interfaces for 
    cubature and Cuba routines are provided; the vector interfaces
    are highly recommended as demonstrated in the package
    vignette.",2021-05-13,Balasubramanian Narasimhan,https://bnaras.github.io/cubature/,TRUE,https://github.com/bnaras/cubature,790168,6,2021-08-19T23:47:55Z,131694.66666666666
cubelyr,"An implementation of a data cube extracted out of
    'dplyr' for backward compatibility.",2020-11-24,Hadley Wickham,https://github.com/hadley/cubelyr,TRUE,https://github.com/hadley/cubelyr,181191,26,2020-11-30T13:06:03Z,6968.884615384615
cubfits,"Estimating mutation and selection coefficients on synonymous
       codon bias usage based on models of ribosome overhead cost (ROC).
       Multinomial logistic regression and Markov Chain Monte Carlo are used to
       estimate and predict protein production rates with/without the presence
       of expressions and measurement errors. Work flows with examples for
       simulation, estimation and prediction processes are also provided
       with parallelization speedup. The whole framework is tested with
       yeast genome and gene expression data of Yassour, et al. (2009)
       <doi:10.1073/pnas.0812841106>.",2017-04-30,Wei-Chen Chen,https://github.com/snoweye/cubfits,TRUE,https://github.com/snoweye/cubfits,19829,4,2021-07-27T00:51:51Z,4957.25
Cubist,Regression modeling using rules with added instance-based corrections.,2021-05-28,Max Kuhn,"https://topepo.github.io/Cubist//,
https://topepo.github.io/Cubist/",TRUE,https://github.com/topepo/cubist,1009646,30,2021-05-28T16:08:39Z,33654.86666666667
CUFF,"Utility functions that provides wrapper to descriptive base functions
  like cor, mean and table.  It makes use of the formula interface to pass
  variables to functions.  It also provides operators to concatenate (%+%), to
  repeat (%n%) and manage character vectors for nice display.",2020-09-18,Charles-Édouard Giguère,https://github.com/giguerch/CUFF,TRUE,https://github.com/giguerch/cuff,20832,0,2021-05-12T15:05:25Z,NA
cuperdec,"Calculates and visualises cumulative percent 'decay' curves,
    which are typically calculated from metagenomic taxonomic profiles.
    These can be used to estimate the level of expected 'endogenous' taxa
    at different abundance levels retrieved from metagenomic samples, when
    comparing to samples of known sampling site or source.",2021-04-07,James A. Fellows Yates,https://github.com/jfy133/cuperdec,TRUE,https://github.com/jfy133/cuperdec,1741,2,2021-07-23T12:32:10Z,870.5
cuRe,"Contains functions for estimating generalized parametric mixture and non-mixture cure models, loss of lifetime, mean residual lifetime, and crude event probabilities.",2020-04-23,Lasse Hjort Jakobsen,http://github.com/LasseHjort/cuRe,TRUE,https://github.com/lassehjort/cure,9405,3,2021-05-17T08:09:27Z,3135
curl,"The curl() and curl_download() functions provide highly
    configurable drop-in replacements for base url() and download.file() with
    better performance, support for encryption (https, ftps), gzip compression,
    authentication, and other 'libcurl' goodies. The core of the package implements a
    framework for performing fully customized requests where data can be processed
    either in memory, on disk, or streaming via the callback or connection
    interfaces. Some knowledge of 'libcurl' is recommended; for a more-user-friendly
    web client see the 'httr' package which builds on this package with http
    specific tools and logic.",2021-06-23,Jeroen Ooms,"https://github.com/jeroen/curl (devel) https://curl.se/libcurl/
(upstream)",TRUE,https://github.com/jeroen/curl,22171209,175,2021-06-22T11:34:16Z,126692.62285714285
cutpointr,"Estimate cutpoints that optimize a specified metric in binary classification tasks
    and validate performance using bootstrapping. Some methods for more robust cutpoint
    estimation are supported, e.g. a parametric method assuming normal distributions,
    bootstrapped cutpoints, and smoothing of the metric values per cutpoint using
    Generalized Additive Models. Various plotting functions are included. For an overview
    of the package see Thiele and Hirschfeld (2021) <doi:10.18637/jss.v098.i11>.",2021-06-29,Christian Thiele,https://github.com/thie1e/cutpointr,TRUE,https://github.com/thie1e/cutpointr,38152,62,2021-06-28T22:15:03Z,615.3548387096774
cvAUC,"This package contains various tools for working with and evaluating cross-validated area under the ROC curve (AUC) estimators.  The primary functions of the package are ci.cvAUC and ci.pooled.cvAUC, which report cross-validated AUC and compute confidence intervals for cross-validated AUC estimates based on influence curves for i.i.d. and pooled repeated measures data, respectively.  One benefit to using influence curve based confidence intervals is that they require much less computation time than bootstrapping methods.  The utility functions, AUC and cvAUC, are simple wrappers for functions from the ROCR package. ",2014-12-09,Erin LeDell,https://github.com/ledell/cvAUC,TRUE,https://github.com/ledell/cvauc,81816,21,2021-01-22T03:28:32Z,3896
cvCovEst,"An efficient cross-validated approach for covariance matrix
    estimation, particularly useful in high-dimensional settings. This
    method relies upon the theory of loss-based estimator selection to
    identify the optimal estimator of the covariance matrix from among a
    prespecified set of candidates.",2021-07-25,Philippe Boileau,https://github.com/PhilBoileau/cvCovEst,TRUE,https://github.com/philboileau/cvcovest,2427,4,2021-07-24T18:18:06Z,606.75
cvms,"Cross-validate one or multiple regression and classification models
    and get relevant evaluation metrics in a tidy format. Validate the
    best model on a test set and compare it to a baseline evaluation.
    Alternatively, evaluate predictions from an external model. Currently
    supports regression and classification (binary and multiclass).
    Described in chp. 5 of Jeyaraman, B. P., Olsen, L. R., 
    & Wambugu M. (2019, ISBN: 9781838550134).",2021-06-17,Ludvig Renbo Olsen,https://github.com/ludvigolsen/cvms,TRUE,https://github.com/ludvigolsen/cvms,26747,28,2021-06-17T11:05:50Z,955.25
CVrisk,"Calculate various cardiovascular disease risk scores from the
    Framingham Heart Study (FHS), the American College of Cardiology (ACC),
    and the American Heart Association (AHA) as described in D’agostino, et al
    (2008) <doi:10.1161/circulationaha.107.699579>, Goff, et al (2013)
    <doi:10.1161/01.cir.0000437741.48606.98>, and Mclelland, et al (2015)
    <doi:10.1016/j.jacc.2015.08.035>.",2020-10-27,Victor Castro,https://github.com/vcastro/CVrisk,TRUE,https://github.com/vcastro/cvrisk,3696,5,2021-05-03T11:00:17Z,739.2
CVXR,"An object-oriented modeling language for disciplined
    convex programming (DCP) as described in Fu, Narasimhan, and Boyd
    (2020, <doi:10.18637/jss.v094.i14>). It allows the user to
    formulate convex optimization problems in a natural way following
    mathematical convention and DCP rules. The system analyzes the
    problem, verifies its convexity, converts it into a canonical
    form, and hands it off to an appropriate solver to obtain the
    solution. Interfaces to solvers on CRAN and elsewhere are
    provided, both commercial and open source.",2021-01-19,Anqi Fu,"https://cvxr.rbind.io, https://www.cvxgrp.org/CVXR/",TRUE,https://github.com/cvxgrp/cvxr,83978,145,2021-01-06T01:50:58Z,579.1586206896552
cwbtools,"The 'Corpus Workbench' ('CWB', <http://cwb.sourceforge.net/>) offers a classic and mature
 approach for working with large, linguistically and structurally annotated corpora. The 'CWB'
 is memory efficient and its design makes running queries fast (Evert and Hardie 2011,
 <http://www.stefan-evert.de/PUB/EvertHardie2011.pdf>). The 'cwbtools' package offers
 pure R tools to create indexed corpus files as well as high-level wrappers for the original C
 implementation of CWB as exposed by the 'RcppCWB' package
 <https://CRAN.R-project.org/package=RcppCWB>. Additional functionality to add and
 modify annotations of corpora from within R makes working with CWB indexed corpora
 much more flexible and convenient. The 'cwbtools' package in combination with the R packages
 'RcppCWB' (<https://CRAN.R-project.org/package=RcppCWB>) and 'polmineR'
 (<https://CRAN.R-project.org/package=polmineR>) offers a lightweight infrastructure
 to support the combination of quantitative and qualitative approaches for working
 with textual data.",2021-06-11,Andreas Blaette,https://github.com/PolMine/cwbtools,TRUE,https://github.com/polmine/cwbtools,14572,1,2021-06-14T12:36:23Z,14572
cxr,"Recent developments in modern coexistence theory have advanced 
    our understanding on how species are able to persist and co-occur 
    with other species at varying abundances. However, applying this 
    mathematical framework to empirical data is still challenging, 
    precluding a larger adoption of the theoretical tools developed 
    by empiricists. This package provides a complete toolbox for modelling 
    interaction effects between species, and calculate fitness and 
    niche differences. 
    The functions are flexible, may accept covariates, 
    and different fitting algorithms can be used. 
    A full description of the underlying methods is available in 
    García-Callejas, D., Godoy, O., and Bartomeus, I. (2020) <doi:10.1111/2041-210X.13443>.",2021-04-16,David Garcia-Callejas,https://github.com/RadicalCommEcol/cxr,TRUE,https://github.com/radicalcommecol/cxr,1638,6,2021-04-22T15:06:28Z,273
cyclestreets,"An interface to the cycle routing/data services provided by
    'CycleStreets', a not-for-profit social enterprise and advocacy
    organisation.  The application programming interfaces (APIs) provided
    by 'CycleStreets' are documented at
    (<https://www.cyclestreets.net/api/>).  The focus of this package is
    the journey planning API, which aims to emulate the routes taken by a
    knowledgeable cyclist.  An innovative feature of the routing service
    of its provision of fastest, quietest and balanced profiles.  These
    represent routes taken to minimise time, avoid traffic and compromise
    between the two, respectively.",2021-06-22,Robin Lovelace,https://github.com/cyclestreets/cyclestreets-r,TRUE,https://github.com/cyclestreets/cyclestreets-r,18310,11,2021-08-21T21:12:24Z,1664.5454545454545
cyclocomp,"Cyclomatic complexity is a software metric (measurement),
    used to indicate the complexity of a program. It is a quantitative
    measure of the number of linearly independent paths through a program's
    source code. It was developed by Thomas J. McCabe, Sr. in 1976.",2016-09-10,Gabor Csardi,https://github.com/MangoTheCat/cyclocomp,TRUE,https://github.com/mangothecat/cyclocomp,579617,36,2021-07-07T11:27:18Z,16100.472222222223
Cyclops,"This model fitting tool incorporates cyclic coordinate descent and
    majorization-minimization approaches to fit a variety of regression models
    found in large-scale observational healthcare data.  Implementations focus
    on computational optimization and fine-scale parallelization to yield
    efficient inference in massive datasets.  Please see:
    Suchard, Simpson, Zorych, Ryan and Madigan (2013) <doi:10.1145/2414416.2414791>.",2021-06-16,Marc A. Suchard,https://github.com/ohdsi/cyclops,TRUE,https://github.com/ohdsi/cyclops,33489,28,2021-06-17T13:32:01Z,1196.0357142857142
cyphr,"Encryption wrappers, using low-level support from
    'sodium' and 'openssl'.  'cyphr' tries to smooth over some pain
    points when using encryption within applications and data analysis
    by wrapping around differences in function names and arguments in
    different encryption providing packages.  It also provides
    high-level wrappers for input/output functions for seamlessly
    adding encryption to existing analyses.",2021-05-17,Rich FitzJohn,"https://github.com/ropensci/cyphr,
https://docs.ropensci.org/cyphr/",TRUE,https://github.com/ropensci/cyphr,21812,87,2021-05-18T07:33:26Z,250.71264367816093
cytominer,"Typical morphological profiling datasets have millions of cells
    and hundreds of features per cell. When working with this data, you must
    clean the data, normalize the features to make them comparable across
    experiments, transform the features, select features based on their
    quality, and aggregate the single-cell data, if needed. 'cytominer' makes
    these steps fast and easy. Methods used in practice in the field are
    discussed in Caicedo (2017) <doi:10.1038/nmeth.4397>. An overview of the
    field is presented in Caicedo (2016) <doi:10.1016/j.copbio.2016.04.003>.",2020-05-09,Shantanu Singh,https://github.com/cytomining/cytominer,TRUE,https://github.com/cytomining/cytominer,10763,32,2021-04-08T10:17:46Z,336.34375
czechrates,"
  Interface to interest and foreign exchange rates published by the Czech National Bank.",2021-05-25,Jindra Lacko,https://github.com/jla-data/czechrates,TRUE,https://github.com/jla-data/czechrates,6701,0,2021-05-26T07:00:49Z,NA
czso,"Get programmatic access to the open data provided by the
    Czech Statistical Office (CZSO, <https://czso.cz>).",2021-03-01,Petr Bouchal,"https://github.com/petrbouchal/czso, https://petrbouchal.xyz/czso/",TRUE,https://github.com/petrbouchal/czso,8314,9,2021-03-04T12:31:09Z,923.7777777777778
D2MCS,"
        Provides a novel framework to able to automatically develop and deploy
    an accurate Multiple Classifier System based on the feature-clustering 
    distribution achieved from an input dataset. 'D2MCS' was developed focused on 
    four main aspects: (i) the ability to determine an effective method to 
    evaluate the independence of features, (ii) the identification of the 
    optimal number of feature clusters, (iii) the training and tuning of ML 
    models and (iv) the execution of voting schemes to combine the outputs of 
    each classifier comprising the Multiple  Classifier System.",2021-05-07,Miguel Ferreiro-Díaz,https://github.com/drordas/D2MCS,TRUE,https://github.com/drordas/d2mcs,1156,0,2021-05-10T10:54:06Z,NA
d3r,"Provides a suite of functions to help ease the use of 'd3.js' in R.
              These helpers include 'htmltools::htmlDependency' functions, hierarchy
              builders, and conversion tools for 'partykit', 'igraph,' 'table',
              and 'data.frame' R objects into the 'JSON' that 'd3.js' expects.",2021-08-15,Mike Bostock,https://github.com/timelyportfolio/d3r,TRUE,https://github.com/timelyportfolio/d3r,322548,137,2021-08-15T17:31:56Z,2354.3649635036495
DA,"Discriminant Analysis (DA) for evolutionary inference (Qin, X. et al, 2020, <doi:10.22541/au.159256808.83862168>), especially for population genetic structure and community structure inference. This package incorporates the commonly used linear and non-linear, local and global supervised learning approaches (discriminant analysis), including Linear Discriminant Analysis of Kernel Principal Components (LDAKPC), Local (Fisher) Linear Discriminant Analysis (LFDA), Local (Fisher) Discriminant Analysis of Kernel Principal Components (LFDAKPC) and Kernel Local (Fisher) Discriminant Analysis (KLFDA). These discriminant analyses can be used to do ecological and evolutionary inference, including demography inference, species identification, and population/community structure inference.",2021-07-12,Xinghu Qin,https://xinghuq.github.io/DA/index.html,TRUE,https://github.com/xinghuq/da,778,1,2021-07-11T04:34:22Z,778
dabestr,"Data Analysis using Bootstrap-Coupled ESTimation.
    Estimation statistics is a simple framework that avoids the pitfalls of
    significance testing. It uses familiar statistical concepts: means,
    mean differences, and error bars. More importantly, it focuses on the
    effect size of one's experiment/intervention, as opposed to a false
    dichotomy engendered by P values.
    An estimation plot has two key features:
    1. It presents all datapoints as a swarmplot, which orders each point to
    display the underlying distribution.
    2. It presents the effect size as a bootstrap 95% confidence interval on a
    separate but aligned axes.
    Estimation plots are introduced in Ho et al., Nature Methods 2019, 1548-7105.
    <doi:10.1038/s41592-019-0470-3>.
    The free-to-view PDF is located at <https://rdcu.be/bHhJ4>.",2020-07-13,Joses W. Ho,https://github.com/ACCLAB/dabestr,TRUE,https://github.com/acclab/dabestr,29326,181,2021-07-26T11:26:36Z,162.02209944751382
dabr,"Provides functions to manage databases: select, update, insert,
    and delete records, list tables, backup tables as CSV files, and import
    CSV files as tables.",2021-05-22,Roberto Villegas-Diaz,"https://github.com/special-uor/dabr/,
https://special-uor.github.io/dabr/,
https://research.reading.ac.uk/palaeoclimate/",TRUE,https://github.com/special-uor/dabr,4486,1,2021-05-22T16:57:47Z,4486
dadjokeapi,"What is funnier than a dad joke?  A dad joke in R!  This package 
    utilizes the API for <https://icanhazdadjoke.com> and returns dad jokes from
    several API endpoints.",2021-03-01,Jeffrey Hollister,https://github.com/jhollist/dadjokeapi/,TRUE,https://github.com/jhollist/dadjokeapi,2238,16,2021-03-01T15:59:28Z,139.875
dagitty,"A port of the web-based software 'DAGitty', available at 
    <http://dagitty.net>, for analyzing structural causal models 
    (also known as directed acyclic graphs or DAGs).
    This package computes covariate adjustment sets for estimating causal
    effects, enumerates instrumental variables, derives testable
    implications (d-separation and vanishing tetrads), generates equivalent
    models, and includes a simple facility for data simulation. ",2021-01-21,Johannes Textor,"http://www.dagitty.net, https://github.com/jtextor/dagitty",TRUE,https://github.com/jtextor/dagitty,104501,147,2021-07-27T15:25:58Z,710.8911564625851
daiR,"R interface for the Google Cloud Services 'Document AI API' 
    <https://cloud.google.com/document-ai/> with additional tools for output file 
    parsing and text reconstruction. 'Document AI' is a powerful server-based 
    OCR processor that extracts text and tables from images and pdf files with 
    high accuracy. 'daiR' gives R users programmatic access to this processor and 
    additional tools to handle and visualize the output. See the package website 
    <https://dair.info/> for more information and examples.",2021-06-11,Thomas Hegghammer,"https://github.com/Hegghammer/daiR, https://dair.info",TRUE,https://github.com/hegghammer/dair,981,23,2021-08-31T17:16:35Z,42.65217391304348
DALEX,"Unverified black box model is the path to the failure. Opaqueness leads to distrust. 
  Distrust leads to ignoration. Ignoration leads to rejection. 
  DALEX package xrays any model and helps to explore and explain its behaviour.
  Machine Learning (ML) models are widely used and have various applications in classification 
  or regression. Models created with boosting, bagging, stacking or similar techniques are often
  used due to their high performance. But such black-box models usually lack of direct interpretability.
  DALEX package contains various methods that help to understand the link between input variables 
  and model output. Implemented methods help to explore model on the level of a single instance 
  as well as a level of the whole dataset.
  All model explainers are model agnostic and can be compared across different models.
  DALEX package is the cornerstone for 'DrWhy.AI' universe of packages for visual model exploration.
  Find more details in (Biecek 2018) <arXiv:1806.08915>.",2021-07-28,Przemyslaw Biecek,"https://dalex.drwhy.ai, https://github.com/ModelOriented/DALEX",TRUE,https://github.com/modeloriented/dalex,115646,888,2021-09-01T13:56:01Z,130.231981981982
DALEXtra,"Provides wrapper of various machine learning models. 
  In applied machine learning, there 
  is a strong belief that we need to strike a balance 
  between interpretability and accuracy. 
  However, in field of the interpretable machine learning, 
  there are more and more new ideas for explaining black-box models, 
  that are implemented in 'R'. 
  'DALEXtra' creates 'DALEX' Biecek (2018) <arXiv:1806.08915> explainer for many type of models
  including those created using 'python' 'scikit-learn' and 'keras' libraries, and 'java' 'h2o' library. 
  Important part of the package is Champion-Challenger analysis and innovative approach
  to model performance across subsets of test data presented in Funnel Plot. 
  Third branch of 'DALEXtra' package is aspect importance analysis
  that provides instance-level explanations for the groups of explanatory variables.",2021-05-09,Szymon Maksymiuk,"https://ModelOriented.github.io/DALEXtra/,
https://github.com/ModelOriented/DALEXtra",TRUE,https://github.com/modeloriented/dalextra,18199,47,2021-05-13T12:39:18Z,387.21276595744683
dampack,"A suite of functions for analyzing and visualizing the health economic outputs of mathematical models.
    This package was developed with funding from the National Institutes of Allergy and Infectious Diseases of the 
    National Institutes of Health under award no. R01AI138783. The content of this package is solely the 
    responsibility of the authors and does not necessarily represent the official views of the National Institutes 
    of Health. The theoretical underpinnings of 'dampack''s functionality are detailed in Hunink et al. (2014) 
    <doi:10.1017/CBO9781139506779>.",2021-05-30,Greg Knowlton,https://github.com/DARTH-git/dampack,TRUE,https://github.com/darth-git/dampack,2451,21,2021-07-28T17:16:31Z,116.71428571428571
damr,"Loads behavioural data from the widely used Drosophila Activity Monitor System (DAMS, TriKinetics <https://trikinetics.com/>) into the rethomics framework.",2020-11-16,Quentin Geissmann,https://github.com/rethomics/damr,TRUE,https://github.com/rethomics/damr,17770,3,2021-06-11T09:11:24Z,5923.333333333333
dams,"The single largest source of dams in the United States is the
    National Inventory of Dams (NID) <http://nid.usace.army.mil> from the US
    Army Corps of Engineers. Entire data from the NID cannot be obtained all at
    once and NID's website limits extraction of more than a couple of thousand
    records at a time. Moreover, selected data from the NID's user interface
    cannot not be saved to a file. In order to make the analysis of this data
    easier, all the data from NID was extracted manually. Subsequently, the raw
    data was checked for potential errors and cleaned. This package provides
    sample cleaned data from the NID and provides functionality to access the
    entire cleaned NID data.",2020-05-20,Joseph Stachelek,https://github.com/jsta/dams,TRUE,https://github.com/jsta/dams,18512,6,2021-08-03T13:19:26Z,3085.3333333333335
dang,A collection of utility functions.,2021-02-17,Dirk Eddelbuettel with contributions by Brodie Gaslam,NA,TRUE,https://github.com/eddelbuettel/dang,14408,4,2021-08-29T18:25:37Z,3602
darksky,"Provides programmatic access to the 'Dark Sky' 'API' 
    <https://darksky.net/dev/docs>, which provides current or historical global 
    weather conditions.",2017-09-20,Bob Rudis,https://github.com/hrbrmstr/darksky,TRUE,https://github.com/hrbrmstr/darksky,22431,81,2021-05-23T11:01:55Z,276.9259259259259
dash,"A framework for building analytical web applications, 'dash' offers a pleasant and productive development experience. No JavaScript required.",2020-06-04,Ryan Patrick Kyle,https://github.com/plotly/dashR,TRUE,https://github.com/plotly/dashr,19481,355,2021-08-23T15:01:34Z,54.87605633802817
dashboardthemes,"Allows manual creation of themes and logos to be used in
    applications created using the 'shinydashboard' package. Removes the need to
    change the underlying css code by wrapping it into a set of convenient R
    functions.",2021-08-21,Nik Lilovski,https://github.com/nik01010/dashboardthemes,TRUE,https://github.com/nik01010/dashboardthemes,24441,288,2021-08-21T13:28:32Z,84.86458333333333
dashCoreComponents,"'Dash' ships with supercharged components for interactive user interfaces. A core set of components, written and maintained by the 'Dash' team, is available in the 'dashCoreComponents' package. The source for this package is on GitHub: plotly/dash-core-components.",2020-05-06,Ryan Patrick Kyle,https://github.com/plotly/dash-core-components,TRUE,https://github.com/plotly/dash-core-components,19012,252,2021-09-01T15:53:36Z,75.44444444444444
dashHtmlComponents,"'Dash' is a web application framework that provides pure Python and R abstraction around HTML, CSS, and JavaScript. Instead of writing HTML or using an HTML templating engine, you compose your layout using R functions within the 'dashHtmlComponents' package. The source for this package is on GitHub: plotly/dash-html-components.",2020-05-06,Ryan Patrick Kyle,https://github.com/plotly/dash-html-components,TRUE,https://github.com/plotly/dash-html-components,18925,149,2021-09-01T15:43:42Z,127.01342281879195
dashPivottable,"Pivot tables are useful for interactive presentation of  summary statistics computed for data contained in another table. The 'dashPivottable' package wraps 'react-pivottable', making it easy to add drag-and-drop tables into your Dash for R applications.",2020-08-12,Ryan Patrick Kyle,https://github.com/plotly/dash-pivottable,TRUE,https://github.com/plotly/dash-pivottable,5035,137,2021-01-28T17:30:51Z,36.75182481751825
dashTable,"An interactive table component designed for editing and exploring large datasets, 'dashDataTable' is rendered with standard, semantic HTML <table/> markup, which makes it accessible, responsive, and easy to style. This component was written from scratch in 'React.js' specifically for the 'dash' community. Its API was designed to be ergonomic and its behaviour is completely customizable through its  properties.",2020-05-14,Ryan Patrick Kyle,https://github.com/plotly/dash-table,TRUE,https://github.com/plotly/dash-table,17798,410,2021-09-01T15:43:14Z,43.40975609756097
dat,"An implementation of common higher order functions with syntactic
    sugar for anonymous function. Provides also a link to 'dplyr' and
    'data.table' for common transformations on data frames to work around non
    standard evaluation by default.",2020-05-15,Sebastian Warnholz,NA,TRUE,https://github.com/wahani/dat,24759,15,2020-12-04T09:34:50Z,1650.6
data.table,"Fast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns, friendly and fast character-separated-value read/write. Offers a natural and flexible syntax, for faster development.",2021-02-21,Matt Dowle,"https://r-datatable.com, https://Rdatatable.gitlab.io/data.table,
https://github.com/Rdatatable/data.table",TRUE,https://github.com/rdatatable/data.table,21653466,2811,2021-09-02T20:38:43Z,7703.118463180363
data.tree,"Create tree structures from hierarchical data, and traverse the
    tree in various orders. Aggregate, cumulate, print, plot, convert to and from
    data.frame and more. Useful for decision trees, machine learning, finance,
    conversion from and to JSON, and many other applications.",2020-08-03,Russ Hyde [ctb,http://github.com/gluc/data.tree,TRUE,https://github.com/gluc/data.tree,1306657,173,2021-01-26T14:58:15Z,7552.93063583815
data.validator,"Validate dataset by columns and rows using convenient predicates inspired by 'assertr' package. 
             Generate good looking HTML report or print console output to display in logs of your data processing pipeline.",2021-03-02,Krystian Igras,NA,TRUE,https://github.com/appsilon/data.validator,2553,40,2021-04-30T08:46:40Z,63.825
DatabaseConnector,"An R 'DataBase Interface' ('DBI') compatible interface to various database platforms ('PostgreSQL', 'Oracle', 'Microsoft SQL Server', 
    'Amazon Redshift', 'Microsoft Parallel Database Warehouse', 'IBM Netezza', 'Apache Impala', 'Google BigQuery', and 'SQLite'). Also includes support for
    fetching data as 'Andromeda' objects. Uses 'Java Database Connectivity' ('JDBC') to connect to databases (except SQLite).",2021-04-15,Martijn Schuemie,"https://ohdsi.github.io/DatabaseConnector/,
https://github.com/OHDSI/DatabaseConnector",TRUE,https://github.com/ohdsi/databaseconnector,47925,32,2021-04-14T05:00:39Z,1497.65625
DatabaseConnectorJars,Provides external JAR dependencies for the 'DatabaseConnector' package.,2019-04-07,Martijn Schuemie,https://github.com/OHDSI/DatabaseConnectorJars,TRUE,https://github.com/ohdsi/databaseconnectorjars,33281,0,2021-07-27T07:29:22Z,NA
DatabionicSwarm,"Algorithms implementing populations of agents that interact with one another and sense their environment may exhibit emergent behavior such as self-organization and swarm intelligence. Here, a swarm system called Databionic swarm (DBS) is introduced which was published in Thrun, M.C., Ultsch A.: ""Swarm Intelligence for Self-Organized Clustering"" (2020), Artificial Intelligence, <DOI:10.1016/j.artint.2020.103237>. DBS is able to adapt itself to structures of high-dimensional data such as natural clusters characterized by distance and/or density based structures in the data space. The first module is the parameter-free projection method called Pswarm (Pswarm()), which exploits the concepts of self-organization and emergence, game theory, swarm intelligence and symmetry considerations. The second module is the parameter-free high-dimensional data visualization technique, which generates projected points on the topographic map with hypsometric tints defined by the generalized U-matrix (GeneratePswarmVisualization()). The third module is the clustering method itself with non-critical parameters (DBSclustering()). Clustering can be verified by the visualization and vice versa. The term DBS refers to the method as a whole. It enables even a non-professional in the field of data mining to apply its algorithms for visualization and/or clustering to data sets with completely different structures drawn from diverse research fields. The comparison to common projection methods can be found in the book of Thrun, M.C.: ""Projection Based Clustering through Self-Organization and Swarm Intelligence"" (2018) <DOI:10.1007/978-3-658-20540-9>. A comparison to 26 common clustering algorithms on 15 datasets is presented on the website.",2021-01-12,Michael Thrun,http://www.deepbionics.org,TRUE,https://github.com/mthrun/databionicswarm,22329,8,2021-01-13T08:21:00Z,2791.125
datacleanr,"Flexible and efficient cleaning of data with interactivity.
  'datacleanr' facilitates best practices in data analyses and reproducibility with built-in features and by translating interactive/manual operations to code. 
  The package is designed for interoperability, and so seamlessly fits into reproducible analyses pipelines in 'R'.",2021-02-10,Alexander Hurley,https://github.com/the-Hull/datacleanr,TRUE,https://github.com/the-hull/datacleanr,4304,12,2021-07-07T13:18:27Z,358.6666666666667
dataCompareR,"Easy comparison of two tabular data
    objects in R. Specifically designed to show differences between two sets of
    data in a useful way that should make it easier to understand the differences,
    and if necessary, help you work out how to remedy them. Aims
    to offer a more useful output than all.equal() when your two data sets do not
    match, but isn't intended to replace all.equal() as a way to test for equality.",2020-04-30,Sarah Johnston,https://github.com/capitalone/dataCompareR,TRUE,https://github.com/capitalone/datacomparer,26340,61,2021-08-31T16:30:43Z,431.8032786885246
DataEditR,"An interactive editor built on 'rhandsontable' to allow the 
  interactive viewing, entering, filtering and editing of data in R 
  <https://dillonhammill.github.io/DataEditR/>.",2021-07-09,Dillon Hammill,https://github.com/DillonHammill/DataEditR,TRUE,https://github.com/dillonhammill/dataeditr,14105,198,2021-07-26T00:26:15Z,71.23737373737374
DataExplorer,"Automated data exploration process for analytic tasks and predictive modeling, so
    that users could focus on understanding data and extracting insights. The package scans and
    analyzes each variable, and visualizes them with typical graphical techniques. Common
    data processing methods are also available to treat and format data.",2020-12-15,Boxuan Cui,http://boxuancui.github.io/DataExplorer/,TRUE,https://github.com/boxuancui/dataexplorer,305727,399,2021-08-12T01:15:10Z,766.2330827067669
datafsm,"Automatic generation of finite state machine models of dynamic 
    decision-making that both have strong predictive power and are 
    interpretable in human terms. We use an efficient model representation and 
    a genetic algorithm-based estimation process to generate simple 
    deterministic approximations that explain most of the structure of complex 
    stochastic processes. We have applied the software to empirical data, and 
    demonstrated it's ability to recover known data-generating processes by 
    simulating data with agent-based models and correctly deriving the 
    underlying decision models for multiple agent models and degrees of
    stochasticity.",2021-05-29,Jonathan M. Gilligan,"https://jonathan-g.github.io/datafsm/,
https://github.com/jonathan-g/datafsm",TRUE,https://github.com/jonathan-g/datafsm,19036,9,2021-05-29T00:06:30Z,2115.1111111111113
datagovindia,"This wrapper allows the user to communicate with more than
    80,000 API posted on data.gov.in - open data
    platform of the government of India <https:data.gov.in/ogpl_apis>. 
    It also allows the user to search for the API required through the universe
    of the API with a better interface than the one the official website provides.
    Once a user has the ID by using the API discovery functionalities, 
    it allows one to converse with the API using a consistent format across all
    available API.",2021-05-31,Abhishek Arora,https://github.com/econabhishek/datagovindia,TRUE,https://github.com/econabhishek/datagovindia,2025,4,2021-09-02T03:28:13Z,506.25
dataMaid,"Data screening is an important first step of any statistical
    analysis. dataMaid auto generates a customizable data report with a thorough
    summary of the checks and the results that a human can use to identify possible
    errors. It provides an extendable suite of test for common potential
    errors in a dataset. ",2019-12-10,Claus Thorn Ekstrøm,"https://github.com/ekstroem/dataMaid,
https://doi.org/10.18637/jss.v090.i06",TRUE,https://github.com/ekstroem/datamaid,51748,127,2020-12-18T16:01:13Z,407.46456692913387
dataMeta,Designed to create a basic data dictionary and append to the original dataset's attributes list. The package makes use of a tidy dataset and creates a data frame that will serve as a linker that will aid in building the dictionary. The dictionary is then appended to the list of the original dataset's attributes. The user will have the option of entering variable and item descriptions by writing code or use alternate functions that will prompt the user to add these.,2017-08-12,Dania M. Rodriguez,https://github.com/dmrodz/dataMeta,TRUE,https://github.com/dmrodz/datameta,18550,16,2020-11-19T03:16:37Z,1159.375
datamods,"'Shiny' modules to import data into an application or 'addin'
    from various sources, and to manipulate them after that.",2021-07-02,Victor Perrier,https://github.com/dreamRs/datamods,TRUE,https://github.com/dreamrs/datamods,40491,66,2021-08-06T14:58:41Z,613.5
dataonderivatives,"Post Global Financial Crisis derivatives reforms have lifted the 
  veil off over-the-counter (OTC) derivative markets. Swap Execution Facilities
  (SEFs) and Swap Data Repositories (SDRs) now publish data on swaps that are 
  traded on or reported to those facilities (respectively). This package provides
  you the ability to get this data from supported sources.",2018-02-10,Imanuel Costigan,"https://imanuelcostigan.github.io/dataonderivatives,
https://github.com/imanuelcostigan/dataonderivatives",TRUE,https://github.com/imanuelcostigan/dataonderivatives,17842,29,2021-01-20T21:27:16Z,615.2413793103449
dataone,"Provides read and write access to data and metadata from
    the DataONE network <https://www.dataone.org> of data repositories.  
    Each DataONE repository implements a consistent repository application 
    programming interface. Users call methods in R to access these remote 
    repository functions, such as methods to query the metadata catalog, get 
    access to metadata for particular data packages, and read the data objects 
    from the data repository. Users can also insert and update data objects on 
    repositories that support these methods.",2020-12-06,Matthew B. Jones,https://github.com/DataONEorg/rdataone,TRUE,https://github.com/dataoneorg/rdataone,29559,31,2021-02-24T02:59:24Z,953.516129032258
datapack,"Provides a flexible container to transport and manipulate complex
    sets of data. These data may consist of multiple data files and associated
    meta data and ancillary files. Individual data objects have associated system
    level meta data, and data files are linked together using the OAI-ORE standard
    resource map which describes the relationships between the files. The OAI-
    ORE standard is described at <https://www.openarchives.org/ore/>. Data packages
    can be serialized and transported as structured files that have been created
    following the BagIt specification. The BagIt specification is described at
    <https://tools.ietf.org/html/draft-kunze-bagit-08>.",2020-11-04,Matthew B. Jones,"https://docs.ropensci.org/datapack/,
https://github.com/ropensci/datapack",TRUE,https://github.com/ropensci/datapack,26978,40,2020-11-03T18:13:57Z,674.45
datapackage.r,"Work with 'Frictionless Data Packages' (<https://specs.frictionlessdata.io//data-package/>). Allows to load and validate any descriptor for a data package profile, create and modify descriptors and provides expose methods for reading and streaming data in the package. When a descriptor is a 'Tabular Data Package', it uses the 'Table Schema' package (<https://CRAN.R-project.org/package=tableschema.r>) and exposes its functionality, for each resource object in the resources field.",2021-04-16,Kleanthis Koupidis,https://github.com/frictionlessdata/datapackage-r,TRUE,https://github.com/frictionlessdata/datapackage-r,15038,34,2021-09-02T12:17:39Z,442.29411764705884
datapasta,RStudio addins and R functions that make copy-pasting vectors and tables to text painless.,2020-01-17,Miles McBain,https://github.com/milesmcbain/datapasta,TRUE,https://github.com/milesmcbain/datapasta,62970,741,2020-12-30T11:17:22Z,84.97975708502024
dataPreparation,Do most of the painful data preparation for a data science project with a minimum amount of code; Take advantages of 'data.table' efficiency and use some algorithmic trick in order to perform data preparation in a time and RAM efficient way.,2020-12-16,Emmanuel-Lin Toulemonde,NA,TRUE,https://github.com/eltoulemonde/datapreparation,67875,26,2020-12-15T20:12:03Z,2610.576923076923
dataReporter,"Data screening is an important first step of any statistical
    analysis. 'dataReporter' auto generates a customizable data report with a thorough
    summary of the checks and the results that a human can use to identify possible
    errors. It provides an extendable suite of test for common potential
    errors in a dataset. See Petersen AH, Ekstrøm CT (2019). ""dataMaid: Your Assistant for Documenting Supervised Data Quality Screening in R."" _Journal of Statistical Software_, *90*(6), 1-38 <doi:10.18637/jss.v090.i06> for more information.",2020-12-14,Claus Thorn Ekstrøm,https://github.com/ekstroem/dataReporter,TRUE,https://github.com/ekstroem/datareporter,4066,48,2021-03-26T08:40:52Z,84.70833333333333
dataRetrieval,"Collection of functions to help retrieve U.S. Geological Survey
    (USGS) and U.S. Environmental Protection Agency (EPA) water quality and
    hydrology data from web services. USGS web services are discovered from 
    National Water Information System (NWIS) <https://waterservices.usgs.gov/> and <https://waterdata.usgs.gov/nwis>. 
    Both EPA and USGS water quality data are obtained from the Water Quality Portal <https://www.waterqualitydata.us/>.",2021-07-30,Laura DeCicco,https://pubs.usgs.gov/tm/04/a10/,TRUE,https://github.com/usgs-r/dataretrieval,67456,187,2021-07-30T19:16:03Z,360.72727272727275
datarium,"Contains data organized by topics: categorical data, regression model, 
            means comparisons, independent and repeated measures ANOVA, mixed ANOVA and ANCOVA.",2019-05-21,Alboukadel Kassambara,NA,TRUE,https://github.com/kassambara/datarium,80414,10,2021-01-20T20:19:53Z,8041.4
datasailr,A row by row data processing tool. You can write data processing code in 'DataSailr' script which is specially intended for data manipulation. The package uses 'libsailr' (C/C++ library) for its 'DataSailr' code parsing and its execution.,2021-08-25,Toshihiro Umehara,https://datasailr.io,TRUE,https://github.com/niceume/datasailr,8238,2,2021-08-14T05:37:50Z,4119
DataSpaceR,"Provides a convenient API interface to access immunological data
    within 'the CAVD DataSpace'(<https://dataspace.cavd.org>), a data sharing 
    and discovery tool that facilitates exploration of HIV immunological data 
    from pre-clinical and clinical HIV vaccine studies.",2021-09-02,Jason Taylor,"https://docs.ropensci.org/DataSpaceR/,
https://github.com/ropensci/DataSpaceR",TRUE,https://github.com/ropensci/dataspacer,14282,4,2021-09-02T22:05:38Z,3570.5
dataspice,"The goal of 'dataspice' is to make it easier for researchers to
  create basic, lightweight, and concise metadata files for their datasets.
  These basic files can then be used to make useful information available during
  analysis, create a helpful dataset ""README"" webpage, and produce more complex
  metadata formats to aid dataset discovery. Metadata fields are based on
  the 'Schema.org' and 'Ecological Metadata Language' standards.",2021-05-16,Carl Boettiger,https://github.com/ropensci/dataspice,TRUE,https://github.com/ropensci/dataspice,4273,146,2021-05-28T07:39:19Z,29.267123287671232
dataverse,"Provides access to Dataverse APIs <https://dataverse.org/> (versions 4-5),
    enabling data search, retrieval, and deposit. For Dataverse versions <= 3.0,
    use the archived 'dvn' package <https://cran.r-project.org/package=dvn>.",2021-07-26,Shiro Kuriwaki,"https://iqss.github.io/dataverse-client-r/,
https://dataverse.org/,
https://github.com/iqss/dataverse-client-r",TRUE,https://github.com/iqss/dataverse-client-r,21630,50,2021-07-26T15:07:35Z,432.6
DataVisualizations,"Gives access to data visualisation methods that are relevant from the data scientist's point of view. The flagship idea of 'DataVisualizations' is the mirrored density plot (MD-plot) for either classified or non-classified multivariate data published in Thrun, M.C. et al.: ""Analyzing the Fine Structure of Distributions"" (2020), PLoS ONE, <DOI:10.1371/journal.pone.0238835>. The MD-plot outperforms the box-and-whisker diagram (box plot), violin plot and bean plot and geom_violin plot of ggplot2. Furthermore, a collection of various visualization methods for univariate data is provided. In the case of exploratory data analysis, 'DataVisualizations' makes it possible to inspect the distribution of each feature of a dataset visually through a combination of four methods. One of these methods is the Pareto density estimation (PDE) of the probability density function (pdf). Additionally, visualizations of the distribution of distances using PDE, the scatter-density plot using PDE for two variables as well as the Shepard density plot and the Bland-Altman plot are presented here. Pertaining to classified high-dimensional data, a number of visualizations are described, such as f.ex. the heat map and silhouette plot. A political map of the world or Germany can be visualized with the additional information defined by a classification of countries or regions. By extending the political map further, an uncomplicated function for a Choropleth map can be used which is useful for measurements across a geographic area. For categorical features, the Pie charts, slope charts and fan plots, improved by the ABC analysis, become usable. More detailed explanations are found in the book by Thrun, M.C.: ""Projection-Based Clustering through Self-Organization and Swarm Intelligence"" (2018) <DOI:10.1007/978-3-658-20540-9>.",2021-01-12,Michael Thrun,http://www.deepbionics.org,TRUE,https://github.com/mthrun/datavisualizations,30808,4,2021-07-09T17:20:07Z,7702
datawizard,"A lightweight package to easily manipulate, clean,
    transform, and prepare your data for analysis. It also forms
    the data wrangling backend for the packages in the 'easystats'
    ecosystem.",2021-09-02,Dominique Makowski (<https://orcid.org/0000-0001-5375-9967>,https://easystats.github.io/datawizard/,TRUE,https://github.com/easystats/datawizard,64922,24,2021-08-25T06:21:43Z,2705.0833333333335
daterangepicker,"A Shiny Input for date-ranges, which pops up two calendars for selecting dates, times, or predefined ranges like ""Last 30 Days"". It wraps the JavaScript library 'daterangepicker' which is available at <https://www.daterangepicker.com>.",2020-03-20,Sebastian Gatscha,"https://github.com/trafficonese/daterangepicker/,
https://www.daterangepicker.com",TRUE,https://github.com/trafficonese/daterangepicker,11057,10,2021-02-03T09:50:41Z,1105.7
datetimeutils,"Utilities for handling dates and times, such
   as selecting particular days of the week or month,
   formatting timestamps as required by RSS feeds, or
   converting timestamp representations of other software
   (such as 'MATLAB' and 'Excel') to R. The package is
   lightweight (no dependencies, pure R implementations) and
   relies only on R's standard classes to represent dates
   and times ('Date' and 'POSIXt'); it aims to provide
   efficient implementations, through vectorisation and the
   use of R's native numeric representations of timestamps
   where possible.",2021-04-01,Enrico Schumann,"http://enricoschumann.net/R/packages/datetimeutils/,
https://github.com/enricoschumann/datetimeutils",TRUE,https://github.com/enricoschumann/datetimeutils,28914,5,2021-07-25T19:56:05Z,5782.8
datos,"Provee una versión traducida de los siguientes
    conjuntos de datos: 'airlines', 'airports', 'AwardsManagers',
    'babynames', 'Batting', 'diamonds', 'faithful', 'fueleconomy',
    'Fielding', 'flights', 'gapminder', 'gss_cat', 'iris', 'Managers',
    'mpg', 'mtcars', 'atmos', 'People, 'Pitching', 'planes',
    'presidential', 'table1', 'table2', 'table3', 'table4a', 'table4b',
    'table5', 'vehicles', 'weather', 'who'.  English: It provides a
    Spanish translated version of the datasets listed above.",2020-06-15,Riva Quiroga,https://github.com/cienciadedatos/datos,TRUE,https://github.com/cienciadedatos/datos,29477,30,2021-07-30T04:56:59Z,982.5666666666667
datplot,"Converting date ranges into dating 'steps' eases
    the visualization of changes in e.g. pottery consumption, style and
    other variables over time. This package provides tools to process and
    prepare data for visualization and employs the concept of aoristic
    analysis.",2021-03-04,Lisa Steinmann,https://github.com/lsteinmann/datplot,TRUE,https://github.com/lsteinmann/datplot,2121,2,2021-06-26T14:24:43Z,1060.5
datr,"Interface with the 'Dat' p2p network protocol <https://datproject.org>. Clone archives from the network, share your own files, and install packages from the network.",2018-03-26,Chris Hartgerink,https://github.com/libscie/datr,TRUE,https://github.com/libscie/datr,14110,54,2020-09-24T14:27:10Z,261.2962962962963
daymetr,"Programmatic interface to the 'Daymet' web services
    (<http://daymet.ornl.gov>). Allows for easy downloads of
    'Daymet' climate data directly to your R workspace or your computer.
    Routines for both single pixel data downloads and
    gridded (netCDF) data are provided.",2021-05-25,Koen Hufkens,https://github.com/bluegreen-labs/daymetr,TRUE,https://github.com/bluegreen-labs/daymetr,21422,16,2021-05-25T12:41:12Z,1338.875
dbarts,"Fits Bayesian additive regression trees (BART; Chipman, George, and McCulloch (2010) <doi:10.1214/09-AOAS285>) while allowing the updating of predictors or response so that BART can be incorporated as a conditional model in a Gibbs/Metropolis-Hastings sampler. Also serves as a drop-in replacement for package 'BayesTree'.",2021-01-05,Vincent Dorie,https://github.com/vdorie/dbarts,TRUE,https://github.com/vdorie/dbarts,41269,38,2021-06-17T18:15:55Z,1086.0263157894738
dbflobr,"Reads and writes files to SQLite databases <https://www.sqlite.org/index.html> as flobs 
    (a flob is a blob that preserves the file extension).",2020-05-13,Sebastian Dalgarno,https://github.com/poissonconsulting/dbflobr,TRUE,https://github.com/poissonconsulting/dbflobr,12124,5,2021-02-26T17:10:52Z,2424.8
dbhydroR,"Client for programmatic access to the South Florida Water
  Management District's 'DBHYDRO' database at 
  <https://www.sfwmd.gov/science-data/dbhydro>, with functions
  for accessing hydrologic and water quality data. ",2021-02-21,Joseph Stachelek,"https://github.com/ropensci/dbhydroR,
https://docs.ropensci.org/dbhydroR/",TRUE,https://github.com/ropensci/dbhydror,14354,10,2021-08-04T14:33:45Z,1435.4
DBI,"A database interface definition for communication
    between R and relational database management systems.  All classes in
    this package are virtual and need to be extended by the various R/DBMS
    implementations.",2021-01-15,Kirill Müller,"https://dbi.r-dbi.org, https://github.com/r-dbi/DBI",TRUE,https://github.com/r-dbi/dbi,12251530,227,2021-08-29T17:01:57Z,53971.49779735683
DBItest,"A helper that tests 'DBI' back ends for conformity
    to the interface.",2021-07-31,Kirill Müller,"https://dbitest.r-dbi.org, https://github.com/r-dbi/DBItest",TRUE,https://github.com/r-dbi/dbitest,243127,17,2021-08-12T04:26:00Z,14301.588235294117
dbmss,"Simple computation of spatial statistic functions of distance to characterize the spatial structures of mapped objects, following Marcon, Traissac, Puech, and Lang (2015) <doi:10.18637/jss.v067.c03>.
             Includes classical functions (Ripley's K and others) and more recent ones used by spatial economists (Duranton and Overman's Kd, Marcon and Puech's M). 
             Relies on 'spatstat' for some core calculation.",2021-08-22,Eric Marcon,https://github.com/EricMarcon/dbmss,TRUE,https://github.com/ericmarcon/dbmss,31720,4,2021-08-22T14:57:47Z,7930
dbnR,"Learning and inference over dynamic Bayesian networks of arbitrary 
    Markovian order. Extends some of the functionality offered by the 'bnlearn' 
    package to learn the networks from data and perform exact inference. 
    It offers two structure learning algorithms for dynamic Bayesian networks
    and the possibility to perform forecasts of arbitrary length. A tool for 
    visualizing the structure of the net is also provided via the 'visNetwork' 
    package.",2020-10-13,David Quesada,https://github.com/dkesada/dbnR,TRUE,https://github.com/dkesada/dbnr,9037,11,2021-07-23T12:00:22Z,821.5454545454545
dbparser,"This tool is for parsing the 'DrugBank' XML database <https://www.drugbank.ca/>. The parsed 
    data are then returned in a proper 'R' dataframe with the ability to save 
    them in a given database.",2020-08-26,Mohammed Ali,"https://docs.ropensci.org/dbparser/,
https://github.com/ropensci/dbparser/",TRUE,https://github.com/ropensci/dbparser,20079,38,2021-05-31T12:00:28Z,528.3947368421053
dbplyr,"A 'dplyr' back end for databases that allows you to
    work with remote database tables as if they are in-memory data frames.
    Basic features works with any database that has a 'DBI' back end; more
    advanced features require 'SQL' translation to be provided by the
    package author.",2021-04-06,Hadley Wickham,"https://dbplyr.tidyverse.org/, https://github.com/tidyverse/dbplyr",TRUE,https://github.com/tidyverse/dbplyr,12822222,313,2021-08-14T05:42:09Z,40965.565495207666
dbscan,"A fast reimplementation of several density-based algorithms of
    the DBSCAN family for spatial data. Includes the clustering algorithms 
    DBSCAN (density-based spatial clustering of applications with noise)
    and HDBSCAN (hierarchical DBSCAN), the ordering algorithm
    OPTICS (ordering points to identify the clustering structure), 
    and the outlier detection algorithm LOF (local outlier factor). 
    The implementations use the kd-tree data structure (from library ANN) for faster k-nearest neighbor search. 
    An R interface to fast kNN and fixed-radius NN search is also provided. 
    Hahsler, Piekenbrock and Doran (2019) <doi:10.18637/jss.v091.i01>.",2021-04-27,Michael Hahsler,https://github.com/mhahsler/dbscan,TRUE,https://github.com/mhahsler/dbscan,725923,175,2021-07-05T19:32:04Z,4148.131428571429
dbx,"Provides select, insert, update, upsert, and delete database operations. Supports 'PostgreSQL', 'MySQL', 'SQLite', and more, and plays nicely with the 'DBI' package.",2021-01-17,Andrew Kane,https://github.com/ankane/dbx,TRUE,https://github.com/ankane/dbx,29409,158,2021-08-17T23:17:53Z,186.13291139240508
dccvalidator,"Performs checks for common metadata quality issues. Used by the
    data coordinating centers for the 'AMP-AD' consortium
    (<https://adknowledgeportal.synapse.org>), 'PsychENCODE' consortium
    (<http://www.psychencode.org>), and others to validate metadata prior to
    data releases.",2020-06-19,Nicole Kauer,"https://sage-bionetworks.github.io/dccvalidator,
https://github.com/Sage-Bionetworks/dccvalidator",TRUE,https://github.com/sage-bionetworks/dccvalidator,10178,8,2021-09-01T20:51:19Z,1272.25
DCLEAR,R codes for distance based cell lineage reconstruction. Our methods won both sub-challenges 2 and 3 of the Allen Institute Cell Lineage Reconstruction DREAM Challenge in 2020. The challenge paper is Gong et al. (2021) <doi:10.1016/j.cels.2021.05.008>.,2021-09-03,Il-Youp Kwak,https://github.com/ikwak2/DCLEAR,TRUE,https://github.com/ikwak2/dclear,708,1,2021-09-03T11:13:40Z,708
dcmodify,"Data cleaning scripts typically contain a lot of 'if this change that'
    type of statements. Such statements are typically condensed expert knowledge.
    With this package, such 'data modifying rules' are taken out of the code and
    become in stead parameters to the work flow. This allows one to maintain, document,
    and reason about data modification rules as separate entities.",2021-04-29,Mark van der Loo,https://github.com/data-cleaning/dcmodify,TRUE,https://github.com/data-cleaning/dcmodify,17363,7,2021-08-02T19:14:15Z,2480.4285714285716
dcurves,"Diagnostic and prognostic models are typically evaluated with
    measures of accuracy that do not address clinical consequences.
    Decision-analytic techniques allow assessment of clinical outcomes,
    but often require collection of additional information may be
    cumbersome to apply to models that yield a continuous result. Decision
    curve analysis is a method for evaluating and comparing prediction
    models that incorporates clinical consequences, requires only the data
    set on which the models are tested, and can be applied to models that
    have either continuous or dichotomous results. See the following references 
    for details on the methods: Vickers (2006) <doi:10.1177/0272989X06295361>,
    Vickers (2008) <doi:10.1186/1472-6947-8-53>, 
    and Pfeiffer (2020) <doi:10.1002/bimj.201800240>.",2021-07-20,Daniel D. Sjoberg,"https://github.com/ddsjoberg/dcurves,
http://www.danieldsjoberg.com/dcurves/",TRUE,https://github.com/ddsjoberg/dcurves,776,15,2021-08-17T14:18:28Z,51.733333333333334
dde,"Solves ordinary and delay differential equations, where
    the objective function is written in either R or C.  Suitable only
    for non-stiff equations, the solver uses a 'Dormand-Prince' method
    that allows interpolation of the solution at any point.  This
    approach is as described by Hairer, Norsett and Wanner (1993)
    <ISBN:3540604529>.  Support is also included for iterating
    difference equations.",2020-01-16,Rich FitzJohn,https://github.com/mrc-ide/dde,TRUE,https://github.com/mrc-ide/dde,14646,14,2021-01-27T09:46:33Z,1046.142857142857
DDIwR,"Useful functions for various DDI (Data Documentation Initiative) related inputs and outputs.
    Converts data files to and from SPSS, Stata, SAS, R and Excel, including user declared missing values.",2021-09-02,Adrian Dusa,https://github.com/dusadrian/DDIwR,TRUE,https://github.com/dusadrian/ddiwr,23098,8,2021-08-31T18:58:48Z,2887.25
ddpcr,"An interface to explore, analyze, and visualize droplet digital PCR
    (ddPCR) data in R. This is the first non-proprietary software for analyzing
    two-channel ddPCR data. An interactive tool was also created and is available
    online to facilitate this analysis for anyone who is not comfortable with
    using R.",2020-06-02,Dean Attali,https://github.com/daattali/ddpcr,TRUE,https://github.com/daattali/ddpcr,26033,47,2021-01-09T02:21:36Z,553.8936170212766
deBInfer,"A Bayesian framework for parameter inference in differential equations.
    This approach offers a rigorous methodology for parameter inference as well as
    modeling the link between unobservable model states and parameters, and
    observable quantities. Provides templates for the DE model, the
    observation model and data likelihood, and the model parameters and their prior
    distributions. A Markov chain Monte Carlo (MCMC) procedure processes these inputs
    to estimate the posterior distributions of the parameters and any derived
    quantities, including the model trajectories. Further functionality is provided
    to facilitate MCMC diagnostics and the visualisation of the posterior distributions
    of model parameters and trajectories.",2018-04-18,Philipp H Boersch-Supan,https://github.com/pboesu/debinfer,TRUE,https://github.com/pboesu/debinfer,18977,11,2021-06-03T14:44:51Z,1725.1818181818182
DeCAFS,"Detect abrupt changes in time series with local fluctuations as a random walk process and autocorrelated noise as an AR(1) process. See Romano, G., Rigaill, G., Runge, V., Fearnhead, P. (2021) <doi:10.1080/01621459.2021.1909598>.",2021-04-09,Gaetano Romano,NA,TRUE,https://github.com/gtromano/decafs,5957,0,2021-04-28T08:54:19Z,NA
declared,"A set of functions that offer an alternative to package 'haven', to deal with
    labelled objects with existing values declared as missing.",2021-09-02,Adrian Dusa,https://github.com/dusadrian/declared,TRUE,https://github.com/dusadrian/declared,1329,0,2021-08-31T19:12:10Z,NA
DeclareDesign,"Researchers can characterize and learn about the properties of
    research designs before implementation using `DeclareDesign`. Ex ante
    declaration and diagnosis of designs can help researchers clarify the 
    strengths and limitations of their designs and to improve their 
    properties, and can help readers evaluate a research strategy prior
    to implementation and without access to results. It can also make it
    easier for designs to be shared, replicated, and critiqued.",2021-08-21,Graeme Blair,"https://declaredesign.org/r/declaredesign/,
https://github.com/DeclareDesign/DeclareDesign",TRUE,https://github.com/declaredesign/declaredesign,28958,84,2021-08-21T15:14:07Z,344.73809523809524
decompr,"Three global value chain (GVC) decompositions are implemented. 
    The Leontief decomposition derives the value added origin of exports by 
    country and industry as in Hummels, Ishii and Yi (2001). The Koopman, 
    Wang and Wei (2014) decomposition splits country-level exports into 9 
    value added components, and the Wang, Wei and Zhu (2013) decomposition 
    splits bilateral exports into 16 value added components. Various GVC 
    indicators based on these decompositions are computed in the 
    complimentary 'gvc' package. 
    --- References: ---
    Hummels, D., Ishii, J., & Yi, K. M. (2001). The nature and growth of 
       vertical specialization in world trade. Journal of international 
       Economics, 54(1), 75-96.
    Koopman, R., Wang, Z., & Wei, S. J. (2014). Tracing value-added and double 
       counting in gross exports. American Economic Review, 104(2), 459-94.
    Wang, Z., Wei, S. J., & Zhu, K. (2013). Quantifying international production 
       sharing at the bilateral and sector levels (No. w19677). 
       National Bureau of Economic Research.",2021-05-10,Bastiaan Quast,"https://qua.st/decompr/, https://github.com/bquast/decompr",TRUE,https://github.com/bquast/decompr,29019,10,2021-05-10T15:22:36Z,2901.9
decor,"Retrieves code comment decorations for C++
    languages of the form '\\ [[xyz]]', which are used for automated
    wrapping of C++ functions.",2020-06-30,Jim Hester,https://github.com/r-lib/decor,TRUE,https://github.com/r-lib/decor,98524,2,2021-01-07T13:12:20Z,49262
decorators,"A decorator is a function that receives a function, extends its 
    behaviour, and returned the altered function. Any caller that uses the 
    decorated function uses the same interface as it were the original, 
    undecorated function. Decorators serve two primary uses: (1) Enhancing the 
    response of a function as it sends data to a second component; (2) 
    Supporting multiple optional behaviours. An example of the first use is a 
    timer decorator that runs a function, outputs its execution time on the 
    console, and returns the original function's result. An example of the 
    second use is input type validation decorator that during running time 
    tests whether the caller has passed input arguments of a particular class.
    Decorators can reduce execution time, say by memoization, or reduce bugs 
    by adding defensive programming routines.",2021-03-22,Harel Lustiger,"https://tidylab.github.io/decorators/,
https://github.com/tidylab/decorators",TRUE,https://github.com/tidylab/decorators,1501,2,2021-03-22T18:09:16Z,750.5
deductive,"Attempt to repair inconsistencies and missing values in
        data records by using information from valid values and
        validation rules restricting the data.",2021-03-29,Mark van der Loo,https://github.com/data-cleaning/deductive,TRUE,https://github.com/data-cleaning/deductive,20611,9,2021-04-01T09:04:12Z,2290.1111111111113
dedupewider,"Duplicated data can exist in different rows and columns and user may need to
    treat observations (rows) connected by duplicated data as one observation,
    e.g. companies can belong to one family (and thus: be one company) by sharing
    some telephone numbers. This package provides a function to find connected rows
    based on data on chosen columns and collapse it into one row.",2021-07-09,Grzegorz Smoliński,https://github.com/gsmolinski/dedupewider,TRUE,https://github.com/gsmolinski/dedupewider,680,2,2021-07-08T06:31:44Z,340
deepdep,"Provides tools for exploration of R package dependencies. 
    The main deepdep() function allows to acquire deep dependencies of any package and plot them in an elegant way.
    It also adds some popularity measures for the packages e.g. in the form of download count through the 'cranlogs' package. 
    Uses the CRAN metadata database <http://crandb.r-pkg.org> and Bioconductor metadata <http://bioconductor.org>.
    Other data acquire functions are: get_dependencies(), get_downloads() and get_description(). 
    The deepdep_shiny() function runs shiny application that helps to produce a nice 'deepdep' plot. ",2021-06-07,Dominik Rafacz,"https://dominikrafacz.github.io/deepdep/,
https://github.com/DominikRafacz/deepdep",TRUE,https://github.com/dominikrafacz/deepdep,9619,41,2021-06-07T10:59:47Z,234.609756097561
deepgmm,"Deep Gaussian mixture models as proposed by Viroli and McLachlan (2019) 
    <doi:10.1007/s11222-017-9793-z> provide a generalization of classical Gaussian mixtures 
    to multiple layers. Each layer contains a set of latent variables that follow a mixture of 
    Gaussian distributions. To avoid overparameterized solutions, dimension reduction is 
    applied at each layer by way of factor models.",2020-10-20,Cinzia Viroli,https://github.com/suren-rathnayake/deepgmm,TRUE,https://github.com/suren-rathnayake/deepgmm,13902,3,2020-10-20T00:26:39Z,4634
deeplr,"A wrapper for the 'DeepL' Pro API <https://www.deepl.com/docs-api>, a web service
    for translating texts between different languages. A DeepL API developer account is required
    to use the service (see <https://www.deepl.com/pro#developer).",2021-04-28,David Zumbach,https://www.deepl.com/translator,TRUE,https://github.com/zumbov2/deeplr,15525,10,2021-05-11T13:42:35Z,1552.5
deepredeff,"A tool that contains trained deep learning models
    for predicting effector proteins. 'deepredeff' has been trained to
    identify effector proteins using a set of known experimentally
    validated effectors from either bacteria, fungi, or oomycetes.
    Documentation is available via several vignettes, and the paper by
    Kristianingsih and MacLean (2020) <doi:10.1101/2020.07.08.193250>.",2021-07-16,Ruth Kristianingsih,https://github.com/ruthkr/deepredeff/,TRUE,https://github.com/ruthkr/deepredeff,4208,2,2021-07-16T09:09:49Z,2104
deeptime,"Extends the functionality of other plotting packages like
    'ggplot2' and 'lattice' to help facilitate the plotting of data over long time
    intervals, including, but not limited to, geological, evolutionary, and ecological
    data. The primary goal of 'deeptime' is to enable users to add highly customizable
    timescales to their visualizations. Other functions are also included to assist
    with other areas of deep time visualization.",2021-09-02,William Gearty,https://github.com/willgearty/deeptime,TRUE,https://github.com/willgearty/deeptime,510,41,2021-09-02T13:46:17Z,12.439024390243903
deisotoper,"Provides a low-level interface for a deisotoper container 
  implemented in the 'Java' programming language and means of S3 helper 
  functions for plotting and debugging isotopes of mass spectrometric data. 
  The deisotoper algorithm detects and aggregates peaks which belong to the 
  same isotopic cluster of a given mass spectrum. ",2019-04-18,Christian Panse,https://github.com/protViz/deisotoper/,TRUE,https://github.com/protviz/deisotoper,14808,0,2020-09-25T12:41:24Z,NA
Delaporte,"Provides probability mass, distribution, quantile, random-variate
    generation, and method-of-moments parameter-estimation functions for the
    Delaporte distribution with parameterization based on Vose (2008)
    <isbn:9780470512845>. The Delaporte is a discrete probability distribution
    which can be considered the convolution of a negative binomial distribution
    with a Poisson distribution. Alternatively, it can be considered a counting
    distribution with both Poisson and negative binomial components. It has been
    studied in actuarial science as a frequency distribution which has more
    variability than the Poisson, but less than the negative binomial.",2021-01-10,Avraham Adler,https://github.com/aadler/Delaporte,TRUE,https://github.com/aadler/delaporte,40110,2,2021-01-10T07:59:02Z,20055
delayed,"Mechanisms to parallelize dependent tasks in a manner that
    optimizes the compute resources available. It provides access to ""delayed""
    computations, which may be parallelized using futures. It is, to an extent,
    a facsimile of the 'Dask' library (<https://dask.org/>), for the 'Python'
    language.",2020-02-28,Jeremy Coyle,https://tlverse.org/delayed,TRUE,https://github.com/tlverse/delayed,11031,14,2021-08-29T16:44:59Z,787.9285714285714
DemoDecomp,"Two general demographic decomposition methods are offered: Pseudo-continuous decomposition proposed by Horiuchi, Wilmoth, and Pletcher (2008) <doi:10.1353/dem.0.0033> and stepwise replacement decomposition proposed by Andreev, Shkolnikov and Begun (2002) <doi:10.4054/DemRes.2002.7.14>.",2018-08-14,Tim Riffe,NA,TRUE,https://github.com/timriffe/demodecomp,14281,0,2020-10-21T09:59:26Z,NA
demography,"Functions for demographic analysis including lifetable
        calculations; Lee-Carter modelling; functional data analysis of
        mortality rates, fertility rates, net migration numbers; and
        stochastic population forecasting.",2019-04-22,Rob J Hyndman with contributions from Heather Booth,https://github.com/robjhyndman/demography,TRUE,https://github.com/robjhyndman/demography,63754,43,2021-04-09T01:01:17Z,1482.6511627906978
dendextend,"Offers a set of functions for extending
    'dendrogram' objects in R, letting you visualize and compare trees of
    'hierarchical clusterings'. You can (1) Adjust a tree's graphical parameters
    - the color, size, type, etc of its branches, nodes and labels. (2)
    Visually and statistically compare different 'dendrograms' to one another.",2021-05-08,Tal Galili,"http://talgalili.github.io/dendextend/,
https://github.com/talgalili/dendextend/,
https://cran.r-project.org/package=dendextend,
https://www.r-statistics.com/tag/dendextend/,
https://academic.oup.com/bioinformatics/article/31/22/3718/240978/dendextend-an-R-package-for-visualizing-adjusting",TRUE,https://github.com/talgalili/dendextend,2946879,128,2021-06-20T06:24:16Z,23022.4921875
dendroTools,"Provides novel dendroclimatological methods, primarily used by the
    Tree-ring research community. There are four core functions. The first one is 
    daily_response(), which finds the optimal sequence of days that are related 
    to one or more tree-ring proxy records. Similar function is daily_response_seascorr(), 
    which implements partial correlations in the analysis of daily response functions.
    For the enthusiast of monthly data, there is monthly_response() function.
    The last core function is compare_methods(), which effectively compares several 
    linear and nonlinear regression algorithms on the task of climate reconstruction.   ",2021-05-18,Jernej Jevsenak,https://github.com/jernejjevsenak/dendroTools,TRUE,https://github.com/jernejjevsenak/dendrotools,31426,3,2021-05-22T18:45:52Z,10475.333333333334
dendsort,"An implementation of functions to optimize ordering of nodes in a dendrogram, without affecting the meaning of the dendrogram. A dendrogram can be sorted based on the average distance of subtrees, or based on the smallest distance value. These sorting methods improve readability and interpretability of tree structure, especially for tasks such as comparison of different distance measures or linkage types and identification of tight clusters and outliers. As a result, it also introduces more meaningful reordering for a coupled heatmap visualization. This method is described in ""dendsort: modular leaf ordering methods for dendrogram representations in R"", F1000Research 2014, 3: 177 <doi:10.12688/f1000research.4784.1>.",2021-04-20,Evan Biederstedt,https://github.com/evanbiederstedt/dendsort,TRUE,https://github.com/evanbiederstedt/dendsort,37888,1,2021-05-31T17:50:10Z,37888
DEoptim,"Implements the Differential Evolution algorithm for global optimization of a real-valued function 
      of a real-valued parameter vector as described in Mullen et al. (2011) <doi:10.18637/jss.v040.i06>.",2021-05-05,David Ardia,https://github.com/ArdiaD/DEoptim,TRUE,https://github.com/ardiad/deoptim,148803,11,2021-05-03T22:36:39Z,13527.545454545454
depigner,"Pigna [_pìn'n'a_] is the Italian word for pine
    cone.  In jargon, it is used to identify a task which is boring,
    banal, annoying, painful, frustrating and maybe even with a not so
    beautiful or rewarding result, just like the obstinate act of trying
    to challenge yourself in extracting pine nuts from a pine cone,
    provided that, in the end, you will find at least one inside it. Here
    you can find a backpack of functions to be used to solve small
    everyday problems of coding or analyzing (clinical) data, which would
    be normally solved using quick-and-dirty patches. You will be able to
    convert 'Hmisc' and 'rms' summary()es into data.frames ready to be
    rendered by 'pander' and 'knitr'. You can access easy-to-use wrappers
    to activate essential but useful progress bars (from 'progress') into
    your loops or functionals. Easy setup and control Telegram's bots
    (from 'telegram.bot') to send messages or to divert error messages to
    a Telegram's chat. You also have some utilities helping you in the
    development of packages, like the activation of the same user
    interface of 'usethis' into your package, or call polite functions to
    ask a user to install other packages. Finally, you find a set of
    thematic sets of packages you may use to set up new environments
    quickly, installing them in a single call.",2021-01-11,Corrado Lanera,"https://corradolanera.github.io/depigner/,
https://github.com/CorradoLanera/depigner",TRUE,https://github.com/corradolanera/depigner,5961,0,2021-01-11T08:25:55Z,NA
Deriv,"R-based solution for symbolic differentiation. It admits
    user-defined function as well as function substitution
    in arguments of functions to be differentiated. Some symbolic
    simplification is part of the work.",2021-02-24,Serguei Sokol,NA,TRUE,https://github.com/sgsokol/deriv,917108,23,2021-02-24T16:16:17Z,39874.260869565216
descr,"Weighted frequency and contingency tables of categorical
       variables and of the comparison of the mean value of a numerical
       variable by the levels of a factor, and methods to produce xtable
       objects of the tables and to plot them. There are also functions to
       facilitate the character encoding conversion of objects, to quickly
       convert fixed width files into csv ones, and to export a data.frame to
       a text file with the necessary R and SPSS codes to reread the data.",2021-02-16,"Jakson Aquino. Includes R source code and/or documentation
        written by Dirk Enzmann",https://github.com/jalvesaq/descr,TRUE,https://github.com/jalvesaq/descr,299819,17,2021-02-16T00:54:29Z,17636.41176470588
DescriptiveStats.OBeu,"Estimate and return the needed parameters for visualizations designed for 'OpenBudgets.eu' <http://openbudgets.eu/> datasets. Calculate descriptive statistical measures in budget data of municipalities across Europe, according to the 'OpenBudgets.eu' data model. There are functions for measuring central tendency and dispersion of amount variables along with their distributions and correlations and the frequencies of categorical variables for a given dataset. Also, can be used generally to other datasets, to extract visualization parameters, convert them to 'JSON' format and use them as input in a different graphical interface. ",2020-05-04,Kleanthis Koupidis,https://github.com/okgreece/DescriptiveStats.OBeu,TRUE,https://github.com/okgreece/descriptivestats.obeu,20280,1,2021-09-02T12:08:35Z,20280
DescriptiveWH,"Exploratory analysis of a data base. Using the functions of this package is possible to filter the data set detecting atypical values (outliers) and to perform exploratory analysis through visual inspection or dispersion measures. With this package you can explore the structure of your data using several parameters at the same time joining statistical parameters with different graphics. Finally, this package aid to confirm or reject the hypothesis that your data structure presents a normal distribution. Therefore this package is useful to get a previous insight of your data before to carry out statistical analysis.",2021-06-17,William Herrera-Caceres,https://github.com/William-HC/DescriptiveWH,TRUE,https://github.com/william-hc/descriptivewh,2501,0,2021-06-14T21:03:56Z,NA
descriptr,"Generate descriptive statistics such as measures of location,
    dispersion, frequency tables, cross tables, group summaries and multiple
    one/two way tables. ",2020-12-09,Aravind Hebbali,"https://descriptr.rsquaredacademy.com/,
https://github.com/rsquaredacademy/descriptr",TRUE,https://github.com/rsquaredacademy/descriptr,45645,32,2021-07-09T06:06:04Z,1426.40625
DescrTab2,"Provides functions to create descriptive statistics tables for continuous and categorical variables.
    By default, summary statistics such as mean, standard deviation, quantiles, minimum and maximum for continuous
    variables and relative and absolute frequencies for categorical variables are calculated. 'DescrTab2' features a sophisticated algorithm to
    choose appropriate test statistics for your data and provides p-values. On top of this, confidence intervals for group
    differences of appropriated summary measures are automatically produces for two-group comparison.
    Tables generated by 'DescrTab2' can be integrated in a variety of document formats, including .html, .tex and .docx documents.
    'DescrTab2' also allows printing tables to console and saving table objects for later use.",2021-03-15,Jan Meis,https://imbi-heidelberg.github.io/DescrTab2/,TRUE,https://github.com/imbi-heidelberg/descrtab2,2824,4,2021-07-16T15:51:13Z,706
desctable,"Easily create descriptive and comparative tables.
    It makes use and integrates directly with the tidyverse family of packages, and pipes.
    Tables are produced as data frames/lists of data frames for easy manipulation after creation,
    and ready to be saved as csv, or piped to DT::datatable() or pander::pander() to integrate into reports.",2020-10-17,Maxime Wack,https://github.com/maximewack/desctable,TRUE,https://github.com/maximewack/desctable,24103,50,2020-10-15T22:58:12Z,482.06
DescTools,"A collection of miscellaneous basic statistic functions and convenience wrappers for efficiently describing data. The author's intention was to create a toolbox, which facilitates the (notoriously time consuming) first descriptive tasks in data analysis, consisting of calculating descriptive statistics, drawing graphical summaries and reporting the results. The package contains furthermore functions to produce documents using MS Word (or PowerPoint) and functions to import data from Excel. Many of the included functions can be found scattered in other packages and other sources written partly by Titans of R. The reason for collecting them here, was primarily to have them consolidated in ONE instead of dozens of packages (which themselves might depend on other packages which are not needed at all), and to provide a common and consistent interface as far as function and arguments naming, NA handling, recycling rules etc. are concerned. Google style guides were used as naming rules (in absence of convincing alternatives). The 'BigCamelCase' style was consequently applied to functions borrowed from contributed R packages as well.",2021-06-17,Andri Signorell,"https://andrisignorell.github.io/DescTools/,
https://github.com/AndriSignorell/DescTools/",TRUE,https://github.com/andrisignorell/desctools,1499376,29,2021-08-27T22:17:10Z,51702.620689655174
DescToolsAddIns,"'RStudio' as of recently offers the option to define addins and assign shortcuts to them. This package contains addins for a few most frequently used functions in a data scientist's (at least mine) daily work (like str(), example(), plot(), head(), view(), Desc()). Most of these functions will use the current selection in the editor window and send the specific command to the console while instantly executing it. Assigning shortcuts to these addins will save you quite a few keystrokes.",2021-03-19,Andri Signorell,https://github.com/AndriSignorell/DescToolsAddIns/,TRUE,https://github.com/andrisignorell/desctoolsaddins,36471,1,2021-07-03T11:17:26Z,36471
DesignCTPB,"Applying 'CUDA' 'GPUs' via 'Numba' for optimal clinical design. It allows the user to utilize a 'reticulate' 'Python' environment and run intensive Monte Carlo simulation to get the optimal cutoff for the clinical design with potential biomarker effect, which can guide the realistic clinical trials.",2021-04-05,Yitao Lu,"https://github.com/ubcxzhang/DesignCTPB, Y Lu (2020)
<doi:10.1002/sim.8868>",TRUE,https://github.com/ubcxzhang/designctpb,2170,0,2021-04-05T07:44:36Z,NA
DesignLibrary,"
    A simple interface to build designs using the package 'DeclareDesign'. 
    In one line of code, users can specify the parameters of individual 
    designs and diagnose their properties. The designers can also be used 
    to compare performance of a given design across a range of combinations 
    of parameters, such as effect size, sample size, and assignment probabilities.",2021-09-02,Jasper Cooper,"https://declaredesign.org/r/designlibrary/,
https://github.com/DeclareDesign/DesignLibrary",TRUE,https://github.com/declaredesign/designlibrary,27753,28,2021-05-31T20:07:34Z,991.1785714285714
designr,Generate balanced factorial designs with crossed and nested random and fixed effects <https://github.com/mmrabe/designr>.,2021-04-22,Maximilian M. Rabe,https://maxrabe.com/designr,TRUE,https://github.com/mmrabe/designr,11578,2,2021-08-18T10:10:41Z,5789
desiR,"Functions for (1) ranking, selecting, and prioritising  genes,
    proteins, and metabolites from high dimensional biology experiments, (2)
    multivariate hit calling in high content screens, and (3) combining data
    from diverse sources.",2021-04-16,Stanley E. Lazic,https://github.com/stanlazic/desiR,TRUE,https://github.com/stanlazic/desir,18339,1,2021-04-16T20:10:26Z,18339
desplot,"A function for plotting maps of agricultural field experiments that
    are laid out in grids.",2020-10-21,Kevin Wright,http://kwstat.github.io/desplot/,TRUE,https://github.com/kwstat/desplot,29217,14,2020-10-20T20:46:04Z,2086.9285714285716
details,"Create a details HTML tag around R objects to place
    in a Markdown, 'Rmarkdown' and 'roxygen2' documentation.",2020-01-12,Jonathan Sidi,https://github.com/yonicd/details,TRUE,https://github.com/yonicd/details,23923,84,2021-01-14T01:54:43Z,284.79761904761904
detectR,"Time series analysis of network connectivity. Detects and visualizes change points
    between networks. Methods included in the package are discussed in depth in Baek, C., Gates, K. M., Leinwand, B., Pipiras, V. (2021) ""Two sample tests for high-dimensional auto- covariances"" <doi:10.1016/j.csda.2020.107067> 
    and Baek, C., Gampe, M., Leinwand B., Lindquist K., Hopfinger J. and Gates K. (2021) “Detecting functional connectivity changes in fMRI data”. Preprint.",2021-02-08,Matthew Gampe,https://github.com/mgampe/detectR,TRUE,https://github.com/mgampe/detectr,2552,1,2021-02-16T20:24:07Z,2552
detectseparation,"Provides pre-fit and post-fit methods for detecting separation and infinite maximum likelihood estimates in generalized linear models with categorical responses. The pre-fit methods apply on binomial-response generalized liner models such as logit, probit and cloglog regression, and can be directly supplied as fitting methods to the glm() function. They solve the linear programming problems for the detection of separation developed in Konis (2007, <https://ora.ox.ac.uk/objects/uuid:8f9ee0d0-d78e-4101-9ab4-f9cbceed2a2a>) using 'ROI' <https://cran.r-project.org/package=ROI> or 'lpSolveAPI' <https://cran.r-project.org/package=lpSolveAPI>. The post-fit methods apply to models with categorical responses, including binomial-response generalized linear models and multinomial-response models, such as baseline category logits and adjacent category logits models; for example, the models implemented in the 'brglm2' <https://cran.r-project.org/package=brglm2> package. The post-fit methods successively refit the model with increasing number of iteratively reweighted least squares iterations, and monitor the ratio of the estimated standard error for each parameter to what it has been in the first iteration. According to the results in Lesaffre & Albert (1989, <https://www.jstor.org/stable/2345845>), divergence of those ratios indicates data separation.",2021-04-22,Ioannis Kosmidis,https://github.com/ikosmidis/detectseparation,TRUE,https://github.com/ikosmidis/detectseparation,11186,3,2021-07-17T15:05:26Z,3728.6666666666665
DetLifeInsurance,"Methods for valuation of life insurance premiums and reserves (including variable-benefit and fractional coverage) based on  ""Actuarial Mathematics"" by Bowers, H.U. Gerber, J.C. Hickman, D.A. Jones and C.J. Nesbitt (1997, ISBN: 978-0938959465), ""Actuarial Mathematics for Life Contingent Risks"" by Dickson, David C. M., Hardy, Mary R. and Waters, Howard R  (2009) <doi:10.1017/CBO9780511800146>  and ""Life Contingencies"" by Jordan, C. W (1952) <doi:10.1017/S002026810005410X>. It also contains functions for  equivalent interest and discount rate calculation, present and future values of annuities, and loan amortization schedule.",2020-09-12,Joaquin Auza,https://github.com/JoaquinAuza/DetLifeInsurance,TRUE,https://github.com/joaquinauza/detlifeinsurance,5364,2,2020-09-12T09:05:58Z,2682
devRate,"A set of functions to quantify the relationship between development
    rate and temperature and to build phenological models. The package comprises 
    a set of models and estimated parameters borrowed from a literature review 
    in ectotherms. The methods and literature review are described in Rebaudo 
    et al. (2018) <doi:10.1111/2041-210X.12935> and Rebaudo and Rabhi (2018) 
    <doi:10.1111/eea.12693>. An example can be found in Rebaudo et al. (2017) 
    <doi:10.1007/s13355-017-0480-5>.",2020-11-06,Francois Rebaudo,https://github.com/frareb/devRate/,TRUE,https://github.com/frareb/devrate,21575,2,2021-01-06T11:29:44Z,10787.5
devtools,Collection of package development tools.,2021-06-07,Jim Hester,"https://devtools.r-lib.org/, https://github.com/r-lib/devtools",TRUE,https://github.com/r-lib/devtools,29365933,2097,2021-08-18T13:01:35Z,14003.783023366714
dexter,"A system for the management, assessment, and psychometric analysis of data from educational and psychological tests. ",2021-05-08,Jesse Koops,https://dexter-psychometrics.github.io/dexter/,TRUE,https://github.com/dexter-psychometrics/dexter,41093,5,2021-09-02T11:25:17Z,8218.6
dextergui,"Classical Test and Item analysis, 
  Item Response analysis and data management for educational and psychological tests.",2021-05-05,jesse koops,https://dexter-psychometrics.github.io/dexter/,TRUE,https://github.com/dexter-psychometrics/dexter,19661,5,2021-09-02T11:25:17Z,3932.2
dexterMST,"Conditional Maximum Likelihood Calibration and data management of multistage tests. 
  Supports polytomous items and incomplete designs with linear as well as multistage tests.
  Extended Nominal Response and Interaction models, DIF and profile analysis.
  See Robert J. Zwitser and Gunter Maris (2015)<doi:10.1007/s11336-013-9369-6>.",2021-01-07,Timo Bechger,https://dexterities.netlify.app,TRUE,https://github.com/jessekps/dexter,20314,5,2021-09-02T11:25:17Z,4062.8
DFA,"Contains the Detrended Fluctuation Analysis (DFA), Detrended Cross-Correlation Analysis (DCCA), Detrended Cross-Correlation Coefficient (rhoDCCA), Delta Amplitude Detrended Cross-Correlation Coefficient (DeltarhoDCCA), log amplitude Detrended Fluctuation Analysis (DeltalogDFA), two DFA automatic methods for identification of crossover points and a Deltalog automatic method for identification of reference channels.",2020-10-26,Victor Barreto Mesquita[aut,NA,TRUE,https://github.com/victormesquita40/dfa,8043,0,2020-11-11T18:52:58Z,NA
dfadjust,"Computes small-sample degrees of freedom adjustment for
    heteroskedasticity robust standard errors, and for clustered standard errors
    in linear regression. See Imbens and Kolesár (2016)
    <doi:10.1162/REST_a_00552> for a discussion of these adjustments.",2021-08-10,Michal Kolesár,https://github.com/kolesarm/Robust-Small-Sample-Standard-Errors,TRUE,https://github.com/kolesarm/robust-small-sample-standard-errors,11547,24,2021-08-10T12:15:48Z,481.125
dfoliatR,"Tools to identify, quantify, analyze, and visualize growth
    suppression events in tree rings that are often produced by insect
    defoliation. Described in Guiterman et al. (2020) <doi:10.1016/j.dendro.2020.125750>.",2020-09-02,Chris Guiterman,https://chguiterman.github.io/dfoliatR/,TRUE,https://github.com/chguiterman/dfoliatr,8578,5,2021-01-04T18:50:42Z,1715.6
dfvad,"Decomposing value added growth into explanatory factors.
    A cost constrained value added function is defined to specify the 
    production frontier. Industry estimates can also be aggregated
    using a weighted average approach.
    Details about the methodology and data can be found in Diewert and Fox (2018)
    <doi:10.1093/oxfordhb/9780190226718.013.19>
    and Zeng, Parsons, Diewert and Fox (2018)
    <https://www.business.unsw.edu.au/research-site/centreforappliedeconomicresearch-site/Documents/emg2018-6_SZeng_EMG-Slides.pdf>.",2020-03-05,Shipei Zeng,https://github.com/shipei-zeng/dfvad,TRUE,https://github.com/shipei-zeng/dfvad,8064,0,2020-12-12T05:00:36Z,NA
DGM,"
    Dynamic graphical models for multivariate time series data to estimate directed
    dynamic networks in functional magnetic resonance imaging (fMRI), see Schwab et al. (2017) <doi:10.1101/198887>.",2021-05-18,Simon Schwab,https://github.com/schw4b/DGM,TRUE,https://github.com/schw4b/dgm,12379,19,2021-05-18T19:21:33Z,651.5263157894736
DHARMa,"The 'DHARMa' package uses a simulation-based approach to create
    readily interpretable scaled (quantile) residuals for fitted (generalized) linear mixed
    models. Currently supported are linear and generalized linear (mixed) models from 'lme4'
    (classes 'lmerMod', 'glmerMod'), 'glmmTMB' 'GLMMadaptive' and 'spaMM', generalized additive models 
    ('gam' from 'mgcv'), 'glm' (including 'negbin' from 'MASS', but excluding quasi-distributions) and 
    'lm' model classes. Moreover, externally created simulations, e.g. posterior predictive simulations
    from Bayesian software such as 'JAGS', 'STAN', or 'BUGS' can be processed as well.
    The resulting residuals are standardized to values between 0 and 1 and can be interpreted
    as intuitively as residuals from a linear regression. The package also provides a number of
    plot and test functions for typical model misspecification problems, such as
    over/underdispersion, zero-inflation, and residual spatial and temporal autocorrelation.",2021-07-07,Florian Hartig,http://florianhartig.github.io/DHARMa/,TRUE,https://github.com/florianhartig/dharma,161447,142,2021-08-24T14:17:44Z,1136.9507042253522
diagmeta,Provides methods by Steinhauser et al. (2016) <DOI:10.1186/s12874-016-0196-1> for meta-analysis of diagnostic accuracy studies with several cutpoints.,2021-05-11,Guido Schwarzer,https://github.com/guido-s/diagmeta,TRUE,https://github.com/guido-s/diagmeta,18057,3,2021-05-11T14:35:15Z,6019
diagonals,"Several tools for handling block-matrix diagonals and similar
    constructs are implemented. Block-diagonal matrices can be extracted or removed
    using two small functions implemented here. In addition, non-square matrices
    are supported. Block diagonal matrices occur when two dimensions of a data set
    are combined along one edge of a matrix. For example, trade-flow data in the
    'decompr' and 'gvc' packages have each country-industry combination occur along
    both edges of the matrix.",2021-05-08,Bastiaan Quast,"https://qua.st/diagonals, https://github.com/bquast/diagonals",TRUE,https://github.com/bquast/diagonals,27851,1,2021-05-08T15:55:55Z,27851
DiagrammeR,"
    Build graph/network structures using functions for stepwise addition and
    deletion of nodes and edges. Work with data available in tables for bulk
    addition of nodes, edges, and associated metadata. Use graph selections
    and traversals to apply changes to specific nodes or edges. A wide
    selection of graph algorithms allow for the analysis of graphs. Visualize
    the graphs and take advantage of any aesthetic properties assigned to
    nodes and edges.",2020-05-08,Richard Iannone,https://github.com/rich-iannone/DiagrammeR,TRUE,https://github.com/rich-iannone/diagrammer,951355,1472,2021-07-07T01:44:25Z,646.3009510869565
dialr,"Parse, format, and validate international phone
    numbers using Google's 'libphonenumber' java library,
    <https://github.com/google/libphonenumber>.",2021-05-24,Danny Smith,"https://socialresearchcentre.github.io/dialr/,
https://github.com/socialresearchcentre/dialr,
https://github.com/socialresearchcentre/dialrjars,
https://github.com/google/libphonenumber",TRUE,https://github.com/socialresearchcentre/dialr,20189,4,2021-08-13T07:12:09Z,5047.25
dialrjars,"Collects 'libphonenumber' jars required for the
    'dialr' package.",2021-08-23,Danny Smith,"https://github.com/socialresearchcentre/dialrjars,
https://github.com/google/libphonenumber",TRUE,https://github.com/socialresearchcentre/dialrjars,21958,2,2021-08-31T10:30:01Z,10979
dials,"Many models contain tuning parameters (i.e. parameters that cannot be directly estimated from the data). These tools can be used to define objects for creating, simulating, or validating values for such parameters. ",2020-09-16,Max Kuhn,"https://dials.tidymodels.org, https://github.com/tidymodels/dials",TRUE,https://github.com/tidymodels/dials,374390,84,2021-09-01T17:18:43Z,4457.023809523809
diceR,"Performs cluster analysis using an ensemble
    clustering framework, Chiu & Talhouk (2018)
    <doi:10.1186/s12859-017-1996-y>.  Results from a diverse set of
    algorithms are pooled together using methods such as majority voting,
    K-Modes, LinkCluE, and CSPA. There are options to compare cluster
    assignments across algorithms using internal and external indices,
    visualizations such as heatmaps, and significance testing for the
    existence of clusters.",2021-07-23,Derek Chiu,"https://github.com/AlineTalhouk/diceR/,
https://alinetalhouk.github.io/diceR/",TRUE,https://github.com/alinetalhouk/dicer,23422,28,2021-07-29T19:33:27Z,836.5
DiceView,"View 2D/3D sections, contour plots, mesh of excursion sets for computer experiments designs, surrogates or test functions.",2020-11-27,Yann Richet,https://github.com/IRSN/DiceView,TRUE,https://github.com/irsn/diceview,21503,0,2020-11-27T16:08:10Z,NA
dictionar6,"Efficient object-oriented R6 dictionary capable of holding objects of any class, including R6. Typed and untyped dictionaries are provided as well as the 'usual' dictionary methods that are available in other OOP languages, for example listing keys, items, values, and methods to get/set these.",2021-07-26,Raphael Sonabend,"https://xoopR.github.io/dictionar6/,
https://github.com/xoopR/dictionar6/",TRUE,https://github.com/xoopr/dictionar6,636,0,2021-08-27T10:57:50Z,NA
did,"The standard Difference-in-Differences (DID) setup involves two periods and two groups -- a treated group and untreated group.  Many applications of DID methods involve more than two periods and have individuals that are treated at different points in time.  This package contains tools for computing average treatment effect parameters in Difference in Differences setups with more than two periods and with variation in treatment timing using the methods developed in Callaway and Sant'Anna (2020) <https://www.ssrn.com/abstract=3148250>.  The main parameters are group-time average treatment effects which are the average treatment effect for a particular group at a a particular time.  These can be aggregated into a fewer number of treatment effect parameters, and the package deals with the cases where there is selective treatment timing, dynamic treatment effects, calendar time effects, or combinations of these.  There are also functions for testing the Difference in Differences assumption, and plotting group-time average treatment effects.",2020-12-11,Brantly Callaway,"https://bcallaway11.github.io/did/,
https://github.com/bcallaway11/did/",TRUE,https://github.com/bcallaway11/did,30283,122,2021-07-27T03:10:12Z,248.22131147540983
dietr,Estimates fractional trophic level from quantitative and qualitative diet data and calculates electivity indices in R. Borstein (2020) <doi:10.1007/s10750-020-04417-5>.,2021-03-12,Samuel R. Borstein,https://github.com/sborstein/dietr,TRUE,https://github.com/sborstein/dietr,11052,3,2021-03-11T18:33:22Z,3684
diffeqr,"An interface to 'DifferentialEquations.jl' <https://diffeq.sciml.ai/dev/> from the R programming language.
  It has unique high performance methods for solving ordinary differential equations (ODE), stochastic differential equations (SDE),
  delay differential equations (DDE), differential-algebraic equations (DAE), and more. Much of the functionality,
  including features like adaptive time stepping in SDEs, are unique and allow for multiple orders of magnitude speedup over more common methods.
  'diffeqr' attaches an R interface onto the package, allowing seamless use of this tooling by R users. For more information,
  see Rackauckas and Nie (2017) <doi:10.5334/jors.151>.",2021-08-05,Christopher Rackauckas,https://github.com/SciML/diffeqr,TRUE,https://github.com/sciml/diffeqr,19263,107,2021-08-04T12:24:21Z,180.02803738317758
diffmatchpatch,"A wrapper for Google's 'diff-match-patch' library. It provides basic tools
    for computing diffs, finding fuzzy matches, and constructing / applying patches to strings.",2021-04-16,Colin Rundel,https://github.com/rundel/diffmatchpatch,TRUE,https://github.com/rundel/diffmatchpatch,1563,4,2021-04-27T13:33:43Z,390.75
diffobj,"Generate a colorized diff of two R objects for an intuitive
    visualization of their differences.",2021-03-22,Brodie Gaslam,https://github.com/brodieG/diffobj,TRUE,https://github.com/brodieg/diffobj,5878548,200,2021-03-23T12:10:45Z,29392.74
diffviewer,"A HTML widget that shows differences between files
    (text, images, and data frames).",2020-09-10,Hadley Wickham,https://github.com/r-lib/diffviewer,TRUE,https://github.com/r-lib/diffviewer,29648,38,2021-06-29T18:42:24Z,780.2105263157895
difNLR,"Detection of differential item functioning (DIF) among dichotomously scored items and differential distractor functioning (DDF) among unscored items with non-linear regression procedures based on generalized logistic regression models (Drabinova & Martinkova, 2017, <doi:10.1111/jedm.12158>).",2021-01-07,Adela Hladka,NA,TRUE,https://github.com/adelahladka/difnlr,34393,4,2021-02-16T15:11:18Z,8598.25
digest,"Implementation of a function 'digest()' for the creation of hash
 digests of arbitrary R objects (using the 'md5', 'sha-1', 'sha-256', 'crc32',
 'xxhash', 'murmurhash', 'spookyhash' and 'blake3' algorithms) permitting easy
 comparison of R language objects, as well as functions such as'hmac()' to
 create hash-based message authentication code. Please note that this package
 is not meant to be deployed for cryptographic purposes for which more
 comprehensive (and widely tested) libraries such as 'OpenSSL' should be
 used.",2020-10-24,"Dirk Eddelbuettel <edd@debian.org> with contributions 
 by Antoine Lucas",http://dirk.eddelbuettel.com/code/digest.html,TRUE,https://github.com/eddelbuettel/digest,27316192,90,2021-06-18T01:35:58Z,303513.2444444444
digitTests,"Provides statistical tests and support functions for detecting irregular digit patterns in numerical data. The package includes tools for extracting digits at various locations in a number, tests for repeated values, and (Bayesian) tests of digit distributions.",2021-08-13,Koen Derks,"https://koenderks.github.io/digitTests/,
https://github.com/koenderks/digitTests",TRUE,https://github.com/koenderks/digittests,293,3,2021-09-03T02:29:49Z,97.66666666666667
DIGSS,"Simulation tool to estimate the rate of success that surveys possessing user-specific characteristics have in identifying archaeological sites (or any groups of clouds of objects), given specific parameters of survey area, survey methods, and site properties. The survey approach used is largely based on the work of Kintigh (1988) <doi:10.2307/281113>.",2021-08-04,Mark Hubbe,https://github.com/markhubbe/DIGSS,TRUE,https://github.com/markhubbe/digss,396,0,2021-08-03T13:28:12Z,NA
dimensio,"Simple Principal Components Analysis (PCA) and
    Correspondence Analysis (CA) based on the Singular Value Decomposition
    (SVD). This package provides S4 classes and methods to compute,
    extract, summarize and visualize results of multivariate data
    analysis. It also includes methods for partial bootstrap validation
    described in Greenacre (1984) <isbn: 978-0-12-299050-2> and Lebart et al. 
    (2006) <isbn: 978-2-10-049616-7>.",2021-05-17,Nicolas Frerebeau,"https://dimensio.tesselle.org,
https://github.com/tesselle/dimensio",TRUE,https://github.com/tesselle/dimensio,2159,7,2021-08-25T16:35:51Z,308.42857142857144
dimensionsR,A set of tools to extract bibliographic content from 'Digital Science Dimensions' using 'DSL' API <https://www.dimensions.ai/dimensions-apis/>.,2020-08-28,Massimo Aria,https://github.com/massimoaria/dimensionsR,TRUE,https://github.com/massimoaria/dimensionsr,53853,12,2021-06-07T12:45:16Z,4487.75
dimRed,"A collection of dimensionality reduction
    techniques from R packages and a common
    interface for calling the methods.",2019-05-08,Guido Kraemer,https://github.com/gdkrmr/dimRed,TRUE,https://github.com/gdkrmr/dimred,370089,71,2020-11-12T12:35:58Z,5212.521126760564
dipsaus,"Works as an ""add-on"" to packages like 'shiny', 'future', as well as 
    'rlang', and provides utility functions. Just like dipping sauce adding 
    flavors to potato chips or pita bread, 'dipsaus' for data analysis and 
    visualizations adds handy functions and enhancements to popular packages. 
    The goal is to provide simple solutions that are frequently asked for 
    online, such as how to synchronize 'shiny' inputs without freezing the app,
    or how to get memory size on 'Linux' or 'MacOS' system. The enhancements 
    roughly fall into these four categories: 1. 'shiny' input widgets; 2. 
    high-performance computing using 'RcppParallel' and 'future' package; 3. 
    modify R calls and convert among numbers, strings, and other objects. 4. 
    utility functions to get system information such like CPU chip-set, memory 
    limit, etc.",2021-07-26,Zhengjia Wang,https://github.com/dipterix/dipsaus,TRUE,https://github.com/dipterix/dipsaus,15644,10,2021-08-28T22:52:01Z,1564.4
diptest,"Compute Hartigan's dip test statistic for unimodality /
 multimodality and provide a test with simulation based p-values,  where
 the original public code has been corrected.",2021-05-04,Martin Maechler (originally from Fortran and S-plus by Dario Ringach,https://github.com/mmaechler/diptest,TRUE,https://github.com/mmaechler/diptest,3299993,2,2021-06-06T10:15:16Z,1649996.5
Dire,"Fit linear models, estimating score distributions for groups of people, following Cohen and Jiang (1999) <doi:10.2307/2669917>. In this model, the response is a latent trait (such as student ability) and raw item responses are combined with item difficulties in an item response theory (IRT) framework to form a density for each unit (student). This latent trait is then integrated out. This software is intended to fit the same models as the existing software 'AM' <http://am.air.org/>.",2021-04-03,Paul Bailey,https://american-institutes-for-research.github.io/Dire/,TRUE,https://github.com/american-institutes-for-research/dire,4695,1,2021-08-20T21:31:43Z,4695
DirectEffects,"A set of functions to estimate the controlled direct effect of treatment fixing a potential mediator to a specific value. Implements the sequential g-estimation estimator described in Vansteelandt (2009) <doi:10.1097/EDE.0b013e3181b6f4c9> and Acharya, Blackwell, and Sen (2016) <doi:10.1017/S0003055416000216>.",2021-05-12,Matthew Blackwell,https://www.mattblackwell.org/software/direct-effects/,TRUE,https://github.com/mattblackwell/directeffects,15391,15,2021-05-17T19:15:44Z,1026.0666666666666
directlabels,"An extensible framework
 for automatically placing direct labels onto multicolor 'lattice' or
 'ggplot2' plots.
 Label positions are described using Positioning Methods
 which can be re-used across several different plots.
 There are heuristics for examining ""trellis"" and ""ggplot"" objects
 and inferring an appropriate Positioning Method.",2021-01-16,Toby Dylan Hocking,https://github.com/tdhock/directlabels,TRUE,https://github.com/tdhock/directlabels,207098,48,2021-07-22T04:27:42Z,4314.541666666667
dirichletprocess,"Perform nonparametric Bayesian analysis using Dirichlet 
    processes without the need to program the inference algorithms. 
    Utilise included pre-built models or specify custom 
    models and allow the 'dirichletprocess' package to handle the 
    Markov chain Monte Carlo sampling. 
    Our Dirichlet process objects can act as building blocks for a variety 
    of statistical models including and not limited to: density estimation, 
    clustering and prior distributions in hierarchical models.
    See Teh, Y. W. (2011) 
    <https://www.stats.ox.ac.uk/~teh/research/npbayes/Teh2010a.pdf>, 
    among many other sources.",2020-06-13,Dean Markwick,https://github.com/dm13450/dirichletprocess,TRUE,https://github.com/dm13450/dirichletprocess,20382,37,2021-06-26T09:58:18Z,550.8648648648649
DirStats,"Nonparametric kernel density estimation, bandwidth selection,
    and other utilities for analyzing directional data. Implements the estimator
    in Bai, Rao and Zhao (1987) <doi:10.1016/0047-259X(88)90113-3>, the
    cross-validation bandwidth selectors in Hall, Watson and Cabrera (1987)
    <doi:10.1093/biomet/74.4.751> and the plug-in bandwidth selectors in
    García-Portugués (2013) <doi:10.1214/13-ejs821>.",2021-01-10,Eduardo García-Portugués,https://github.com/egarpor/DirStats,TRUE,https://github.com/egarpor/dirstats,5520,5,2021-08-26T21:30:25Z,1104
disaggR,"The twoStepsBenchmark() and threeRuleSmooth() functions allow you to 
    disaggregate a low-frequency time-serie with higher frequency time-series, 
    using the French National Accounts methodology. The aggregated sum of the 
    resulting time-serie is strictly equal to the low-frequency serie within the 
    benchmarking window. Typically, the low-frequency serie is an annual one, 
    unknown for the last year, and the high frequency one is either quarterly or 
    mensual. See ""Methodology of quarterly national accounts"", Insee Méthodes 
    N°126, by Insee (2012, ISBN:978-2-11-068613-8).",2021-08-23,Arnaud Feldmann (<https://orcid.org/0000-0003-0109-7505>,NA,TRUE,https://github.com/inseefr/disaggr,8081,6,2021-08-23T15:21:21Z,1346.8333333333333
DIscBIO,"An open, multi-algorithmic pipeline for easy, fast and efficient
	analysis of cellular sub-populations and the molecular signatures that
	characterize them. The pipeline consists of four successive steps: data
	pre-processing, cellular clustering with pseudo-temporal ordering, defining
	differential expressed genes and biomarker identification. More details on
	Ghannoum et. al. (2021) <doi:10.3390/ijms22031399>. This package implements
	extensions of the work published by Ghannoum et. al. (2019)
	<doi:10.1101/700989>.",2021-04-28,Waldir Leoncio,https://github.com/ocbe-uio/DIscBIO,TRUE,https://github.com/ocbe-uio/discbio,2637,7,2021-04-28T11:27:33Z,376.7142857142857
discgolf,"Client for the Discourse API. Discourse is a open source
    discussion forum platform (<https://www.discourse.org/>). It comes with 'RESTful'
    API access to an installation. This client requires that you are authorized
    to access a Discourse installation, either yours or another.",2018-01-03,Scott Chamberlain,https://github.com/sckott/discgolf,TRUE,https://github.com/sckott/discgolf,17870,7,2021-03-01T16:35:14Z,2552.8571428571427
discord,"Functions for discordant kinship modeling (and other sibling-based quasi-experimental designs). Currently, the package contains data restructuring functions and functions for generating biometrically informed data for kin pairs.",2021-07-15,S. Mason Garrison,https://github.com/R-Computing-Lab/discord,TRUE,https://github.com/r-computing-lab/discord,16116,0,2021-08-25T20:52:32Z,NA
DiscreteFDR,"Multiple testing procedures described in the paper Döhler, Durand and Roquain (2018) ""New FDR bounds for discrete and heterogeneous tests"" <doi:10.1214/18-EJS1441>. The main procedures of the paper (HSU and HSD), their adaptive counterparts (AHSU and AHSD), and the HBR variant are available and are coded to take as input a set of observed p-values and their discrete support under the null. A function to compute such p-values and supports for Fisher's exact tests is also provided, along with a wrapper allowing to apply discrete procedures directly from contingency tables.",2021-09-03,Florian Junge,https://github.com/DISOhda/DiscreteFDR,TRUE,https://github.com/disohda/discretefdr,17564,0,2021-09-02T08:26:26Z,NA
discrim,"Bindings for additional classification models for use with
    the 'parsnip' package. Models include flavors of discriminant
    analysis, such as linear (Fisher (1936)
    <doi:10.1111/j.1469-1809.1936.tb02137.x>), regularized (Friedman
    (1989) <doi:10.1080/01621459.1989.10478752>), and flexible (Hastie,
    Tibshirani, and Buja (1994) <doi:10.1080/01621459.1994.10476866>), as
    well as naive Bayes classifiers (Hand and Yu (2007)
    <doi:10.1111/j.1751-5823.2001.tb00465.x>).",2021-07-21,Max Kuhn,https://discrim.tidymodels.org,TRUE,https://github.com/tidymodels/discrim,21931,19,2021-07-21T19:57:03Z,1154.2631578947369
diseq,"Provides estimation methods for markets in equilibrium and
    disequilibrium. Supports the estimation of an equilibrium and
    four disequilibrium models with both correlated and independent shocks.
    Also provides post-estimation analysis tools, such as aggregation,
    marginal effect, and shortage calculations. The estimation methods are
    based on full information maximum likelihood techniques given in
    Maddala and Nelson (1974) <doi:10.2307/1914215>. They are implemented
    using the analytic derivative expressions calculated in
    Karapanagiotis (2020) <doi:10.2139/ssrn.3525622>. Standard
    errors can be estimated by adjusting for heteroscedasticity or clustering.
    The equilibrium estimation constitutes a case of a system of linear,
    simultaneous equations. Instead, the disequilibrium models replace the
    market-clearing condition with a non-linear,
    short-side rule and allow for different specifications of price dynamics. ",2021-05-12,Pantelis Karapanagiotis,"https://github.com/pi-kappa-devel/diseq/,
https://diseq.pikappa.eu/",TRUE,https://github.com/pi-kappa-devel/diseq,7683,2,2021-08-26T14:41:01Z,3841.5
DisImpact,"Implements methods for calculating disproportionate impact: the percentage point gap, proportionality index, and the 80% index.
 California Community Colleges Chancellor's Office (2017).  Percentage Point Gap Method. <https://www.cccco.edu/-/media/CCCCO-Website/About-Us/Divisions/Digital-Innovation-and-Infrastructure/Research/Files/PercentagePointGapMethod2017.ashx>.
 California Community Colleges Chancellor's Office (2014).  Guidelines for Measuring Disproportionate Impact in Equity Plans. <https://www.cccco.edu/-/media/CCCCO-Website/Files/DII/guidelines-for-measuring-disproportionate-impact-in-equity-plans-tfa-ada.pdf>.",2021-09-02,Vinh Nguyen,https://github.com/vinhdizzo/DisImpact,TRUE,https://github.com/vinhdizzo/disimpact,18660,1,2021-09-02T20:34:28Z,18660
disk.frame,"A disk-based data manipulation tool for working with 
  large-than-RAM datasets. Aims to lower the barrier-to-entry for 
  manipulating large datasets by adhering closely to popular and 
  familiar data manipulation paradigms like 'dplyr' verbs and 
  'data.table' syntax.",2021-05-13,Dai ZJ,https://diskframe.com,TRUE,https://github.com/xiaodaigh/disk.frame,28801,554,2021-02-17T23:51:40Z,51.98736462093863
dismo,"Methods for species distribution modeling, that is, predicting the environmental similarity of any site to that of the locations of known occurrences of a species.",2020-11-17,Robert J. Hijmans,https://rspatial.org/raster/sdm/,TRUE,https://github.com/rspatial/dismo,992688,9,2021-07-12T23:13:39Z,110298.66666666667
disordR,"Functionality for manipulating values of associative
  maps.  Ordinary R vectors are unsuitable for working with values of
  associative maps because elements of an R vector may be accessed by
  reference to their location in the vector, but associative maps are
  stored in arbitrary order.  However, when associating keys with
  values one needs both parts to be in 1-1 correspondence, so one
  cannot dispense with the order entirely.  The 'disordR' package
  includes a single S4 class, disord.  This class allows one to
  perform only those operations appropriate for manipulating values of
  associative maps and prevents any other operation (such as accessing
  an element at a particular location).  A useful heuristic is that
  one is only allowed to access or modify a disord object using a
  python list comprehension.  The idea is to prevent ill-defined
  operations on values (or keys) of associative maps, whose order is
  undefined or at best implementation-specific, while allowing and
  facilitating sensible operations.  The package is needed for
  development versions of 'mvp', 'hyper2', 'spray', 'clifford', and
  'freealg'.",2021-08-05,Robin K. S. Hankin,https://github.com/RobinHankin/disordR,TRUE,https://github.com/robinhankin/disordr,998,1,2021-08-30T21:32:24Z,998
dispositionEffect,"Evaluate the presence of disposition effect and others irrational
    investor's behaviors based solely on investor's transactions and financial
    market data. Experimental data can also be used to perform the analysis.  
    Four different methodologies are implemented to account for the different
    nature of human behaviors on financial markets.  
    Novel analyses such as portfolio driven and time series disposition effect
    are also allowed.",2021-08-02,Marco Zanotti,"https://marcozanotti.github.io/dispositionEffect/,
https://github.com/marcozanotti/dispositionEffect",TRUE,https://github.com/marcozanotti/dispositioneffect,300,0,2021-08-04T12:33:02Z,NA
dispRity,"A modular package for measuring disparity (multidimensional space occupancy). Disparity can be calculated from any matrix defining a multidimensional space. The package provides a set of implemented metrics to measure properties of the space and allows users to provide and test their own metrics (Guillerme (2018) <doi:10.1111/2041-210X.13022>). The package also provides functions for looking at disparity in a serial way (e.g. disparity through time - Guillerme and Cooper (2018) <doi:10.1111/pala.12364>) or per groups as well as visualising the results. Finally, this package provides several statistical tests for disparity analysis.",2021-04-17,Thomas Guillerme,https://github.com/TGuillerme/dispRity,TRUE,https://github.com/tguillerme/disprity,26906,15,2021-09-02T10:39:03Z,1793.7333333333333
Distance,"A simple way of fitting detection functions to distance sampling
    data for both line and point transects. Adjustment term selection, left and
    right truncation as well as monotonicity constraints and binning are
    supported. Abundance and density estimates can also be calculated (via a
    Horvitz-Thompson-like estimator) if survey area information is provided. See
    Miller et al. (2019) <doi:10.18637/jss.v089.i01> for more information on
    methods and <https://examples.distancesampling.org/> for example analyses.",2021-08-12,David Lawrence Miller,https://github.com/DistanceDevelopment/Distance/,TRUE,https://github.com/distancedevelopment/distance,49279,2,2021-08-16T14:38:11Z,24639.5
distill,"Scientific and technical article format for the web. 'Distill' articles 
    feature attractive, reader-friendly typography, flexible layout options
    for visualizations, and full support for footnotes and citations.",2021-01-13,JJ Allaire,"https://github.com/rstudio/distill,
https://pkgs.rstudio.com/distill",TRUE,https://github.com/rstudio/distill,47782,328,2021-07-12T12:27:45Z,145.6768292682927
distr6,"An R6 object oriented distributions package. Unified interface for 42 probability distributions and 11 kernels including functionality for multiple scientific types. Additionally functionality for composite distributions and numerical imputation. Design patterns including wrappers and decorators are described in Gamma et al. (1994, ISBN:0-201-63361-2). For quick reference of probability distributions including d/p/q/r functions and results we refer to McLaughlin, M. P. (2001). Additionally Devroye (1986, ISBN:0-387-96305-7) for sampling the Dirichlet distribution, Gentle (2009) <doi:10.1007/978-0-387-98144-4> for sampling the Multivariate Normal distribution and Michael et al. (1976) <doi:10.2307/2683801> for sampling the Wald distribution.",2021-07-17,Raphael Sonabend,"https://alan-turing-institute.github.io/distr6/,
https://github.com/alan-turing-institute/distr6/",TRUE,https://github.com/alan-turing-institute/distr6,109378,80,2021-07-31T08:23:20Z,1367.225
distreg.vis,"Functions for visualizing distributional regression models fitted using the 'gamlss', 'bamlss' or 'betareg' R package. The core of the package consists of a 'shiny' application, where the model results can be interactively explored and visualized.",2021-05-13,Stanislaus Stadlmann,https://github.com/Stan125/distreg.vis,TRUE,https://github.com/stan125/distreg.vis,16193,1,2021-05-24T04:20:40Z,16193
distribglm,"Distributed generalized linear models (GLM) fitting using 
    Fisher scoring from McCullagh and Nelder (1989) <ISBN:0412317605>.  
    Models are to be fit using a primary-secondary 
    relationship, where the results are written to a synced folder, but
    data can be elsewhere though it is loaded in memory.  Additional functions
    are available for deploying a plumber 'API'.",2021-04-15,John Muschelli,https://github.com/muschellij2/distribglm,TRUE,https://github.com/muschellij2/distribglm,1591,0,2021-04-14T13:18:18Z,NA
distributional,"Vectorised distribution objects with tools for manipulating, 
    visualising, and using probability distributions. Designed to allow model
    prediction outputs to return distributions rather than their parameters, 
    allowing users to directly interact with predictive distributions in a
    data-oriented workflow. In addition to providing generic replacements for
    p/d/q/r functions, other useful statistics can be computed including means,
    variances, intervals, and highest density regions.",2021-02-02,Mitchell OHara-Wild,"https://pkg.mitchelloharawild.com/distributional/,
https://github.com/mitchelloharawild/distributional",TRUE,https://github.com/mitchelloharawild/distributional,229695,38,2021-07-26T06:05:47Z,6044.605263157895
distributions3,"Tools to create and manipulate probability
    distributions using S3.  Generics random(), pdf(), cdf() and
    quantile() provide replacements for base R's r/d/p/q style functions.
    Functions and arguments have been named carefully to minimize
    confusion for students in intro stats courses. The documentation for
    each distribution contains detailed mathematical notes.",2019-09-03,Alex Hayes,https://github.com/alexpghayes/distributions3,TRUE,https://github.com/alexpghayes/distributions3,17612,87,2021-04-03T19:46:52Z,202.4367816091954
distrom,"Fast distributed/parallel estimation for multinomial logistic regression via Poisson factorization and the 'gamlr' package.  For details see: Taddy (2015, AoAS), Distributed Multinomial Regression, <arXiv:1311.6139>.",2018-02-11,Matt Taddy,http://github.com/TaddyLab/distrom,TRUE,https://github.com/taddylab/distrom,30925,13,2021-06-09T13:19:30Z,2378.846153846154
distTails,"A full definition for Weibull tails and Full-Tails Gamma and tools for fitting these distributions to empirical tails. This package build upon the paper by del Castillo, Joan & Daoudi, Jalila & Serra, Isabel. (2012) <doi:10.1017/asb.2017.9>.",2019-09-07,Sergi Vilardell,https://github.com/SergiVilardell/distTails,TRUE,https://github.com/sergivilardell/disttails,9389,0,2021-01-15T11:16:50Z,NA
dittodb,"Testing and documenting code that communicates with remote
  databases can be painful. Although the interaction with R is usually relatively 
  simple (e.g. data(frames) passed to and from a database), because they rely on 
  a separate service and the data there, testing them can be difficult to set up,
  unsustainable in a continuous integration environment, or impossible without 
  replicating an entire production cluster. This package addresses that by 
  allowing you to make recordings from your database interactions and then play 
  them back while testing (or in other contexts) all without needing to spin up 
  or have access to the database your code would typically connect to.",2020-10-10,Jonathan Keane,"https://dittodb.jonkeane.com/, https://github.com/ropensci/dittodb",TRUE,https://github.com/ropensci/dittodb,8128,55,2020-10-10T12:16:37Z,147.78181818181818
div,"Facilitate the analysis of teams in a corporate setting:
    assess the diversity per grade and job, present the results,
    search for bias (in hiring and/or promoting processes).
    It also provides methods to simulate the effect of bias, random team-data, etc.
    White paper: 'Philippe J.S. De Brouwer' (2021) <http://www.de-brouwer.com/assets/div/div-white-paper.pdf>.
    Book (chapter 36): 'Philippe J.S. De Brouwer' (2020, ISBN:978-1-119-63272-6) and 'Philippe J.S. De Brouwer' (2020) <doi:10.1002/9781119632757>.",2021-05-06,Philippe J.S. De Brouwer,http://www.de-brouwer.com/div/,TRUE,https://github.com/drphilippedb/div,1524,0,2021-04-29T22:05:47Z,NA
diveMove,"Utilities to represent, visualize, filter, analyse, and summarize
	     time-depth recorder (TDR) data.  Miscellaneous functions for
	     handling location data are also provided.",2021-05-12,Sebastian P. Luque,https://github.com/spluque/diveMove,TRUE,https://github.com/spluque/divemove,118589,3,2021-05-12T02:59:48Z,39529.666666666664
diveR,"A suite of 'loon' related packages providing data analytic tools for 
    Direct Interactive Visual Exploration in R ('diveR').
    These tools work with and complement those of the 'tidyverse' suite, 
    extending the grammar of 'ggplot2' to become a grammar of interactive
    graphics.
    The suite provides many visual tools designed for moderately (100s of variables)
    high dimensional data analysis, through 'zenplots' and novel tools in 'loon', and
    extends the 'ggplot2' grammar to provide parallel coordinates, Andrews plots, and arbitrary 
    glyphs through 'ggmulti'.
    The  'diveR' package gathers together and installs all these related packages
    in a single step. ",2021-05-10,R. Wayne Oldford,https://github.com/great-northern-diver/diver/,TRUE,https://github.com/great-northern-diver/diver,1512,1,2021-05-14T20:49:45Z,1512
divest,"Provides tools to sort DICOM-format medical image files, and
    convert them to NIfTI-1 format.",2021-04-02,Jon Clayden,https://github.com/jonclayden/divest,TRUE,https://github.com/jonclayden/divest,36107,10,2021-04-02T11:25:13Z,3610.7
divseg,Implements common measures of diversity and spatial segregation. This package has tools to compute the majority of measures are reviewed in Douglas and Massey (1988) <doi:10.2307/2579183>. Multiple common measures of within-geography diversity are implemented as well. All functions operate on data frames with a 'tidyselect' based workflow.,2021-08-09,Christopher T. Kenny,"https://github.com/christopherkenny/divseg/,
https://christopherkenny.github.io/divseg/",TRUE,https://github.com/christopherkenny/divseg,297,0,2021-08-11T21:27:48Z,NA
diyar,"An R package for record linkage and implementing epidemiological case definitions in R.
    Record linkage is implemented either through a multistage deterministic approach or a probabilistic approach. 
    Matching records are assigned to unique groups. There are mechanisms to address missing data and conflicting matches across linkage stages.
    Track and assign events (e.g. sample collection) and periods (e.g. hospital admission) to unique groups based on a case definition. 
    The tracking process permits several options such as episode lengths and recurrence.
    Duplicate events or records can then be identified for removal or further analyses.  ",2021-08-09,Olisaeloka Nsonwu,https://olisansonwu.github.io/diyar/index.html,TRUE,https://github.com/olisansonwu/diyar,12899,3,2021-08-23T21:05:15Z,4299.666666666667
DIZutils,"Utility functions used for the R package development
    infrastructure inside the data integration centers ('DIZ') to
    standardize and facilitate repetitive tasks such as setting up a
    database connection or issuing notification messages and to avoid
    redundancy.",2021-05-25,Jonathan M. Mang,https://github.com/miracum/misc-dizutils,TRUE,https://github.com/miracum/misc-dizutils,6691,2,2021-06-03T06:45:55Z,3345.5
dlnm,Collection of functions for distributed lag linear and non-linear models.,2021-06-15,Antonio Gasparrini,"https://github.com/gasparrini/dlnm,
http://www.ag-myresearch.com/package-dlnm",TRUE,https://github.com/gasparrini/dlnm,66019,34,2021-06-15T06:51:48Z,1941.735294117647
dlookr,"A collection of tools that support data diagnosis, exploration, and transformation. 
    Data diagnostics provides information and visualization of missing values and outliers and 
    unique and negative values to help you understand the distribution and quality of your data. 
    Data exploration provides information and visualization of the descriptive statistics of 
    univariate variables, normality tests and outliers, correlation of two variables, and 
    relationship between target variable and predictor. Data transformation supports binning 
    for categorizing continuous variables, imputates missing values and outliers, resolving skewness. 
    And it creates automated reports that support these three tasks.",2021-09-02,Choonghyun Ryu,NA,TRUE,https://github.com/choonghyunryu/dlookr,90075,119,2021-09-01T23:29:39Z,756.9327731092437
dlstats,"Monthly download stats of 'CRAN' and 'Bioconductor' packages.
	     Download stats of 'CRAN' packages is from the 'RStudio' 'CRAN mirror', see <https://cranlogs.r-pkg.org:443>.
	     'Bioconductor' package download stats is at <https://bioconductor.org/packages/stats/>.",2021-04-23,Guangchuang Yu,https://github.com/GuangchuangYu/dlstats,TRUE,https://github.com/guangchuangyu/dlstats,35255,9,2021-04-23T09:17:09Z,3917.222222222222
dm,"Provides tools for working with multiple related
    tables, stored as data frames or in a relational database.  Multiple
    tables (data and metadata) are stored in a compound object, which can
    then be manipulated with a pipe-friendly syntax.",2021-06-20,Kirill Müller,"https://cynkra.github.io/dm/, https://github.com/cynkra/dm",TRUE,https://github.com/cynkra/dm,18207,299,2021-09-01T04:15:32Z,60.89297658862876
DMCfun,"
  DMC model simulation detailed in Ulrich, R., Schroeter, H., Leuthold, H., & Birngruber, T. (2015).
  Automatic and controlled stimulus processing in conflict tasks: Superimposed diffusion processes and delta functions.
  Cognitive Psychology, 78, 148-174. Ulrich et al. (2015) <doi:10.1016/j.cogpsych.2015.02.005>.
  Decision processes within choice reaction-time (CRT) tasks are often modelled using evidence accumulation models (EAMs),
  a variation of which is the Diffusion Decision Model (DDM, for a review, see Ratcliff & McKoon, 2008).
  Ulrich et al. (2015) introduced a Diffusion Model for Conflict tasks (DMC). The DMC model combines common
  features from within standard diffusion models with the addition of superimposed controlled and automatic activation.
  The DMC model is used to explain distributional reaction time (and error rate) patterns in common behavioural
  conflict-like tasks (e.g., Flanker task, Simon task). This R-package implements the DMC model and provides functionality
  to fit the model to observed data.",2021-01-10,Ian G. Mackenzie,"https://github.com/igmmgi/DMCfun,
https://CRAN.R-project.org/package=DMCfun",TRUE,https://github.com/igmmgi/dmcfun,6341,7,2021-08-30T09:46:41Z,905.8571428571429
dmdScheme,"Forms the core for developing own domain specific metadata schemes. 
                  It contains the basic functionality needed for all metadata schemes based on the
                  'dmdScheme'. See R.M. Krug and O.L. Petchey (2019) <DOI:10.5281/zenodo.3581970>.",2020-09-16,Rainer M. Krug,"https://exp-micro-ecol-hub.github.io/dmdScheme/,
https://github.com/Exp-Micro-Ecol-Hub/dmdScheme/",TRUE,https://github.com/exp-micro-ecol-hub/dmdscheme,9870,2,2021-08-24T11:40:52Z,4935
dmri.tracking,It provides functions to apply the deterministic tracking algorithm - DiST (Wong et al 2016) <doi:10.1214/15-AOAS880> and to plot tractography results.,2021-06-09,Seungyong Hwang,https://github.com/vic-dragon/dmri.tracking,TRUE,https://github.com/vic-dragon/dmri.tracking,1052,1,2021-06-09T16:10:33Z,1052
DMTL,"
	Implementation of a transfer learning framework employing distribution mapping based domain transfer. Uses the renowned concept of histogram matching (see Gonzalez and Fittes (1977) <doi:10.1016/0094-114X(77)90062-3>, Gonzalez and Woods (2008) <isbn:9780131687288>) and extends it to include distribution measures like kernel density estimates (KDE; see Wand and Jones (1995) <isbn:978-0-412-55270-0>, Jones et al. (1996) <doi:10.2307/2291420). In the typical application scenario, one can use the underlying sample distributions (histogram or KDE) to generate a map between two distinct but related domains to transfer the target data to the source domain and utilize the available source data for better predictive modeling design. Suitable for the case where a one-to-one sample matching is not possible, thus one needs to transform the underlying data distribution to utilize the more available data for modeling. ",2021-02-18,Saugato Rahman Dhruba,https://github.com/dhruba018/DMTL,TRUE,https://github.com/dhruba018/dmtl,2216,2,2021-06-19T00:04:31Z,1108
dmtools,"For checking the dataset from EDC(Electronic Data Capture) in clinical trials.
             'dmtools' reshape your dataset in a tidy view and check events.
             You can reshape the dataset and choose your target to check, for example, the laboratory reference range.",2020-11-08,Konstantin Ryabov,https://github.com/KonstantinRyabov/dmtools,TRUE,https://github.com/konstantinryabov/dmtools,6458,1,2020-12-19T18:57:23Z,6458
do,"Flexibly convert data between long and wide format using just two
    functions: reshape_toLong() and reshape_toWide().",2021-08-03,Jing Zhang,https://github.com/yikeshu0611/do,TRUE,https://github.com/yikeshu0611/do,30526,3,2021-07-31T11:53:41Z,10175.333333333334
doc2vec,"Learn vector representations of sentences, paragraphs or documents by using the 'Paragraph Vector' algorithms,
    namely the distributed bag of words ('PV-DBOW') and the distributed memory ('PV-DM') model. 
    The techniques in the package are detailed in the paper ""Distributed Representations of Sentences and Documents"" by Mikolov et al. (2014), available at <arXiv:1405.4053>.
    The package also provides an implementation to cluster documents based on these embedding using a technique called top2vec. 
    Top2vec finds clusters in text documents by combining techniques to embed documents and words and density-based clustering.
    It does this by embedding documents in the semantic space as defined by the 'doc2vec' algorithm. Next it maps
    these document embeddings to a lower-dimensional space using the 'Uniform Manifold Approximation and Projection' (UMAP) clustering algorithm 
    and finds dense areas in that space using a 'Hierarchical Density-Based Clustering' technique (HDBSCAN). These dense
    areas are the topic clusters which can be represented by the corresponding topic vector which is an aggregate of the 
    document embeddings of the documents which are part of that topic cluster. In the same semantic space similar words can 
    be found which are representative of the topic.
    More details can be found in the paper 'Top2Vec: Distributed Representations of Topics' by D. Angelov available at <arXiv:2008.09470>. ",2021-03-28,Jan Wijffels,https://github.com/bnosac/doc2vec,TRUE,https://github.com/bnosac/doc2vec,6652,24,2021-06-22T07:28:36Z,277.1666666666667
dockerfiler,"Build a Dockerfile straight from your R session.
    'dockerfiler' allows you to create step by step a Dockerfile, and
    provide convenient tools to wrap R code inside this Dockerfile.",2021-09-03,Colin Fay,https://github.com/ColinFay/dockerfiler,TRUE,https://github.com/colinfay/dockerfiler,73120,80,2021-04-06T07:42:03Z,914
DockerParallel,"This is the core package that provides both the user API and developer API to deploy 
    the parallel cluster on the cloud using the container service. The user can call 
    clusterPreset() to define the cloud service provider and container and makeDockerCluster()
    to create the cluster. The developer should see ""developer's cookbook"" on how to define
    the cloud provider and container.",2021-06-23,Jiefei Wang,https://github.com/Jiefei-Wang/DockerParallel,TRUE,https://github.com/jiefei-wang/dockerparallel,1924,6,2021-06-07T06:37:08Z,320.6666666666667
doconv,"Functions to convert 'Microsoft Word' or 'Microsoft PowerPoint' 
 documents to 'PDF' format and also for converting them into a thumbnail. 
 In order to work, 'LibreOffice' must be installed on the machine and 
 possibly 'python' and 'Microsoft Word'. If the latter is available, 
 it can be used to produce PDF documents identical to the originals, 
 otherwise, 'LibreOffice' is used.",2021-05-19,David Gohel,NA,TRUE,https://github.com/ardata-fr/doconv,2237,5,2021-09-01T20:56:03Z,447.4
docopt,"Define a command-line interface by just giving it
    a description in the specific format.",2020-06-24,Edwin de Jonge,https://github.com/docopt/docopt.R,TRUE,https://github.com/docopt/docopt.r,231135,179,2021-02-09T10:21:12Z,1291.2569832402235
docreview,High quality documentation can make for a great experience for your users. You can use 'docreview' to check that your R package documentation passes a number of configurable checks relating to documentation content.,2021-08-17,Nic Crane,https://thisisnic.github.io/docreview/,TRUE,https://github.com/thisisnic/docreview,289,7,2021-08-25T20:15:58Z,41.285714285714285
docxtools,"A set of helper functions for using R Markdown to create documents
    in docx format, especially documents for use in a classroom or workshop
    setting.",2020-06-03,Richard Layton,https://github.com/graphdr/docxtools,TRUE,https://github.com/graphdr/docxtools,21747,18,2021-06-09T14:38:11Z,1208.1666666666667
Dodge,"A variety of sampling plans are able to be compared using evaluations of their operating characteristics (OC), average outgoing quality (OQ), average total inspection (ATI) etc.",2018-06-05,A. Jonathan R. Godfrey,https://github.com/ajrgodfrey/Dodge,TRUE,https://github.com/ajrgodfrey/dodge,23823,0,2020-12-23T07:12:14Z,NA
dodgr,"Distances on dual-weighted directed graphs using
    priority-queue shortest paths (Padgham (2019) <doi:10.32866/6945>).
    Weighted directed graphs have weights from A to B which may differ
    from those from B to A.  Dual-weighted directed graphs have two sets
    of such weights. A canonical example is a street network to be used
    for routing in which routes are calculated by weighting distances
    according to the type of way and mode of transport, yet lengths of
    routes must be calculated from direct distances.",2021-08-07,Mark Padgham,https://github.com/ATFutures/dodgr,TRUE,https://github.com/atfutures/dodgr,34773,98,2021-08-30T18:23:36Z,354.8265306122449
doFuture,"Provides a '%dopar%' adapter such that any type of futures can
    be used as backends for the 'foreach' framework.",2021-01-04,Henrik Bengtsson,https://github.com/HenrikBengtsson/doFuture,TRUE,https://github.com/henrikbengtsson/dofuture,112593,61,2021-09-03T11:07:37Z,1845.7868852459017
domir,"Provides tools that support relative importance analysis focusing 
    on dominance analysis.  Dominance analysis is a methodology for 
    determining the relative importance of predictors/features/independent
    variables (Azen, R., & Budescu, D. V. (2003) <doi:10.1037/1082-989X.8.2.129>; 
    Groemping, U. (2007) <doi:10.1198/000313007X188252>) 
    as well as parameter estimates (Luchman, J. N, Lei, X., & Kaplan, S. 
    (2020) <doi:10.47263/JASEM.4(2)02>). 
    These tools are intended to extend relative importance analysis to, 
    effectively, any statistical or machine learning function as defined or 
    desired by the user-especially those where the user wants to use custom 
    importance/fit statistic or modeling function.",2021-08-09,Joseph Luchman,https://github.com/jluchman/domir,TRUE,https://github.com/jluchman/domir,2284,1,2021-08-30T19:04:58Z,2284
doMIsaul,"Algorithms for (i) unsupervised learning for dataset with
    missing data and/or left-censored data, using multiple imputation and
    consensus clustering ; (ii) semi-supervised learning with a survival
    endpoint (right-censored) for complete or incomplete datasets, using
    multiple imputation and consensus clustering in the latter case. The
    methods are described in Faucheux et al. (2021)
    <doi:10.1002/bimj.201900366> and Faucheux et al. (2021)
    <doi:10.1002/bimj.202000365>, respectively.",2021-08-05,Lilith Faucheux,https://github.com/LilithF/doMIsaul,TRUE,https://github.com/lilithf/domisaul,376,2,2021-08-04T13:31:30Z,188
donut,"Finds the k nearest neighbours in a dataset of specified points, 
  adding the option to wrap certain variables on a torus.  The user chooses
  the algorithm to use to find the nearest neighbours. Two such algorithms,
  provided by the packages 'RANN' <https://cran.r-project.org/package=RANN>, 
  and 'nabor' <https://cran.r-project.org/package=nabor>, are suggested.",2020-02-18,Paul J. Northrop,"http://github.com/paulnorthrop/donut,
https://paulnorthrop.github.io/donut/",TRUE,https://github.com/paulnorthrop/donut,9951,0,2021-04-09T20:48:42Z,NA
DOPE,"Provides information on drug names (brand, 
    generic and street) for drugs tracked by the DEA. There are functions that 
    will search synonyms and return the drug names and types. The vignettes 
    have extensive information on the work done to create the data for the package.",2021-06-18,Raymond Balise,"https://ctn-0094.github.io/DOPE/, https://github.com/CTN-0094/DOPE",TRUE,https://github.com/ctn-0094/dope,2754,9,2021-06-18T18:05:58Z,306
doRedis,Create and manage fault-tolerant task queues for the 'foreach' package using the 'Redis' key/value database.,2021-08-23,B. W. Lewis,NA,TRUE,https://github.com/bwlewis/doredis,59327,67,2021-08-24T22:29:26Z,885.4776119402985
doseminer,"Utilities for converting unstructured electronic prescribing instructions into structured medication data. Extracts drug dose, units, daily dosing frequency and intervals from English-language prescriptions. Based on Karystianis et al. (2015) <doi:10.1186/s12911-016-0255-x>.",2021-07-19,David Selby,NA,TRUE,https://github.com/selbosh/doseminer,1482,0,2021-07-19T10:28:44Z,NA
dotenv,"Load configuration from a '.env' file, that is
    in the current working directory, into environment variables.",2021-04-22,Gábor Csárdi,https://github.com/gaborcsardi/dotenv,TRUE,https://github.com/gaborcsardi/dotenv,44068,45,2021-04-22T13:30:25Z,979.2888888888889
dotgen,"Decorrelates a set of summary statistics (i.e., Z-scores or P-values per SNP) via Decorrelation by Orthogonal Transformation (DOT) approach and performs gene-set analyses by combining transformed statistic values; operations are performed with algorithms that rely only on the association summary results and the linkage disequilibrium (LD). For more details on DOT and its power, see Olga (2020) <doi:10.1371/journal.pcbi.1007819>.",2020-11-16,Olga Vsevolozhskaya,https://github.com/xiaoran831213/dotgen,TRUE,https://github.com/xiaoran831213/dotgen,4795,0,2020-11-23T20:11:14Z,NA
dotwhisker,Quick and easy dot-and-whisker plots of regression results.,2021-09-02,Yue Hu,NA,TRUE,https://github.com/fsolt/dotwhisker,109530,51,2021-08-31T02:11:40Z,2147.6470588235293
DoubleML,"Implementation of the double/debiased machine learning framework of
    Chernozhukov et al. (2018) <doi:10.1111/ectj.12097> for partially linear
    regression models, partially linear instrumental variable regression models,
    interactive regression models and interactive instrumental variable
    regression models. 'DoubleML' allows estimation of the nuisance parts in
    these models by machine learning methods and computation of the Neyman
    orthogonal score functions. 'DoubleML' is built on top of 'mlr3' and the
    'mlr3' ecosystem. The object-oriented implementation of 'DoubleML' based on
    the 'R6' package is very flexible. ",2021-08-02,Malte S. Kurz,"https://docs.doubleml.org/stable/index.html,
https://github.com/DoubleML/doubleml-for-r/",TRUE,https://github.com/doubleml/doubleml-for-r,5040,38,2021-08-13T07:12:09Z,132.6315789473684
downlit,"Syntax highlighting of R code, specifically designed
    for the needs of 'RMarkdown' packages like 'pkgdown', 'hugodown', and
    'bookdown'. It includes linking of function calls to their documentation
    on the web, and automatic translation of ANSI escapes in output to the
    equivalent HTML.",2020-11-04,Hadley Wickham,"https://downlit.r-lib.org/, https://github.com/r-lib/downlit",TRUE,https://github.com/r-lib/downlit,368948,53,2021-07-15T22:43:04Z,6961.2830188679245
downloadthis,Implement download buttons in HTML output from 'rmarkdown' without the need for 'runtime:shiny'.,2020-09-17,Felipe Mattioni Maturana,https://github.com/fmmattioni/downloadthis,TRUE,https://github.com/fmmattioni/downloadthis,14980,91,2021-02-08T09:16:29Z,164.6153846153846
dparser,"A Scannerless GLR parser/parser generator.  Note that GLR standing for ""generalized LR"", where L stands for ""left-to-right"" and
   R stands for ""rightmost (derivation)"".  For more information see <https://en.wikipedia.org/wiki/GLR_parser>. This parser is based on the Tomita
   (1987) algorithm. (Paper can be found at <https://www.aclweb.org/anthology/P84-1073.pdf>).
   The original 'dparser' package documentation can be found at <http://dparser.sourceforge.net/>.  This allows you to add mini-languages to R (like
   RxODE's ODE mini-language Wang, Hallow, and James 2015 <DOI:10.1002/psp4.12052>) or to parse other languages like 'NONMEM' to automatically translate
   them to R code.   To use this in your code, add a LinkingTo dparser in your DESCRIPTION file and instead of using #include <dparse.h> use
   #include <dparser.h>.  This also provides a R-based port of the make_dparser <http://dparser.sourceforge.net/d/make_dparser.cat> command called
   mkdparser().  Additionally you can parse an arbitrary grammar within R using the dparse() function, which works on most OSes and is mainly for grammar
   testing.  The fastest parsing, of course, occurs at the C level, and is suggested.",2021-04-07,Matthew Fidler,"https://nlmixrdevelopment.github.io/dparser-R/,
https://github.com/nlmixrdevelopment/dparser-R/",TRUE,https://github.com/nlmixrdevelopment/dparser-r,39890,1,2021-04-06T19:32:53Z,39890
dplR,"Perform tree-ring analyses such as detrending, chronology
        building, and cross dating.  Read and write standard file formats
        used in dendrochronology.",2021-01-31,Andy Bunn,https://github.com/AndyBunn/dplR,TRUE,https://github.com/andybunn/dplr,115273,21,2021-08-19T20:38:25Z,5489.190476190476
dplyr,"A fast, consistent tool for working with data frame
    like objects, both in memory and out of memory.",2021-06-18,Hadley Wickham,"https://dplyr.tidyverse.org, https://github.com/tidyverse/dplyr",TRUE,https://github.com/tidyverse/dplyr,41422544,3825,2021-08-31T16:38:46Z,10829.423267973856
dplyr.teradata,"A 'Teradata' backend for 'dplyr'. It makes it possible to operate
  'Teradata' database <https://www.teradata.com/products-and-services/teradata-database/>
  in the same way as manipulating data frames with 'dplyr'.",2020-11-12,Koji Makiyama,https://github.com/hoxo-m/dplyr.teradata,TRUE,https://github.com/hoxo-m/dplyr.teradata,39250,16,2020-12-23T08:41:38Z,2453.125
dqrng,"Several fast random number generators are provided as C++
  header only libraries: The PCG family by O'Neill (2014
  <https://www.cs.hmc.edu/tr/hmc-cs-2014-0905.pdf>) as well as
  Xoroshiro128+ and Xoshiro256+ by Blackman and Vigna (2018
  <arXiv:1805.01407>). In addition fast functions for generating random
  numbers according to a uniform, normal and exponential distribution
  are included. The latter two use the Ziggurat algorithm originally
  proposed by Marsaglia and Tsang (2000, <doi:10.18637/jss.v005.i08>).
  These functions are exported to R and as a C++ interface and are
  enabled for use with the default 64 bit generator from the PCG family,
  Xoroshiro128+ and Xoshiro256+ as well as the 64 bit version of the 20 rounds
  Threefry engine (Salmon et al., 2011 <doi:10.1145/2063384.2063405>) as
  provided by the package 'sitmo'.",2021-05-01,Ralf Stubner,"https://daqana.github.io/dqrng/, https://github.com/daqana/dqrng",TRUE,https://github.com/daqana/dqrng,307794,21,2021-05-01T09:47:05Z,14656.857142857143
dracor,"Decodes meshes and point cloud data encoded by the Draco mesh
    compression library from Google. Note that this is only designed for basic
    decoding and not intended as a full scale wrapping of the Draco library.",2020-10-27,Gregory Jefferis,"https://github.com/jefferis/dracor,
https://github.com/google/draco",TRUE,https://github.com/jefferis/dracor,5720,0,2020-10-24T16:31:05Z,NA
dragon,"Create, visualize, manipulate, and analyze bipartite mineral-chemistry
    networks for set of focal element(s) across deep-time on Earth. The method is 
    described in Spielman and Moore (2020) <doi:10.3389/feart.2020.585087>.",2021-07-07,Stephanie J. Spielman,https://github.com/sjspielman/dragon,TRUE,https://github.com/sjspielman/dragon,14531,0,2021-07-07T16:10:03Z,NA
drake,"A general-purpose computational engine for data
  analysis, drake rebuilds intermediate data objects when their
  dependencies change, and it skips work when the results are already up
  to date.  Not every execution starts from scratch, there is native
  support for parallel and distributed computing, and completed projects
  have tangible evidence that they are reproducible.  Extensive
  documentation, from beginner-friendly tutorials to practical examples
  and more, is available at the reference website
  <https://docs.ropensci.org/drake/> and the online manual
  <https://books.ropensci.org/drake/>.",2021-04-22,William Michael Landau,"https://github.com/ropensci/drake,
https://docs.ropensci.org/drake/,
https://books.ropensci.org/drake/",TRUE,https://github.com/ropensci/drake,95928,1319,2021-08-26T16:40:16Z,72.72782410917361
DramaAnalysis,"Analysis of preprocessed dramatic texts, with respect to literary research. 
  The package provides functions to analyze and visualize information about characters,
  stage directions, the dramatic structure and the text itself.
  The dramatic texts are expected to be in CSV format, which can be installed from within 
  the package, sample texts are provided. The package and the reasoning behind it are described in 
  Reiter et al. (2017) <doi:10.18420/in2017_119>.",2020-09-18,Nils Reiter,https://github.com/quadrama/DramaAnalysis,TRUE,https://github.com/quadrama/dramaanalysis,9669,10,2020-09-26T08:10:00Z,966.9
drat,"Creation and use of R Repositories via helper functions 
 to insert packages into a repository, and to add repository information 
 to the current R session. Two primary types of repositories are support:
 gh-pages at GitHub, as well as local repositories on either the same machine
 or a local network. Drat is a recursive acronym: Drat R Archive Template. ",2021-07-10,Dirk Eddelbuettel with contributions by Carl Boettiger,"https://github.com/eddelbuettel/drat,
https://dirk.eddelbuettel.com/code/drat.html",TRUE,https://github.com/eddelbuettel/drat,244458,130,2021-07-10T10:42:52Z,1880.446153846154
drawer,"An interactive image editing tool that can be added as part of the HTML in Shiny,
    R markdown or any type of HTML document. Often times, plots, photos are embedded
    in the web application/file. 'drawer' can take screenshots of these image-like elements, or 
    any part of the HTML document and send to an image editing space called 'canvas' to allow users 
    immediately edit the screenshot(s) within the same document. Users can quickly 
    combine, compare different screenshots, upload their own images 
    and maybe make a scientific figure. ",2021-03-02,Le Zhang,https://github.com/lz100/drawer,TRUE,https://github.com/lz100/drawer,1970,6,2021-05-04T07:05:49Z,328.3333333333333
drawsample,"A tool to sample data with the desired properties.Samples can be 
    drawn by purposive sampling with determining distributional 
    conditions, such as deviation from normality (skewness and kurtosis), 
    and sample size in quantitative research studies.
    For purposive sampling, a researcher has something in mind and participants that 
    fit the purpose of the study are included (Etikan,Musa, & Alkassim, 2015) 
    <doi:10.11648/j.ajtas.20160501.11>.Purposive sampling can be useful for answering
    many research questions (Klar & Leeper, 2019) 
    <doi:10.1002/9781119083771.ch21>.",2021-03-01,Kubra Atalay Kabasakal,https://github.com/atalay-k/drawsample,TRUE,https://github.com/atalay-k/drawsample,3572,2,2021-03-04T08:53:23Z,1786
drda,"Fit logistic functions to observed dose-response data and evaluate
  goodness of fit measures. See Malyutina A., Tang J., and Pessia A. (2021)
  <doi:10.1101/2021.06.07.447323>.",2021-06-10,Alberto Pessia,https://github.com/albertopessia/drda,TRUE,https://github.com/albertopessia/drda,1076,0,2021-08-18T10:05:51Z,NA
DRDID,"Implements the locally efficient doubly robust difference-in-differences (DiD)
    estimators for the average treatment effect proposed by Sant'Anna and Zhao (2020)
    <doi:10.1016/j.jeconom.2020.06.003>. The estimator combines inverse probability weighting and outcome
    regression estimators (also implemented in the package) to form estimators with
    more attractive statistical properties. Two different estimation methods can be used
    to estimate the nuisance functions.",2021-01-07,Pedro H. C. SantAnna,"https://pedrohcgs.github.io/DRDID/,
https://github.com/pedrohcgs/DRDID",TRUE,https://github.com/pedrohcgs/drdid,10468,36,2021-06-23T01:05:24Z,290.77777777777777
dreamer,"Fits (longitudinal) dose-response models utilizing a Bayesian model
  averaging approach as outlined in Gould (2019) <doi:10.1002/bimj.201700211>
  for both continuous and binary responses.  Functions
  for plotting and calculating various posterior quantities
  (e.g. posterior mean, quantiles, probability of minimum efficacious dose,
  etc.) are also implemented.  Copyright Eli Lilly and Company (2019).",2021-08-20,Richard Payne,https://github.com/rich-payne/dreamer,TRUE,https://github.com/rich-payne/dreamer,166,0,2021-08-25T18:55:27Z,NA
dreamerr,"Set of tools to facilitate package development and make R a more user-friendly place. Mostly for developers (or anyone who writes/shares functions). Provides a simple, powerful and flexible way to check the arguments passed to functions. 
    The developer can easily describe the type of argument needed. If the user provides a wrong argument, then an informative error message is prompted with the requested type and the problem clearly stated--saving the user a lot of time in debugging. ",2020-12-05,Laurent Berge,NA,TRUE,https://github.com/lrberge/dreamerr,60309,9,2020-10-03T07:32:16Z,6701
drf,An implementation of distributional random forests as introduced in Cevid & Michel & Meinshausen & Buhlmann (2020) <arXiv:2005.14458>.,2021-03-29,Loris Michel,https://github.com/lorismichel/drf,TRUE,https://github.com/lorismichel/drf,5116,7,2021-06-29T13:55:47Z,730.8571428571429
drhur,"A fast, interactive tool built upon the 'learnr' function which will open a shiny app for learners to interact with the instructions and tasks. The best way to learn these skills together with the “Learning R with Dr. Hu” online/offline workshops.",2020-10-30,Yue Hu,NA,TRUE,https://github.com/sammo3182/drhur,3706,7,2021-08-31T11:58:46Z,529.4285714285714
DriveML,"Implementing some of the pillars of an automated machine learning pipeline such as (i) Automated data preparation, (ii) Feature engineering, (iii) Model building in classification context that includes techniques such as (a) Regularised regression [1], (b) Logistic regression [2], (c) Random Forest [3], (d) Decision tree [4] and (e) Extreme Gradient Boosting (xgboost) [5], and finally, (iv) Model explanation (using lift chart and partial dependency plots). Accomplishes the above tasks  by running the function instead of writing lengthy R codes. Also provides some additional features such as generating missing at random (MAR) variables and automated exploratory data analysis. Moreover, function exports the model results with the required plots in an HTML vignette report format that follows the best practices of the industry and the academia. [1] Gonzales G B and De Saeger (2018) <doi:10.1038/s41598-018-21851-7>, [2] Sperandei S (2014) <doi:10.11613/BM.2014.003>, [3] Breiman L (2001) <doi:10.1023/A:1010933404324>, [4] Kingsford C and Salzberg S (2008) <doi:10.1038/nbt0908-1011>, [5] Chen Tianqi and Guestrin Carlos (2016) <doi:10.1145/2939672.2939785>.",2021-06-14,Dayanand Ubrangala,NA,TRUE,https://github.com/daya6489/driveml,5811,9,2021-07-19T15:17:01Z,645.6666666666666
driveR,"Cancer genomes contain large numbers of somatic alterations but few
    genes drive tumor development. Identifying cancer driver genes is critical 
    for precision oncology. Most of current approaches either identify driver 
    genes based on mutational recurrence or using estimated scores predicting 
    the functional consequences of mutations. 'driveR' is a tool for 
    personalized or batch analysis of genomic data for driver gene prioritization 
    by combining genomic information and prior biological knowledge. As features, 
    'driveR' uses coding impact metaprediction scores, non-coding impact scores, 
    somatic copy number alteration scores, hotspot gene/double-hit gene 
    condition, 'phenolyzer' gene scores and memberships to cancer-related KEGG 
    pathways. It uses these features to estimate cancer-type-specific 
    probability for each gene of being a cancer driver using the related task of 
    a multi-task learning classification model. The method is described in detail 
    in Ulgen E, Sezerman OU. 2020. driveR: A Novel Method for Prioritizing Cancer 
    Driver Genes Using Somatic Genomics Data. bioRxiv 
    <doi:10.1101/2020.11.10.376707>.",2020-11-26,Ege Ulgen,"https://egeulgen.github.io/driveR/,
https://github.com/egeulgen/driveR/",TRUE,https://github.com/egeulgen/driver,2464,8,2021-08-20T14:57:34Z,308
DRomics,"Several functions are provided for dose-response (or concentration-response) characterization from omics data. 'DRomics' is especially dedicated to omics data obtained using a typical dose-response design, favoring a great number of tested doses (or concentrations) rather than a great number of replicates (no need of replicates). 'DRomics' provides functions 1) to check, normalize and or transform data, 2) to select monotonic or biphasic significantly responding items (e.g. probes, metabolites), 3) to choose the best-fit model among a predefined family of monotonic and biphasic models to describe each selected item, 4) to derive a benchmark dose or concentration and a typology of response from each fitted curve. In the available version data are supposed to be single-channel microarray data in log2, RNAseq data in raw counts, or already pretreated continuous omics data (such as metabolomic data) in log scale. In order to link responses across biological levels based on a common method, 'DRomics' also handles apical data as long as they are continuous and follow a normal distribution for each dose or concentration, with a common standard error. For further details see Larras et al (2018) <DOI:10.1021/acs.est.8b04752> at <https://hal.archives-ouvertes.fr/hal-02309919>.",2021-02-09,Aurelie Siberchicot,"https://lbbe.univ-lyon1.fr/-DRomics-.html,
https://github.com/aursiber/DRomics",TRUE,https://github.com/aursiber/dromics,13522,0,2021-08-04T12:16:58Z,NA
drord,"Efficient covariate-adjusted estimators of quantities that are useful for 
    establishing the effects of treatments on ordinal outcomes.",2021-05-20,David Benkeser,https://github.com/benkeser/drord,TRUE,https://github.com/benkeser/drord,4792,4,2021-05-21T13:31:42Z,1198
DRR,"An Implementation of Dimensionality Reduction
    via Regression using Kernel Ridge Regression.",2020-02-12,Guido Kraemer,https://github.com/gdkrmr/DRR,TRUE,https://github.com/gdkrmr/drr,300575,8,2020-11-11T19:12:42Z,37571.875
drtmle,"Targeted minimum loss-based estimators of counterfactual means and
    causal effects that are doubly-robust with respect both to consistency and
    asymptotic normality (Benkeser et al (2017), <doi:10.1093/biomet/asx053>; MJ
    van der Laan (2014), <doi:10.1515/ijb-2012-0038>).",2021-06-02,David Benkeser,https://github.com/benkeser/drtmle,TRUE,https://github.com/benkeser/drtmle,17407,12,2021-06-03T13:28:44Z,1450.5833333333333
ds4psy,"All datasets and functions required for the examples and exercises of the book ""Data Science for Psychologists"" (by Hansjoerg Neth, Konstanz University, 2021), available at <https://bookdown.org/hneth/ds4psy/>. The book and course introduce principles and methods of data science to students of psychology and other biological or social sciences. The 'ds4psy' package primarily provides datasets, but also functions for data generation and manipulation (e.g., of text and time data) and graphics that are used in the book and its exercises. All functions included in 'ds4psy' are designed to be explicit and instructive, rather than efficient or elegant. ",2021-05-12,Hansjoerg Neth,"https://bookdown.org/hneth/ds4psy/,
https://github.com/hneth/ds4psy/",TRUE,https://github.com/hneth/ds4psy,15998,10,2021-08-14T06:27:54Z,1599.8
DSAIDE,"Exploration of simulation models (apps) of various infectious disease transmission dynamics scenarios.
    The purpose of the package is to help individuals learn 
    about infectious disease epidemiology (ecology/evolution) from a dynamical systems perspective.
    All apps include explanations of the underlying models and instructions on what to do with the models. ",2021-07-23,Andreas Handel,"https://ahgroup.github.io/DSAIDE/,
https://github.com/ahgroup/DSAIDE/",TRUE,https://github.com/ahgroup/dsaide,21830,16,2021-08-04T12:00:37Z,1364.375
DSAIRM,"Simulation models (apps) of various within-host immune response scenarios.
    The purpose of the package is to help individuals learn 
    about within-host infection and immune response modeling from a dynamical systems perspective.
    All apps include explanations of the underlying models and instructions on what to do with the models. ",2021-07-28,Andreas Handel,"https://ahgroup.github.io/DSAIRM/,
https://github.com/ahgroup/DSAIRM/",TRUE,https://github.com/ahgroup/dsairm,18619,21,2021-08-04T11:57:06Z,886.6190476190476
dsb,"This lightweight R package provides a method for normalizing and denoising protein expression data from droplet based single cell experiments. Raw protein Unique Molecular Index (UMI) counts from sequencing DNA-conjugated antibodies in droplets (e.g. 'CITE-seq') have substantial measurement noise. Our experiments and computational modeling revealed two major components of this noise: 1) protein-specific noise originating from ambient, unbound antibody encapsulated in droplets that can be accurately inferred via the expected protein counts detected in empty droplets, and 2) droplet/cell-specific noise revealed via the shared variance component associated with isotype antibody controls and background protein counts in each cell. This package normalizes and removes both of these sources of noise from raw protein data derived from methods such as 'CITE-seq', 'REAP-seq', 'ASAP-seq', 'TEA-seq', 'proteogenomic' data from the Mission Bio platform, etc. See the vignette for tutorials on how to integrate dsb with 'Seurat', 'Bioconductor' and the AnnData class in 'Python'. Please also see our preprint Mulè M.P., Martins A.J., and Tsang J.S. (2020) <https://www.biorxiv.org/content/10.1101/2020.02.24.963603v3> for more details on the dsb method.",2021-09-03,Matthew Mulè,https://github.com/niaid/dsb,TRUE,https://github.com/niaid/dsb,2828,23,2021-09-03T13:22:54Z,122.95652173913044
dscore,"The D-score is a quantitative measure of child development. 
    The D-score follows the Rasch model. See Jacobusse, van Buuren and 
    Verkerk (2006) <doi:10.1002/sim.2351>. The user can convert 
    milestone scores from 19 assessment instruments into the D-score 
    and the DAZ (D-score adjusted for age). Several tools assist in 
    mapping milestone names into the 9-position Global Scale of Early 
    Development (GSED) convention. Supports calculation of the D-score 
    using 'dutch' <doi:10.1177/0962280212473300>, 
    'gcdg' <doi:10.1136/bmjgh-2019-001724> and 'gsed' conversion keys.
    The user can calculate DAZ using 'dutch' and 'gcdg' age-conditional 
    references.",2020-11-29,Stef van Buuren,"https://github.com/d-score/dscore, https://d-score.org/dscore/,
https://d-score.org/dbook1/",TRUE,https://github.com/d-score/dscore,11328,2,2021-05-25T13:01:38Z,5664
DSI,"'DataSHIELD' is an infrastructure and series of R packages that 
    enables the remote and 'non-disclosive' analysis of sensitive research data. 
    This package defines the API that is to be implemented by 'DataSHIELD' compliant 
    data repositories.",2021-06-22,Yannick Marcon,"https://github.com/datashield/DSI/,
https://datashield.github.io/DSI/, https://datashield.org/",TRUE,https://github.com/datashield/dsi,14209,0,2021-06-22T20:48:32Z,NA
dsims,"Performs distance sampling simulations. 'dsims' repeatedly generates
    instances of a user defined population within a given survey region. It then 
    generates realisations of a survey design and simulates the detection process. 
    The data are then analysed so that the results can be compared for accuracy 
    and precision across all replications. This process allows users to optimise 
    survey designs for their specific set of survey conditions. The effects of 
    uncertainty in population distribution or parameters can be investigated
    under a number of simulations so that users can be confident that they have
    achieved a robust survey design before deploying vessels into the field. The
    distance sampling designs used in this package from 'dssd' are detailed in
    Chapter 7 of Advanced Distance Sampling, Buckland et. al. (2008, ISBN-13: 
    978-0199225873). General distance sampling methods are detailed in Introduction 
    to Distance Sampling: Estimating Abundance of Biological Populations, Buckland 
    et. al. (2004, ISBN-13: 978-0198509271). Find out more about estimating 
    animal/plant abundance with distance sampling at <http://distancesampling.org/>.",2021-09-01,Laura Marshall,https://github.com/DistanceDevelopment/dsims,TRUE,https://github.com/distancedevelopment/dsims,1574,1,2021-09-01T09:00:21Z,1574
DSLite,"'DataSHIELD' is an infrastructure and series of R packages that 
    enables the remote and 'non-disclosive' analysis of sensitive research data.
    This 'DataSHIELD Interface' implementation is for analyzing datasets living
    in the current R session. The purpose of this is primarily for lightweight
    'DataSHIELD' analysis package development.",2021-06-23,Yannick Marcon,"https://github.com/datashield/DSLite/,
https://datashield.github.io/DSLite/,
https://www.datashield.org/, https://doi.org/10.1093/ije/dyu188",TRUE,https://github.com/datashield/dslite,10406,1,2021-06-23T07:02:07Z,10406
dsm,"Density surface modelling of line transect data. A Generalized
    Additive Model-based approach is used to calculate spatially-explicit estimates
    of animal abundance from distance sampling (also presence/absence and strip
    transect) data. Several utility functions are provided for model checking,
    plotting and variance estimation.",2021-03-27,David L. Miller,https://github.com/DistanceDevelopment/dsm,TRUE,https://github.com/distancedevelopment/dsm,23407,6,2021-08-02T10:41:48Z,3901.1666666666665
DSMolgenisArmadillo,"'DataSHIELD' is an infrastructure and series of R packages that 
    enables the remote and 'non-disclosive' analysis of sensitive research data.
    This package is the 'DataSHIELD' interface implementation to analyze data
    shared on a 'MOLGENIS Armadillo' server. 'MOLGENIS Armadillo' is a
    light-weight 'DataSHIELD' server using a file store and an 'RServe' server.",2021-03-20,Sido Haakma,"https://github.com/molgenis/molgenis-r-datashield/,
https://molgenis.github.io/molgenis-r-datashield/",TRUE,https://github.com/molgenis/molgenis-r-datashield,3736,0,2021-06-07T14:09:10Z,NA
DSOpal,"'DataSHIELD' is an infrastructure and series of R packages that 
    enables the remote and 'non-disclosive' analysis of sensitive research data.
    This package is the 'DataSHIELD' interface implementation for 'Opal', which is
    the data integration application for biobanks by 'OBiBa'. Participant data, once
    collected from any data source, must be integrated and stored in a central
    data repository under a uniform model. 'Opal' is such a central repository.
    It can import, process, validate, query, analyze, report, and export data.
    'Opal' is the reference implementation of the 'DataSHIELD' infrastructure.",2021-08-23,Yannick Marcon,"https://github.com/datashield/DSOpal/,
https://datashield.github.io/DSOpal/, https://www.obiba.org,
https://www.obiba.org/pages/products/opal/,
https://www.datashield.org,
https://academic.oup.com/ije/article/43/6/1929/707730,
https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008880",TRUE,https://github.com/datashield/dsopal,10043,0,2021-08-23T08:37:28Z,NA
DSSAT,"The purpose of this package is to provide a comprehensive
    R interface to the Decision Support System for Agrotechnology
    Transfer Cropping Systems Model (DSSAT-CSM; see <https://dssat.net> for more information).
    The package provides cross-platform functions to read and
    write input files, run DSSAT-CSM, and read output files.",2021-09-02,Phillip D. Alderman,NA,TRUE,https://github.com/palderman/dssat,9398,7,2021-07-26T16:27:39Z,1342.5714285714287
dssd,"Creates survey designs for distance sampling surveys. These
    designs can be assessed for various effort and coverage statistics.
    Once the user is satisfied with the design characteristics they can 
    generate a set of transects to use in their distance sampling survey.
    Many of the designs implemented in this R package were first made 
    available in our 'Distance' for Windows software and are detailed in 
    Chapter 7 of Advanced Distance Sampling, Buckland et. al. (2008, 
    ISBN-13: 978-0199225873). Find out more about estimating animal/plant 
    abundance with distance sampling at <http://distancesampling.org/>. ",2021-09-01,Laura Marshall,NA,TRUE,https://github.com/distancedevelopment/dssd,13171,1,2021-09-02T23:45:42Z,13171
dst,"Using the Theory of Belief Functions for evidence calculus. Basic probability assignments, or mass functions, can be defined on the subsets of a set of possible values and combined. A mass function can be extended to a larger frame. Marginalization, i.e. reduction to a smaller frame can also be done. These features can be combined to analyze small belief networks and take into account situations where information cannot be satisfactorily described by probability distributions.",2020-03-28,Claude Boivin,NA,TRUE,https://github.com/rapler/dst-1,20738,2,2021-08-20T01:08:35Z,10369
DSWE,"Data science methods used in wind energy applications. 
              Current functionalities include creating a multi-dimensional power curve model, 
              performing power curve function comparison, and covariate matching. 
              Relevant works for the developed functions are: 
              funGP() - Prakash et al. (2020) <arxiv:2003.07899>, 
              AMK() - Lee et al. (2015) <doi:10.1080/01621459.2014.977385>, 
              tempGP() - Prakash et al. (2020) <arxiv:2012.01349>, 
              ComparePCurve() - Ding et al. (2020) <arxiv:2005.08652>,
              All other functions - Ding (2019, ISBN:9780429956508).",2021-01-11,Yu Ding,"https://github.com/TAMU-AML/DSWE-Package,
https://aml.engr.tamu.edu/book-dswe/",TRUE,https://github.com/tamu-aml/dswe-package,2741,7,2021-01-12T03:49:24Z,391.57142857142856
DT,"Data objects in R can be rendered as HTML tables using the
    JavaScript library 'DataTables' (typically via R Markdown or Shiny). The
    'DataTables' library has been included in this R package. The package name
    'DT' is an abbreviation of 'DataTables'.",2021-09-02,Yihui Xie,https://github.com/rstudio/DT,TRUE,https://github.com/rstudio/dt,8532705,482,2021-09-02T14:48:24Z,17702.707468879667
DtD,"Provides fast methods to work with Merton's distance to default 
  model introduced in Merton (1974) <doi:10.1111/j.1540-6261.1974.tb03058.x>. 
  The methods includes simulation and estimation of the parameters.",2020-02-11,Benjamin Christoffersen,NA,TRUE,https://github.com/boennecd/dtd,20305,3,2020-11-04T06:20:59Z,6768.333333333333
dtplyr,"Provides a data.table backend for 'dplyr'. The goal
    of 'dtplyr' is to allow you to write 'dplyr' code that is
    automatically translated to the equivalent, but usually much faster,
    data.table code.",2021-02-20,Hadley Wickham,https://github.com/tidyverse/dtplyr,TRUE,https://github.com/tidyverse/dtplyr,3010668,504,2021-09-01T12:44:44Z,5973.547619047619
DTSg,"Basic time series functionalities such as listing of missing
    values, application of arbitrary aggregation as well as rolling (asymmetric)
    window functions and automatic detection of periodicity. As it is mainly
    based on 'data.table', it is fast and - in combination with the 'R6'
    package - offers reference semantics. In addition to its native R6
    interface, it provides an S3 interface inclusive an S3 wrapper method 
    generator for those who prefer the latter. Finally yet importantly, its
    functional approach allows incorporating functionalities from many other
    packages.",2021-05-30,Gerold Hepp,https://github.com/gisler/DTSg,TRUE,https://github.com/gisler/dtsg,18176,1,2021-07-21T17:16:22Z,18176
dttr2,"Manipulates date ('Date'), date time ('POSIXct') and
    time ('hms') vectors.  Date/times are considered discrete and are
    floored whenever encountered.  Times are wrapped and time zones are
    maintained unless explicitly altered by the user.",2020-07-10,Joe Thorley,https://github.com/poissonconsulting/dttr2,TRUE,https://github.com/poissonconsulting/dttr2,18186,8,2021-08-03T21:41:41Z,2273.25
dtwclust,"Time series clustering along with optimized techniques related
    to the Dynamic Time Warping distance and its corresponding lower bounds.
    Implementations of partitional, hierarchical, fuzzy, k-Shape and TADPole
    clustering are available. Functionality can be easily extended with
    custom distance measures and centroid definitions. Implementations of
    DTW barycenter averaging, a distance based on global alignment kernels,
    and the soft-DTW distance and centroid routines are also provided. 
    All included distance functions have custom loops optimized for the 
    calculation of cross-distance matrices, including parallelization support.
    Several cluster validity indices are included.",2019-12-11,Alexis Sarda-Espinosa,https://github.com/asardaes/dtwclust,TRUE,https://github.com/asardaes/dtwclust,110032,202,2021-07-01T18:35:36Z,544.7128712871287
duawranglr,"Create shareable data sets from raw data files that
	     contain protected elements. Relying on master crosswalk
	     files that list restricted variables, package functions
	     warn users about possible violations of data usage
	     agreement and prevent writing protected elements.",2021-04-15,Benjamin Skinner,https://github.com/btskinner/duawranglr,TRUE,https://github.com/btskinner/duawranglr,15135,8,2021-06-13T17:24:54Z,1891.875
duckdb,"The DuckDB project is an embedded analytical data
    management system with support for the Structured Query Language (SQL). This package includes all of
    DuckDB and a R Database Interface (DBI) connector.",2021-08-02,Hannes Mühleisen,"https://duckdb.org/, https://github.com/duckdb/duckdb",TRUE,https://github.com/duckdb/duckdb,27782,3435,2021-09-03T08:52:54Z,8.08791848617176
dumbbell,Creates a Dumbbell Plot.,2021-02-25,Foo Cheung,https://github.com/foocheung2/dumbbell,TRUE,https://github.com/foocheung2/dumbbell,2189,0,2021-02-25T21:26:18Z,NA
dupree,"Identifies code blocks that have a high level of similarity
  within a set of R files.",2020-04-21,Russ Hyde,https://github.com/russHyde/dupree,TRUE,https://github.com/russhyde/dupree,10958,13,2021-03-15T16:46:47Z,842.9230769230769
DVHmetrics,"Functionality for analyzing dose-volume histograms (DVH)
        in radiation oncology: Read DVH text files, calculate DVH
        metrics as well as generalized equivalent uniform dose (gEUD),
        biologically effective dose (BED), equivalent dose in 2 Gy
        fractions (EQD2), normal tissue complication probability
        (NTCP), and tumor control probability (TCP). Show DVH
        diagrams, check and visualize quality assurance constraints
        for the DVH. Includes web-based graphical user interface.",2021-04-16,Daniel Wollschlaeger,https://github.com/dwoll/DVHmetrics/,TRUE,https://github.com/dwoll/dvhmetrics,21926,5,2021-08-23T10:44:42Z,4385.2
dvir,"Joint DNA-based disaster victim identification (DVI), as described in 
    Vigeland and Egeland (2021) <doi:10.21203/rs.3.rs-296414/v1>. Identification is 
    performed by optimising the joint likelihood of all victim samples and 
    reference individuals. Individual identification probabilities, conditional on
    all available information, are derived from the joint solution in the form of 
    posterior pairing probabilities. 'dvir' is part of the 'ped suite' collection
    of packages for pedigree analysis. In particular it uses 'forrel' for 
    calculation of likelihood ratios.",2021-05-18,Thore Egeland,https://github.com/thoree/dvir,TRUE,https://github.com/thoree/dvir,2202,1,2021-06-03T09:33:48Z,2202
dygraphs,"An R interface to the 'dygraphs' JavaScript charting library
    (a copy of which is included in the package). Provides rich facilities
    for charting time-series data in R, including highly configurable
    series- and axis-display and interactive features like zoom/pan and
    series/point highlighting.",2018-07-11,Dan Vanderkam,https://github.com/rstudio/dygraphs,TRUE,https://github.com/rstudio/dygraphs,1447193,327,2021-03-03T11:21:12Z,4425.666666666667
dynamac,"While autoregressive distributed lag (ARDL) models allow for extremely flexible dynamics, interpreting substantive significance of complex lag structures remains difficult. This package is designed to assist users in dynamically simulating and plotting the results of various ARDL models. It also contains post-estimation diagnostics, including a test for cointegration when estimating the error-correction variant of the autoregressive distributed lag model (Pesaran, Shin, and Smith 2001 <doi:10.1002/jae.616>).",2020-04-03,Soren Jordan,https://github.com/andyphilips/dynamac/,TRUE,https://github.com/andyphilips/dynamac,24689,1,2020-10-09T19:16:19Z,24689
dynamichazard,"Contains functions that lets you fit dynamic hazard models using 
  state space models. The first implemented model is described in Fahrmeir 
  (1992) <doi:10.1080/01621459.1992.10475232> and Fahrmeir (1994) 
  <doi:10.1093/biomet/81.2.317>. Extensions hereof are available where the  
  Extended Kalman filter is replaced by an unscented Kalman filter and other 
  options including particle filters. The implemented particle filters support
  more general state space models. ",2021-02-15,Benjamin Christoffersen,https://github.com/boennecd/dynamichazard,TRUE,https://github.com/boennecd/dynamichazard,32011,6,2021-09-03T11:45:26Z,5335.166666666667
dynaSpec,A set of tools to generate dynamic spectrogram visualizations in video format.,2021-03-10,Marcelo Araya-Salas,https://github.com/maRce10/dynaSpec,TRUE,https://github.com/marce10/dynaspec,5161,6,2021-02-04T23:55:18Z,860.1666666666666
dyndimred,"
  Provides a common interface for applying dimensionality reduction methods,
  such as Principal Component Analysis ('PCA'), Independent Component Analysis ('ICA'), diffusion maps, 
  Locally-Linear Embedding ('LLE'), t-distributed Stochastic Neighbor Embedding ('t-SNE'), 
  and Uniform Manifold Approximation and Projection ('UMAP'). 
  Has built-in support for sparse matrices.",2021-03-23,Robrecht Cannoodt  (<https://orcid.org/0000-0003-3641-729X>,https://github.com/dynverse/dyndimred,TRUE,https://github.com/dynverse/dyndimred,12994,6,2021-03-23T08:50:42Z,2165.6666666666665
dyngen,"A novel, multi-modal simulation engine for
    studying dynamic cellular processes at single-cell resolution. 'dyngen'
    is more flexible than current single-cell simulation engines. It
    allows better method development and benchmarking, thereby stimulating
    development and testing of novel computational methods. Cannoodt et
    al. (2021) <doi:10.1038/s41467-021-24152-2>.",2021-06-24,Robrecht Cannoodt,"https://dyngen.dynverse.org, https://github.com/dynverse/dyngen",TRUE,https://github.com/dynverse/dyngen,4765,41,2021-06-30T03:27:04Z,116.21951219512195
dynplot,"Visualise a single-cell trajectory as a graph or dendrogram, 
  as a dimensionality reduction or heatmap of the expression data, 
  or a comparison between two trajectories as a pairwise scatterplot
  or dimensionality reduction projection. Saelens and Cannoodt et
  al. (2019) <doi:10.1038/s41587-019-0071-9>.",2021-06-28,Robrecht Cannoodt,https://github.com/dynverse/dynplot,TRUE,https://github.com/dynverse/dynplot,1413,12,2021-06-18T12:14:37Z,117.75
dynr,"Intensive longitudinal data have become increasingly prevalent in
    various scientific disciplines. Many such data sets are noisy, multivariate,
    and multi-subject in nature. The change functions may also be continuous, or
    continuous but interspersed with periods of discontinuities (i.e., showing
    regime switches). The package 'dynr' (Dynamic Modeling in R) is an R package
    that implements a set of computationally efficient algorithms for handling a
    broad class of linear and nonlinear discrete- and continuous-time models with
    regime-switching properties under the constraint of linear Gaussian measurement
    functions. The discrete-time models can generally take on the form of a state-
    space or difference equation model. The continuous-time models are generally
    expressed as a set of ordinary or stochastic differential equations. All
    estimation and computations are performed in C, but users are provided with the
    option to specify the model of interest via a set of simple and easy-to-learn
    model specification functions in R. Model fitting can be performed using single-
    subject time series data or multiple-subject longitudinal data. Ou, Hunter, &
    Chow (2019) <doi:10.32614/RJ-2019-012> provided a detailed introduction to the
    interface and more information on the algorithms.",2021-03-16,Michael D. Hunter,"https://dynrr.github.io/, https://github.com/mhunter1/dynr",TRUE,https://github.com/mhunter1/dynr,33962,1,2021-06-04T19:08:39Z,33962
dynsim,"Dynamic simulations and graphical depictions of autoregressive
    relationships.",2021-06-20,Christopher Gandrud,https://cran.r-project.org/package=dynsim,TRUE,https://github.com/christophergandrud/dynsim,20579,1,2021-06-20T16:13:54Z,20579
dynsurv,"Time-varying coefficient models for interval censored and
    right censored survival data including
    1) Bayesian Cox model with time-independent, time-varying or
    dynamic coefficients for right censored and interval censored data studied by
    Sinha et al. (1999) <doi:10.1111/j.0006-341X.1999.00585.x> and
    Wang et al. (2013) <doi:10.1007/s10985-013-9246-8>,
    2) Spline based time-varying coefficient Cox model for right censored data
    proposed by Perperoglou et al. (2006) <doi:10.1016/j.cmpb.2005.11.006>, and
    3) Transformation model with time-varying coefficients for right censored data
    using estimating equations proposed by
    Peng and Huang (2007) <doi:10.1093/biomet/asm058>.",2020-09-07,Wenjie Wang,https://github.com/wenjie2wang/dynsurv,TRUE,https://github.com/wenjie2wang/dynsurv,51208,5,2020-11-23T17:25:59Z,10241.6
dynutils,"
  Provides common functionality for the 'dynverse' packages. 
  'dynverse' is created to support the development, execution, and benchmarking of trajectory inference methods.
  For more information, check out <https://dynverse.org>.",2021-03-22,Robrecht Cannoodt  (<https://orcid.org/0000-0003-3641-729X>,https://github.com/dynverse/dynutils,TRUE,https://github.com/dynverse/dynutils,32329,1,2021-03-22T18:42:10Z,32329
dynwrap,"Provides functionality to infer trajectories from single-cell data,
  represent them into a common format, and adapt them. Other biological information
  can also be added, such as cellular grouping, RNA velocity and annotation.
  Saelens et al. (2019) <doi:10.1038/s41587-019-0071-9>.",2021-03-23,Robrecht Cannoodt  (<https://orcid.org/0000-0003-3641-729X>,https://github.com/dynverse/dynwrap,TRUE,https://github.com/dynverse/dynwrap,13993,11,2021-03-23T23:11:25Z,1272.090909090909
eaf,"Computation and visualization of the empirical attainment function (EAF) for the analysis of random sets in multi-criterion optimization. M. López-Ibáñez, L. Paquete, and T. Stützle (2010) <doi:10.1007/978-3-642-02538-9_9>.",2021-05-07,Manuel López-Ibáñez,"http://lopez-ibanez.eu/eaftools,
https://github.com/MLopez-Ibanez/eaf",TRUE,https://github.com/mlopez-ibanez/eaf,68037,10,2021-08-31T11:57:18Z,6803.7
earlyR,"Implements a simple, likelihood-based estimation of the reproduction number (R0) using a branching process with a Poisson likelihood. This model requires knowledge of the serial interval distribution, and dates of symptom onsets. Infectiousness is determined by weighting R0 by the probability mass function of the serial interval on the corresponding day. It is a simplified version of the model introduced by Cori et al. (2013) <doi:10.1093/aje/kwt133>.",2020-10-27,Thibaut Jombart,https://www.repidemicsconsortium.org/earlyR/,TRUE,https://github.com/reconhub/earlyr,24254,6,2020-10-27T06:28:22Z,4042.3333333333335
earthtide,"This is a port of 'Fortran ETERNA 3.4' 
             <http://igets.u-strasbg.fr/soft_and_tool.php> by H.G. Wenzel
             for calculating synthetic Earth tides using the 
             Hartmann and Wenzel (1994) <doi:10.1029/95GL03324> or 
             Kudryavtsev (2004) <doi:10.1007/s00190-003-0361-2> tidal catalogs. ",2021-06-04,Jonathan Kennel,https://github.com/jkennel/earthtide,TRUE,https://github.com/jkennel/earthtide,14558,10,2021-06-04T13:48:44Z,1455.8
easyalluvial,"Alluvial plots are similar to sankey diagrams and visualise categorical data 
    over multiple dimensions as flows. (Rosvall M, Bergstrom CT (2010) Mapping Change in 
    Large Networks. PLoS ONE 5(1): e8694. <doi:10.1371/journal.pone.0008694> 
    Their graphical grammar however is a bit more complex then that of a regular x/y 
    plots. The 'ggalluvial' package made a great job of translating that grammar into 
    'ggplot2' syntax and gives you many options to tweak the appearance of an alluvial 
    plot, however there still remains a multi-layered complexity that makes it difficult
    to use 'ggalluvial' for explorative data analysis. 'easyalluvial' provides a simple 
    interface to this package that allows you to produce a decent alluvial plot from any 
    dataframe in either long or wide format from a single line of code while also handling 
    continuous data. It is meant to allow a quick visualisation of entire dataframes 
    with a focus on different colouring options that can make alluvial plots a great 
    tool for data exploration. ",2021-01-13,Bjoern Koneswarakantha,https://github.com/erblast/easyalluvial/,TRUE,https://github.com/erblast/easyalluvial,30139,72,2021-01-13T09:37:58Z,418.59722222222223
easyCODA,"Univariate and multivariate methods for compositional data 
    analysis, based on logratios. The package implements the approach in the
    book Compositional Data Analysis in Practice by Michael Greenacre (2018),
    where accent is given to simple pairwise logratios. Selection can be made
    of logratios that account for a maximum percentage of logratio variance.
    Various multivariate analyses of logratios are included in the package. ",2020-09-19,Michael Greenacre,https://github.com/michaelgreenacre/CODAinPractice/,TRUE,https://github.com/michaelgreenacre/codainpractice,18926,10,2021-08-05T21:00:45Z,1892.6
easyr,"Makes difficult operations easy. Includes these types of functions: 
    shorthand, type conversion, data wrangling, and work flow. 
    Also includes some helpful data objects: NA strings, U.S. state list, color blind charting colors. 
    Built and shared by Oliver Wyman Actuarial Consulting. Accepting proposed contributions through GitHub.",2021-06-02,Bryce Chamberlain,https://github.com/oliver-wyman-actuarial/easyr,TRUE,https://github.com/oliver-wyman-actuarial/easyr,13579,15,2021-08-31T15:49:31Z,905.2666666666667
easySdcTable,"The main function, ProtectTable(), performs table suppression according to a 
 frequency rule with a data set as the only required input. Within this function, 
 protectTable(), protect_linked_tables() or runArgusBatchFile() in package 'sdcTable' is called. 
 Lists of level-hierarchy (parameter 'dimList') and other required input to these functions 
 are created automatically. 
 The suppression method Gauss (default) is an additional method that is not available in 'sdcTable'.
 The function, PTgui(), starts a graphical user interface based on the 'shiny' package.",2021-08-12,Øyvind Langsrud,https://github.com/statisticsnorway/easySdcTable,TRUE,https://github.com/statisticsnorway/easysdctable,21716,0,2021-08-13T06:28:56Z,NA
eat,"Functions are provided to determine production frontiers and technical 
    efficiency measures through non-parametric techniques based upon regression trees. 
    The package includes code for estimating radial input, output, directional and 
    additive measures, plotting graphical representations of the scores and the production 
    frontiers by means of trees, and determining rankings of importance of input variables 
    in the analysis. Additionally, an adaptation of Random Forest by a set of individual 
    Efficiency Analysis Trees for estimating technical efficiency is also included. More 
    details in: <doi:10.1016/j.eswa.2020.113783>.",2021-04-09,Miriam Esteve,https://efficiencytools.wordpress.com/,TRUE,https://github.com/miriamesteve/eat,4018,1,2021-08-26T08:30:23Z,4018
eatATA,"Provides simple functions to create constraints for small test assembly problems 
    (e.g. van der Linden (2005, ISBN: 978-0-387-29054-6)) using sparse matrices. Currently, 
    'GLPK', 'lpSolve', 'Symphony', and 'Gurobi' are supported as solvers. The 'gurobi' package is not available from 
    any mainstream repository; see <https://www.gurobi.com/downloads/>.",2021-07-06,Benjamin Becker,https://github.com/beckerbenj/eatATA,TRUE,https://github.com/beckerbenj/eatata,5446,1,2021-07-30T15:39:14Z,5446
eatGADS,"Import 'SPSS' data, handle and change 'SPSS' meta data, store and access large hierarchical data in 'SQLite' data bases.",2021-07-19,Benjamin Becker,https://github.com/beckerbenj/eatGADS,TRUE,https://github.com/beckerbenj/eatgads,4139,0,2021-08-31T09:52:57Z,NA
eatRep,"Replication methods to compute some basic statistic operations (means, standard deviations,
  frequency tables, percentiles and generalized linear models) in complex survey designs comprising multiple
  imputed variables and/or a clustered sampling structure which both deserve special procedures at least in
  estimating standard errors. See the package documentation for a more detailed description along with references.",2021-08-10,Sebastian Weirich,https://github.com/weirichs/eatRep,TRUE,https://github.com/weirichs/eatrep,3374,0,2021-08-10T16:44:00Z,NA
eatTools,"
   Miscellaneous functions for data cleaning and data analysis of educational assessments. Includes functions for descriptive 
   analyses, character vector manipulations and weighted statistics. Mainly a lightweight dependency for the packages 'eatRep', 
   'eatGADS', 'eatPrep' and 'eatModel' (which will be subsequently submitted to 'CRAN').
   The function for defining (weighted) contrasts in weighted effect coding refers to
   te Grotenhuis et al. (2017) <doi:10.1007/s00038-016-0901-1>.
   Functions for weighted statistics refer to
   Wolter (2007) <doi:10.1007/978-0-387-35099-8>.",2021-08-11,Sebastian Weirich,https://github.com/weirichs/eatTools,TRUE,https://github.com/weirichs/eattools,6154,0,2021-08-10T16:42:55Z,NA
EBCHS,We provide the main R functions to compute the posterior interval for the noncentrality parameter of the chi-squared distribution. The skewness estimate of the posterior distribution is also available to improve the coverage rate of posterior intervals. Details can be found in Du and Hu (2020) <doi:10.1080/01621459.2020.1777137>.  ,2021-06-01,Lilun Du,https://github.com/dulilun/EBCHS,TRUE,https://github.com/dulilun/ebchs,1088,0,2021-06-01T08:02:29Z,NA
EBMAforecast,"Create forecasts from multiple predictions using ensemble Bayesian model averaging (EBMA). EBMA models can be estimated using an expectation maximization (EM) algorithm or as fully Bayesian models via Gibbs sampling. The methods in this package are Montgomery, Hollenbach, and Ward (2015) <doi:10.1016/j.ijforecast.2014.08.001> and Montgomery, Hollenbach, and Ward (2012) <doi:10.1093/pan/mps002>.",2020-10-28,Florian M. Hollenbach,<https://github.com/fhollenbach/EBMA/>,TRUE,https://github.com/fhollenbach/ebma,18680,0,2020-10-29T18:17:28Z,NA
ec50estimator,"An implementation for estimating Effective control to 50% of growth
    inhibition (EC50) for multi isolates and stratified datasets. It implements 
    functions from the drc package in a way that is displayed a tidy data.frame 
    as output. Info about the drc package is available in Ritz C, Baty F, Streibig JC,
    Gerhard D (2015) <doi:10.1371/journal.pone.0146021>.",2020-09-15,Kaique dos S. Alves,https://github.com/AlvesKS/ec50estimator,TRUE,https://github.com/alvesks/ec50estimator,4420,0,2020-10-05T17:30:30Z,NA
eCAR,"Fits Leroux model in spectral domain to estimate causal spatial effect as detailed in 
             Guan, Y; Page, G.L.; Reich, B.J.; Ventrucci, M.; Yang, S; (2020) <arXiv:2012.11767>.  
             Both the parametric and semi-parametric models are available.  The semi-parametric model 
             relies on 'INLA'.  The 'INLA' package can be obtained from <https://www.r-inla.org/>.",2021-05-15,Garritt L. Page,https://github.com/gpage2990/eCAR,TRUE,https://github.com/gpage2990/ecar,2664,1,2021-05-02T21:59:32Z,2664
ECharts2Shiny,"Embed interactive charts to their Shiny applications. These charts will be generated by ECharts library developed by Baidu (<http://echarts.baidu.com/>). Current version supports line chart, bar chart, pie chart, scatter plot, gauge, word cloud, radar chart, tree map, and heat map.",2017-12-11,Xiaodong Deng,https://github.com/XD-DENG/ECharts2Shiny,TRUE,https://github.com/xd-deng/echarts2shiny,27979,122,2020-10-02T13:11:30Z,229.3360655737705
echarts4r,"Easily create interactive charts by leveraging the 'Echarts Javascript' library which includes
    36 chart types, themes, 'Shiny' proxies and animations.",2021-07-14,John Coene,"https://echarts4r.john-coene.com/,
https://github.com/JohnCoene/echarts4r",TRUE,https://github.com/johncoene/echarts4r,99181,413,2021-08-31T14:50:55Z,240.14769975786925
echarty,The goal is to deliver the full functionality of 'ECharts' with minimal overhead. 'ECharts' is based on data structures and 'echarty' users build R lists for these same data structures. One to three 'echarty' commands are usually sufficient to produce any chart. ,2021-07-30,Larry Helgason,https://github.com/helgasoft/echarty,TRUE,https://github.com/helgasoft/echarty,4069,8,2021-08-11T19:38:59Z,508.625
echogram,"Easily import multi-frequency acoustic data stored in 'HAC' files (see <http://biblio.uqar.ca/archives/30005500.pdf> for more information on the format), and produce echogram visualisations with predefined or customized color palettes. It is also possible to merge consecutive echograms; mask or delete unwanted echogram areas; model and subtract background noise; and more important, develop, test and interpret different combinations of frequencies in order to perform acoustic filtering of the echogram's data. ",2019-12-16,Héctor Villalobos,https://github.com/hvillalo/echogram,TRUE,https://github.com/hvillalo/echogram,15987,1,2021-07-27T00:10:16Z,15987
echor,"An R interface to United States Environmental 
    Protection Agency (EPA) Environmental Compliance 
    History Online ('ECHO') Application Program Interface
    (API). 'ECHO' provides information about EPA permitted 
    facilities, discharges, and other reporting info 
    associated with permitted entities. Data are obtained 
    from <https://echo.epa.gov/>. ",2021-08-21,Michael Schramm,NA,TRUE,https://github.com/mps9506/echor,18762,3,2021-08-21T01:58:05Z,6254
ecm,Functions for easy building of error correction models (ECM) for time series regression. ,2021-05-28,Gaurav Bansal,https://github.com/gaurbans/ecm,TRUE,https://github.com/gaurbans/ecm,40573,3,2021-05-28T15:03:53Z,13524.333333333334
ecmwfr,"Programmatic interface to the European Centre for Medium-Range
    Weather Forecasts dataset web services (ECMWF; <https://www.ecmwf.int/>)
    and Copernicus's Climate Data Store (CDS; 
    <https://cds.climate.copernicus.eu>). Allows for easy downloads of weather 
    forecasts and climate reanalysis data in R.",2020-07-13,Koen Hufkens,https://github.com/bluegreen-labs/ecmwfr,TRUE,https://github.com/bluegreen-labs/ecmwfr,23393,64,2021-04-20T16:03:48Z,365.515625
ecocomDP,"Tools to create, use, and convert 'ecocomDP' datasets. 'ecocomDP' is a dataset design pattern for harmonizing ecological community surveys in a research question agnostic format, from source datasets published across multiple repositories, and with methods that keep the derived datasets up-to-date as the underlying sources change. Described in O'Brien et al. (2021), <doi:10.1016/j.ecoinf.2021.101374>.",2021-07-31,Colin Smith,https://github.com/EDIorg/ecocomDP,TRUE,https://github.com/ediorg/ecocomdp,1308,23,2021-09-01T20:15:46Z,56.869565217391305
EcoDiet,"Biotracers and stomach content analyses are combined in a Bayesian hierarchical model
    to estimate a probabilistic topology matrix (all trophic link probabilities) and a diet matrix 
    (all diet proportions).
    The package relies on the JAGS software and the 'rjags' package to run a Markov chain Monte Carlo 
    approximation of the different variables.",2020-03-05,Pierre-Yves Hernvann,https://github.com/pyhernvann/EcoDiet,TRUE,https://github.com/pyhernvann/ecodiet,6610,1,2021-06-21T09:47:15Z,6610
ECoL,"Provides measures to characterize the complexity of classification 
    and regression problems based on aspects that quantify the linearity of the 
    data, the presence of informative feature, the sparsity and dimensionality 
    of the datasets. This package provides bug fixes, generalizations and 
    implementations of many state of the art measures. The measures are 
    described in the papers: Lorena et al. (2019) <doi:10.1145/3347711> and 
    Lorena et al. (2018) <doi:10.1007/s10994-017-5681-1>.",2019-11-05,Luis Garcia,https://github.com/lpfgarcia/ECoL/,TRUE,https://github.com/lpfgarcia/ecol,16002,42,2020-12-18T18:04:10Z,381
ecolottery,"Coalescent-Based Simulation of Ecological Communities as proposed
    by Munoz et al. (2017) <doi:10.13140/RG.2.2.31737.26728>. The package includes
    a tool for estimating parameters of community assembly by using Approximate 
    Bayesian Computation.",2017-07-03,François Munoz,https://github.com/frmunoz/ecolottery,TRUE,https://github.com/frmunoz/ecolottery,16621,11,2021-04-15T07:38:09Z,1511
ecolTest,"Functions and data sets to perform and demonstrate community ecology statistical tests, including Hutcheson's t-test (Hutcheson (1970) <doi:10.1016/0022-5193(70)90124-4>, Zar (2010) ISBN:9780321656865). ",2021-02-15,Hugo Salinas,https://github.com/hugosal/ecolTest,TRUE,https://github.com/hugosal/ecoltest,3029,0,2021-02-11T18:28:24Z,NA
economiccomplexity,"A wrapper of different methods from Linear Algebra for the equations
  introduced in The Atlas of Economic Complexity and related literature. This
  package provides standard matrix and graph output that can be used seamlessly
  with other packages.",2020-09-19,Mauricio Vargas,https://pacha.dev/economiccomplexity/,TRUE,https://github.com/pachamaltese/economiccomplexity,14792,22,2021-03-22T01:34:57Z,672.3636363636364
econullnetr,"Tools for using null models to analyse ecological
    networks (e.g. food webs, flower-visitation networks, seed-dispersal
    networks) and detect resource preferences or non-random interactions among
    network nodes. Tools are provided to run null models, test for and plot
    preferences, plot and analyse bipartite networks, and export null model
    results in a form compatible with other network analysis packages. The 
    underlying null model was developed by Agusti et al. (2003) Molecular 
    Ecology <doi:10.1046/j.1365-294X.2003.02014.x> and the full application to 
    ecological networks by Vaughan et al. (2018) econullnetr: an R package 
    using null models to analyse the structure of ecological networks and 
    identify resource selection. Methods in Ecology & Evolution, 
    <doi:10.1111/2041-210X.12907>.",2021-06-02,Ian Vaughan,NA,TRUE,https://github.com/ivaughan/econullnetr,12294,6,2021-05-28T15:46:56Z,2049
ECOSolveR,"R interface to the Embedded COnic Solver (ECOS), an efficient
	     and robust C library for convex problems. Conic and equality
	     constraints can be specified in addition to integer and
	     boolean variable constraints for mixed-integer problems. This
	     R interface is inspired by the python interface and has
	     similar calling conventions.",2021-01-14,Balasubramanian Narasimhan,https://bnaras.github.io/ECOSolveR/,TRUE,https://github.com/bnaras/ecosolver,79909,5,2021-01-13T23:16:21Z,15981.8
ecospat,"Collection of R functions and data sets for the support of spatial ecology analyses with a focus on pre, core and post modelling analyses of species distribution, niche quantification and community assembly. Written by current and former members and collaborators of the ecospat group of Antoine Guisan, Department of Ecology and Evolution (DEE) and Institute of Earth Surface Dynamics (IDYST), University of Lausanne, Switzerland. Read Di Cola et al. (2016) <doi:10.1111/ecog.02671> for details.",2021-02-19,Olivier Broennimann,http://www.unil.ch/ecospat/home/menuguid/ecospat-resources/tools.html,TRUE,https://github.com/ecospat/ecospat,39649,19,2021-06-04T09:58:25Z,2086.7894736842104
ecr,"Framework for building evolutionary algorithms for both single- and multi-objective continuous or discrete optimization problems. A set of predefined evolutionary building blocks and operators is included. Moreover, the user can easily set up custom objective functions, operators, building blocks and representations sticking to few conventions. The package allows both a black-box approach for standard tasks (plug-and-play style) and a much more flexible white-box approach where the evolutionary cycle is written by hand.",2017-07-10,Jakob Bossek,https://github.com/jakobbossek/ecr2,TRUE,https://github.com/jakobbossek/ecr2,21466,31,2021-06-29T18:03:33Z,692.4516129032259
edbuildmapr,"Import US Census Bureau, Education Demographic and Geographic Estimates Program,
  Composite School District Boundaries Files for 2013-2019 with the option to attach the 'EdBuild'
  master dataset of school district finance, student demographics, and community economic
  indicators for every school district in the United States. The master dataset is built
  from the US Census, Annual Survey of School System Finances (F33) and joins data from the
  National Center for Education Statistics, Common Core of Data; the US Census, Small Area
  Income and Poverty Estimates; and the US Census, Education Demographic and Geographic
  Estimates. Additional functions in the package create a dataset of all pairs of school
  district neighbors as either a dataframe or a shapefile and create formatted maps of
  selected districts at the state or neighbor level, symbolized by a selected variable
  in the 'EdBuild' master dataset. For full details about 'EdBuild' data processing please
  see 'EdBuild' (2020) <http://data.edbuild.org/>. ",2021-06-15,Megan Brodzik,https://github.com/EdBuild/edbuildmapr,TRUE,https://github.com/edbuild/edbuildmapr,10673,1,2021-06-14T14:01:02Z,10673
edbuildr,"Import the 'EdBuild' master dataset of school district finance,
  student demographics, and community economic indicators for every school district in the United States.  
  The master dataset is built from the US Census, Annual Survey of School System Finances (F33)
  and joins data from the National Center for Education Statistics, Common Core of Data;
  the US Census, Small Area Income and Poverty Estimates; and the US Census,
  Education Demographic and Geographic Estimates. We apply 'EdBuild' standard processing to the
  dataset and provide the option to select from four different exclusion criteria - see the masterpull() help file for more details.
  The master dataset is available for any school year from 2013 to 2019 or longitudinally for all years 2013-2019.
  School year is identified by the end year. For example, the 2018-19 school year is 2019.
  Additional functions in the package use 'EdBuild' master data to analyze the difference 
  between neighboring school districts and create formatted excel tables of school district data. For full details about
  'EdBuild' data processing please see 'EdBuild' (2020) <http://data.edbuild.org>. ",2021-06-02,Megan Brodzik,"https://github.com/EdBuild/edbuildr,
http://viz.edbuild.org/workshops/edbuildr/,
http://viz.edbuild.org/workshops/data-overview/",TRUE,https://github.com/edbuild/edbuildr,9273,3,2021-06-01T20:10:39Z,3091
eddi,"Finds and downloads raw Evaporative Demand Drought
    Index (EDDI) data, then reads the data into 'R' using the 'raster'
    package. The EDDI product detects drought at multiple time scales,
    from weekly ""flash droughts"" to long-term droughts. More information
    about the EDDI data product can be found at
    <https://www.esrl.noaa.gov/psd/eddi/>.",2019-05-22,Max Joseph,https://github.com/earthlab/eddi,TRUE,https://github.com/earthlab/eddi,11241,1,2020-12-23T04:50:27Z,11241
eddington,"Compute a cyclist's Eddington number, including efficiently
    computing cumulative E over a vector. A cyclist's Eddington number
    <https://en.wikipedia.org/wiki/Arthur_Eddington#Eddington_number_for_cycling>
    is the maximum number satisfying the condition such that a cyclist has
    ridden E miles or greater in E days. The algorithm in this package is an
    improvement over the conventional approach because both summary statistics
    and cumulative statistics can be computed in linear time, since it does not
    require initial sorting of the data. These functions may also be used for
    computing h-indices for authors, a metric described by Hirsch (2005)
    <doi:10.1073/pnas.0507655102>. Both are specific applications of computing
    the side length of a Durfee square 
    <https://en.wikipedia.org/wiki/Durfee_square>.",2020-03-24,Paul Egeler,https://github.com/pegeler/eddington2,TRUE,https://github.com/pegeler/eddington2,10773,2,2020-09-09T07:48:32Z,5386.5
edeaR,"Exploratory and descriptive analysis of event based data. Provides methods for describing and selecting process data, and for preparing event log data for process mining. Builds on the S3-class for event logs implemented in the package 'bupaR'.",2020-10-01,Gert Janssenswillen,"https://www.bupar.net, https://github.com/bupaverse/edeaR",TRUE,https://github.com/bupaverse/edear,73150,8,2020-12-18T14:35:29Z,9143.75
edgarWebR,"A set of methods to access and parse live filing information from the
    U.S. Securities and Exchange Commission (SEC - <https://www.sec.gov/>) including
    company and fund filings along with all associated metadata.",2021-04-24,Micah J Waldstein,"https://mwaldstein.github.io/edgarWebR/,
https://github.com/mwaldstein/edgarWebR",TRUE,https://github.com/mwaldstein/edgarwebr,20525,57,2021-04-18T20:23:18Z,360.0877192982456
editData,"An 'RStudio' addin for editing a 'data.frame' or a 'tibble'. You can delete, add or update a 'data.frame'
    without coding. You can get resultant data as a 'data.frame'. In the package, modularized 'shiny' app codes are provided. 
    These modules are intended for reuse across applications.",2021-04-02,Keon-Woong Moon,https://github.com/cardiomoon/editData,TRUE,https://github.com/cardiomoon/editdata,59664,21,2021-07-13T12:11:52Z,2841.1428571428573
edmdata,"Collection of data sets from various assessments that can be used to 
    evaluate psychometric models. These data sets have been analyzed in the
    following papers that introduced new methodology as part of the application section:
    Yinghan Chen et al. (2021) <doi:10.1007/s11336-021-09750-9>,
    Yinyin Chen et al. (2020) <doi:10.1007/s11336-019-09693-2>,
    Culpepper, S. A. (2019a) <doi:10.1007/s11336-019-09683-4>,
    Culpepper, S. A. (2019b) <doi:10.1007/s11336-018-9643-8>,
    Culpepper, S. A., & Chen, Y. (2019) <doi:10.3102/1076998618791306>,
    Culpepper, S. A., & Balamuta, J. J. (2017) <doi:10.1007/s11336-015-9484-7>,
    and Culpepper, S. A. (2015) <doi:10.3102/1076998615595403>.",2021-07-25,James Joseph Balamuta,"https://tmsalab.github.io/edmdata/,
https://github.com/tmsalab/edmdata/",TRUE,https://github.com/tmsalab/edmdata,8296,2,2021-07-25T15:18:43Z,4148
EDOIF,"A non-parametric framework based on  estimation statistics principle. Its main purpose is to  infer orders of empirical distributions from different categories based on a probability of finding a value in one distribution that is greater than an expectation of another distribution. Given a set of ordered-pair of real-category values the framework is capable of 1) inferring orders of  domination  of  categories  and  representing  orders  in  the form of a graph; 2) estimating  magnitude  of  difference  between  a  pair  of categories in forms of mean-difference confidence intervals; and 3) visualizing  domination  orders  and  magnitudes  of  difference of categories. The publication of this package is at Chainarong Amornbunchornvej, Navaporn Surasvadi, Anon Plangprasopchok, and Suttipong Thajchayapong (2020) <doi:10.1016/j.heliyon.2020.e05435>.",2021-03-28,Chainarong Amornbunchornvej,https://github.com/DarkEyes/EDOIF,TRUE,https://github.com/darkeyes/edoif,10094,1,2021-03-27T04:17:23Z,10094
educationdata,"Allows R users to retrieve and parse data from the Urban 
    Institute's Education Data API <https://ed-data-portal.urban.org/> into a 
    'data.frame' for analysis.",2021-05-31,Kyle Ueyama,https://urbaninstitute.github.io/education-data-package-r/,TRUE,https://github.com/urbaninstitute/education-data-package-r,1271,54,2021-08-09T17:39:08Z,23.537037037037038
eechidna,"Data from the seven Australian Federal Elections (House of
    Representatives) between 2001 and 2019, and from the four Australian
    Censuses over the same period. Includes tools for visualizing and
    analysing the data, as well as imputing Census data for years in
    which a Census does not occur. This package incorporates
    data that is copyright Commonwealth of Australia (Australian
    Electoral Commission and Australian Bureau of Statistics) 2019.",2021-02-25,Jeremy Forbes,"https://github.com/jforbes14/eechidna/,
https://jforbes14.github.io/eechidna/",TRUE,https://github.com/jforbes14/eechidna,16958,36,2021-02-26T00:55:38Z,471.05555555555554
eefAnalytics,"Analysing data from evaluations of educational interventions using a randomised controlled trial design. Various analytical tools to perform sensitivity analysis using different methods are supported (e.g. frequentist models with bootstrapping and permutations options, Bayesian models). The included commands can be used for simple randomised trials, cluster randomised trials and multisite trials. The methods can also be used more widely beyond education trials. This package can be used to evaluate other intervention designs using Frequentist and Bayesian multilevel models.",2021-03-16,Germaine Uwimpuhwe,https://github.com/germaine86/eefAnalytics,TRUE,https://github.com/germaine86/eefanalytics,11607,0,2021-03-15T10:54:33Z,NA
eemR,"Provides various tools for preprocessing Emission-Excitation-Matrix (EEM) for Parallel Factor Analysis (PARAFAC). Different
  methods are also provided to calculate common metrics such as humification index and fluorescence index.",2019-06-26,Philippe Massicotte,https://github.com/PMassicotte/eemR,TRUE,https://github.com/pmassicotte/eemr,21372,14,2020-11-20T17:00:23Z,1526.5714285714287
EFAtools,"Provides functions to perform exploratory factor analysis (EFA) procedures and compare their solutions. The goal is to provide state-of-the-art factor retention methods and a high degree of flexibility in the EFA procedures. This way, for example, implementations from R 'psych' and 'SPSS' can be compared. Moreover, functions for Schmid-Leiman transformation and the computation of omegas are provided. To speed up the analyses, some of the iterative procedures, like principal axis factoring (PAF), are implemented in C++.",2021-03-27,Markus Steiner,https://github.com/mdsteiner/EFAtools,TRUE,https://github.com/mdsteiner/efatools,8436,4,2021-06-14T17:50:28Z,2109
efdm,"An implementation of European Forestry Dynamics Model (EFDM) and
    an estimation algorithm for the transition probabilities.
    The EFDM is a large-scale forest model that simulates the development of
    the forest and estimates volume of wood harvested for any given forested
    area. This estimate can be broken down by, for example, species, site
    quality, management regime and ownership category.
    See Packalen et al. (2015) <doi:10.2788/153990>.",2021-08-16,Mikko Kuronen,https://github.com/mikkoku/efdm,TRUE,https://github.com/mikkoku/efdm,208,0,2021-08-17T15:08:51Z,NA
EFDR,"Enhanced False Discovery Rate (EFDR) is a tool to detect anomalies
    in an image. The image is first transformed into the wavelet domain in
    order to decorrelate any noise components, following which the coefficients
    at each resolution are standardised. Statistical tests (in a multiple
    hypothesis testing setting) are then carried out to find the anomalies. The
    power of EFDR exceeds that of standard FDR, which would carry out tests on
    every wavelet coefficient: EFDR choose which wavelets to test based on a
    criterion described in Shen et al. (2002). The package also provides
    elementary tools to interpolate spatially irregular data onto a grid of the
    required size. The work is based on Shen, X., Huang, H.-C., and Cressie, N.
    'Nonparametric hypothesis testing for a spatial signal.' Journal of the
    American Statistical Association 97.460 (2002): 1122-1140.",2021-04-18,Andrew Zammit-Mangion,https://github.com/andrewzm/EFDR/,TRUE,https://github.com/andrewzm/efdr,19389,4,2021-04-16T20:56:07Z,4847.25
eff2,"Estimate a total causal effect from observational data under 
    linearity and causal sufficiency. The observational data is supposed to 
    be generated from a linear structural equation model (SEM) with independent 
    and additive noise. The underlying causal DAG associated the SEM is required
    to be known up to a maximally oriented partially directed graph (MPDAG), 
    which is a general class of graphs consisting of both directed and 
    undirected edges, including CPDAGs (i.e., essential graphs) and DAGs. Such
    graphs are usually obtained with structure learning algorithms with added 
    background knowledge. The program is able to estimate every identified 
    effect, including single and multiple treatment variables. Moreover, the 
    resulting estimate has the minimal asymptotic covariance (and hence 
    shortest confidence intervals) among all estimators that are based on the 
    sample covariance. ",2021-05-21,Richard Guo,https://github.com/richardkwo/eff2,TRUE,https://github.com/richardkwo/eff2,1190,0,2021-05-22T02:16:24Z,NA
EffectLiteR,"Use structural equation modeling to estimate average and
    conditional effects of a treatment variable on an outcome variable, taking into
    account multiple continuous and categorical covariates.",2019-12-10,Axel Mayer,https://github.com/amayer2010/EffectLiteR,TRUE,https://github.com/amayer2010/effectliter,22902,4,2021-07-11T12:38:28Z,5725.5
effectsize,"Provide utilities to work with indices of effect size and
    standardized parameters for a wide variety of models (see list of
    supported models in insight; Lüdecke, Waggoner & Makowski (2019)
    <doi:10.21105/joss.01412>), allowing computation of and conversion
    between indices such as Cohen's d, r, odds, etc.",2021-05-25,"Mattan S. Ben-Shachar 
    (<https://orcid.org/0000-0002-4287-4801>",https://easystats.github.io/effectsize/,TRUE,https://github.com/easystats/effectsize,585693,189,2021-08-31T14:39:00Z,3098.904761904762
eflm,"Efficient Fitting of Linear and Generalized Linear Models by using
  just base R. As an alternative to lm() and glm(), this package provides elm()
  and eglm(), with a significant speedup when the number of 
  observations is larger than the number of parameters to estimate. The speed
 gains are obtained by reducing the NxP model matrix to a PxP matrix, and the 
 best computational performance is obtained when R is linked against 'OpenBLAS',
 'Intel MKL' or other optimized 'BLAS' library. This implementation aims at being
 compatible with 'broom' and 'sandwich' packages for summary statistics and
 clustering by providing S3 methods.",2021-05-31,Mauricio Vargas,https://github.com/pachadotdev/eflm/,TRUE,https://github.com/pachadotdev/eflm,2345,12,2021-05-30T20:24:46Z,195.41666666666666
efts,"The binary file format 'netCDF' is developed primarily for climate, ocean and
    meteorological data, and 'efts' is a package to read and write Ensemble Forecast Time 
    Series data in 'netCDF'. 'netCDF' has traditionally been used to store time slices 
    of gridded data, rather than complete time series of point data. 
    'efts' facilitates data handling stored in 'netCDF' files that follow a convention
    devised in the domain of ensemble hydrologic forecasting, but possibly applicable 
    in other domains. 'efts' uses reference class objects to 
    provide a high level interface to read and write such data, wrapping 
    lower level operations performed using 'ncdf4'.",2018-04-26,Jean-Michel Perraud,https://github.com/jmp75/efts,TRUE,https://github.com/jmp75/efts,13558,0,2020-10-25T04:47:09Z,NA
egor,"Tools for importing, analyzing and visualizing ego-centered
    network data. Supports several data formats, including the export formats of
    'EgoNet', 'EgoWeb 2.0' and 'openeddi'. An interactive (shiny) app for the
    intuitive visualization of ego-centered networks is provided. Also included
    are procedures for creating and visualizing Clustered Graphs 
    (Lerner 2008 <DOI:10.1109/PACIFICVIS.2008.4475458>).",2021-07-01,Till Krenz,"https://github.com/tilltnet/egor, https://egor.tillt.net/",TRUE,https://github.com/tilltnet/egor,20107,17,2021-09-01T20:40:28Z,1182.764705882353
EGRETci,"Collection of functions to evaluate uncertainty of results from
    water quality analysis using the Weighted Regressions on Time Discharge and
    Season (WRTDS) method. This package is an add-on to the EGRET package that
    performs the WRTDS analysis. The WRTDS modeling
    method was initially introduced and discussed in Hirsch et al. (2010) <doi:10.1111/j.1752-1688.2010.00482.x>,
    and expanded in Hirsch and De Cicco (2015) <doi:10.3133/tm4A10>. The 
    paper describing the uncertainty and confidence interval calculations 
    is Hirsch et al. (2015) <doi:10.1016/j.envsoft.2015.07.017>.",2021-04-13,Laura DeCicco,https://github.com/USGS-R/EGRETci,TRUE,https://github.com/usgs-r/egretci,19519,6,2021-04-13T15:15:52Z,3253.1666666666665
eha,"Parametric proportional hazards fitting with left truncation and
        right censoring for common families of distributions, piecewise constant 
        hazards, and discrete models. Parametric accelerated failure time models
        for left truncated and right censored data. Proportional hazards
        models for tabular and register data. Sampling of risk sets in Cox 
        regression, selections in the Lexis diagram, bootstrapping. 
        Broström (2012) <doi:10.1201/9781315373942>.",2021-04-30,Göran Broström,http://ehar.se/r/eha/,TRUE,https://github.com/goranbrostrom/eha,148668,1,2021-07-10T12:26:22Z,148668
EHRtemporalVariability,"Functions to delineate temporal dataset shifts in Electronic Health 
             Records through the projection and visualization of dissimilarities 
             among data temporal batches. This is done through the estimation of 
             data statistical distributions over time and their projection in 
             non-parametric statistical manifolds, uncovering the patterns of the 
             data latent temporal variability. 'EHRtemporalVariability' is 
             particularly suitable for multi-modal data and categorical variables 
             with a high number of values, common features of biomedical data where 
             traditional statistical process control or time-series methods may not 
             be appropriate. 'EHRtemporalVariability' allows you to explore and 
             identify dataset shifts through visual analytics formats such as 
             Data Temporal heatmaps and Information Geometric Temporal (IGT) plots. 
             An additional 'EHRtemporalVariability' Shiny app can be used to load 
             and explore the package results and even to allow the use of these 
             functions to those users non-experienced in R coding. (Sáez et al. 2020)
             <doi:10.1093/gigascience/giaa079>.",2021-05-31,Carlos Sáez,https://github.com/hms-dbmi/EHRtemporalVariability,TRUE,https://github.com/hms-dbmi/ehrtemporalvariability,15994,6,2021-05-31T11:51:28Z,2665.6666666666665
eicm,"Model fitting and species biotic interaction network topology selection for explicit
  interaction community models. Explicit interaction community models are an extension of binomial
  linear models for joint modelling of species communities, that incorporate both the effects of
  species biotic interactions and the effects of missing covariates. Species interactions are modelled
  as direct effects of each species on each of the others, and are estimated alongside the effects of
  missing covariates, modelled as latent factors. The package includes a penalized maximum likelihood
  fitting function, and a genetic algorithm for selecting the most parsimonious species interaction
  network topology.",2020-10-20,Miguel Porto,https://github.com/miguel-porto/eicm,TRUE,https://github.com/miguel-porto/eicm,7236,4,2021-07-08T13:48:17Z,1809
eiCompare,"Compares estimates from three ecological inference routines, based on King (1997) <ISBN: 0691012407>, <https://gking.harvard.edu/eicamera/kinroot.html>; King (2004) <ISBN: 0521542804>, <https://gking.harvard.edu/files/abs/ecinf04-abs.shtml>.",2020-09-18,Loren Collingwood,https://github.com/RPVote/eiCompare,TRUE,https://github.com/rpvote/eicompare,15410,3,2020-09-08T05:22:47Z,5136.666666666667
EigenR,"Matrix algebra using the 'Eigen' C++ library: determinant, rank, inversion, kernel and image, QR decomposition, Cholesky decomposition, linear least-squares problems. Complex matrices are supported.",2020-11-25,Stéphane Laurent,https://github.com/stla/EigenR,TRUE,https://github.com/stla/eigenr,3407,1,2020-12-05T23:44:20Z,3407
einet,"Methods and utilities for causal emergence.
    Used to explore and compute various information theory metrics for networks, such as effective information, effectiveness and causal emergence.",2020-04-23,Travis Byrum,https://github.com/travisbyrum/einet,TRUE,https://github.com/travisbyrum/einet,5324,2,2021-08-23T19:21:28Z,2662
eirm,"Analysis of dichotomous and polytomous response data using the explanatory item response modeling framework, as described in Stanke & Bulut (2019) <doi:10.21449/ijate.515085> and De Boeck & Wilson (2004) <doi:10.1007/978-1-4757-3990-9>. Generalized linear mixed modeling is used for estimating the effects of item-related and person-related variables on dichotomous and polytomous item responses. ",2021-07-04,Okan Bulut,https://github.com/okanbulut/eirm,TRUE,https://github.com/okanbulut/eirm,4411,2,2021-07-31T16:37:15Z,2205.5
EIX,"Structure mining from 'XGBoost' and 'LightGBM' models.
    Key functionalities of this package cover: visualisation of tree-based ensembles models,
    identification of interactions, measuring of variable importance,
    measuring of interaction importance, explanation of single prediction 
    with break down plots (based on 'xgboostExplainer' and 'iBreakDown' packages). 
    To download the 'LightGBM' use the following link: <https://github.com/Microsoft/LightGBM>.
    'EIX' is a part of the 'DrWhy.AI' universe.",2021-03-23,Szymon Maksymiuk,https://github.com/ModelOriented/EIX,TRUE,https://github.com/modeloriented/eix,13319,15,2021-02-14T21:57:41Z,887.9333333333333
eixport,"Emissions are the mass of pollutants released into the atmosphere. Air quality models need emissions data, with spatial and temporal distribution, to represent air pollutant concentrations. This package, eixport, creates inputs for the air quality models 'WRF-Chem' Grell et al (2005) <doi:10.1016/j.atmosenv.2005.04.027>, 'MUNICH' Kim et al (2018) <doi:10.5194/gmd-11-611-2018> , 'BRAMS-SPM' Freitas et al (2005) <doi:10.1016/j.atmosenv.2005.07.017> and 'RLINE' Snyder et al (2013) <doi:10.1016/j.atmosenv.2013.05.074>. See the 'eixport' website (<https://atmoschem.github.io/eixport/>) for more information, documentations and examples. More details in Ibarra-Espinosa et al (2018) <doi:10.21105/joss.00607>.",2020-11-13,Sergio Ibarra-Espinosa,https://atmoschem.github.io/eixport/,TRUE,https://github.com/atmoschem/eixport,19645,18,2021-07-17T22:19:33Z,1091.388888888889
elastic,"Connect to 'Elasticsearch', a 'NoSQL' database built on the 'Java'
    Virtual Machine. Interacts with the 'Elasticsearch' 'HTTP' API
    (<https://www.elastic.co/elasticsearch/>), including functions for
    setting connection details to 'Elasticsearch' instances, loading bulk data,
    searching for documents with both 'HTTP' query variables and 'JSON' based body
    requests. In addition, 'elastic' provides functions for interacting with API's
    for 'indices', documents, nodes, clusters, an interface to the cat API, and
    more.",2021-03-16,Scott Chamberlain,"https://docs.ropensci.org/elastic/ (website),
https://github.com/ropensci/elastic",TRUE,https://github.com/ropensci/elastic,53627,231,2021-05-06T23:36:18Z,232.15151515151516
electionsBR,"Offers a set of functions to easily download and clean 
    Brazilian electoral data from the Superior Electoral Court website. 
    Among others, the package retrieves data on local and
    federal elections for all positions (city councilor, mayor, state deputy,
    federal deputy, governor, and president) aggregated by
    state, city, and electoral zones. ",2021-01-30,Denisson Silva,http://electionsbr.com/,TRUE,https://github.com/silvadenisson/electionsbr,21472,52,2021-07-29T01:37:56Z,412.9230769230769
elevatr,"Several web services are available that provide access to elevation
             data. This package provides access to several of those services and 
             returns elevation data either as a SpatialPointsDataFrame from 
             point elevation services or as a raster object from raster 
             elevation services.  Currently, the package supports access to the
             Amazon Web Services Terrain Tiles <https://registry.opendata.aws/terrain-tiles/>, 
             the Open Topography Global Datasets API <https://opentopography.org/developers/>, 
             and the USGS Elevation Point Query Service <https://nationalmap.gov/epqs/>.",2021-07-22,Jeffrey Hollister,https://github.com/jhollist/elevatr/,TRUE,https://github.com/jhollist/elevatr,39954,113,2021-07-21T22:41:34Z,353.57522123893807
elisr,"An alternative to Exploratory Factor Analysis (EFA) for
    metrical data in R. Drawing on characteristics of classical test
    theory, Exploratory Likert Scaling (ELiS) supports the user exploring
    multiple one-dimensional data structures. In common research practice,
    however, EFA remains the go-to method to uncover the (underlying)
    structure of a data set. Orthogonal dimensions and the potential of
    overextraction are often accepted as side effects. As described in
    Müller-Schneider (2001) <doi:10.1515/zfsoz-2001-0404>), ELiS confronts
    these problems. As a result, 'elisr' provides the platform to fully
    exploit the exploratory potential of the multiple scaling approach
    itself.",2021-05-15,Steven Bißantz,https://github.com/sbissantz/elisr,TRUE,https://github.com/sbissantz/elisr,1169,0,2021-05-13T11:10:57Z,NA
eList,"
    Create list comprehensions (and other types of comprehension) similar to those in
    'python', 'haskell', and other languages. List comprehension in 'R' converts a 
    regular for() loop into a vectorized lapply() function. Support for looping 
    with multiple variables, parallelization, and across non-standard objects included. Package 
    also contains a variety of functions to help with list comprehension.",2021-01-22,Chris Mann,NA,TRUE,https://github.com/cmann3/elist,2888,1,2021-01-22T22:59:56Z,2888
ellipsis,"The ellipsis is a powerful tool for extending functions. Unfortunately 
    this power comes at a cost: misspelled arguments will be silently ignored. 
    The ellipsis package provides a collection of functions to catch problems
    and alert the user.",2021-04-29,Hadley Wickham,"https://ellipsis.r-lib.org, https://github.com/r-lib/ellipsis",TRUE,https://github.com/r-lib/ellipsis,28622493,129,2021-04-29T12:34:51Z,221879.7906976744
elliptic,"
 A suite of elliptic and related functions including Weierstrass and
 Jacobi forms.  Also includes various tools for manipulating and
 visualizing complex functions.",2019-03-14,Robin K. S. Hankin,https://github.com/RobinHankin/elliptic.git,TRUE,https://github.com/robinhankin/elliptic,705511,1,2021-08-16T23:25:38Z,705511
elmNNRcpp,"Training and predict functions for Single Hidden-layer Feedforward Neural Networks (SLFN) using the Extreme Learning Machine (ELM) algorithm. The ELM algorithm differs from the traditional gradient-based algorithms for very short training times (it doesn't need any iterative tuning, this makes learning time very fast) and there is no need to set any other parameters like learning rate, momentum, epochs, etc. This is a reimplementation of the 'elmNN' package using 'RcppArmadillo' after the 'elmNN' package was archived. For more information, see ""Extreme learning machine: Theory and applications"" by Guang-Bin Huang, Qin-Yu Zhu, Chee-Kheong Siew (2006), Elsevier B.V, <doi:10.1016/j.neucom.2005.12.126>.",2021-05-04,Lampros Mouselimis,https://github.com/mlampros/elmNNRcpp,TRUE,https://github.com/mlampros/elmnnrcpp,26298,10,2021-05-04T10:04:13Z,2629.8
elo,"A flexible framework for calculating Elo ratings and resulting
    rankings of any two-team-per-matchup system (chess, sports leagues, 'Go',
    etc.). This implementation is capable of evaluating a variety of matchups,
    Elo rating updates, and win probabilities, all based on the basic Elo
    rating system. It also includes methods to benchmark performance,
    including logistic regression and Markov chain models.",2020-01-14,Ethan Heinzen,"https://github.com/eheinzen/elo,
https://cran.r-project.org/package=elo,
https://eheinzen.github.io/elo/",TRUE,https://github.com/eheinzen/elo,23909,28,2021-05-07T13:38:30Z,853.8928571428571
EloOptimized,"Provides an implementation of the maximum likelihood methods for deriving
    Elo scores as published in Foerster, Franz et al. (2016) <DOI:10.1038/srep35404>.",2021-06-10,Joseph Feldblum,https://github.com/jtfeld/EloOptimized,TRUE,https://github.com/jtfeld/elooptimized,14184,0,2021-08-22T01:17:45Z,NA
EloRating,"Provides functions to quantify animal dominance hierarchies. The major focus is on Elo rating and its ability to deal with temporal dynamics in dominance interaction sequences. For static data, David's score and de Vries' I&SI are also implemented. In addition, the package provides functions to assess transitivity, linearity and stability of dominance networks. See Neumann et al (2011) <doi:10.1016/j.anbehav.2011.07.016> for an introduction.",2020-03-12,Christof Neumann,https://github.com/gobbios/EloRating,TRUE,https://github.com/gobbios/elorating,18101,1,2020-11-18T14:34:22Z,18101
eltr,"Provides a tool to run Monte Carlo simulation of catastrophe model event loss tables, using a Poisson frequency and Beta severity distribution.",2021-01-16,Randhir Bilkhu,"https://randhirbilkhu.github.io/eltr/,
https://github.com/RandhirBilkhu/eltr",TRUE,https://github.com/randhirbilkhu/eltr,2632,0,2021-01-26T21:07:07Z,NA
emayili,"A light, simple tool for sending emails with minimal dependencies.",2021-09-02,Andrew B. Collier,https://datawookie.github.io/emayili/,TRUE,https://github.com/datawookie/emayili,31543,96,2021-09-03T06:33:11Z,328.5729166666667
emba,"Analysis and visualization of an ensemble of boolean models for 
  biomarker discovery in cancer cell networks. The package allows to easily 
  load the simulation data results of the DrugLogics software pipeline which predicts synergistic drug 
  combinations in cancer cell lines (developed by the DrugLogics research group 
  in NTNU). It has generic functions that can be used to split a boolean model 
  dataset to model groups with regards to the models predictive performance (number of true 
  positive predictions/Matthews correlation coefficient score) or synergy prediction based on a given set 
  of gold standard synergies and find the average activity difference per network 
  node between all model group pairs. Thus, given user-specific thresholds,
  important nodes (biomarkers) can be accessed in the sense that they make the 
  models predict specific synergies (synergy biomarkers) or have better 
  performance in general (performance biomarkers). Lastly, if the 
  boolean models have a specific equation form and differ only in their link operator, 
  link operator biomarkers can also be found.",2021-01-07,John Zobolas,"https://bblodfon.github.io/emba/,
https://github.com/bblodfon/emba,
https://github.com/druglogics/",TRUE,https://github.com/bblodfon/emba,11537,0,2021-01-07T17:11:18Z,NA
embed,Predictors can be converted to one or more numeric representations using simple generalized linear models <arXiv:1611.09477> or nonlinear models <arXiv:1604.06737>. Most encoding methods are supervised.,2021-01-16,Max Kuhn,"https://embed.tidymodels.org, https://github.com/tidymodels/embed",TRUE,https://github.com/tidymodels/embed,25948,107,2021-07-03T19:35:26Z,242.50467289719626
EMCluster,"EM algorithms and several efficient
        initialization methods for model-based clustering of finite
        mixture Gaussian distribution with unstructured dispersion
        in both of unsupervised and semi-supervised learning.",2021-03-16,Wei-Chen Chen,https://github.com/snoweye/EMCluster,TRUE,https://github.com/snoweye/emcluster,37977,15,2021-03-08T20:02:37Z,2531.8
emdi,"Functions that support estimating, assessing and mapping regional
    disaggregated indicators. So far, estimation methods comprise direct estimation,
    the model-based unit-level approach Empirical Best Prediction (see ""Small area
    estimation of poverty indicators"" by Molina and Rao (2010) <doi:10.1002/cjs.10051>), 
    the area-level model (see ""Estimates of income for small places: An 
    application of James-Stein procedures to Census Data"" by Fay and Herriot (1979) 
    <doi:10.1080/01621459.1979.10482505>) and various extensions of it (adjusted variance 
    estimation methods, log and arcsin transformation, spatial, robust and measurement 
    error models), as well as their precision estimates. The assessment of the used model
    is supported by a summary and diagnostic plots. For a suitable presentation of
    estimates, map plots can be easily created. Furthermore, results can easily be
    exported to excel. For a detailed description of the package and the methods used
    see ""The R Package emdi for Estimating and Mapping Regionally Disaggregated Indicators"" 
    by Kreutzmann et al. (2019) <doi:10.18637/jss.v091.i07> and the second package vignette ""A Framework for Producing Small Area Estimates Based on Area-Level Models in {R}"".",2021-07-07,Soeren Pannier,https://github.com/SoerenPannier/emdi,TRUE,https://github.com/soerenpannier/emdi,25770,8,2021-08-17T12:05:41Z,3221.25
EmissV,"Processing tools to create emissions for use in numerical air 
  quality models. Emissions can be calculated both using emission factors 
  and activity data (Schuch et al 2018) <doi:10.21105/joss.00662> or using 
  pollutant inventories (Schuch et al., 2018) <doi:10.30564/jasr.v1i1.347>. 
  Functions to process individual point emissions, line emissions and 
  area emissions of pollutants are available as well as methods to 
  incorporate alternative data for Spatial distribution of emissions 
  such as satellite images (Martins et al, 2012) <doi:10.3389/fenvs.2015.00009> 
  or openstreetmap data (Andrade et al, 2015) <doi:10.3389/fenvs.2015.00009>.",2021-03-31,Daniel Schuch,https://atmoschem.github.io/EmissV/,TRUE,https://github.com/atmoschem/emissv,19583,19,2021-08-30T23:54:53Z,1030.6842105263158
EML,"Work with Ecological Metadata Language ('EML') files. 
    'EML' is a widely used metadata standard in the ecological and
    environmental sciences, described in Jones et al. (2006),
    <doi:10.1146/annurev.ecolsys.37.091305.110031>.",2021-02-27,Carl Boettiger,"https://docs.ropensci.org/EML/, https://github.com/ropensci/EML/",TRUE,https://github.com/ropensci/eml,43023,86,2021-06-07T17:37:04Z,500.2674418604651
emld,"This is a utility for transforming Ecological Metadata Language
        ('EML') files into 'JSON-LD' and back into 'EML.'  Doing so creates a
        list-based representation of 'EML' in R, so that 'EML' data can easily
        be manipulated using standard 'R' tools. This makes this package an
        effective backend for other 'R'-based tools  working with 'EML.' By
        abstracting away the complexity of 'XML' Schema, developers can
        build around native 'R' list objects and not have to worry about satisfying
        many of the additional constraints of set by the schema (such as element
        ordering, which is handled automatically). Additionally, the 'JSON-LD' 
        representation enables the use of developer-friendly 'JSON' parsing and
        serialization that may facilitate the use of 'EML' in contexts outside of 'R,'
        as well as the informatics-friendly serializations such as 'RDF' and
        'SPARQL' queries.",2020-09-27,Carl Boettiger,"https://docs.ropensci.org/emld/, https://github.com/ropensci/emld",TRUE,https://github.com/ropensci/emld,34061,11,2020-10-23T21:42:08Z,3096.4545454545455
emmeans,"Obtain estimated marginal means (EMMs) for many linear, generalized 
  linear, and mixed models. Compute contrasts or linear functions of EMMs,
  trends, and comparisons of slopes. Plots and other displays.
  Least-squares means are discussed, and the term ""estimated marginal means""
  is suggested, in Searle, Speed, and Milliken (1980) Population marginal means 
  in the linear model: An alternative to least squares means, The American 
  Statistician 34(4), 216-221 <doi:10.1080/00031305.1980.10483031>.",2021-08-20,Russell V. Lenth,https://github.com/rvlenth/emmeans,TRUE,https://github.com/rvlenth/emmeans,1702401,182,2021-08-22T01:59:57Z,9353.851648351649
emoji,"Contains data about emojis with relevant metadata, and functions
    to work with emojis when they are in strings.",2021-07-27,Emil Hvitfeldt,"https://emilhvitfeldt.github.io/emoji/,
https://github.com/EmilHvitfeldt/emoji",TRUE,https://github.com/emilhvitfeldt/emoji,683,16,2021-08-06T05:22:04Z,42.6875
emojifont,"An implementation of using emoji and fontawesome for using in both
    base and 'ggplot2' graphics.",2021-04-20,Guangchuang Yu,https://github.com/GuangchuangYu/emojifont,TRUE,https://github.com/guangchuangyu/emojifont,56346,60,2021-04-19T07:36:02Z,939.1
EmpiricalCalibration,"Routines for performing empirical calibration of observational
  study estimates. By using a set of negative control hypotheses we can
  estimate the empirical null distribution of a particular observational
  study setup. This empirical null distribution can be used to compute a
  calibrated p-value, which reflects the probability of observing an
  estimated effect size when the null hypothesis is true taking both random
  and systematic error into account. A similar approach can be used to
  calibrate confidence intervals, using both negative and positive controls. 
  For more details, see Schuemie et al. (2013) <doi:10.1002/sim.5925> and
  Schuemie et al. (2018) <doi:10.1073/pnas.1708282114>.",2021-03-04,Martijn Schuemie,"https://ohdsi.github.io/EmpiricalCalibration/,
https://github.com/OHDSI/EmpiricalCalibration",TRUE,https://github.com/ohdsi/empiricalcalibration,24381,8,2021-03-04T07:20:25Z,3047.625
EMSS,"Some EM-type algorithms to estimate parameters for the well-known Heckman selection model are provided in the package. Such algorithms are as follow: ECM(Expectation/Conditional Maximization), ECM(NR)(the Newton-Raphson method is adapted to the ECM) and ECME(Expectation/Conditional Maximization Either). Since the algorithms are based on the EM algorithm, they also have EM’s main advantages, namely, stability and ease of implementation. Further details and explanations of the algorithms can be found in Zhao et al. (2020) <doi: 10.1016/j.csda.2020.106930>.",2021-05-10,Kexuan Yang,https://github.com/SangkyuStat/EMSS,TRUE,https://github.com/sangkyustat/emss,6350,0,2021-05-10T15:45:30Z,NA
emstreeR,"Fast and easily computes an Euclidean Minimum Spanning Tree (EMST) from data. 
    This package relies on 'RcppMLPACK' to provide an R interface to the Dual-Tree Boruvka 
    algorithm (March, Ram, Gray, 2010, <doi:10.1145/1835804.1835882>) implemented in 
    'mlpack', the C++ Machine Learning Library (Curtin et. al., 2013). The Dual-Tree 
    Boruvka is theoretically and empirically the fastest algorithm for computing an EMST. 
    This package also provides functions and an S3 method for readily plotting Minimum 
    Spanning Trees (MST) using either the style of the 'base', 'scatterplot3d', or 
    'ggplot2' libraries.",2020-11-30,Allan Quadros,NA,TRUE,https://github.com/allanvc/emstreer,17450,4,2020-11-30T12:53:54Z,4362.5
emulator,"
 Allows one to estimate the output of a computer program,
 as a function of the input parameters, without actually running it.
 The computer program is assumed to be a Gaussian process, whose
 parameters are estimated using Bayesian techniques that give a PDF of
 expected program output.  This PDF is conditional on a training set
 of runs, each consisting of a point in parameter space and the model
 output at that point.  The emphasis is on complex codes that take
 weeks or months to run, and that have a large number of undetermined
 input parameters; many climate prediction models fall into this
 class.  The emulator essentially determines Bayesian posterior
 estimates of the PDF of the output of a model, conditioned on results
 from previous runs and a user-specified prior linear model.  The
 package includes functionality to evaluate quadratic forms 
 efficiently. ",2021-04-25,Robin K. S. Hankin,https://github.com/RobinHankin/emulator,TRUE,https://github.com/robinhankin/emulator,57494,1,2021-08-08T00:04:16Z,57494
emuR,"An R package that provides the EMU Speech 
    Database Management System (EMU-SDMS) with database management, data 
    extraction, data preparation and data visualization facilities. See
    <https://ips-lmu.github.io/The-EMU-SDMS-Manual/> for more details.",2021-06-11,Raphael Winkelmann,"https://github.com/IPS-LMU/emuR,
https://ips-lmu.github.io/The-EMU-SDMS-Manual/",TRUE,https://github.com/ips-lmu/emur,32892,21,2021-08-17T12:36:43Z,1566.2857142857142
enc,"
    Implements an S3 class for storing 'UTF-8' strings, based on regular character vectors.
    Also contains routines to portably read and write 'UTF-8' encoded text files,
    to convert all strings in an object to 'UTF-8',
    and to create character vectors with various encodings.",2019-12-19,Kirill Müller,https://github.com/krlmlr/enc,TRUE,https://github.com/krlmlr/enc,43773,12,2021-07-29T04:13:54Z,3647.75
encryptedRmd,"Create encrypted html files that are fully self contained and do
  not require any additional software. Using the package you can encrypt
  arbitrary html files and also directly create encrypted 'rmarkdown' html reports.",2020-12-09,Dirk Schumacher,https://github.com/dirkschumacher/encryptedRmd,TRUE,https://github.com/dirkschumacher/encryptedrmd,6074,143,2021-05-08T20:28:54Z,42.47552447552447
endoSwitch,Maximum likelihood estimation of endogenous switching regression models from Heckman (1979) <doi:10.2307/1912352> and estimation of treatment effects.  ,2020-02-21,Bowen Chen,https://github.com/cbw1243/endoSwitch,TRUE,https://github.com/cbw1243/endoswitch,6642,0,2021-02-15T15:31:35Z,NA
energy,"E-statistics (energy) tests and statistics for multivariate and univariate inference,
             including distance correlation, one-sample, two-sample, and multi-sample tests for
             comparing multivariate distributions, are implemented. Measuring and testing
             multivariate independence based on distance correlation, partial distance correlation,
             multivariate goodness-of-fit tests, k-groups and hierarchical clustering based on energy 
             distance, testing for multivariate normality, distance components (disco) for non-parametric 
             analysis of structured data, and other energy statistics/methods are implemented.",2021-02-22,Maria Rizzo,https://github.com/mariarizzo/energy,TRUE,https://github.com/mariarizzo/energy,297487,27,2021-02-21T21:42:34Z,11018.037037037036
enpls,"An algorithmic framework for measuring feature importance,
    outlier detection, model applicability domain evaluation,
    and ensemble predictive modeling with (sparse)
    partial least squares regressions.",2019-05-18,Nan Xiao,"https://nanx.me/enpls/, https://github.com/nanxstats/enpls",TRUE,https://github.com/nanxstats/enpls,22899,15,2021-07-19T06:02:25Z,1526.6
entropart,"Measurement and partitioning of diversity, based on Tsallis entropy, following Marcon and Herault (2015) <doi:10.18637/jss.v067.i08>.
             'entropart' provides functions to calculate alpha, beta and gamma diversity of communities, including phylogenetic and functional diversity.
             Estimation-bias corrections are available.",2021-06-04,Eric Marcon,https://github.com/EricMarcon/entropart,TRUE,https://github.com/ericmarcon/entropart,34426,3,2021-06-04T06:27:51Z,11475.333333333334
envalysis,"Small toolbox for data analyses in environmental chemistry and
    ecotoxicology. Provides, for example, calibration() to calculate calibration
    curves and corresponding limits of detection (LODs) and  limits of 
    quantification (LOQs) according to German DIN 32645 (2008). texture() makes
    it easy to estimate soil particle size distributions from hydrometer
    measurements (ASTM D422-63, 2007).",2021-03-05,Zacharias Steinmetz,https://github.com/zsteinmetz/envalysis,TRUE,https://github.com/zsteinmetz/envalysis,16133,1,2021-05-22T13:31:02Z,16133
EnvCpt,"Tools for automatic model selection and diagnostics for Climate and Environmental data.  In particular the envcpt() function does automatic model selection between a variety of trend, changepoint and autocorrelation models.  The envcpt() function should be your first port of call.",2021-03-29,Rebecca Killick,https://github.com/rkillick/EnvCpt/,TRUE,https://github.com/rkillick/envcpt,62921,4,2021-03-29T10:55:40Z,15730.25
envDocument,"Prints out information about the R working environment
    (system, R version,loaded and attached packages and versions) from a single
    function ""env_doc()"".  Optionally adds information on git repository,
    tags, commits and remotes (if available).",2019-08-19,Donald Jackson,https://github.com/dgJacks0n/envDocument,TRUE,https://github.com/dgjacks0n/envdocument,21785,1,2021-04-05T01:09:29Z,21785
EnvExpInd,"Tools for the assessment of the environmental exposure. The package provides three methods (nearest monitoring site, inverse distance weighted as described in Li Wu (2017) <doi:10.1016/j.envint.2016.11.013>,and ordinary kriging) to calculate the environmental exposure (e.g. air pollution) on the individual level.",2020-10-23,Bing Zhang,https://github.com/Spatial-R/EnvExpInd,TRUE,https://github.com/spatial-r/envexpind,3740,5,2020-10-19T03:55:45Z,748
envi,"Estimates an ecological niche using occurrence data, covariates, and kernel
        density-based estimation methods. For a single species with presence and absence data,
        the 'envi' package uses the spatial relative risk function that is estimated using the
        'sparr' package. Details about the 'sparr' package methods can be found in the tutorial:
        Davies et al. (2018) <doi:10.1002/sim.7577>. Details about kernel density estimation can
        be found in J. F. Bithell (1990) <doi:10.1002/sim.4780090616>.  More information about
        relative risk functions using kernel density estimation can be found in J. F. Bithell
        (1991) <doi:10.1002/sim.4780101112>.",2021-08-02,Ian D. Buller,https://github.com/Waller-SUSAN/envi,TRUE,https://github.com/waller-susan/envi,3977,0,2021-08-31T16:45:12Z,NA
enviGCMS,"Gas/Liquid Chromatography-Mass Spectrometer(GC/LC-MS) Data Analysis for Environmental Science. This package covered topics such molecular isotope ratio, matrix effects and Short-Chain Chlorinated Paraffins analysis etc. in environmental analysis.",2020-06-04,Miao YU,https://github.com/yufree/enviGCMS,TRUE,https://github.com/yufree/envigcms,18574,12,2021-07-20T20:21:23Z,1547.8333333333333
envir,"Provides a small set of functions for managing R environments, with defaults designed to encourage usage patterns that scale well to larger code bases. It provides: import_from(), a flexible way to assign bindings that defaults to the current environment; include(), a vectorized alternative to base::source() that also default to the current environment; and attach_eval() and attach_source(), a way to evaluate expressions in attached environments. Together, these (and other) functions pair to provide a robust alternative to base::library() and base::source().",2021-02-27,Tomasz Kalinowski,https://t-kalinowski.github.io/envir/,TRUE,https://github.com/t-kalinowski/envir,6116,4,2021-03-23T15:12:12Z,1529
envirem,Generation of bioclimatic rasters that are complementary to the typical 19 bioclim variables.  ,2021-05-14,Pascal O. Title,http://envirem.github.io,TRUE,https://github.com/ptitle/envirem,23363,7,2021-05-14T14:33:47Z,3337.5714285714284
EnvStats,"Graphical and statistical analyses of environmental data, with 
  focus on analyzing chemical concentrations and physical parameters, usually in 
  the context of mandated environmental monitoring.  Major environmental 
  statistical methods found in the literature and regulatory guidance documents, 
  with extensive help that explains what these methods do, how to use them, 
  and where to find them in the literature.  Numerous built-in data sets from 
  regulatory guidance documents and environmental statistics literature.  Includes 
  scripts reproducing analyses presented in the book ""EnvStats:  An R Package for 
  Environmental Statistics"" (Millard, 2013, Springer, ISBN 978-1-4614-8455-4, 
  <https://www.springer.com/book/9781461484554>).",2020-10-21,Steven P. Millard,https://github.com/alexkowa/EnvStats,TRUE,https://github.com/alexkowa/envstats,402284,7,2021-02-04T10:05:53Z,57469.142857142855
epanet2toolkit,"Enables simulation of water piping networks using 'EPANET'.
    The package provides functions from the 'EPANET' programmer's toolkit as R
    functions so that basic or customized simulations can be carried out from R.
    The package uses 'EPANET' version 2.2 from Open Water Analytics
    <https://github.com/OpenWaterAnalytics/EPANET/releases/tag/v2.2>.  ",2021-04-22,Bradley Eck,https://github.com/bradleyjeck/epanet2toolkit,TRUE,https://github.com/bradleyjeck/epanet2toolkit,9421,13,2021-04-22T20:31:07Z,724.6923076923077
epcc,"Provides several functions that allow model and simulate 
             the effects of thermal sensitivity and the exposition to 
             different trends in environmental temperature on the 
             abundance dynamics of ectotherms populations. It allows 
             an easy implementation of the possible consequences of 
             warming at global and local scales, constituting a useful 
             tool for understanding the extinction risk of populations.
             (Víctor Saldaña-Núñez, Fernando Córdova-Lepe, & Felipe N. 
             Moreno-Gómez, 2021) <doi:10.5281/zenodo.5034087>.",2021-06-29,Víctor Saldaña-Núñez,https://github.com/Victor-Saldana/epcc,TRUE,https://github.com/victor-saldana/epcc,864,0,2021-06-25T23:46:31Z,NA
eph,"Tools to download and manipulate the Permanent Household Survey from Argentina
    (EPH is the Spanish acronym for Permanent Household Survey).
    e.g: get_microdata() for downloading the datasets, get_poverty_lines() for downloading the official poverty baskets,
    calculate_poverty() for the calculation of stating if a household is in poverty or not, following the official methodology.
    organize_panels() is used to concatenate observations from different periods, and organize_labels()
    adds the official labels to the data. The implemented methods are based on INDEC (2016) <http://www.estadistica.ec.gba.gov.ar/dpe/images/SOCIEDAD/EPH_metodologia_22_pobreza.pdf>.
    As this package works with the argentinian Permanent Household Survey and its main audience is from this country,
    the documentation was written in Spanish.",2020-06-25,Diego Kozlowski,https://github.com/holatam/eph,TRUE,https://github.com/holatam/eph,16663,39,2021-07-15T08:29:30Z,427.2564102564103
EpiContactTrace,"Routines for epidemiological contact tracing
    and visualisation of network of contacts.",2020-12-12,Maria Noremark,https://github.com/stewid/EpiContactTrace,TRUE,https://github.com/stewid/epicontacttrace,27686,7,2021-08-30T14:19:06Z,3955.1428571428573
EpiCurve,"Creates simple or stacked epidemic curves for hourly, daily, weekly or monthly outcome data.",2021-07-14,Jean Pierre Decorps,https://github.com/IamKDO/EpiCurve,TRUE,https://github.com/iamkdo/epicurve,24139,0,2021-07-29T13:54:09Z,NA
EpiEstim,"Tools to quantify transmissibility throughout
    an epidemic from the analysis of time series of incidence as described in
    Cori et al. (2013) <doi:10.1093/aje/kwt133> and Wallinga and Teunis (2004) 
    <doi:10.1093/aje/kwh255>.",2021-01-07,Anne Cori,https://github.com/mrc-ide/EpiEstim,TRUE,https://github.com/mrc-ide/epiestim,90801,58,2021-06-23T08:47:45Z,1565.5344827586207
epifitter,"Analysis and visualization of plant disease progress curve data. Functions for
    fitting two-parameter population dynamics models (exponential, monomolecular, logistic
    and Gompertz) to proportion data for single or multiple epidemics using either linear
    or no-linear regression.  Statistical and visual outputs are provided to aid in model
    selection. Synthetic curves can be simulated for any of the models given the parameters.
    See Laurence V. Madden, Gareth Hughes, and Frank van den Bosch (2007) <doi:10.1094/9780890545058>
    for further information on the methods.",2021-06-14,Kaique dos S. Alves,https://github.com/AlvesKS/epifitter,TRUE,https://github.com/alvesks/epifitter,6279,4,2021-06-14T14:36:46Z,1569.75
epigraphdb,"The interface package to access data from the
    'EpiGraphDB' <https://epigraphdb.org> platform.
    It provides easy access to the 'EpiGraphDB' platform with functions that
    query the corresponding REST endpoints on the API <https://api.epigraphdb.org>
    and return the response data in the 'tibble' data frame format.",2021-03-29,Yi Liu,https://mrcieu.github.io/epigraphdb-r/,TRUE,https://github.com/mrcieu/epigraphdb-r,4989,11,2021-03-29T09:36:14Z,453.54545454545456
EpiILM,Provides tools for simulating from discrete-time individual level models for infectious disease data analysis. This epidemic model class contains spatial and contact-network based models with two disease types: Susceptible-Infectious (SI) and Susceptible-Infectious-Removed (SIR).,2020-09-30,Vineetha Warriyar. K. V.,https://github.com/waleedalmutiry/EpiILM,TRUE,https://github.com/waleedalmutiry/epiilm,19910,4,2020-09-30T11:53:23Z,4977.5
epikit,"Contains tools for formatting inline code, renaming redundant
  columns, aggregating age categories, and calculating proportions with
  confidence intervals. This is part of the 'R4Epis' project
  <https://r4epis.netlify.com>.",2020-09-07,Zhian N. Kamvar,"https://github.com/R4EPI/epikit, https://r4epis.netlify.com,
https://r4epi.github.io/epikit",TRUE,https://github.com/r4epi/epikit,11545,1,2020-09-07T22:03:06Z,11545
EpiModel,"Tools for simulating mathematical models of infectious disease dynamics. 
    Epidemic model classes include deterministic compartmental models, stochastic 
    individual-contact models, and stochastic network models. Network models use the
    robust statistical methods of exponential-family random graph models (ERGMs) 
    from the Statnet suite of software packages in R. Standard templates for epidemic 
    modeling include SI, SIR, and SIS disease types. EpiModel features an API for 
    extending these templates to address novel scientific research aims. Full 
    methods for EpiModel are detailed in Jenness et al. (2018, <doi:10.18637/jss.v084.i08>).",2021-06-25,Samuel Jenness,http://www.epimodel.org/,TRUE,https://github.com/statnet/epimodel,79523,189,2021-08-25T16:05:28Z,420.7566137566138
epinetr,"Allows for forward-in-time simulation of epistatic networks with associated
    phenotypic output.",2021-01-10,Dion Detterer,https://github.com/diondetterer/epinetr,TRUE,https://github.com/diondetterer/epinetr,3787,0,2020-12-22T01:05:57Z,NA
EpiNow2,"Estimates the time-varying reproduction number, rate of spread,
             and doubling time using a range of open-source tools (Abbott et al. (2020) <doi:10.12688/wellcomeopenres.16006.1>),
             and current best practices (Gostic et al. (2020) <doi:10.1101/2020.06.18.20134858>).
             It aims to help users avoid some of the limitations of naive implementations in a framework
             that is informed by community feedback and is under active development.",2020-12-14,Sam Abbott,"https:/epiforecasts.io/EpiNow2/,
https:/epiforecasts.io/EpiNow2/dev/,
https://github.com/epiforecasts/EpiNow2",TRUE,https://github.com/epiforecasts/epinow2,10099,53,2021-07-15T12:13:16Z,190.54716981132074
episensr,"Basic sensitivity analysis of the observed relative risks adjusting
    for unmeasured confounding and misclassification of the
    exposure/outcome, or both. It follows the bias analysis methods and
    examples from the book by Lash T.L, Fox M.P, and Fink A.K.
    ""Applying Quantitative Bias Analysis to Epidemiologic Data"",
    ('Springer', 2009).",2021-08-20,Denis Haine,https://github.com/dhaine/episensr,TRUE,https://github.com/dhaine/episensr,23433,6,2021-08-20T17:27:18Z,3905.5
epitweetr,"It allows you to automatically monitor trends of tweets by time, place and topic aiming at detecting public health threats early through the detection of signals (e.g. an unusual increase in the number of tweets). It was designed to focus on infectious diseases, and it can be extended to all hazards or other fields of study by modifying the topics and keywords.",2021-04-08,Francisco Orchard,https://github.com/EU-ECDC/epitweetr,TRUE,https://github.com/eu-ecdc/epitweetr,7362,29,2021-04-08T05:36:11Z,253.86206896551724
eplusr,"A rich toolkit of using the whole building
    simulation program 'EnergyPlus'(<https://energyplus.net>), which
    enables programmatic navigation, modification of 'EnergyPlus' models
    and makes it less painful to do parametric simulations and analysis.",2021-05-28,Hongyuan Jia,"https://hongyuanjia.github.io/eplusr/,
https://github.com/hongyuanjia/eplusr",TRUE,https://github.com/hongyuanjia/eplusr,26123,44,2021-06-11T06:29:23Z,593.7045454545455
epocakir,"Clinical coding and diagnosis of patients with kidney using
    clinical practice guidelines. The guidelines used are the evidence-based
    KDIGO guidelines, see <https://kdigo.org/guidelines/> for more information.
    This package covers acute kidney injury (AKI), anemia, and
    chronic liver disease (CKD).",2021-06-09,Alwin Wang,https://github.com/alwinw/epocakir,TRUE,https://github.com/alwinw/epocakir,1085,3,2021-06-09T12:31:11Z,361.6666666666667
EPP,A toolbox for coverage evaluation of proximity programs.,2021-08-05,Richard Detomasi,NA,TRUE,https://github.com/richdeto/epp,375,1,2021-09-02T15:40:32Z,375
epsiwal,"Implements the conditional estimation procedure of
  Lee, Sun, Sun and Taylor (2016) <doi:10.1214/15-AOS1371>.
  This procedure allows hypothesis testing on the mean of
  a normal random vector subject to linear constraints.",2019-07-02,Steven E. Pav,https://github.com/shabbychef/epsiwal,TRUE,https://github.com/shabbychef/epsiwal,10356,0,2021-04-03T23:10:45Z,NA
epwshiftr,"
    Query, download climate change projection data from the 'CMIP6' (Coupled
    Model Intercomparison Project Phase 6) project
    <https://pcmdi.llnl.gov/CMIP6/> in the 'ESGF' (Earth System Grid Federation)
    platform <https://esgf.llnl.gov>, and create future 'EnergyPlus'
    <https://energyplus.net> Weather ('EPW') files adjusted from climate changes
    using data from Global Climate Models ('GCM').",2021-05-26,Hongyuan Jia,https://github.com/ideas-lab-nus/epwshiftr,TRUE,https://github.com/ideas-lab-nus/epwshiftr,6515,8,2021-06-11T06:26:18Z,814.375
eq5d,"EQ-5D is a popular health related quality of life instrument used 
    in the clinical and economic evaluation of health care. Developed by the 
    EuroQol group <https://euroqol.org/>, the instrument consists of two 
    components: health state description and evaluation. For the description 
    component a subject self-rates their health in terms of five dimensions; 
    mobility, self-care, usual activities, pain/discomfort, and 
    anxiety/depression using either a three-level (EQ-5D-3L,
    <https://euroqol.org/eq-5d-instruments/eq-5d-3l-about/>) or a five-level
    (EQ-5D-5L, <https://euroqol.org/eq-5d-instruments/eq-5d-5l-about/>) 
    scale. Frequently the scores on these five dimensions are converted to a 
    single utility index using country specific value sets, which can be used
    in the clinical and economic evaluation of health care as well as in 
    population health surveys. The eq5d package provides methods to calculate 
    index scores from a subject's dimension scores. 29 TTO and 11 VAS EQ-5D-3L
    value sets including those for countries in Szende et al (2007) 
    <doi:10.1007/1-4020-5511-0> and Szende et al (2014) 
    <doi:10.1007/978-94-007-7596-1>, 28 EQ-5D-5L EQ-VT value sets from the 
    EuroQol website, the EQ-5D-5L crosswalk value sets developed by 
    van Hout et al. (2012) <doi:10.1016/j.jval.2012.02.008>, the
    crosswalk value set for Russia and reverse crosswalk value sets. Two 
    EQ-5D-Y value sets are also included. Methods are also included for the 
    analysis of EQ-5D profiles along with a shiny web tool to enable the 
    calculation, visualisation and automated statistical analysis of EQ-5D 
    data via a web browser using EQ-5D dimension scores stored in CSV or 
    Excel files. ",2021-07-27,Fraser Morton,https://github.com/fragla/eq5d,TRUE,https://github.com/fragla/eq5d,20070,12,2021-07-27T13:21:37Z,1672.5
equaltestMI,"Functions for examining measurement invariance via equivalence testing are included in this package. The traditionally used RMSEA (Root Mean Square Error of Approximation) cutoff values are adjusted based on simulation results. In addition, a projection-based method is implemented to test the equality of latent factor means across groups without assuming the equality of intercepts. For more information, see Yuan, K. H., & Chan, W. (2016) <doi:10.1037/met0000080>, Deng, L., & Yuan, K. H. (2016) <doi:10.1007/s11336-015-9491-8>, and Jiang, G., Mai, Y., & Yuan, K. H. (2017) <doi:10.3389/fpsyg.2017.01823>. ",2021-01-06,Ge Jiang,NA,TRUE,https://github.com/gabriellajg/equaltestmi,17809,1,2021-01-02T05:03:58Z,17809
equatags,"Provides function transform_mathjax() to transform equations 
 defined using 'MathML', 'LaTeX' or 'ASCIIMathML' notation
 into format 'SVG' or 'Office Open XML Math'. The 'XML' 
 result can then be included in 'HTML', 'Microsoft Word' 
 documents or 'Microsoft PowerPoint' presentations by using 
 a 'Markdown' document or the R package 'officer'. ",2021-08-04,David Gohel,NA,TRUE,https://github.com/ardata-fr/equatags,9324,5,2021-08-04T07:17:18Z,1864.8
equate,"Contains methods for observed-score linking
  and equating under the single-group, equivalent-groups,
  and nonequivalent-groups with anchor test(s) designs.
  Equating types include identity, mean, linear, general
  linear, equipercentile, circle-arc, and composites of
  these. Equating methods include synthetic, nominal
  weights, Tucker, Levine observed score, Levine true
  score, Braun/Holland, frequency estimation, and chained
  equating. Plotting and summary methods, and methods for
  multivariate presmoothing and bootstrap error estimation
  are also provided.",2018-04-08,Anthony Albano,https://github.com/talbano/equate,TRUE,https://github.com/talbano/equate,37312,2,2021-02-08T17:48:46Z,18656
equatiomatic,"The goal of 'equatiomatic' is to reduce the pain associated with
             writing 'LaTeX' formulas from fitted models. The primary function of
             the package, extract_eq(), takes a fitted model object as its input
             and returns the corresponding 'LaTeX' code for the model.",2021-01-30,Daniel Anderson,https://github.com/datalorax/equatiomatic,TRUE,https://github.com/datalorax/equatiomatic,14495,492,2021-08-31T20:53:18Z,29.461382113821138
era,"Provides a consistent vector representation of years with an 
    associated calendar era or time scale. It includes built-in definitions of 
    many contemporary and historic calendars; time scales commonly used in
    archaeology, astronomy, geology, and other palaeosciences (e.g. Before 
    Present, SI-prefixed 'annus'); and support for arbitrary user-defined eras. 
    Functions for converting between eras and for type-stable arithmetic with 
    years are also provided.",2021-01-29,Joe Roe,"https://era.joeroe.io, https://github.com/joeroe/era",TRUE,https://github.com/joeroe/era,2643,4,2021-02-09T15:29:13Z,660.75
ergm,"An integrated set of tools to analyze and simulate networks based on exponential-family random graph models (ERGMs). 'ergm' is a part of the Statnet suite of packages for network analysis. See Hunter, Handcock, Butts, Goodreau, and Morris (2008) <doi:10.18637/jss.v024.i03> and Krivitsky, Hunter, Morris, and Klumb (2021) <arXiv:2106.04997>.",2021-07-27,Pavel N. Krivitsky,https://statnet.org,TRUE,https://github.com/statnet/ergm,272242,65,2021-09-03T02:46:14Z,4188.338461538461
ergm.count,"A set of extensions for the 'ergm' package to fit weighted networks whose edge weights are counts. See Krivitsky (2012) <doi:10.1214/12-EJS696> and Krivitsky, Hunter, Morris, and Klumb (2021) <arXiv:2106.04997>.",2021-06-18,Pavel N. Krivitsky,https://statnet.org,TRUE,https://github.com/statnet/ergm.count,135040,5,2021-06-23T12:17:58Z,27008
ergm.ego,Utilities for managing egocentrically sampled network data and a wrapper around the 'ergm' package to facilitate ERGM inference and simulation from such data. See Krivitsky and Morris (2017) <doi:10.1214/16-AOAS1010>.,2021-06-23,Pavel N. Krivitsky,https://statnet.org,TRUE,https://github.com/statnet/ergm.ego,38678,9,2021-06-29T10:07:08Z,4297.555555555556
ergm.rank,"A set of extensions for the 'ergm' package to fit weighted networks whose edge weights are ranks. See Krivitsky and Butts (2017) <doi:10.1177/0081175017692623> and Krivitsky, Hunter, Morris, and Klumb (2021) <arXiv:2106.04997>.",2021-06-20,Pavel N. Krivitsky,http://statnet.org,TRUE,https://github.com/statnet/ergm.rank,37472,1,2021-06-23T12:20:05Z,37472
ergm.userterms,A template package to demonstrate the use of user-specified statistics for use in 'ergm' models as part of the Statnet suite of packages.,2019-05-15,Pavel N. Krivitsky,http://statnet.org,TRUE,https://github.com/statnet/ergm.userterms,75745,0,2021-04-26T17:16:36Z,NA
ergmito,"Simulation and estimation of Exponential Random Graph Models (ERGMs)
  for small networks using exact statistics as shown in Vega Yon et al. (2020)
  <DOI:10.1016/j.socnet.2020.07.005>. As a difference from the 'ergm'
  package, 'ergmito' circumvents using Markov-Chain Maximum Likelihood Estimator
  (MC-MLE) and instead uses Maximum Likelihood Estimator (MLE) to fit ERGMs
  for small networks. As exhaustive enumeration is computationally feasible for
  small networks, this R package takes advantage of this and provides tools for
  calculating likelihood functions, and other relevant functions, directly,
  meaning that in many cases both estimation and simulation of ERGMs for
  small networks can be faster and more accurate than simulation-based
  algorithms.",2020-08-10,George Vega Yon,https://muriteams.github.io/ergmito/,TRUE,https://github.com/muriteams/ergmito,10712,7,2021-06-11T08:25:35Z,1530.2857142857142
erify,"Provides several validator functions to check if arguments passed
    by users have valid types, lengths, etc., and if not, to generate
    informative and good-formatted error messages in a consistent style. Also
    provides tools for users to create their own validator functions. The
    error message style used is adopted from
    <https://style.tidyverse.org/error-messages.html>.",2021-05-30,Renfei Mao,"https://github.com/flujoo/erify, https://flujoo.github.io/erify/",TRUE,https://github.com/flujoo/erify,1983,4,2021-06-13T11:12:12Z,495.75
err,"Messages should provide users with readable information 
    about R objects without flooding their console. 
    'cc()' concatenates vector and data frame values 
    into a grammatically correct string using commas, an ellipsis and conjunction. 
    'cn()' allows the user to define a string which varies based on a count.
    'co()' combines the two to produce a customizable object aware string.
    The package further facilitates this process by providing five 'sprintf'-like 
    types such as '%n' for the length of an object and '%o' for its name as
    well as wrappers for pasting objects and issuing errors, warnings and messages.",2019-04-25,Joe Thorley,https://github.com/poissonconsulting/err,TRUE,https://github.com/poissonconsulting/err,33838,6,2021-02-15T22:04:41Z,5639.666666666667
erratum,Elegantly handle error and warning messages.,2021-02-22,John Coene,NA,TRUE,https://github.com/devopifex/erratum,3123,18,2021-05-17T16:48:55Z,173.5
errorlocate,Errors in data can be located and removed using validation rules from package 'validate'.,2021-05-03,Edwin de Jonge,https://github.com/data-cleaning/errorlocate,TRUE,https://github.com/data-cleaning/errorlocate,22638,17,2021-08-04T09:10:19Z,1331.6470588235295
errors,"Support for measurement errors in R vectors, matrices and arrays:
    automatic uncertainty propagation and reporting.
    Documentation about 'errors' is provided in the paper by Ucar, Pebesma &
    Azcorra (2018, <doi:10.32614/RJ-2018-075>), included in this package as a
    vignette; see 'citation(""errors"")' for details.",2020-11-10,Iñaki Ucar,https://github.com/r-quantities/errors,TRUE,https://github.com/r-quantities/errors,24956,37,2021-04-03T13:21:44Z,674.4864864864865
esaddle,Tools for fitting the Extended Empirical Saddlepoint (EES) density of Fasiolo et al. (2018) <doi:10.1214/18-EJS1433>.,2021-04-26,Matteo Fasiolo and Simon N. Wood,https://github.com/mfasiolo/esaddle,TRUE,https://github.com/mfasiolo/esaddle,16948,1,2021-06-24T11:04:33Z,16948
esaps,"It allows to construct two types of indicators used in the study of
    Electoral Systems and Party Systems starting from electoral results data.
    The Effective Number of Parties (Laakso and Taagepera (1979) <doi:10.1177/001041407901200101>)
    and Electoral Volatility in its three versions (Pedersen (1979) <doi:10.1111/j.1475-6765.1979.tb01267.x>,
    Powell and Tucker (2014) <doi:10.1017/S0007123412000531> and Torcal and Lago (2015, ISBN:9788415260356)).",2018-03-15,Nicolas Schmidt,https://github.com/Nicolas-Schmidt/esaps,TRUE,https://github.com/nicolas-schmidt/esaps,13789,3,2021-07-01T02:44:14Z,4596.333333333333
eSDM,"A tool which allows users to create and evaluate ensembles 
    of species distribution model (SDM) predictions. 
    Functionality is offered through R functions or a GUI (R Shiny app). 
    This tool can assist users in identifying spatial uncertainties and 
    making informed conservation and management decisions. The package is 
    further described in Woodman et al (2019) <doi:10.1111/2041-210X.13283>.",2021-05-04,Sam Woodman,"https://smwoodman.github.io/eSDM/,
https://github.com/smwoodman/eSDM/",TRUE,https://github.com/smwoodman/esdm,14813,5,2021-05-04T16:14:15Z,2962.6
esquisse,"A 'shiny' gadget to create 'ggplot2' figures interactively with drag-and-drop to map your variables to different aesthetics.
    You can quickly visualize your data accordingly to their type, export in various formats,
    and retrieve the code to reproduce the plot.",2021-07-05,Victor Perrier,https://github.com/dreamRs/esquisse,TRUE,https://github.com/dreamrs/esquisse,213019,1239,2021-08-31T09:05:28Z,171.92816787732042
ess,"An implementation of the ESS algorithm following Amol Deshpande, Minos Garofalakis,
	     Michael I Jordan (2013) <arXiv:1301.2267>. The ESS algorithm
	     is used for model selection in decomposable graphical models.",2021-05-31,Mads Lindskou,https://github.com/mlindsk/ess,TRUE,https://github.com/mlindsk/ess,6341,0,2021-05-27T18:50:36Z,NA
essurvey,Download data from the European Social Survey directly from their website <http://www.europeansocialsurvey.org/>. There are two families of functions that allow you to download and interactively check all countries and rounds available.,2021-03-22,Jorge Cimentada,"https://docs.ropensci.org/essurvey/,
https://github.com/ropensci/essurvey",TRUE,https://github.com/ropensci/essurvey,22235,45,2021-03-11T19:00:17Z,494.1111111111111
EstimationTools,"Routines for parameter estimation for any probability density or
    mass function implemented in R via maximum likelihood (ML) given a data set.
    The main routines 'maxlogL' and 'maxlogLreg' are  wrapper functions specifically
    developed for ML estimation. There are included optimization procedures such as
    'nlminb' and 'optim' from base package, and 'DEoptim' Mullen (2011)
    <doi: 10.18637/jss.v040.i06>. Standard errors are estimated with 'numDeriv'
    Gilbert (2011) <https://CRAN.R-project.org/package=numDeriv>
    or the option 'Hessian = TRUE' of 'optim' function.",2021-03-10,Jaime Mosquera,"https://jaimemosg.github.io/EstimationTools/,
https://github.com/Jaimemosg/EstimationTools",TRUE,https://github.com/jaimemosg/estimationtools,16431,1,2021-06-10T03:05:31Z,16431
estimatr,"Fast procedures for small set of commonly-used, design-appropriate estimators with robust standard errors and confidence intervals. Includes estimators for linear regression, instrumental variables regression, difference-in-means, Horvitz-Thompson estimation, and regression improving precision of experimental estimates by interacting treatment with centered pre-treatment covariates introduced by Lin (2013) <doi:10.1214/12-AOAS583>.",2021-01-17,Graeme Blair,"https://declaredesign.org/r/estimatr/,
https://github.com/DeclareDesign/estimatr",TRUE,https://github.com/declaredesign/estimatr,139582,115,2021-06-06T00:24:21Z,1213.7565217391304
estudy2,"An implementation of a most commonly used event study methodology,
    including both parametric and nonparametric tests. It contains variety
    aspects of the rate of return estimation (the core calculation is done in
    C++), as well as three classical for event study market models: mean
    adjusted returns, market adjusted returns and single-index market models.
    There are 6 parametric and 6 nonparametric tests provided, which examine
    cross-sectional daily abnormal return (see the documentation of the
    functions for more information). Parametric tests include tests proposed by 
    Brown and Warner (1980) <DOI:10.1016/0304-405X(80)90002-1>, Brown and Warner
    (1985) <DOI:10.1016/0304-405X(85)90042-X>, Boehmer et al. (1991)
    <DOI:10.1016/0304-405X(91)90032-F>, Patell (1976) <DOI:10.2307/2490543>, and
    Lamb (1995) <DOI:10.2307/253695>. Nonparametric tests covered in estudy2 are
    tests described in Corrado and Zivney (1992) <DOI:10.2307/2331331>,
    McConnell and Muscarella (1985) <DOI:10.1016/0304-405X(85)90006-6>,
    Boehmer et al. (1991) <DOI:10.1016/0304-405X(91)90032-F>, Cowan (1992)
    <DOI:10.1007/BF00939016>, Corrado (1989) <DOI:10.1016/0304-405X(89)90064-0>,
    Campbell and Wasley (1993) <DOI:10.1016/0304-405X(93)90025-7>, Savickas (2003)
    <DOI:10.1111/1475-6803.00052>, Kolari and Pynnonen (2010)
    <DOI:10.1093/rfs/hhq072>. Furthermore, tests for the cumulative
    abnormal returns proposed by Brown and Warner (1985)
    <DOI:10.1016/0304-405X(85)90042-X> and Lamb (1995) <DOI:10.2307/253695>
    are included.",2021-08-18,Iegor Rudnytskyi,"https://github.com/irudnyts/estudy2,
https://irudnyts.github.io/estudy2/",TRUE,https://github.com/irudnyts/estudy2,20869,4,2021-08-15T19:55:58Z,5217.25
ETAS,"Fits the space-time Epidemic Type Aftershock Sequence
    ('ETAS') model to earthquake catalogs using a stochastic 'declustering' 
    approach. The 'ETAS' model is a 'spatio-temporal' marked point process
    model and a special case of the 'Hawkes' process. The package is based 
    on a Fortran program by 'Jiancang Zhuang'
    (available at <http://bemlar.ism.ac.jp/zhuang/software.html>),
    which is modified and translated into C++ and C such that it 
    can be called from R. Parallel computing with 'OpenMP' is possible 
    on supported platforms.",2021-03-15,Abdollah Jalilian,https://github.com/jalilian/ETAS,TRUE,https://github.com/jalilian/etas,23764,13,2021-03-15T19:23:47Z,1828
ethnobotanyR,"An implementation of the quantitative ethnobotany indices in R. The goal is to provide an easy-to-use platform for ethnobotanists to assess the cultural significance of plant species based on informant consensus. The package closely follows the paper by Tardio and Pardo-de-Santayana (2008). Tardio, J., and M. Pardo-de-Santayana, 2008. Cultural Importance Indices: A Comparative Analysis Based on the Useful Wild Plants of Southern Cantabria (Northern Spain) 1. Economic Botany, 62(1), 24-39. <doi:10.1007/s12231-007-9004-5>.",2021-01-06,Cory Whitney,https://CRAN.R-project.org/package=ethnobotanyR,TRUE,https://github.com/cwwhitney/ethnobotanyr,17431,4,2021-05-15T07:45:04Z,4357.75
etl,"A predictable and pipeable framework for performing ETL 
    (extract-transform-load) operations on publicly-accessible medium-sized data 
    set. This package sets up the method structure and implements generic 
    functions. Packages that depend on this package download specific data sets 
    from the Internet, clean them up, and import them into a local or remote 
    relational database management system.",2021-05-17,Benjamin S. Baumer,https://github.com/beanumber/etl,TRUE,https://github.com/beanumber/etl,28355,112,2021-05-17T17:49:46Z,253.16964285714286
etrader,"Use R to interface with the 'ETRADE' API <https://developer.etrade.com/home>.
    Functions include authentication, trading, quote requests, account information, and option 
    chains. A user will need an ETRADE brokerage account and 'ETRADE' API approval. See README 
    for authentication process and examples.",2021-02-22,Anthony Balentine,https://exploringfinance.github.io/etrader/,TRUE,https://github.com/exploringfinance/etrader,3496,4,2021-02-22T01:01:54Z,874
eudract,"The remit of the European Clinical Trials Data Base (EudraCT <https://eudract.ema.europa.eu/> ) is to provide open access to summaries of all registered clinical trial results; thus aiming to prevent non-reporting of negative results and provide open-access to results to inform future research. The amount of information required and the format of the results, however, imposes a large extra workload at the end of studies on clinical trial units. In particular, the adverse-event-reporting component requires entering: each unique combination of treatment group and safety event; for every such event above, a further 4 pieces of information (body system, number of occurrences, number of subjects, number exposed) for non-serious events, plus an extra three pieces of data for serious adverse events (numbers of causally related events, deaths, causally related deaths). This package prepares the required statistics needed by EudraCT and formats them into the precise requirements to directly upload an XML file into the web portal, with no further data entry by hand.",2021-02-24,Simon Bond,https://eudract-tool.medschl.cam.ac.uk/,TRUE,https://github.com/shug0131/eudract,10802,3,2021-07-30T15:31:24Z,3600.6666666666665
eulerr,"Generate area-proportional Euler diagrams
    using numerical optimization. An Euler diagram is a generalization of a Venn
    diagram, relaxing the criterion that all interactions need to be
    represented. Diagrams may be fit with ellipses and circles via
    a wide range of inputs and can be visualized in numerous ways.",2020-03-09,Johan Larsson,"https://github.com/jolars/eulerr, https://jolars.github.io/eulerr/",TRUE,https://github.com/jolars/eulerr,62537,93,2021-06-11T06:35:23Z,672.4408602150538
Eunomia,"A sample dataset in the OMOP (Observational Medical Outcomes Partnership) Common Data Model (CDM) format. The CDM enables uniform storage of observational health care data, and is widely used for health care analytics. 'Eunomia' contains simulated data as well as a subset of the OMOP Vocabulary, and enables testing of additional packages and is used for educational and demonstration purposes.",2020-11-04,Frank DeFalco,https://github.com/OHDSI/Eunomia,TRUE,https://github.com/ohdsi/eunomia,4337,12,2020-11-10T16:54:57Z,361.4166666666667
eurocordexr,"
    Daily 'netCDF' data from e.g. regional climate models (RCMs) are not trivial
    to work with. This package, which relies on 'data.table', makes it easier
    to deal with large data from RCMs, such as from EURO-CORDEX 
    (<https://www.euro-cordex.net/>, <https://cordex.org/data-access/>). It has 
    functions to extract single grid cells from rotated pole grids as well as 
    the whole array in long format. Can handle non-standard calendars (360, 
    noleap) and interpolate them to a standard one. Potentially works with many 
    CF-conform 'netCDF' files. ",2021-08-19,Michael Matiu,https://github.com/mitmat/eurocordexr,TRUE,https://github.com/mitmat/eurocordexr,192,0,2021-08-24T09:44:18Z,NA
europepmc,"An R Client for the Europe PubMed Central RESTful Web Service
    (see <https://europepmc.org/RestfulWebService> for more information). It
    gives access to both metadata on life science literature and open access
    full texts. Europe PMC indexes all PubMed content and other literature
    sources including Agricola, a bibliographic database of citations to the
    agricultural literature, or Biological Patents. In addition to bibliographic
    metadata, the client allows users to fetch citations and reference lists.
    Links between life-science literature and other EBI databases, including
    ENA, PDB or ChEMBL are also accessible. No registration or API key is
    required. See the vignettes for usage examples.",2021-09-02,Najko Jahn,"https://docs.ropensci.org/europepmc/,
https://github.com/ropensci/europepmc/",TRUE,https://github.com/ropensci/europepmc,189184,20,2021-09-02T08:17:17Z,9459.2
eurostat,"Tools to download data from the Eurostat database
    <https://ec.europa.eu/eurostat> together with search and
    manipulation utilities.",2021-05-14,Leo Lahti,https://ropengov.github.io/eurostat/,TRUE,https://github.com/ropengov/eurostat,87762,180,2021-09-01T10:10:06Z,487.56666666666666
evabic,"Evaluates the performance of binary classifiers.
    Computes confusion measures (TP, TN, FP, FN), derived measures (TPR,
    FDR, accuracy, F1, DOR, ..), and area under the curve.  Outputs are
    well suited for nested dataframes.",2020-06-12,Antoine Bichat,"https://abichat.github.io/evabic,
https://github.com/abichat/evabic",TRUE,https://github.com/abichat/evabic,8340,5,2021-02-10T09:53:57Z,1668
evalITR,"Provides various statistical methods for evaluating Individualized Treatment Rules under randomized data. The provided metrics include Population Average Value (PAV), Population Average Prescription Effect (PAPE), Area Under Prescription Effect Curve (AUPEC). It also provides the tools to analyze Individualized Treatment Rules under budget constraints. Detailed reference in Imai and Li (2019) <arXiv:1905.05389>.",2021-07-07,Michael Lingzhi Li,https://github.com/MichaelLLi/evalITR,TRUE,https://github.com/michaellli/evalitr,9346,5,2021-07-07T18:51:09Z,1869.2
evaluate,"Parsing and evaluation tools that make it easy to recreate the
    command line behaviour of R.",2019-05-28,Yihui Xie,https://github.com/r-lib/evaluate,TRUE,https://github.com/r-lib/evaluate,20612489,88,2021-02-26T05:35:31Z,234232.82954545456
evaluator,"An open source risk analysis toolkit based on the OpenFAIR ontology 
  <https://publications.opengroup.org/c20b> and risk analysis standard 
  <https://publications.opengroup.org/c20a>. Empowers an organization to 
  perform a quantifiable, repeatable, and data-driven risk review.",2021-07-06,David Severski,https://evaluator.tidyrisk.org,TRUE,https://github.com/davidski/evaluator,23644,108,2021-07-19T00:22:34Z,218.92592592592592
EventDetectR,Detect events in time-series data. Combines multiple well-known R packages like 'forecast' and 'neuralnet' to deliver an easily configurable tool for multivariate event detection.,2020-10-10,Sowmya Chandrasekaran,https://github.com/frehbach/EventDetectR,TRUE,https://github.com/frehbach/eventdetectr,15897,14,2020-12-18T11:10:16Z,1135.5
eventglm,"A user friendly, easy to understand way of doing event
    history regression for marginal estimands of interest,
    including the cumulative incidence and the restricted mean
    survival, using the pseudo observation framework for 
    estimation. For a review of the methodology, see Andersen and
    Pohar Perme (2010) <doi:10.1177/0962280209105020>. The
    interface uses the well known formulation of a generalized
    linear model and allows for features including plotting of 
    residuals, the use of sampling weights, and corrected
    variance estimation. ",2021-06-01,Michael C Sachs,https://sachsmc.github.io/eventglm/,TRUE,https://github.com/sachsmc/eventglm,4012,3,2021-06-01T09:42:57Z,1337.3333333333333
EvidenceSynthesis,"Routines for combining causal effect estimates and study diagnostics across multiple data sites in a distributed study, without sharing patient-level data. 
  Allows for normal and non-normal approximations of the data-site likelihood of the effect parameter. ",2021-01-29,Martijn Schuemie,"https://ohdsi.github.io/EvidenceSynthesis/,
https://github.com/OHDSI/EvidenceSynthesis",TRUE,https://github.com/ohdsi/evidencesynthesis,3886,3,2021-02-10T14:34:50Z,1295.3333333333333
EviewsR,It allows running 'EViews'(<https://eviews.com>) program from R Markdown. 'EViews' (Econometric Views) is a statistical software for Econometric analysis.  This package serves as an 'EViews' Knit-Engine for 'knitr' package. Write all your 'EViews' commands in R Markdown chunk.,2020-06-04,Sagiru Mati,https://smati.com.ng,TRUE,https://github.com/sagirumati/eviewsr,4884,1,2021-07-03T10:22:59Z,4884
exactextractr,"Provides a replacement for the 'extract' function from the 'raster' package
    that is suitable for extracting raster values using 'sf' polygons.",2021-05-10,Daniel Baston,"https://isciences.gitlab.io/exactextractr/,
https://github.com/isciences/exactextractr",TRUE,https://github.com/isciences/exactextractr,79453,159,2021-08-26T02:04:26Z,499.70440251572325
ExamPAData,"Contains all data sets for Exam PA: Predictive Analytics at 
    <https://exampa.net/>.",2021-05-26,Guanglai Li,https://github.com/sdcastillo/ExamPAData,TRUE,https://github.com/sdcastillo/exampadata,4720,5,2021-05-27T12:50:08Z,944
exams.mylearn,"Randomized multiple-select and single-select
  question generation for the 'MyLearn' teaching and learning
  platform. Question templates
  in the form of the R/exams package (see <http://www.r-exams.org/>)
  are transformed into XML format required by 'MyLearn'.",2021-04-19,Darjus Hosszejni,https://github.com/hdarjus/exams.mylearn,TRUE,https://github.com/hdarjus/exams.mylearn,6153,2,2021-04-19T15:06:58Z,3076.5
exams2sakai,"Automatic Generation of Exams in R for 'Sakai'.
  Question templates in the form of the 'exams' package (see <http://www.r-exams.org/>)
  are transformed into XML format required by 'Sakai'.",2021-04-22,Jesús María Méndez Pérez,https://github.com/jesusmmp/exams2sakai,TRUE,https://github.com/jesusmmp/exams2sakai,2137,1,2021-04-22T10:06:24Z,2137
excel.link,"Allows access to data in running instance of Microsoft Excel
    (e. g. 'xl[a1] = xl[b2]*3' and so on). Graphics can be transferred with
    'xl[a1] = current.graphics()'. Additionally there are function for reading/writing 
    'Excel' files - 'xl.read.file'/'xl.save.file'. They are not fast but able to read/write 
    '*.xlsb'-files and password-protected files. There is an Excel workbook with 
    examples of calling R from Excel in the 'doc' folder. It tries to keep things as
    simple as possible - there are no needs in any additional
    installations besides R, only 'VBA' code in the Excel workbook.
    Microsoft Excel is required for this package.",2021-03-21,"Gregory Demin <excel.link.feedback@gmail.com>. To comply CRAN policy
    includes source code from RDCOMClient (http://www.omegahat.net/RDCOMClient/) by
    Duncan Temple Lang <duncan@wald.ucdavis.edu>.",https://github.com/gdemin/excel.link,TRUE,https://github.com/gdemin/excel.link,48444,42,2021-03-21T17:18:06Z,1153.4285714285713
excelR,An R interface to 'jExcel' library to create web-based interactive tables and spreadsheets compatible with 'Excel' or any other spreadsheet software.,2020-03-09,Swechhya Bista,https://github.com/Swechhya/excelR,TRUE,https://github.com/swechhya/excelr,27685,125,2020-10-28T07:19:03Z,221.48
excursions,"Functions that compute probabilistic excursion sets, contour credibility regions, contour avoiding regions, and simultaneous confidence bands for latent Gaussian random processes and fields. The package also contains functions that calculate these quantities for models estimated with the INLA package. The main references for excursions are Bolin and Lindgren (2015) <doi:10.1111/rssb.12055>, Bolin and Lindgren (2017) <doi:10.1080/10618600.2016.1228537>, and Bolin and Lindgren (2018) <doi:10.18637/jss.v086.i05>. These can be generated by the citation function in R.",2021-01-21,David Bolin,https://github.com/davidbolin/excursions,TRUE,https://github.com/davidbolin/excursions,23965,0,2021-01-20T20:24:53Z,NA
exdex,"Performs frequentist inference for the extremal index of a 
    stationary time series.  Two types of methodology are used.  One type is
    based on a model that relates the distribution of block maxima to the 
    marginal distribution of series and leads to the semiparametric maxima 
    estimators described in Northrop (2015) <doi:10.1007/s10687-015-0221-5> and 
    Berghaus and Bucher (2018) <doi:10.1214/17-AOS1621>.  Sliding block maxima
    are used to increase precision of estimation. The other type of methodology
    uses a model for the distribution of threshold inter-exceedance times
    (Ferro and Segers (2003) <doi:10.1111/1467-9868.00401>). Two 
    versions of this type of approach are provided, following Suveges (2007) 
    <doi:10.1007/s10687-007-0034-2> and Suveges and Davison (2010)  
    <doi:10.1214/09-AOAS292>.",2019-08-06,Paul J. Northrop,http://github.com/paulnorthrop/exdex,TRUE,https://github.com/paulnorthrop/exdex,10834,0,2021-07-02T16:03:19Z,NA
ExhaustiveSearch,"The goal of this package is to provide an easy to use, fast and
    scalable exhaustive search framework. Exhaustive feature selections 
    typically require a very large number of models to be fitted and evaluated. 
    Execution speed and memory management are crucial factors here. This package 
    provides solutions for both. Execution speed is optimized by using a 
    multi-threaded C++ backend, and memory issues are solved by by only storing 
    the best results during execution and thus keeping memory usage constant.",2021-01-18,Rudolf Jagdhuber,https://github.com/RudolfJagdhuber/ExhaustiveSearch,TRUE,https://github.com/rudolfjagdhuber/exhaustivesearch,2759,3,2021-01-22T00:03:55Z,919.6666666666666
exifr,"Reads EXIF data using ExifTool <https://exiftool.org>
    and returns results as a data frame.
    ExifTool is a platform-independent Perl library plus a command-line
    application for reading, writing and editing meta information in a wide variety
    of files. ExifTool supports many different metadata formats including EXIF,
    GPS, IPTC, XMP, JFIF, GeoTIFF, ICC Profile, Photoshop IRB, FlashPix, AFCP and
    ID3, as well as the maker notes of many digital cameras by Canon, Casio, FLIR,
    FujiFilm, GE, HP, JVC/Victor, Kodak, Leaf, Minolta/Konica-Minolta, Motorola, Nikon,
    Nintendo, Olympus/Epson, Panasonic/Leica, Pentax/Asahi, Phase One, Reconyx, Ricoh,
    Samsung, Sanyo, Sigma/Foveon and Sony.",2021-03-20,Dewey Dunnington,https://github.com/paleolimbot/exifr,TRUE,https://github.com/paleolimbot/exifr,33664,30,2021-03-20T21:49:40Z,1122.1333333333334
exoplanets,"The goal of exoplanets is to provide access to
    NASA's Exoplanet Archive TAP Service. For more information regarding
    the API please read the documentation
    <https://exoplanetarchive.ipac.caltech.edu/index.html>.",2021-07-24,Tyler Littlefield,"https://docs.ropensci.org/exoplanets/,
https://github.com/ropensci/exoplanets",TRUE,https://github.com/ropensci/exoplanets,1765,10,2021-07-24T19:30:00Z,176.5
ExPanDaR,"Provides a shiny-based front end (the 'ExPanD' app) and
    a set of functions for exploratory data analysis. Run as a web-based 
    app, 'ExPanD' enables users to assess the robustness of empirical evidence 
    without providing them access to the underlying data. You can export a 
    notebook containing the analysis of 'ExPanD' and/or use the functions of the 
    package to support your exploratory data analysis workflow. Refer to the 
    vignettes of the package for more information on how to use 'ExPanD' and/or 
    the functions of this package.",2020-12-06,Joachim Gassen,https://joachim-gassen.github.io/ExPanDaR/,TRUE,https://github.com/joachim-gassen/expandar,28393,95,2021-04-11T09:45:30Z,298.87368421052633
expands,"Expanding Ploidy and Allele Frequency on Nested Subpopulations (expands) characterizes coexisting subpopulations in a single tumor sample using copy number and allele frequencies derived from exome- or whole genome sequencing input data (<https://pubmed.ncbi.nlm.nih.gov/24177718/>). The model detects coexisting genotypes by leveraging run-specific tradeoffs between depth of coverage and breadth of coverage. This package predicts the number of clonal expansions, the size of the resulting subpopulations in the tumor bulk, the mutations specific to each subpopulation, tumor purity and phylogeny. The main function runExPANdS() provides the complete functionality needed to predict coexisting subpopulations from single nucleotide variations (SNVs) and associated copy numbers.  The robustness of subpopulation predictions increases with the number of mutations provided. It is recommended that at least 200 mutations are used as input to obtain stable results. Updates in version 2.1 include: (i) new parameter ploidy in runExPANdS.R allows specification of non-diploid background ploidies (e.g. for near-triploid cell lines); (ii) parallel computing option is available.",2021-09-03,Noemi Andor,"https://github.com/noemiandor/expands,
https://groups.google.com/d/forum/expands",TRUE,https://github.com/noemiandor/expands,18550,2,2021-08-23T12:10:16Z,9275
experDesign,"Distributes samples in batches while making batches homogeneous 
    according to their description. Allows for an arbitrary number of variables, 
    both numeric and categorical. For quality control it provides functions to 
    subset a representative sample.",2021-04-22,Lluís Revilla Sancho,"https://experdesign.llrs.dev, https://github.com/llrs/experDesign/",TRUE,https://github.com/llrs/experdesign,4805,5,2021-04-22T14:13:43Z,961
explor,Shiny interfaces and graphical functions for multivariate analysis results exploration.,2021-06-01,Julien Barnier,https://juba.github.io/explor/,TRUE,https://github.com/juba/explor,44923,164,2021-06-11T17:15:31Z,273.9207317073171
exploratory,"Conduct numerous exploratory analyses in an instant with a 
    point-and-click interface. With one simple command, this tool 
    launches a Shiny App on the local machine. Drag and drop variables 
    in a data set to categorize them as possible independent, 
    dependent, moderating, or mediating variables. Then run dozens 
    (or hundreds) of analyses instantly to uncover any statistically 
    significant relationships among variables. Any relationship 
    thus uncovered should be tested in follow-up studies. 
    This tool is designed only to facilitate exploratory 
    analyses and should NEVER be used for p-hacking. Many of 
    the functions used in this package are direct copies of functions
    in the R Package 'kim' and 'ezr'.
    Selected References:
    Chang et al. (2021) <https://CRAN.R-project.org/package=shiny>.
    Chang et al. (2018) <https://CRAN.R-project.org/package=shinydashboard>.
    Cohen (1988) <doi:10.4324/9780203771587>.
    Dowle et al. (2021) <https://CRAN.R-project.org/package=data.table>.
    Ioannidis (2005) <doi:10.1371/journal.pmed.0020124>
    Kim (2021) <doi:10.5281/zenodo.4619237>.
    Kim (2020) <https://CRAN.R-project.org/package=ezr>.
    Simmons et al. (2011) <doi:10.1177/0956797611417632>
    Tingley et al. (2019) <https://CRAN.R-project.org/package=mediation>.
    Wickham et al. (2020) <https://CRAN.R-project.org/package=ggplot2>.",2021-04-22,Jin Kim,https://exploratoryonly.com,TRUE,https://github.com/jinkim3/exploratory,1907,1,2021-04-23T09:55:46Z,1907
explore,"Interactive data exploration with one line of code or use an easy 
    to remember set of tidy functions for exploratory data analysis. 
    Introduces three main verbs. explore() to graphically explore a variable or 
    table, describe() to describe a variable or table and report() to create an 
    automated report.",2021-06-04,Roland Krasser,https://github.com/rolkra/explore/,TRUE,https://github.com/rolkra/explore,29414,77,2021-06-03T21:38:17Z,382
ExPosition,"A variety of descriptive multivariate analyses with the singular value decomposition,
    such as principal components analysis, correspondence analysis, and multidimensional scaling.
    See An ExPosition of the Singular Value Decomposition in R (Beaton et al 2014) <doi:10.1016/j.csda.2013.11.006>.",2019-01-07,Derek Beaton,NA,TRUE,https://github.com/derekbeaton/exposition-family_old,86377,3,2020-11-20T04:12:09Z,28792.333333333332
expss,"Package computes and displays tables with support for 'SPSS'-style 
        labels, multiple and nested banners, weights, multiple-response variables 
        and significance testing. There are facilities for nice output of tables 
        in 'knitr', 'Shiny', '*.xlsx' files, R and 'Jupyter' notebooks. Methods 
        for labelled variables add value labels support to base R functions and to 
        some functions from other packages. Additionally, the package brings 
        popular data transformation functions from 'SPSS' Statistics and 'Excel': 
        'RECODE', 'COUNT', 'COMPUTE', 'DO IF', 'COUNTIF', 'VLOOKUP' and etc. 
        These functions are very useful for data processing in marketing research 
        surveys. Package intended to help people to move data 
        processing from 'Excel' and 'SPSS' to R.",2020-11-15,Gregory Demin,https://gdemin.github.io/expss/,TRUE,https://github.com/gdemin/expss,228396,69,2020-11-15T20:57:42Z,3310.086956521739
extraDistr,"Density, distribution function, quantile function
    and random generation for a number of univariate
    and multivariate distributions. This package implements the
    following distributions: Bernoulli, beta-binomial, beta-negative
    binomial, beta prime, Bhattacharjee, Birnbaum-Saunders,
    bivariate normal, bivariate Poisson, categorical, Dirichlet,
    Dirichlet-multinomial, discrete gamma, discrete Laplace,
    discrete normal, discrete uniform, discrete Weibull, Frechet,
    gamma-Poisson, generalized extreme value, Gompertz,
    generalized Pareto, Gumbel, half-Cauchy, half-normal, half-t,
    Huber density, inverse chi-squared, inverse-gamma, Kumaraswamy,
    Laplace, location-scale t, logarithmic, Lomax, multivariate
    hypergeometric, multinomial, negative hypergeometric, 
    non-standard beta, normal mixture, Poisson mixture, Pareto,
    power, reparametrized beta, Rayleigh, shifted Gompertz, Skellam,
    slash, triangular, truncated binomial, truncated normal,
    truncated Poisson, Tukey lambda, Wald, zero-inflated binomial,
    zero-inflated negative binomial, zero-inflated Poisson.",2020-09-07,Tymoteusz Wolodzko,https://github.com/twolodzko/extraDistr,TRUE,https://github.com/twolodzko/extradistr,662338,25,2020-09-07T12:58:56Z,26493.52
extrafont,"Tools to using fonts other than the standard PostScript fonts.
    This package makes it easy to use system TrueType fonts and with PDF or
    PostScript output files, and with bitmap output files in Windows. extrafont
    can also be used with fonts packaged specifically to be used with, such as
    the fontcm package, which has Computer Modern PostScript fonts with math
    symbols. See https://github.com/wch/extrafont for instructions and
    examples.",2014-12-08,Winston Chang,https://github.com/wch/extrafont,TRUE,https://github.com/wch/extrafont,1287035,256,2021-03-26T17:20:13Z,5027.48046875
extras,"Functions to 'numericise' 'R' objects (coerce to numeric
    objects), summarise 'MCMC' (Monte Carlo Markov Chain) samples and
    calculate deviance residuals as well as 'R' translations of 'BUGS'
    (Bayesian Using Gibbs Sampling) and 'JAGS' (Just Another Gibbs
    Sampler) functions.",2021-08-05,Joe Thorley,"https://poissonconsulting.github.io/extras/,
https://github.com/poissonconsulting/extras",TRUE,https://github.com/poissonconsulting/extras,16935,3,2021-08-30T02:17:08Z,5645
extremeStat,"Code to fit, plot and compare several (extreme value)
    distribution functions. Can also compute (truncated) distribution quantile estimates and
    draw a plot with return periods on a linear scale.",2017-11-05,Berry Boessenkool,https://github.com/brry/extremeStat,TRUE,https://github.com/brry/extremestat,20490,7,2020-09-07T17:32:47Z,2927.1428571428573
exuber,"Testing for and dating periods of explosive
    dynamics (exuberance) in time series using the univariate and panel
    recursive unit root tests proposed by Phillips et al. (2015)
    <doi:10.1111/iere.12132> and Pavlidis et al. (2016)
    <doi:10.1007/s11146-015-9531-2>.  The recursive least-squares
    algorithm utilizes the matrix inversion lemma to avoid matrix
    inversion which results in significant speed improvements. Simulation
    of a variety of periodically-collapsing bubble processes.",2020-12-18,Kostas Vasilopoulos,https://github.com/kvasilopoulos/exuber,TRUE,https://github.com/kvasilopoulos/exuber,18695,13,2021-07-16T14:52:59Z,1438.076923076923
eye,"There is no ophthalmic researcher who has not had headaches from 
    the handling of visual acuity entries. Different notations, untidy entries. 
    This shall now be a matter of the past. Eye makes it as easy as pie to work
    with VA data - easy cleaning, easy conversion between 
    Snellen, logMAR, ETDRS letters, and qualitative visual acuity 
    shall never pester you again. The eye 
    package automates the pesky task to count number of patients and eyes, 
    and can help to clean data with easy re-coding for right and left eyes. 
    It also contains functions to help reshaping eye side specific variables 
    between wide and long format. Visual acuity conversion is based on 
    Schulze-Bonsel et al. (2006) <doi:10.1167/iovs.05-0981>, 
    Gregori et al. (2010) <doi:10.1097/iae.0b013e3181d87e04>, 
    Beck et al. (2003) <doi:10.1016/s0002-9394(02)01825-1> and 
    Bach (2007) <http:michaelbach.de/sci/acuity.html>.",2021-03-22,Tjebo Heeren,https://github.com/tjebo/eye,TRUE,https://github.com/tjebo/eye,4040,3,2021-08-25T16:57:48Z,1346.6666666666667
eyedata,"Open source data allows for reproducible research and helps advance
    our knowledge. The purpose of this package is to collate open source 
    ophthalmic data sets curated for direct use. This is real life data of 
    people with intravitreal injections with anti-vascular endothelial growth
    factor (anti-VEGF), due to age-related macular degeneration or diabetic 
    macular edema. Associated publications of the data sets: 
    Fu et al. (2020) <doi:10.1001/jamaophthalmol.2020.5044>, 
    Moraes et al (2020) <doi:10.1016/j.ophtha.2020.09.025>,
    Fasler et al. (2019) <doi:10.1136/bmjopen-2018-027441>, 
    Arpa et al. (2020) <doi:10.1136/bjophthalmol-2020-317161>,
    Kern et al. 2020, <doi:10.1038/s41433-020-1048-0>.",2020-12-09,Tjebo Heeren,https://github.com/tjebo/eyedata,TRUE,https://github.com/tjebo/eyedata,2333,3,2020-12-09T12:38:32Z,777.6666666666666
eyelinker,"Imports plain-text ASC data files from EyeLink eye trackers 
    into (relatively) tidy data frames for analysis and visualization.",2021-06-03,Simon Barthelme,https://github.com/a-hurst/eyelinker,TRUE,https://github.com/a-hurst/eyelinker,20049,3,2021-05-17T17:58:29Z,6683
ezcox,"A tool to operate a batch of univariate or multivariate 
    Cox models and return tidy result.",2021-03-16,Shixiang Wang,https://github.com/ShixiangWang/ezcox,TRUE,https://github.com/shixiangwang/ezcox,13445,11,2021-08-08T09:24:05Z,1222.2727272727273
ezEDA,"Enables users to create visualizations using functions 
    based on the data analysis task rather than on plotting mechanics. It hides
    the details of the individual 'ggplot2' function calls and 
    allows the user to focus on the end goal. Useful for quick preliminary explorations. 
    Provides functions for common exploration patterns. Some of the ideas in this
    package are motivated by Fox (2015, ISBN:1938377052).",2021-06-29,Viswa Viswanathan,https://github.com/kviswana/ezEDA,TRUE,https://github.com/kviswana/ezeda,4977,0,2021-06-28T22:36:20Z,NA
ezknitr,"An extension of 'knitr' that adds flexibility in several
    ways. One common source of frustration with 'knitr' is that it assumes
    the directory where the source file lives should be the working directory,
    which is often not true. 'ezknitr' addresses this problem by giving you
    complete control over where all the inputs and outputs are, and adds several
    other convenient features to make rendering markdown/HTML documents easier.",2016-09-16,Dean Attali,https://github.com/ropenscilabs/ezknitr,TRUE,https://github.com/ropenscilabs/ezknitr,23042,94,2021-01-09T02:13:01Z,245.12765957446808
ezpickr,"Choosing any rectangular data file using interactive GUI dialog box, and seamlessly manipulating tidy data between an 'Excel' window and R session.",2020-11-16,JooYoung Seo,https://github.com/jooyoungseo/ezpickr,TRUE,https://github.com/jooyoungseo/ezpickr,22473,4,2020-12-27T18:33:56Z,5618.25
ezr,"Runs a Shiny App in the local machine for basic
    statistical and graphical analyses. The point-and-click interface 
    of Shiny App enables obtaining the same analysis outputs (e.g., plots and
    tables) more quickly, as compared with typing the required code in R,
    especially for users without much experience or expertise with coding.
    Examples of possible analyses include tabulating descriptive
    statistics for a variable, creating histograms by experimental groups,
    and creating a scatter plot and calculating the correlation between
    two variables.",2020-11-25,Jin Kim,https://github.com/jinkim3/ezr,TRUE,https://github.com/jinkim3/ezr,3529,0,2021-04-15T21:43:46Z,NA
fabisearch,"Implementation of the Factorized Binary Search (FaBiSearch) methodology for the estimation of the number and location of multiple change points in the network (or clustering) structure of multivariate high-dimensional time series. The method is motivated by the detection of change points in functional connectivity networks for functional magnetic resonance imaging (fMRI) data. FaBiSearch uses non-negative matrix factorization (NMF), an unsupervised dimension reduction technique, and a new binary search algorithm to identify multiple change points.  It also requires minimal assumptions. The  main routines of the package are detect.cps(), for multiple change point detection, est.net(), for estimating a network between stationary multivariate time series, net.3dplot(), for plotting the estimated functional connectivity networks, and opt.rank(), for finding the optimal rank in NMF for a given data set. The functions have been extensively tested on simulated multivariate high-dimensional time series data and fMRI data. For details on the FaBiSearch methodology, please see Ondrus et al. (2021).",2021-02-24,Martin Ondrus,https://github.com/mondrus96/FaBiSearch,TRUE,https://github.com/mondrus96/fabisearch,1751,0,2021-07-30T23:46:09Z,NA
fable,"Provides a collection of commonly used univariate and multivariate
    time series forecasting models including automatically selected exponential 
    smoothing (ETS) and autoregressive integrated moving average (ARIMA) models.
    These models work within the 'fable' framework provided by the 'fabletools'
    package, which provides the tools to evaluate, visualise, and combine models 
    in a workflow consistent with the tidyverse.",2021-05-16,Mitchell OHara-Wild,"https://fable.tidyverts.org, https://github.com/tidyverts/fable",TRUE,https://github.com/tidyverts/fable,181636,428,2021-08-26T03:13:20Z,424.38317757009344
fabletools,"Provides tools, helpers and data structures for
    developing models and time series functions for 'fable' and extension
    packages. These tools support a consistent and tidy interface for time
    series modelling and analysis.",2021-03-16,Mitchell OHara-Wild,"https://fabletools.tidyverts.org/,
https://github.com/tidyverts/fabletools",TRUE,https://github.com/tidyverts/fabletools,213559,72,2021-08-11T06:36:03Z,2966.097222222222
fabricatr,"Helps you imagine your data before you collect it. Hierarchical data structures
   and correlated data can be easily simulated, either from random number generators or
   by resampling from existing data sources. This package is faster with 'data.table' and
   'mvnfast' installed.",2021-02-09,Graeme Blair,"https://declaredesign.org/r/fabricatr,
https://github.com/DeclareDesign/fabricatr",TRUE,https://github.com/declaredesign/fabricatr,42483,78,2021-06-06T00:22:54Z,544.6538461538462
fabricerin,"Allows the user to implement easily canvas elements within a 'shiny' app or an 'RMarkdown' document. 
    The user can create shapes, images and text elements within the canvas which can also be used as a drawing tool for taking notes.
    The package relies on the 'fabricjs' 'JavaScript' library. See <http://fabricjs.com/>.",2020-08-14,Mohamed El Fodil Ihaddaden,https://github.com/feddelegrand7/fabricerin,TRUE,https://github.com/feddelegrand7/fabricerin,5435,46,2021-02-01T17:55:42Z,118.15217391304348
factiv,Implements instrumental variable estimators for 2^K factorial experiments with noncompliance.,2021-05-21,Matthew Blackwell,https://github.com/mattblackwell/factiv,TRUE,https://github.com/mattblackwell/factiv,1194,1,2021-05-20T01:15:56Z,1194
factset.analyticsapi.engines,"Allow clients to fetch 'analytics' through API for Portfolio 
    'Analytics'('PA'), Style Performance Risk('SPAR') and 'Vault' products of 
    'FactSet'. Visit 
    <https://github.com/factset/analyticsapi-engines-r-sdk/tree/master/Engines>
    for more information on the usage of package. Visit 
    <https://developer.factset.com/> for more information on products.",2020-02-02,Akshay Sheth,https://github.com/factset/analyticsapi-engines-r-sdk,TRUE,https://github.com/factset/analyticsapi-engines-r-sdk,8707,3,2020-11-18T01:01:29Z,2902.3333333333335
factset.protobuf.stach,"Generates 'RProtobuf' classes for 'FactSet' 'STACH' tabular 
    format which represents complex multi-dimensional array of data. These 
    classes help in the 'serialization' and 'deserialization' of 'STACH' 
    formatted data. See 'GitHub' repository documentation for more 
    information.",2020-01-14,analytics-reporting,https://github.com/factset/stachschema,TRUE,https://github.com/factset/stachschema,9361,4,2021-02-16T14:37:17Z,2340.25
fad,"Compute maximum likelihood estimators of parameters in a Gaussian factor model using
  the the matrix-free methodology described in Dai et al. (2020) <doi:10.1080/10618600.2019.1704296>.
  In contrast to the factanal() function from 'stats' package, fad() can handle high-dimensional datasets where
  number of variables exceed the sample size and is also substantially faster than the EM algorithms.",2021-01-10,Somak Dutta,https://github.com/somakd/fad,TRUE,https://github.com/somakd/fad,10077,3,2021-09-01T02:31:58Z,3359
fairadapt,"An implementation of the fair data adaptation with quantile
    preservation described in Plecko & Meinshausen (2019) <arXiv:1911.06685>.
    The adaptation procedure uses the specified causal graph to pre-process the
    given training and testing data in such a way to remove the bias caused by
    the protected attribute. The procedure uses tree ensembles for quantile
    regression.",2021-07-28,Drago Plecko,https://github.com/dplecko/fairadapt,TRUE,https://github.com/dplecko/fairadapt,9248,0,2021-08-20T13:01:42Z,NA
fairmodels,"Measure fairness metrics in one place for many models. Check how big is model's bias towards different races, sex, nationalities etc. Use measures such as Statistical Parity, Equal odds to detect the discrimination against unprivileged groups. Visualize the bias using heatmap, radar plot, biplot, bar chart (and more!). There are various pre-processing and post-processing bias mitigation algorithms implemented. Package also supports calculating fairness metrics for regression models. Find more details in (Wiśniewski, Biecek (2021)) <arXiv:2104.00507>.  ",2021-05-31,Jakub Wiśniewski,https://fairmodels.drwhy.ai/,TRUE,https://github.com/modeloriented/fairmodels,7051,54,2021-05-31T14:39:49Z,130.57407407407408
fairness,"Offers calculation, visualization and comparison of algorithmic fairness metrics. Fair machine learning is an emerging topic with the overarching aim to critically assess whether ML algorithms reinforce existing social biases. Unfair algorithms can propagate such biases and produce predictions with a disparate impact on various sensitive groups of individuals (defined by sex, gender, ethnicity, religion, income, socioeconomic status, physical or mental disabilities). Fair algorithms possess the underlying foundation that these groups should be treated similarly or have similar prediction outcomes. The fairness R package offers the calculation and comparisons of commonly and less commonly used fairness metrics in population subgroups. These methods are described by Calders and Verwer (2010) <doi:10.1007/s10618-010-0190-x>, Chouldechova (2017) <doi:10.1089/big.2016.0047>, Feldman et al. (2015) <doi:10.1145/2783258.2783311> , Friedler et al. (2018) <doi:10.1145/3287560.3287589> and Zafar et al. (2017) <doi:10.1145/3038912.3052660>. The package also offers convenient visualizations to help understand fairness metrics.",2021-04-14,Nikita Kozodoi,https://kozodoi.me/r/fairness/packages/2020/05/01/fairness-tutorial.html,TRUE,https://github.com/kozodoi/fairness,13646,23,2021-07-29T10:52:21Z,593.304347826087
fanplot,"Visualise sequential distributions using a range of plotting
    styles. Sequential distribution data can be input as either simulations or
    values corresponding to percentiles over time. Plots are added to
    existing graphic devices using the fan function. Users can choose from four
    different styles, including fan chart type plots, where a set of coloured
    polygon, with shadings corresponding to the percentile values are layered
    to represent different uncertainty levels. Full details in R Journal article; Abel (2015) <doi:10.32614/RJ-2015-002>.",2021-08-02,Guy J. Abel,http://guyabel.github.io/fanplot/,TRUE,https://github.com/guyabel/fanplot,47817,2,2021-08-25T13:20:45Z,23908.5
fansi,"Counterparts to R string manipulation functions that account for
   the effects of ANSI text formatting control sequences.",2021-05-25,Brodie Gaslam,https://github.com/brodieG/fansi,TRUE,https://github.com/brodieg/fansi,25047664,47,2021-05-25T13:26:27Z,532929.0212765958
faoutlier,"Tools for detecting and summarize influential cases that
    can affect exploratory and confirmatory factor analysis models as well as
    structural equation models more generally (Chalmers, 2015, <doi:10.1177/0146621615597894>; 
    Flora, D. B., LaBrish, C. & Chalmers, R. P., 2012, <doi:10.3389/fpsyg.2012.00055>).",2021-01-10,Phil Chalmers,https://github.com/philchalmers/faoutlier,TRUE,https://github.com/philchalmers/faoutlier,26225,6,2021-06-28T12:08:53Z,4370.833333333333
farff,"Reads and writes 'ARFF' files. 'ARFF' (Attribute-Relation
    File Format) files are like 'CSV' files, with a little bit of added
    meta information in a header and standardized NA values. They are
    quite often used for machine learning data sets and were introduced
    for the 'WEKA' machine learning 'Java' toolbox. See
    <https://waikato.github.io/weka-wiki/formats_and_processing/arff_stable/>
    for further info on 'ARFF' and for
    <http://www.cs.waikato.ac.nz/ml/weka/> for more info on 'WEKA'.
    'farff' gets rid of the 'Java' dependency that 'RWeka' enforces, and
    it is at least a faster reader (for bigger files). It uses 'readr' as
    parser back-end for the data section of the 'ARFF' file. Consistency
    with 'RWeka' is tested on 'Github' and 'Travis CI' with hundreds of
    'ARFF' files from 'OpenML'.",2021-05-10,Marc Becker,https://github.com/mlr-org/farff,TRUE,https://github.com/mlr-org/farff,24982,7,2021-05-10T20:43:59Z,3568.8571428571427
FarmTest,"Performs robust multiple testing for means in the presence of known and unknown latent factors presented in Fan et al.(2019) ""FarmTest: Factor-Adjusted Robust Multiple Testing With Approximate False Discovery Control"" <doi:10.1080/01621459.2018.1527700>.
             Implements a series of adaptive Huber methods combined with fast data-drive tuning schemes proposed in Ke et al.(2019) ""User-Friendly Covariance Estimation for Heavy-Tailed Distributions"" <doi:10.1214/19-STS711> to estimate model parameters and construct test statistics that are robust against heavy-tailed and/or asymmetric error distributions. 
             Extensions to two-sample simultaneous mean comparison problems are also included. 
             As by-products, this package contains functions that compute adaptive Huber mean, covariance and regression estimators that are of independent interest.",2020-09-07,Xiaoou Pan,https://github.com/XiaoouPan/FarmTest,TRUE,https://github.com/xiaooupan/farmtest,18693,2,2021-02-28T00:56:11Z,9346.5
farrell,"Allows the user to execute interactively radial data envelopment analysis models. The user has the ability to upload a data frame, 
    select the input/output variables, choose the technology assumption to adopt and decide whether to run an input or an output oriented model. 
    When the model is executed a set of results are displayed which include efficiency scores, peers' determination, scale efficiencies' evaluation 
    and slacks' calculation. Fore more information about the theoretical background of the package, 
    please refer to Bogetoft & Otto (2011) <doi:10.1007/978-1-4419-7961-2>.",2020-09-26,Mohamed El Fodil Ihaddaden,https://github.com/feddelegrand7/farrell,TRUE,https://github.com/feddelegrand7/farrell,6001,6,2020-12-09T21:53:29Z,1000.1666666666666
farver,"The encoding of colour can be handled in many different ways, using
    different colour spaces. As different colour spaces have different uses,
    efficient conversion between these representations are important. The 
    'farver' package provides a set of functions that gives access to very fast
    colour space conversion and comparisons implemented in C++, and offers 
    speed improvements over the 'convertColor' function in the 'grDevices' 
    package.",2021-02-28,Thomas Lin Pedersen,"https://farver.data-imaginist.com,
https://github.com/thomasp85/farver",TRUE,https://github.com/thomasp85/farver,12674751,74,2021-02-28T19:24:07Z,171280.4189189189
fasano.franceschini.test,"An implementation of the 2-D Kolmogorov-Smirnov (KS) two-sample test as defined by Fasano and Franceschini (Fasano and Franceschini 1987). The 'fasano.franceschini.test' package provides three improvements over the current 2-D KS test on the Comprehensive R Archive Network (CRAN): (i) the Fasano and Franceschini test has been shown to run in O(n^2) versus the Peacock implementation which runs in O(n^3); (ii) the package implements a procedure for handling ties in the data; and (iii) the package implements a parallelized permutation procedure for improved significance testing. Ultimately, the 'fasano.franceschini.test' package presents a robust statistical test for analyzing random samples defined in 2-dimensions.",2021-09-02,Elan Ness-Cohn,https://github.com/nesscoder/fasano.franceschini.test,TRUE,https://github.com/nesscoder/fasano.franceschini.test,1049,1,2021-09-02T23:30:50Z,1049
fasstr,"The Flow Analysis Summary Statistics Tool for R, 'fasstr', provides various 
    functions to tidy and screen daily stream discharge data; calculate and visualize various summary statistics
    and metrics; and compute annual trending (using 'zyp' package methods <https://CRAN.R-project.org/package=zyp>)
    and volume frequency analyses (using methods similar to HEC-SSP (2019) 
    <https://www.hec.usace.army.mil/software/hec-ssp/>). It features useful function arguments for filtering of and
    handling dates, customizing data and metrics, and the ability to pull daily data directly from the Water Survey
    of Canada hydrometric database (<https://collaboration.cmc.ec.gc.ca/cmc/hydrometrics/www/>).",2020-11-07,Jon Goetz,"https://bcgov.github.io/fasstr/, https://github.com/bcgov/fasstr,
https://www2.gov.bc.ca/gov/content/environment/air-land-water/water",TRUE,https://github.com/bcgov/fasstr,12115,39,2021-05-19T21:33:08Z,310.64102564102564
fastai,"The 'fastai' <https://docs.fast.ai/index.html> library 
             simplifies training fast and accurate neural networks 
             using modern best practices. It is based on research 
             in to deep learning best practices undertaken 
             at 'fast.ai', including 'out of the box' support
             for vision, text, tabular, audio, time series, and 
             collaborative filtering models. ",2021-07-28,Turgut Abdullayev [ctb,https://github.com/EagerAI/fastai,TRUE,https://github.com/eagerai/fastai,7973,99,2021-07-26T11:43:57Z,80.53535353535354
fastDummies,"Creates dummy columns from columns that have categorical variables (character or factor types). You can also specify which columns to make dummies out of, or which columns to ignore. Also creates dummy rows from character, factor, and Date columns. This package provides a significant speed increase from creating dummy variables through model.matrix().",2020-11-29,Jacob Kaplan,"https://github.com/jacobkap/fastDummies,
https://jacobkap.github.io/fastDummies/",TRUE,https://github.com/jacobkap/fastdummies,448484,30,2020-11-28T23:54:22Z,14949.466666666667
fastLink,"Implements a Fellegi-Sunter probabilistic record linkage model that allows for missing data
    and the inclusion of auxiliary information. This includes functionalities to conduct a merge of two 
    datasets under the Fellegi-Sunter model using the Expectation-Maximization algorithm. In addition, 
    tools for preparing, adjusting, and summarizing data merges are included. The package implements methods 
    described in Enamorado, Fifield, and Imai (2019) ''Using a Probabilistic Model to Assist Merging of 
    Large-scale Administrative Records'', American Political Science Review and is available 
    at <http://imai.fas.harvard.edu/research/linkage.html>.",2020-04-29,Ted Enamorado,NA,TRUE,https://github.com/kosukeimai/fastlink,23572,186,2020-11-20T03:25:00Z,126.73118279569893
fastmap,"Fast implementation of data structures, including a key-value
    store, stack, and queue. Environments are commonly used as key-value stores
    in R, but every time a new key is used, it is added to R's global symbol
    table, causing a small amount of memory leakage. This can be problematic in
    cases where many different keys are used. Fastmap avoids this memory leak
    issue by implementing the map using data structures in C++.",2021-01-25,Winston Chang,"https://r-lib.github.io/fastmap/, https://github.com/r-lib/fastmap",TRUE,https://github.com/r-lib/fastmap,7636139,97,2021-05-14T22:14:17Z,78723.0824742268
fastpos,"Finds the critical sample size (""critical point of stability"") for a 
    correlation to stabilize in Schoenbrodt and Perugini's definition of 
    sequential stability (see <doi:10.1016/j.jrp.2013.05.009>).",2020-09-29,Johannes Titz,https://github.com/johannes-titz/fastpos,TRUE,https://github.com/johannes-titz/fastpos,11385,0,2020-10-01T12:01:23Z,NA
fastqq,"New and faster implementations for quantile quantile plots. 
    The package also includes a function to prune data for quantile quantile 
    plots. This can drastically reduce the running time for large samples, 
    for 100 million samples, you can expect a factor 80X speedup.",2021-08-19,Gudmundur Einarsson,https://github.com/gumeo/fastqq,TRUE,https://github.com/gumeo/fastqq,153,0,2021-08-21T07:54:34Z,NA
fastRG,"Samples generalized random product graph, a
    generalization of a broad class of network models. Given matrices X,
    S, and Y with with non-negative entries, samples a matrix with
    expectation X S Y^T and independent Poisson or Bernoulli entries. The
    algorithm first samples the number of edges and then puts them down
    one-by-one.  As a result it is O(m) where m is the number of edges, a
    dramatic improvement over element-wise algorithms that which require
    O(n^2) operations to sample a random graph, where n is the number of
    nodes.",2021-02-26,Alex Hayes,https://github.com/RoheLab/fastRG,TRUE,https://github.com/rohelab/fastrg,2092,0,2021-03-18T22:22:54Z,NA
fastrmodels,"A data package that hosts all models for the
    'nflfastR' package.",2021-02-20,Sebastian Carl,https://github.com/mrcaseb/fastrmodels,TRUE,https://github.com/mrcaseb/fastrmodels,6706,1,2021-02-19T14:21:20Z,6706
fastshap,"Computes fast (relative to other implementations) approximate 
    Shapley values for any supervised learning model. Shapley values help to 
    explain the predictions from any black box model using ideas from game 
    theory; see Strumbel and Kononenko (2014) <doi:10.1007/s10115-013-0679-x> 
    for details.",2020-02-02,Brandon Greenwell,https://github.com/bgreenwell/fastshap,TRUE,https://github.com/bgreenwell/fastshap,36495,64,2021-03-03T02:13:05Z,570.234375
fastverse,"Easy installation, loading and management, of a complementary set of 
             high-performance packages for statistical computing and data manipulation. 
             The core 'fastverse' consists of 6 packages: 'data.table', 'collapse', 
             'matrixStats', 'kit', 'magrittr' and 'fst', that jointly only depend on 
             'Rcpp'. These packages are attached and harmonized through the 'fastverse'. 
             In addition, the 'fastverse' can be freely and permanently extended with 
             additional packages, both globally or for individual projects. Entirely 
             separate package verses can also be created. Selected fast and low-dependency 
             packages are suggested for various topics such as time series, dates and times, 
             strings, spatial data, statistics and data serialization (see GitHub / website).  ",2021-09-02,Sebastian Krantz,"https://sebkrantz.github.io/fastverse/,
https://github.com/SebKrantz/fastverse",TRUE,https://github.com/sebkrantz/fastverse,405,60,2021-09-01T09:48:11Z,6.75
faux,Create datasets with factorial structure through simulation by specifying variable parameters. Extended documentation at <https://debruine.github.io/faux/>. Described in DeBruine (2020) <doi:10.5281/zenodo.2669586>.,2021-03-27,Lisa DeBruine,https://github.com/debruine/faux,TRUE,https://github.com/debruine/faux,9974,56,2021-08-30T12:58:53Z,178.10714285714286
fauxnaif,"Provides a replacement for dplyr::na_if().  Allows
    you to specify multiple values to be replaced with NA using a single
    function.",2020-09-04,Alexander Rossell Hayes,https://github.com/rossellhayes/fauxnaif,TRUE,https://github.com/rossellhayes/fauxnaif,9678,0,2020-09-06T22:01:36Z,NA
fauxpas,"HTTP error helpers. Methods included for general purpose HTTP 
    error handling, as well as individual methods for every HTTP status
    code, both via status code numbers as well as their descriptive names.
    Supports ability to adjust behavior to stop, message or warning.
    Includes ability to use custom whisker template to have any configuration
    of status code, short description, and verbose message. Currently 
    supports integration with 'crul', 'curl', and 'httr'.",2020-04-13,Scott Chamberlain,"https://docs.ropensci.org/fauxpas,
https://github.com/ropensci/fauxpas",TRUE,https://github.com/ropensci/fauxpas,76673,12,2020-11-02T19:47:30Z,6389.416666666667
faviconPlease,"Finds the URL to the 'favicon' for a website. This is useful if you
  want to display the 'favicon' in an HTML document or web application,
  especially if the website is behind a firewall.",2021-01-14,John Blischak,https://github.com/jdblischak/faviconPlease,TRUE,https://github.com/jdblischak/faviconplease,2803,0,2021-09-01T14:18:36Z,NA
fbRads,"Wrapper functions around the Facebook Marketing 'API' to create, read, update and delete custom audiences, images, campaigns, ad sets, ads and related content.",2016-04-06,Ajaykumar Gopal,https://github.com/cardcorp/fbRads,TRUE,https://github.com/cardcorp/fbrads,22638,133,2021-07-19T11:48:07Z,170.21052631578948
fcaR,"Provides tools to perform fuzzy formal concept
    analysis, presented in Wille (1982) <doi:10.1007/978-3-642-01815-2_23>
    and in Ganter and Obiedkov (2016) <doi:10.1007/978-3-662-49291-8>.  It
    provides functions to load and save a formal context, extract its
    concept lattice and implications.  In addition, one can use the
    implications to compute semantic closures of fuzzy sets and, thus,
    build recommendation systems.",2021-06-28,Domingo Lopez Rodriguez,https://github.com/Malaga-FCA-group/fcaR,TRUE,https://github.com/malaga-fca-group/fcar,12717,0,2021-06-28T19:41:41Z,NA
fcci,"
	Provides support for building Feldman-Cousins confidence intervals 
	[G. J. Feldman and R. D. Cousins (1998) <doi:10.1103/PhysRevD.57.3873>].",2021-04-06,Valerio Gherardi,https://github.com/vgherard/fcci,TRUE,https://github.com/vgherard/fcci,1606,0,2021-07-07T13:56:11Z,NA
FCPS,"Over sixty clustering algorithms are provided in this package with consistent input and output, which enables the user to try out algorithms swiftly. Additionally, 26 statistical approaches for the estimation of the number of clusters as well as the mirrored density plot (MD-plot) of clusterability are implemented. The packages is published in Thrun, M.C., Stier Q.: ""Fundamental Clustering Algorithms Suite"" (2021), SoftwareX, <DOI:10.1016/j.softx.2020.100642>. Moreover, the fundamental clustering problems suite (FCPS) offers a variety of clustering challenges any algorithm should handle when facing real world data, see Thrun, M.C., Ultsch A.: ""Clustering Benchmark Datasets Exploiting the Fundamental Clustering Problems"" (2020), Data in Brief, <DOI:10.1016/j.dib.2020.105501>.",2021-07-07,Michael Thrun,NA,TRUE,https://github.com/mthrun/fcps,13532,6,2021-08-03T11:00:00Z,2255.3333333333335
fctbases,"Easy-to-use, very fast implementation of various functional bases. Easily used together with other packages.
    A functional basis is a collection of basis functions [\phi_1, ..., \phi_n] that can represent a smooth function, i.e. $f(t) = \sum c_k \phi_k(t)$.
    First- and second-order derivatives are also included. These are the mathematically correct ones, no approximations applied.
    As of version 1.0, this package includes B-splines, Fourier bases and polynomials.",2021-02-02,Niels Olsen,https://github.com/naolsen/fctbases,TRUE,https://github.com/naolsen/fctbases,3480,1,2021-09-03T12:31:20Z,3480
FCVAR,"Estimation and inference using the Fractionally Cointegrated 
    Vector Autoregressive (VAR) model. It includes functions for model specification, 
    including lag selection and cointegration rank selection, as well as a comprehensive
    set of options for hypothesis testing, including tests of hypotheses on the 
    cointegrating relations, the adjustment coefficients and the fractional 
    differencing parameters. 
    An article describing the FCVAR model with examples is available on the Webpage 
    <https://sites.google.com/view/mortennielsen/software>.",2021-08-09,Lealand Morin,https://github.com/LeeMorinUCF/FCVAR,TRUE,https://github.com/leemorinucf/fcvar,432,1,2021-08-27T12:22:54Z,432
fda.usc,"Routines for exploratory and descriptive analysis of functional data such as depth measurements, atypical curves detection, regression models, supervised classification, unsupervised classification and functional analysis of variance.",2020-02-17,Manuel Oviedo de la Fuente,"https://github.com/moviedo5/fda.usc,
http://www.jstatsoft.org/v51/i04/",TRUE,https://github.com/moviedo5/fda.usc,105703,0,2021-07-29T09:27:31Z,NA
fdaACF,"Quantify the serial correlation across lags of a given functional 
    time series using the autocorrelation function and a partial autocorrelation
    function for functional time series proposed in 
    Mestre et al. (2021) <doi:10.1016/j.csda.2020.107108>.
    The autocorrelation functions are based on the L2 norm of the lagged covariance 
    operators of the series. Functions are available for estimating the 
    distribution of the autocorrelation functions under the assumption 
    of strong functional white noise.",2020-10-20,Guillermo Mestre Marcos,https://github.com/GMestreM/fdaACF,TRUE,https://github.com/gmestrem/fdaacf,11843,4,2021-02-10T16:22:08Z,2960.75
fdaoutlier,"A collection of functions for outlier detection in functional data analysis. 
  Methods implemented include directional outlyingness by 
  Dai and Genton (2019) <doi:10.1016/j.csda.2018.03.017>,
  MS-plot by Dai and Genton (2018) <doi:10.1080/10618600.2018.1473781>,
  total variation depth and modified shape similarity index by 
  Huang and Sun (2019) <doi:10.1080/00401706.2019.1574241>, and sequential transformations by
  Dai et al. (2020) <doi:10.1016/j.csda.2020.106960 among others. Additional outlier detection
  tools and depths for functional data like functional boxplot, (modified) band depth etc.,
  are also available. ",2021-03-02,Oluwasegun Taiwo Ojo,https://github.com/otsegun/fdaoutlier,TRUE,https://github.com/otsegun/fdaoutlier,3762,0,2021-03-18T10:15:37Z,NA
fdapace,"A versatile package that provides implementation of various
    methods of Functional Data Analysis (FDA) and Empirical Dynamics. The core of this
    package is Functional Principal Component Analysis (FPCA), a key technique for
    functional data analysis, for sparsely or densely sampled random trajectories
    and time courses, via the Principal Analysis by Conditional Estimation
    (PACE) algorithm. This core algorithm yields covariance and mean functions,
    eigenfunctions and principal component (scores), for both functional data and
    derivatives, for both dense (functional) and sparse (longitudinal) sampling designs.
    For sparse designs, it provides fitted continuous trajectories with confidence bands,
    even for subjects with very few longitudinal observations. PACE is a viable and
    flexible alternative to random effects modeling of longitudinal data. There is also a
    Matlab version (PACE) that contains some methods not available on fdapace and vice
    versa. Updates to fdapace were supported by grants from NIH Echo and NSF DMS-1712864 and DMS-2014626. Please cite our package if you use it (You may run the command citation(""fdapace"") to get the citation format and bibtex entry).
    References: Wang, J.L., Chiou, J., Müller, H.G. (2016) <doi:10.1146/annurev-statistics-041715-033624>;
    Chen, K., Zhang, X., Petersen, A., Müller, H.G. (2017) <doi:10.1007/s12561-015-9137-5>.",2021-05-24,Alvaro Gajardo,https://github.com/functionaldata/tPACE,TRUE,https://github.com/functionaldata/tpace,33272,20,2021-05-17T22:22:53Z,1663.6
fdaPOIFD,"Integrated Depths for Partially Observed Functional Data (POFD). Applications to visualization, outlier detection and classification. Software companion for Elías, Antonio, Jiménez, Raúl, Paganoni, Anna M. and Sangalli, Laura M., (2020), ""Integrated Depth for Partially Observed Functional Data"".",2021-08-04,Antonio Elías,https://github.com/aefdz/fdaPOIFD,TRUE,https://github.com/aefdz/fdapoifd,2874,0,2021-08-02T13:47:58Z,NA
FDboost,"Regression models for functional data, i.e., scalar-on-function,
    function-on-scalar and function-on-function regression models, are fitted
    by a component-wise gradient boosting algorithm. 
	For a manual on how to use 'FDboost', see Brockhaus, Ruegamer, Greven (2017) <doi:10.18637/jss.v094.i10>.",2020-09-08,David Ruegamer,https://github.com/boost-R/FDboost,TRUE,https://github.com/boost-r/fdboost,73357,12,2021-05-11T07:39:55Z,6113.083333333333
fddm,"Provides the probability density function (PDF) and cumulative
  distribution function (CDF) of the diffusion decision model
  (DDM; e.g., Ratcliff & McKoon, 2008, <doi:10.1162/neco.2008.12-06-420>)
  with across-trial variability in the drift rate. Because the PDF and CDF of
  the DDM both contain an infinite sum, they needs to be approximated. 'fddm'
  implements all published approximations
  (Navarro & Fuss, 2009, <doi:10.1016/j.jmp.2009.02.003>;
  Gondan, Blurton, & Kesselmeier, 2014, <doi:10.1016/j.jmp.2014.05.002>;
  Blurton, Kesselmeier, & Gondan, 2017, <doi:10.1016/j.jmp.2016.11.003>) plus
  new approximations. All approximations are implemented purely in 'C++'
  providing faster speed than existing packages.",2021-08-06,Kendal B. Foster,https://github.com/rtdists/fddm,TRUE,https://github.com/rtdists/fddm,15460,6,2021-08-13T22:45:04Z,2576.6666666666665
fdistr,"Provides functionality to generate a frequency 
    distribution table from a set of observations and plot the frequency
    distribution using a Pareto chart.",2019-12-02,Donnie Minnick,https://github.com/dtminnick/fdistr,TRUE,https://github.com/dtminnick/fdistr,7145,1,2021-05-14T16:13:05Z,7145
feamiR,"Comprises a pipeline for predicting microRNA/mRNA interactions, as detailed in Williams, Calinescu, Mohorianu (2020) <doi:10.1101/2020.12.23.424130>. Its input consists of [a] a messenger RNA (mRNA) dataset (either in fasta format, focused on 3' UTRs or in gtf format; for the latter, the sequences of the 3’ UTRs are generated using the genomic coordinates), [b] a microRNA dataset (in fasta format, retrieved from miRBase,  <http://www.mirbase.org/>) and [c] an interaction dataset (in csv format, from miRTarBase <http://mirtarbase.cuhk.edu.cn/php/index.php>). To characterise and predict microRNA/mRNA interactions, we use [a] statistical analyses based on Chi-squared and Fisher exact tests and [b] Machine Learning classifiers (decision trees, random forests and support vector machines). To enhance the accuracy of the classifiers we also employ feature selection approaches used in on conjunction with the classifiers. The feature selection approaches include a voting scheme for decision trees, a measure based on Gini index for random forests, forward feature selection and Genetic Algorithms on SVMs. The pipeline also includes a novel approach based on embryonic Genetic Algorithms which combines and optimises the forward feature selection and Genetic Algorithms. All analyses, including the classification and feature selection, are applicable on the microRNA seed features (default), on the full microRNA features and/or flanking features on the mRNA. The sets of features can be combined. ",2021-01-19,Eleanor Williams,https://github.com/Core-Bioinformatics/feamiR,TRUE,https://github.com/core-bioinformatics/feamir,2612,1,2021-04-29T13:04:58Z,2612
feasts,"Provides a collection of features, decomposition methods, 
    statistical summaries and graphics functions for the analysing tidy time
    series data. The package name 'feasts' is an acronym comprising of its key
    features: Feature Extraction And Statistics for Time Series.",2021-06-03,Mitchell OHara-Wild,"http://feasts.tidyverts.org/, https://github.com/tidyverts/feasts/",TRUE,https://github.com/tidyverts/feasts,169824,248,2021-08-26T02:36:08Z,684.7741935483871
featureflag,"Feature flags allow developers to turn features of their
    software on and off in form of configuration. This package provides
    functions for creating feature flags in code. It exposes an interface
    for defining own feature flags which are enabled based on custom criteria.",2021-02-18,Ryszard Szymański,https://github.com/szymanskir/featureflag,TRUE,https://github.com/szymanskir/featureflag,2195,4,2021-03-07T17:42:19Z,548.75
fec16,"Easily analyze relational data from the United States 2016 federal 
    election cycle as reported by the Federal Election Commission.
    This package contains data about candidates, committees, and a
    variety of different financial expenditures. Data is from <https://www.fec.gov/data/browse-data/?tab=bulk-data>. ",2020-11-15,Marium Tapal,https://github.com/baumer-lab/fec16,TRUE,https://github.com/baumer-lab/fec16,7111,3,2020-11-16T14:10:06Z,2370.3333333333335
FedData,"Functions to automate downloading geospatial data available from
    several federated data sources (mainly sources maintained by the US Federal
    government). Currently, the package enables extraction from seven datasets:
    The National Elevation Dataset digital elevation models (1 and 1/3 arc-second;
    USGS); The National Hydrography Dataset (USGS); The Soil Survey Geographic
    (SSURGO) database from the National Cooperative Soil Survey (NCSS), which is
    led by the Natural Resources Conservation Service (NRCS) under the USDA; the
    Global Historical Climatology Network (GHCN), coordinated by National Climatic
    Data Center at NOAA; the Daymet gridded estimates of daily weather parameters 
    for North America, version 3, available from the Oak Ridge National Laboratory's
    Distributed Active Archive Center (DAAC); the International Tree Ring Data Bank; 
    and the National Land Cover Database (NLCD).",2019-04-22,R. Kyle Bocinsky,https://github.com/ropensci/FedData,TRUE,https://github.com/ropensci/feddata,34278,73,2021-07-26T21:31:13Z,469.56164383561645
fedstatAPIr,"An API for automatic data queries to the fedstat <https://www.fedstat.ru>, using a small set of functions with a common interface.",2021-07-19,Denis Krylov,https://github.com/DenchPokepon/fedstatAPIr,TRUE,https://github.com/denchpokepon/fedstatapir,647,1,2021-08-08T17:40:02Z,647
feedeR,Retrieve data from RSS/Atom feeds.,2020-09-28,Andrew Collier,https://github.com/datawookie/feedeR,TRUE,https://github.com/datawookie/feeder,33259,23,2020-09-28T09:44:41Z,1446.0434782608695
feisr,"Provides the function feis() to estimate fixed effects individual 
    slope (FEIS) models. The FEIS model constitutes a more general version of 
    the often-used fixed effects (FE) panel model, as implemented in the 
    package 'plm' by Croissant and Millo (2008) <doi:10.18637/jss.v027.i02>. 
    In FEIS models, data are not only person demeaned like in conventional 
    FE models, but detrended by the predicted individual slope of each 
    person or group. Estimation is performed by applying least squares lm() 
    to the transformed data. For more details on FEIS models see Bruederl and 
    Ludwig (2015, ISBN:1446252442); Frees (2001) <doi:10.2307/3316008>; 
    Polachek and Kim (1994) <doi:10.1016/0304-4076(94)90075-2>; 
	Ruettenauer and Ludwig (2020) <doi:10.1177/0049124120926211>;
    Wooldridge (2010, ISBN:0262294354). To test consistency of conventional FE 
    and random effects estimators against heterogeneous slopes, the package 
    also provides the functions feistest() for an artificial regression test 
    and bsfeistest() for a bootstrapped version of the Hausman test.",2021-02-25,Tobias Ruettenauer,https://github.com/ruettenauer/feisr,TRUE,https://github.com/ruettenauer/feisr,36678,5,2021-05-22T14:27:36Z,7335.6
felp,"
    Provides pseudo-postfix operators and more to enhance displaying documents.
    The `?.` pseudo-postfix operator and the `?` prefix operator displays documents and contents (source or structure) of objects simultaneously to help understanding the objects.
    The `?p` pseudo-postfix operator displays package documents, and is shorter than help(package = foo).",2020-09-14,Atsushi Yasumoto,https://github.com/atusy/felp,TRUE,https://github.com/atusy/felp,11873,11,2020-09-13T23:08:00Z,1079.3636363636363
FER,"R implementations of standard financial engineering codes;
  vanilla option pricing models such as Black-Scholes, Bachelier, CEV, and
  SABR.",2021-03-05,Jaehyuk Choi,https://github.com/PyFE/FE-R,TRUE,https://github.com/pyfe/fe-r,2374,6,2021-03-07T12:44:46Z,395.6666666666667
ferrn,"Diagnostic plots for optimisation, with a focus on projection pursuit. These show paths the optimiser 
    takes in the high-dimensional space in multiple ways: by reducing the dimension using principal component analysis, and
    also using the tour to show the path on the high-dimensional space. Several botanical colour palettes are included, reflecting the 
    name of the package. ",2021-03-17,H. Sherry Zhang,https://github.com/huizezhang-sherry/ferrn/,TRUE,https://github.com/huizezhang-sherry/ferrn,1898,3,2021-09-03T02:39:20Z,632.6666666666666
ff,"The ff package provides data structures that are stored on
	disk but behave (almost) as if they were in RAM by transparently 
	mapping only a section (pagesize) in main memory - the effective 
	virtual memory consumption per ff object. ff supports R's standard 
	atomic data types 'double', 'logical', 'raw' and 'integer' and 
	non-standard atomic types boolean (1 bit), quad (2 bit unsigned), 
	nibble (4 bit unsigned), byte (1 byte signed with NAs), ubyte (1 byte 
	unsigned), short (2 byte signed with NAs), ushort (2 byte unsigned), 
	single (4 byte float with NAs). For example 'quad' allows efficient 
	storage of genomic data as an 'A','T','G','C' factor. The unsigned 
	types support 'circular' arithmetic. There is also support for 
	close-to-atomic types 'factor', 'ordered', 'POSIXct', 'Date' and 
	custom close-to-atomic types. 
	ff not only has native C-support for vectors, matrices and arrays 
	with flexible dimorder (major column-order, major row-order and 
	generalizations for arrays). There is also a ffdf class not unlike 
	data.frames and import/export filters for csv files.
	ff objects store raw data in binary flat files in native encoding,
	and complement this with metadata stored in R as physical and virtual
	attributes. ff objects have well-defined hybrid copying semantics, 
	which gives rise to certain performance improvements through 
	virtualization. ff objects can be stored and reopened across R 
	sessions. ff files can be shared by multiple ff R objects 
	(using different data en/de-coding schemes) in the same process 
	or from multiple R processes to exploit parallelism. A wide choice of 
	finalizer options allows to work with 'permanent' files as well as 
	creating/removing 'temporary' ff files completely transparent to the 
	user. On certain OS/Filesystem combinations, creating the ff files
	works without notable delay thanks to using sparse file allocation.
	Several access optimization techniques such as Hybrid Index 
	Preprocessing and Virtualization are implemented to achieve good 
	performance even with large datasets, for example virtual matrix 
	transpose without touching a single byte on disk. Further, to reduce 
	disk I/O, 'logicals' and non-standard data types get stored native and 
	compact on binary flat files i.e. logicals take up exactly 2 bits to 
	represent TRUE, FALSE and NA. 
	Beyond basic access functions, the ff package also provides 
	compatibility functions that facilitate writing code for ff and ram 
	objects and support for batch processing on ff objects (e.g. as.ram, 
	as.ff, ffapply). ff interfaces closely with functionality from package 
	'bit': chunked looping, fast bit operations and coercions between 
	different objects that can store subscript information ('bit', 
	'bitwhich', ff 'boolean', ri range index, hi hybrid index). This allows
	to work interactively with selections of large datasets and quickly 
	modify selection criteria. 
	Further high-performance enhancements can be made available upon request. ",2020-10-13,Jens Oehlschlägel,https://github.com/truecluster/ff,TRUE,https://github.com/truecluster/ff,970231,6,2020-10-13T17:10:41Z,161705.16666666666
ffbase,"Extends the out of memory vectors of 'ff' with
    statistical functions and other utilities to ease their usage.",2021-02-27,Edwin de Jonge,https://github.com/edwindj/ffbase,TRUE,https://github.com/edwindj/ffbase,292576,32,2021-02-27T13:08:23Z,9143
ffp,"Implements numerical entropy-pooling for scenario analysis as
    described in Meucci, Attilio (2010) <doi:10.2139/ssrn.1696802>.",2021-07-14,Bernardo Reckziegel,https://github.com/Reckziegel/FFP,TRUE,https://github.com/reckziegel/ffp,763,0,2021-07-14T14:41:53Z,NA
ffscrapr,"Helps access various Fantasy Football APIs by handling
    authentication and rate-limiting, forming appropriate calls, and
    returning tidy dataframes which can be easily connected to other data
    sources.",2021-09-03,Tan Ho,"https://ffscrapr.ffverse.com, https://github.com/ffverse/ffscrapr,
https://api.myfantasyleague.com/2020/api_info,
https://docs.sleeper.app,
https://www.fleaflicker.com/api-docs/index.html,
https://www.espn.com/fantasy/,
https://www.nflfastr.com/reference/load_player_stats.html",TRUE,https://github.com/ffverse/ffscrapr,7483,38,2021-09-03T01:42:12Z,196.92105263157896
ffsimulator,"Uses bootstrap resampling to run fantasy football season
    simulations supported by historical rankings and 'nflfastR' data,
    calculating optimal lineups, and returning aggregated results.",2021-07-20,Tan Ho,"https://ffsimulator.ffverse.com,
https://github.com/ffverse/ffsimulator",TRUE,https://github.com/ffverse/ffsimulator,836,6,2021-08-14T16:20:38Z,139.33333333333334
fftwtools,"Provides a wrapper for several 'FFTW' functions. This package provides access to the two-dimensional 'FFT', the multivariate 'FFT', and the one-dimensional real to complex 'FFT' using the 'FFTW3' library. The package includes the functions fftw() and mvfftw() which are designed to mimic the functionality of the R functions fft() and mvfft(). The 'FFT' functions have a parameter that allows them to not return the redundant complex conjugate when the input is real data. ",2021-03-01,Karim Rahim,https://github.com/krahim/fftwtools,TRUE,https://github.com/krahim/fftwtools,238377,7,2021-02-27T22:03:39Z,34053.857142857145
fgdr,"Read and Parse for Fundamental Geo-Spatial Data (FGD) which downloads XML file 
    from providing site (<https://fgd.gsi.go.jp/download/menu.php>). The JPGIS format file 
    provided by FGD so that it can be handled as an R spatial object such as 'sf' and 
    'raster', 'terra' or 'stars'.
    Supports the FGD version 4.1, and accepts fundamental items and digital elevation models.",2020-09-30,Shinya Uryu,https://github.com/uribo/fgdr,TRUE,https://github.com/uribo/fgdr,11890,6,2020-11-26T03:32:14Z,1981.6666666666667
fgeo.analyze,"To help you access, transform, analyze, and
    visualize ForestGEO data, we developed a collection of R packages
    (<https://forestgeo.github.io/fgeo/>). This package, in particular,
    helps you to implement analyses of plot species distributions,
    topography, demography, and biomass. It also includes a torus
    translation test to determine habitat associations of tree species as
    described by Zuleta et al. (2018) <doi:10.1007/s11104-018-3878-0>. To
    learn more about ForestGEO visit <https://forestgeo.si.edu/>.",2020-12-05,Mauro Lepore,https://github.com/forestgeo/fgeo.analyze,TRUE,https://github.com/forestgeo/fgeo.analyze,13817,2,2020-12-05T01:01:14Z,6908.5
fgeo.plot,"To help you access, transform, analyze, and
    visualize ForestGEO data, we developed a collection of R packages
    (<https://forestgeo.github.io/fgeo/>). This package, in particular,
    helps you to plot ForestGEO data. To learn more about ForestGEO visit
    <https://forestgeo.si.edu/>.",2020-12-16,Mauro Lepore,"https://github.com/forestgeo/fgeo.plot,
https://forestgeo.github.io/fgeo.plot/",TRUE,https://github.com/forestgeo/fgeo.plot,13391,3,2020-12-16T11:38:27Z,4463.666666666667
fgeo.tool,"To help you access, transform, analyze, and visualize
    'ForestGEO' data, we developed a collection of R packages
    (<https://forestgeo.github.io/fgeo/>). This package, in particular,
    helps you to easily import, filter, and modify 'ForestGEO' data. To
    learn more about 'ForestGEO' visit <https://forestgeo.si.edu/>.",2021-05-25,Mauro Lepore,"https://forestgeo.github.io/fgeo.tool/,
https://github.com/forestgeo/fgeo.tool",TRUE,https://github.com/forestgeo/fgeo.tool,14325,3,2021-05-25T20:23:42Z,4775
fhircrackr,"Useful tools for conveniently downloading FHIR resources in xml format 
    and converting them to R data frames. The package uses FHIR-search to download bundles 
    from a FHIR server, provides functions to save and read xml-files containing such bundles 
    and allows flattening the bundles to data.frames using XPath expressions.",2021-06-17,Thomas Peschel,NA,TRUE,https://github.com/polar-fhir/fhircrackr,6543,8,2021-08-03T13:42:21Z,817.875
FielDHub,"A shiny design of experiments (DOE) app that aids in the creation of traditional, 
 un-replicated, augmented and partially-replicated designs applied to agriculture, 
 plant breeding, forestry, animal and biological sciences.",2021-05-19,Didier Murillo,https://github.com/DidierMurilloF/FielDHub,TRUE,https://github.com/didiermurillof/fieldhub,1235,14,2021-06-17T16:01:09Z,88.21428571428571
fields,"For curve, surface and function fitting with an emphasis
 on splines, spatial data, geostatistics,  and spatial statistics. The major methods
 include cubic, and thin plate splines, Kriging, and compactly supported
 covariance functions for large data sets. The splines and Kriging methods are
 supported by functions that can determine the smoothing parameter
 (nugget and sill variance) and other covariance function parameters by cross
 validation and also by restricted maximum likelihood. For Kriging
 there is an easy to use function that also estimates the correlation
 scale (range parameter).  A major feature is that any covariance function
 implemented in R and following a simple format can be used for
 spatial prediction. There are also many useful functions for plotting
 and working with spatial data as images. This package also contains
 an implementation of sparse matrix methods for large spatial data
 sets and currently requires the sparse matrix (spam) package. Use
 help(fields) to get started and for an overview.  The fields source
 code is deliberately commented and provides useful explanations of
 numerical details as a companion to the manual pages. The commented
 source code can be viewed by expanding the source code version
 and looking in the R subdirectory. The reference for fields can be generated
 by the citation function in R and has DOI <doi:10.5065/D6W957CT>. Development
 of this package was supported in part by the National Science Foundation  Grant
 1417857,  the National Center for Atmospheric Research, and Colorado School of Mines.
 See the Fields URL
 for a vignette on using this package and some background on spatial statistics.",2021-06-25,Douglas Nychka,https://github.com/NCAR/Fields,TRUE,https://github.com/ncar/fields,1301414,15,2021-08-24T18:31:10Z,86760.93333333333
fiery,"A very flexible framework for building server side logic in R. The 
    framework is unopinionated when it comes to how HTTP requests and WebSocket
    messages are handled and supports all levels of app complexity; from serving
    static content to full-blown dynamic web-apps. Fiery does not hold your hand
    as much as e.g. the shiny package does, but instead sets you free to create
    your web app the way you want.",2020-12-15,Thomas Lin Pedersen,"https://fiery.data-imaginist.com,
https://github.com/thomasp85/fiery",TRUE,https://github.com/thomasp85/fiery,42606,214,2020-12-16T07:26:38Z,199.09345794392524
figpatch,"For including external figures into an assembled 
    {patchwork}. This enables the creation of more complex figures that include 
    images alongside plots. ",2021-07-09,Brady Johnston,https://github.com/BradyAJohnston/figpatch,TRUE,https://github.com/bradyajohnston/figpatch,781,34,2021-07-09T08:53:33Z,22.970588235294116
filesstrings,"This started out as a package for file and string
    manipulation.  Since then, the 'fs' and 'strex' packages emerged,
    offering functionality previously given by this package (but it's done
    better in these new ones).  Those packages have hence almost pushed
    'filesstrings' into extinction.  However, it still has a small number
    of unique, handy file manipulation functions which can be seen in the
    vignette.  One example is a function to remove spaces from all file
    names in a directory.",2021-04-18,Rory Nolan,https://github.com/rorynolan/filesstrings,TRUE,https://github.com/rorynolan/filesstrings,171903,21,2021-04-18T02:10:18Z,8185.857142857143
finalfit,"Generate regression results tables and plots in final 
    format for publication. Explore models and export directly to PDF 
    and 'Word' using 'RMarkdown'. ",2021-06-11,Ewen Harrison,https://github.com/ewenharrison/finalfit,TRUE,https://github.com/ewenharrison/finalfit,78734,241,2021-08-04T13:36:58Z,326.6970954356847
finbif,"A programmatic interface to the 'Finnish Biodiversity Information
    Facility' ('FinBIF') API (<https://api.laji.fi>). 'FinBIF' aggregates
    Finnish biodiversity data from multiple sources in a single open access
    portal for researchers, citizen scientists, industry and government.
    'FinBIF' allows users of biodiversity information to find, access, combine
    and visualise data on Finnish plants, animals and microorganisms. The
    'finbif' package makes the publicly available data in 'FinBIF' easily
    accessible to programmers. Biodiversity information is available on taxonomy
    and taxon occurrence. Occurrence data can be filtered by taxon, time,
    location and other variables. The data accessed are conveniently
    preformatted for subsequent analyses.",2021-09-03,William K. Morris,"https://github.com/luomus/finbif, https://luomus.github.io/finbif/",TRUE,https://github.com/luomus/finbif,11832,2,2021-09-03T11:35:22Z,5916
finch,"Parse and create Darwin Core (<http://rs.tdwg.org/dwc/>) Simple
    and Archives. Functionality includes reading and parsing all the
    files in a Darwin Core Archive, including the datasets and metadata;
    read and parse simple Darwin Core files; and validation of Darwin
    Core Archives.",2020-08-11,Scott Chamberlain,"https://docs.ropensci.org/finch/,
https://github.com/ropensci/finch",TRUE,https://github.com/ropensci/finch,29477,19,2020-11-02T19:55:19Z,1551.421052631579
findInFiles,"Creates a HTML widget which displays the results of searching for a pattern in files in a given folder. The results can be viewed in the 'RStudio' viewer pane, included in a 'R Markdown' document or in a 'Shiny' app.",2021-08-03,Stéphane Laurent,https://github.com/stla/findInFiles,TRUE,https://github.com/stla/findinfiles,4356,5,2021-08-02T11:57:26Z,871.2
findInGit,"Creates a HTML widget which displays the results of searching for a pattern in files in a given 'git' repository, including all its branches. The results can also be returned in a dataframe.",2021-07-28,Stéphane Laurent,https://github.com/stla/findInGit,TRUE,https://github.com/stla/findingit,565,1,2021-07-28T09:56:40Z,565
findpython,Package designed to find an acceptable python binary.,2021-01-27,Trevor L Davis,https://github.com/trevorld/findpython,TRUE,https://github.com/trevorld/findpython,94487,5,2021-01-26T06:29:19Z,18897.4
finetune,"The ability to tune models is important. 'finetune' enhances
    the 'tune' package by providing more specialized methods for finding
    reasonable values of model tuning parameters.  Two racing methods
    described by Kuhn (2014) <arXiv:1405.6974> are included. An iterative
    search method using generalized simulated annealing (Bohachevsky,
    Johnson and Stein, 1986) <doi:10.1080/00401706.1986.10488128> is also
    included.",2021-07-21,Max Kuhn,"https://github.com/tidymodels/finetune,
https://finetune.tidymodels.org",TRUE,https://github.com/tidymodels/finetune,5591,34,2021-08-27T15:55:48Z,164.44117647058823
fingerprint,"Functions to manipulate binary fingerprints
 of arbitrary length. A fingerprint is represented by an object of S4 class 'fingerprint'
 which is internally represented a vector of integers, such
 that each element represents the position in the fingerprint that is set to 1.
 The bitwise logical functions in R are overridden so that they can be used directly
 with 'fingerprint' objects. A number of distance metrics are also
 available (many contributed by Michael Fadock). Fingerprints 
 can be converted to Euclidean vectors (i.e., points on the unit hypersphere) and
 can also be folded using OR.  Arbitrary fingerprint formats can be handled via line
 handlers. Currently handlers are provided for CDK, MOE and BCI fingerprint data.",2018-01-07,Rajarshi Guha,NA,TRUE,https://github.com/rajarshi/cdkr,49372,35,2021-07-11T18:44:09Z,1410.6285714285714
firebase,"Authenticate users in 'Shiny' applications using 'Google Firebase' 
    with any of the many methods provided; email and password, email link, or
    using a third-party provider such as 'Github', 'Twitter', or 'Google'.",2020-03-30,John Coene,"https://firebase.john-coene.com/,
https://github.com/JohnCoene/firebase",TRUE,https://github.com/johncoene/firebase,7260,67,2020-12-03T08:46:44Z,108.35820895522389
fishbc,"Provides raw and curated data on the codes,
    classification and conservation status of freshwater fishes in British
    Columbia. Marine fishes will be added in a future release.",2021-05-12,Evan Amies-Galonski,https://github.com/poissonconsulting/fishbc,TRUE,https://github.com/poissonconsulting/fishbc,5902,3,2021-05-28T20:31:12Z,1967.3333333333333
fishflux,"Model fluxes of carbon, nitrogen, and phosphorus with the use of a coupled 
    bioenergetics and stoichiometric model that incorporates flexible elemental limitation. 
    Additional functions to help the user to find parameters are included. Finally, functions to 
    extract and visualize results are available as well. For an introduction, see vignette. 
    For more information on the theoretical background of this model, 
    see Schiettekatte et al. (2020) <doi:10.1111/1365-2435.13618>. ",2021-09-02,Nina M. D. Schiettekatte,https://nschiett.github.io/fishflux/,TRUE,https://github.com/nschiett/fishflux,5902,4,2021-08-27T13:26:57Z,1475.5
fishtree,"An interface to the Fish Tree of Life API to download taxonomies,
    phylogenies, fossil calibrations, and diversification rate information for
    ray-finned fishes.",2021-01-31,Jonathan Chang,"https://fishtreeoflife.org/, https://github.com/jonchang/fishtree",TRUE,https://github.com/jonchang/fishtree,17030,7,2021-09-03T02:44:20Z,2432.8571428571427
fishualize,Implementation of color palettes based on fish species. ,2021-04-09,Nina M. D. Schiettekatte,https://github.com/nschiett/fishualize,TRUE,https://github.com/nschiett/fishualize,17142,127,2021-06-23T12:13:23Z,134.9763779527559
fitbitr,"Many 'Fitbit' users, and R-friendly 'Fitbit' users
    especially, have found themselves curious about their 'Fitbit' data.
    'Fitbit' aggregates a large amount of personal data, much of which is
    interesting for personal research and to satisfy curiosity, and is
    even potentially useful in medical settings. The goal of 'fitbitr' is
    to make interfacing with the 'Fitbit' API as streamlined as possible,
    to make it simple for R users of all backgrounds and comfort levels to
    analyze their 'Fitbit' data and do whatever they want with it!
    Currently, 'fitbitr' includes methods for pulling data on activity,
    sleep, and heart rate, but this list is likely to grow in the future
    as the package gains more traction and more requests for new methods
    to be implemented come in.  You can find details on the 'Fitbit' API
    at <https://dev.fitbit.com/build/reference/web-api/>.",2021-08-22,Matt Kaye,"https://github.com/mrkaye97/fitbitr,
https://mrkaye97.github.io/fitbitr/",TRUE,https://github.com/mrkaye97/fitbitr,1241,2,2021-08-21T20:10:22Z,620.5
fitbitViz,"Connection to the 'Fitbit' Web API <https://dev.fitbit.com/build/reference/web-api/> by including 'ggplot2' Visualizations, 'Leaflet' and 3-dimensional 'Rayshader' Maps. The 3-dimensional 'Rayshader' Map requires the installation of the 'CopernicusDEM' R package which includes the 30- and 90-meter elevation data.",2021-06-30,Lampros Mouselimis,https://github.com/mlampros/fitbitViz,TRUE,https://github.com/mlampros/fitbitviz,2391,2,2021-06-30T09:05:23Z,1195.5
fitdistrplus,"Extends the fitdistr() function (of the MASS package) with several functions 
  to help the fit of a parametric distribution to non-censored or censored data. 
  Censored data may contain left censored, right censored and interval censored values, 
  with several lower and upper bounds. In addition to maximum likelihood estimation (MLE), 
  the package provides moment matching (MME), quantile matching (QME), maximum goodness-of-fit 
  estimation (MGE) and maximum spacing estimation (MSE) methods (available only for 
  non-censored data). Weighted versions of MLE, MME, QME and MSE are available. See e.g. 
  Casella & Berger (2002), Statistical inference, Pacific Grove, for a general introduction 
  to parametric estimation.",2021-05-28,Aurelie Siberchicot,"https://lbbe.univ-lyon1.fr/fitdistrplus.html,
https://github.com/aursiber/fitdistrplus",TRUE,https://github.com/aursiber/fitdistrplus,1039210,24,2021-08-03T09:48:34Z,43300.416666666664
fitHeavyTail,"Robust estimation methods for the mean vector and covariance matrix 
    from data (possibly containing NAs) under multivariate heavy-tailed 
    distributions such as angular Gaussian (via Tyler's method), Cauchy, 
    and Student's t. 
    Additionally, a factor model structure can be specified for the covariance 
    matrix.
    The package is based on the papers: Sun, Babu, and Palomar (2014),
    Sun, Babu, and Palomar (2015), Liu and Rubin (1995), and 
    Zhou, Liu, Kumar, and Palomar (2019).",2020-01-07,Daniel P. Palomar,https://github.com/dppalomar/fitHeavyTail,TRUE,https://github.com/dppalomar/fitheavytail,10241,10,2021-05-25T01:21:35Z,1024.1
fitmix,"Fits the lifespan datasets of biological systems such as yeast, fruit flies, and other similar biological units with well-known finite mixture models introduced by Farewell V. (1982) <doi:10.2307/2529885> and Al-Hussaini et al. (2000) <doi:10.1080/00949650008812033>. Estimates parameter space fitting of a lifespan dataset with finite mixtures of parametric distributions. Computes the following tasks; 1) Estimates parameter space of the finite mixture model 
             by implementing the expectation maximization (EM) algorithm. 2) Finds a sequence of four goodness-of-fit measures consist of Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), Kolmogorov-Smirnov (KS), and log-likelihood (log-likelihood) statistics. 3)The initial values is determined by k-means clustering.",2021-04-19,Emine Guven,https://github.com/guven-code/fitmix/,TRUE,https://github.com/guven-code/fitmix,1518,0,2021-04-16T22:38:01Z,NA
fitur,"Wrapper for computing parameters for univariate distributions using MLE. It creates an object that stores d, p, q, r functions as well as parameters and statistics for diagnostics. Currently supports automated fitting from base and actuar packages. A manually fitting distribution fitting function is included to support directly specifying parameters for any distribution from ancillary packages.",2018-09-01,Thomas Roh,https://github.com/tomroh/fitur,TRUE,https://github.com/tomroh/fitur,22557,4,2021-07-28T04:29:05Z,5639.25
fitzRoy,"An easy package for scraping and processing Australia Rules Football (AFL)
    data. 'fitzRoy' provides a range of functions for accessing publicly available data 
    from 'AFL Tables' <https://afltables.com/afl/afl_index.html>, 'Footy Wire' <https://www.footywire.com> and
    'The Squiggle' <https://squiggle.com.au>. Further functions allow for easy processing, 
    cleaning and transformation of this data into formats that can be used for analysis. ",2021-03-16,James Day,"https://jimmyday12.github.io/fitzRoy/,
https://github.com/jimmyday12/fitzRoy",TRUE,https://github.com/jimmyday12/fitzroy,13876,91,2021-07-27T01:11:44Z,152.4835164835165
fixerapi,"An R client for the ""fixer.io"" currency conversion and exchange 
  rate API. The API requires registration and some features are only available 
  on paid accounts. The full API documentation is available at 
  <https://fixer.io/documentation>.",2018-08-23,Evan Odell,https://docs.evanodell.com/fixerapi,TRUE,https://github.com/evanodell/fixerapi,14572,6,2020-11-22T13:30:43Z,2428.6666666666665
fixest,"Fast and user-friendly estimation of econometric models with multiple fixed-effects. Includes ordinary least squares (OLS), generalized linear models (GLM) and the negative binomial.
    The core of the package is based on optimized parallel C++ code, scaling especially well for large data sets. The method to obtain the fixed-effects coefficients is based on Berge (2018) <https://wwwen.uni.lu/content/download/110162/1299525/file/2018_13>.
    Further provides tools to export and view the results of several estimations with intuitive design to cluster the standard-errors.",2021-06-19,Laurent Berge,"https://lrberge.github.io/fixest/,
https://github.com/lrberge/fixest",TRUE,https://github.com/lrberge/fixest,100012,194,2021-08-31T12:10:50Z,515.5257731958762
FKF,"This is a fast and flexible implementation of the Kalman
        filter, which can deal with NAs. It is entirely written in C
        and relies fully on linear algebra subroutines contained in
        BLAS and LAPACK. Due to the speed of the filter, the fitting of
        high-dimensional linear state space models to large datasets
        becomes possible. This package also contains a plot function
        for the visualization of the state vector and graphical
        diagnostics of the residuals.",2020-06-14,Paul Smith,"https://waternumbers.github.io/FKF/,
https://github.com/waternumbers/FKF",TRUE,https://github.com/waternumbers/fkf,39375,3,2021-05-11T16:47:50Z,13125
FKF.SP,"Fast and flexible Kalman filtering implementation utilizing sequential processing, designed for efficient parameter estimation through maximum likelihood estimation. Sequential processing is a univariate treatment of a multivariate series of observations and can benefit from computational efficiency over traditional Kalman filtering when independence is assumed in the variance of the disturbances of the measurement equation. Sequential processing is described in the textbook of Durbin and Koopman (2001, ISBN:978-0-19-964117-8). 'FKF.SP' was built upon the existing 'FKF' package and is, in general, a faster Kalman filter.",2021-05-04,Thomas Aspinall,https://github.com/TomAspinall/FKF.SP,TRUE,https://github.com/tomaspinall/fkf.sp,4711,0,2021-05-04T06:09:07Z,NA
flair,"Facilitates easier formatting and highlighting of R
    source code in a R Markdown-based presentation. The main goal of the
    package is to allow users to preserve their code creation process
    within code chunks, then to specify formatting details for the source
    code, such as highlighting of particular syntactical elements.",2020-04-23,Kelly Bodwin,"https://github.com/kbodwin/flair,
https://kbodwin.github.io/flair/index.html",TRUE,https://github.com/kbodwin/flair,6545,172,2021-02-10T22:45:57Z,38.05232558139535
FLAME,"Efficient implementations of the algorithms in the 
    Almost-Matching-Exactly framework for interpretable matching in causal
    inference. These algorithms match units via a learned, weighted Hamming
    distance that determines which covariates are more important to match on.
    For more information and examples, see the Almost-Matching-Exactly website. ",2021-07-07,Vittorio Orlandi,"https://almost-matching-exactly.github.io,https://vittorioorlandi.github.io/",TRUE,https://github.com/vittorioorlandi/flame,13880,5,2021-07-14T01:06:38Z,2776
flametree,"A generative art system for producing tree-like
    images using an L-system to create the structures. The package 
    includes tools for generating the data structures and visualise
    them in a variety of styles.",2021-04-27,Danielle Navarro,https://github.com/djnavarro/flametree,TRUE,https://github.com/djnavarro/flametree,1822,137,2021-08-12T08:56:03Z,13.299270072992702
flan,"Tools for fluctuations analysis of mutant cells counts. Main reference is A. Mazoyer, R. Drouilhet, S. Despreaux and B. Ycart (2017)  <doi:10.32614/RJ-2017-029>.",2021-06-07,Adrien Mazoyer,"https://www.r-project.org, https://github.com/AdriMaz/flan",TRUE,https://github.com/adrimaz/flan,14986,1,2020-11-25T12:01:55Z,14986
flashlight,"Shed light on black box machine learning models by the help
    of model performance, variable importance, global surrogate models,
    ICE profiles, partial dependence (Friedman J. H. (2001)
    <doi:10.1214/aos/1013203451>), accumulated local effects (Apley D. W.
    (2016) <arXiv:1612.08468>), further effects plots, scatter plots,
    interaction strength, and variable contribution breakdown (approximate
    SHAP) for single observations (Gosiewska and Biecek (2019)
    <arxiv:1903.11420>). All tools are implemented to work with case
    weights and allow for stratified analysis. Furthermore, multiple
    flashlights can be combined and analyzed together.",2021-04-21,Michael Mayer,https://github.com/mayer79/flashlight,TRUE,https://github.com/mayer79/flashlight,18124,13,2021-05-07T14:34:05Z,1394.1538461538462
flatxml,"On import, the XML information is converted to a dataframe that reflects the hierarchical XML structure. Intuitive functions allow to navigate within this transparent XML data structure (without any knowledge of 'XPath'). 'flatXML' also provides tools to extract data from the XML into a flat dataframe that can be used to perform statistical operations. It also supports converting dataframes to XML.",2020-12-01,Joachim Zuckarelli,https://github.com/jsugarelli/flatxml/,TRUE,https://github.com/jsugarelli/flatxml,20412,21,2020-12-01T21:21:17Z,972
flexdashboard,"Format for converting an R Markdown document to a grid oriented
  dashboard. The dashboard flexibly adapts the size of it's components to the
  containing web page.",2020-06-24,Richard Iannone,http://rmarkdown.rstudio.com/flexdashboard,TRUE,https://github.com/rstudio/flexdashboard,491167,528,2021-08-20T14:45:45Z,930.2405303030303
flexiblas,"Provides functions to switch the 'BLAS'/'LAPACK' optimized backend
    and change the number of threads without leaving the R session, which needs
    to be linked against the 'FlexiBLAS' wrapper library
    <https://www.mpi-magdeburg.mpg.de/projects/flexiblas>.",2020-09-29,Iñaki Ucar,https://github.com/Enchufa2/r-flexiblas,TRUE,https://github.com/enchufa2/r-flexiblas,2730,6,2020-09-30T12:38:03Z,455
flexpolyline,"Binding to the C++ implementation of the flexible polyline
    encoding by HERE <https://github.com/heremaps/flexible-polyline>. The
    flexible polyline encoding is a lossy compressed representation of a list of
    coordinate pairs or coordinate triples. The encoding is achieved by:
    (1) Reducing the decimal digits of each value;
    (2) encoding only the offset from the previous point;
    (3) using variable length for each coordinate delta; and
    (4) using 64 URL-safe characters to display the result.",2021-04-30,Merlin Unterfinger,"https://munterfinger.github.io/flexpolyline/,
https://github.com/munterfinger/flexpolyline/",TRUE,https://github.com/munterfinger/flexpolyline,9193,6,2021-04-30T17:50:20Z,1532.1666666666667
flexsurv,"Flexible parametric models for time-to-event data,
    including the Royston-Parmar spline model, generalized gamma and
    generalized F distributions.  Any user-defined parametric
    distribution can be fitted, given at least an R function defining
    the probability density or hazard. There are also tools for
    fitting and predicting from fully parametric multi-state models,
    based on either cause-specific hazards or mixture models.",2021-02-22,Christopher Jackson,https://github.com/chjackson/flexsurv-dev,TRUE,https://github.com/chjackson/flexsurv-dev,160460,29,2021-08-31T11:10:46Z,5533.103448275862
flextable,"Create pretty tables for 'HTML', 'PDF', 'Microsoft Word' and 'Microsoft PowerPoint' 
  documents from 'R Markdown'. Functions are provided to let users create tables, modify and format 
  their content. It also extends package 'officer' that does not contain any feature for customized 
  tabular reporting.",2021-07-22,David Gohel,"https://ardata-fr.github.io/flextable-book/,
https://davidgohel.github.io/flextable/",TRUE,https://github.com/davidgohel/flextable,563470,338,2021-09-01T21:22:17Z,1667.0710059171597
flightplanning,"Utility functions for creating flight plans for unmanned aerial vehicles (UAV), specially for the Litchi Hub platform. It calculates the flight and camera settings based on the camera specifications, exporting the flight plan CSV format ready to import into Litchi Hub.",2021-02-24,Caio Hamamura,https://github.com/caiohamamura/flightplanning-R,TRUE,https://github.com/caiohamamura/flightplanning-r,11593,4,2021-06-14T18:49:49Z,2898.25
flightplot,Provides functionality to plot airplane flight paths on maps. The plotted flight paths follow the great circle of the Earth.,2020-06-29,Mingchu Xu,https://github.com/xmc811/flightplot,TRUE,https://github.com/xmc811/flightplot,4498,5,2021-05-19T17:28:28Z,899.6
FLightR,"Spatio-temporal locations of an animal are computed 
    from annotated data with a hidden Markov  model via particle
    filter algorithm. The package is relatively robust to varying
    degrees of shading.
    The hidden Markov model is described in Movement Ecology (Rakhimberdiev et al., 2015) <doi:10.1186/s40462-015-0062-5>,
    general package description is in the Methods in Ecology and Evolution (Rakhimberdiev et al., 2017) <doi:10.1111/2041-210X.12765>
    and package accuracy assessed in the Journal of Avian Biology (Rakhimberdiev et al. 2016) <doi:10.1111/jav.00891>.",2021-07-06,Eldar Rakhimberdiev,https://CRAN.R-project.org/package=FLightR,TRUE,https://github.com/eldarrak/flightr,19575,15,2021-09-01T12:52:05Z,1305
flipdownr,"Allows the user to create a countdown in 'RMarkdown' documents and 'shiny' applications. 
    The package is a wrapper of the 'JavaScript' library 'flipdown.js'. See <https://pbutcher.uk/flipdown/> for more info.",2020-11-29,Mohamed El Fodil Ihaddaden,https://github.com/feddelegrand7/flipdownr,TRUE,https://github.com/feddelegrand7/flipdownr,3860,29,2021-01-23T11:25:33Z,133.10344827586206
flipdownWidgets,Include a countdown <https://github.com/PButcher/flipdown> in all R contexts with the convenience of 'htmlwidgets'.,2021-06-30,Yongchao Fang,https://github.com/fanggong/flipdownWidgets,TRUE,https://github.com/fanggong/flipdownwidgets,833,0,2021-07-01T13:04:27Z,NA
flipr,"A flexible permutation framework for making 
    inference such as point estimation, confidence 
    intervals or hypothesis testing, on any kind of data, 
    be it univariate, multivariate, or more complex such 
    as network-valued data, topological data, functional 
    data or density-valued data.",2021-09-01,Aymeric Stamm,"https://astamm.github.io/flipr/, https://github.com/astamm/flipr/",TRUE,https://github.com/astamm/flipr,2614,1,2021-08-31T22:18:51Z,2614
float,"R comes with a suite of utilities for linear algebra with ""numeric""
    (double precision) vectors/matrices. However, sometimes single precision (or
    less!) is more than enough for a particular task.  This package extends R's
    linear algebra facilities to include 32-bit float (single precision) data.
    Float vectors/matrices have half the precision of their ""numeric""-type
    counterparts but are generally faster to numerically operate on, for a
    performance vs accuracy trade-off.  The internal representation is an S4
    class, which allows us to keep the syntax identical to that of base R's.
    Interaction between floats and base types for binary operators is generally
    possible; in these cases, type promotion always defaults to the higher
    precision.  The package ships with copies of the single precision 'BLAS' and
    'LAPACK', which are automatically built in the event they are not available
    on the system.",2020-04-22,Drew Schmidt,https://github.com/wrathematics/float,TRUE,https://github.com/wrathematics/float,232007,39,2021-01-10T15:33:55Z,5948.897435897436
flobr,"Converts files to and from flobs. 
    A flob is a file that was 
    read into binary in integer-mode as little endian, 
    saved as the single element of a named list (where the name is the name 
    of the original file) and then serialized before being coerced into a blob.
    Flobs are useful for writing and reading files to and from databases.",2020-05-15,Joe Thorley,https://github.com/poissonconsulting/flobr,TRUE,https://github.com/poissonconsulting/flobr,15792,7,2021-02-26T17:28:23Z,2256
flora,"Tools to quickly compile taxonomic and distribution data from
    the Brazilian Flora 2020.",2020-04-28,Gustavo Carvalho,http://www.github.com/gustavobio/flora,TRUE,https://github.com/gustavobio/flora,21501,20,2021-07-05T19:24:40Z,1075.05
flowr,"This framework allows you to design and implement complex
    pipelines, and deploy them on your institution's computing cluster. This has
    been built keeping in mind the needs of bioinformatics workflows. However, it is
    easily extendable to any field where a series of steps (shell commands) are to
    be executed in a (work)flow.",2021-03-02,Sahil Seth,https://github.com/flow-r/flowr,TRUE,https://github.com/flow-r/flowr,20415,76,2021-03-10T15:43:53Z,268.61842105263156
fmcmc,"Provides a friendly (flexible) Markov Chain Monte Carlo (MCMC)
         framework for implementing Metropolis-Hastings algorithm in a modular way
         allowing users to specify automatic convergence checker, personalized
         transition kernels, and out-of-the-box multiple MCMC chains using
         parallel computing. Most of the methods implemented in this package can
         be found in Brooks et al. (2011, ISBN 9781420079425). Among the methods
         included, we have: Haario (2001) <doi:10.1007/s11222-011-9269-5>
         Adaptive Metropolis, Vihola (2012) <doi:10.1007/s11222-011-9269-5>
         Robust Adaptive Metropolis, and Thawornwattana et
         al. (2018) <doi:10.1214/17-BA1084> Mirror transition kernels.",2021-09-03,George Vega Yon,https://github.com/USCbiostats/fmcmc,TRUE,https://github.com/uscbiostats/fmcmc,11895,11,2021-08-16T23:26:04Z,1081.3636363636363
fmpcloudr,"Use R to access to the 'FMP Cloud' API <https://fmpcloud.io/> and 
    'Financial Modeling Prep' API <https://financialmodelingprep.com/developer/docs/>.
    Data available includes stock prices, market indexes, company fundamentals,
    13F holdings data, and much more. A valid API token must be set to enable
    functions. ",2021-03-01,Anthony Balentine,https://exploringfinance.github.io/fmpcloudr/,TRUE,https://github.com/exploringfinance/fmpcloudr,4629,5,2021-04-23T13:47:53Z,925.8
FMradio,"Functions that support stable prediction and classification with radiomics data through factor-analytic modeling. For details, see Peeters et al. (2019) <arXiv:1903.11696>.",2019-12-16,Carel F.W. Peeters,https://github.com/CFWP/FMradio,TRUE,https://github.com/cfwp/fmradio,11487,8,2020-09-08T15:10:13Z,1435.875
fmtr,"Contains a set of functions that can be used to apply
  formats to data frames or vectors.  The package aims to provide to 
  functionality similar to that of SAS® formats. Formats are assigned to
  the format attribute on data frame columns.  Then when the fdata() 
  function is called, a new data frame is created with the column data
  formatted as specified.  The package also contains a value() function
  to create a user-defined format, similar to a SAS® user-defined format.",2021-07-25,David Bosak,https://fmtr.r-sassy.org,TRUE,https://github.com/dbosak01/fmtr,7324,7,2021-08-08T18:00:47Z,1046.2857142857142
foghorn,"The CRAN check results and where your package stands in the
    CRAN submission queue in your R terminal.",2021-07-11,Francois Michonneau,https://github.com/fmichonneau/foghorn,TRUE,https://github.com/fmichonneau/foghorn,690051,53,2021-07-11T10:17:12Z,13019.830188679245
foieGras,"Fits continuous-time random walk and correlated random walk state-space models for quality control animal tracking data ('Argos', processed light-level 'geolocation', 'GPS'). Template Model Builder ('TMB') is used for fast estimation. The 'Argos' data can be: (older) least squares-based locations; (newer) Kalman filter-based locations with error ellipse information; or a mixture of both. The models estimate two sets of location states corresponding to: 1) each observation, which are (usually) irregularly timed; and 2) user-specified time intervals (regular or irregular). Latent variable models are provided to estimate move persistence along tracks as an index of behaviour. Track simulation functions are provided. 'Jonsen I', 'McMahon CR', 'Patterson TA', 'Auger-Méthé M', 'Harcourt R', 'Hindell MA', 'Bestley S' (2019) Movement responses to environment: fast inference of variation among southern elephant seals with a mixed effects model. Ecology 100:e02566 <doi:10.1002/ecy.2566>.",2021-04-26,Ian Jonsen,https://github.com/ianjonsen/foieGras/,TRUE,https://github.com/ianjonsen/foiegras,22804,10,2021-07-28T12:06:40Z,2280.4
folio,"Datasets for teaching quantitative approaches and
    modeling in archaeology and paleontology. This package provides
    several types of data related to broad topics (cultural evolution,
    radiocarbon dating, paleoenvironments, etc.), which can be used to
    illustrate statistical methods in the classroom (multivariate data
    analysis, compositional data analysis, diversity measurement, etc.).",2021-02-12,Nicolas Frerebeau,"https://folio.tesselle.org, https://github.com/tesselle/folio",TRUE,https://github.com/tesselle/folio,3254,3,2021-08-25T16:36:35Z,1084.6666666666667
fontawesome,"Easily and flexibly insert 'Font Awesome' icons into 'R Markdown'
    documents and 'Shiny' apps. These icons can be inserted into HTML content
    through inline 'SVG' tags or 'i' tags. There is also a utility function for
    exporting 'Font Awesome' icons as 'PNG' images for those situations where
    raster graphics are needed.",2021-07-02,Richard Iannone,https://github.com/rstudio/fontawesome,TRUE,https://github.com/rstudio/fontawesome,12652,190,2021-07-03T04:20:53Z,66.58947368421053
footprint,"A handy tool to calculate carbon footprints from
    air travel based on three-letter International Air Transport Association (IATA) airport codes or latitude and longitude.
    footprint first calculates the great-circle distance between departure and arrival 
    destinations. It then uses the Department of Environment, Food & Rural Affairs (DEFRA) 
    greenhouse gas conversion factors for business air travel to estimate the carbon footprint.
    These conversion factors consider trip length, flight class (e.g. economy, business), and emissions 
    metric (e.g. carbon dioxide equivalent, methane).",2021-01-06,Anthony Schmidt,https://github.com/acircleda/footprint,TRUE,https://github.com/acircleda/footprint,2769,10,2020-12-31T14:25:02Z,276.9
foqat,"Tools for quickly processing and analyzing 
	field observation data and air quality data. This 
	tools contain functions that facilitate analysis 
	in atmospheric chemistry (especially in ozone 
	pollution). Some functions of time series are also 
	applicable to other fields. For detail please view 
	homepage<https://github.com/tianshu129/foqat>.
	Scientific Reference:
	1. The Hydroxyl Radical (OH) Reactivity: Roger Atkinson and Janet Arey (2003) <doi:10.1021/cr0206420>.
	2. Ozone Formation Potential (OFP): <https://ww2.arb.ca.gov/sites/default/files/classic/regact/2009/mir2009/mir10.pdf>, Zhang et al.(2021) <doi:10.5194/acp-21-11053-2021>.
	3. Aerosol Formation Potential (AFP): Wenjing Wu et al. (2016) <doi:10.1016/j.jes.2016.03.025>.
	4. TUV model: <https://www2.acom.ucar.edu/modeling/tropospheric-ultraviolet-and-visible-tuv-radiation-model>.",2021-08-18,Tianshu Chen,https://github.com/tianshu129/foqat,TRUE,https://github.com/tianshu129/foqat,2486,21,2021-09-01T08:54:05Z,118.38095238095238
forcats,"Helpers for reordering factor levels (including
    moving specified levels to front, ordering by first appearance,
    reversing, and randomly shuffling), and tools for modifying factor
    levels (including collapsing rare levels into other, 'anonymising',
    and manually 'recoding').",2021-01-27,Hadley Wickham,"https://forcats.tidyverse.org,
https://github.com/tidyverse/forcats",TRUE,https://github.com/tidyverse/forcats,13179407,454,2021-01-27T20:07:02Z,29029.530837004404
foreach,"Support for the foreach looping construct.  Foreach is an
        idiom that allows for iterating over elements in a collection,
        without the use of an explicit loop counter.  This package in
        particular is intended to be used for its return value, rather
        than for its side effects.  In that sense, it is similar to the
        standard lapply function, but doesn't require the evaluation
        of a function.  Using foreach without side effects also
        facilitates executing the loop in parallel.",2020-10-15,Michelle Wallig,https://github.com/RevolutionAnalytics/foreach,TRUE,https://github.com/revolutionanalytics/foreach,6116604,34,2020-10-15T16:49:32Z,179900.11764705883
forecast,"Methods and tools for displaying and analysing
             univariate time series forecasts including exponential smoothing
             via state space models and automatic ARIMA modelling.",2021-06-01,Rob Hyndman,"https://pkg.robjhyndman.com/forecast/,
https://github.com/robjhyndman/forecast",TRUE,https://github.com/robjhyndman/forecast,7826605,924,2021-08-29T03:38:10Z,8470.351731601731
forecastHybrid,"Convenient functions for ensemble forecasts in R combining
    approaches from the 'forecast' package. Forecasts generated from auto.arima(), ets(),
    thetaf(), nnetar(), stlm(), tbats(), and snaive() can be combined with equal weights, weights
    based on in-sample errors (introduced by Bates & Granger (1969) <doi:10.1057/jors.1969.103>),
    or cross-validated weights. Cross validation for time series data with user-supplied models
    and forecasting functions is also supported to evaluate model accuracy.",2020-08-28,David Shaub,"https://gitlab.com/dashaub/forecastHybrid,
https://github.com/ellisp/forecastHybrid",TRUE,https://github.com/ellisp/forecasthybrid,183512,74,2020-09-20T14:22:18Z,2479.891891891892
FoReco,"Classical (bottom-up and top-down), optimal and heuristic combination forecast 
    reconciliation procedures for cross-sectional, temporal, and cross-temporal linearly 
    constrained time series.",2021-07-23,Daniele Girolimetto,"https://github.com/daniGiro/FoReco,
https://danigiro.github.io/FoReco/",TRUE,https://github.com/danigiro/foreco,5515,7,2021-07-24T10:11:57Z,787.8571428571429
forestControl,"Approximate false positive rate control in selection frequency for
    random forest using the methods described by Ender Konukoglu and Melanie Ganz (2014) <arXiv:1410.2838>.
    Methods for calculating the selection frequency threshold at false positive rates
    and selection frequency false positive rate feature selection.",2019-11-18,Tom Wilson,https://github.com/aberHRML/forestControl,TRUE,https://github.com/aberhrml/forestcontrol,18294,2,2020-12-15T22:44:25Z,9147
forestecology,"Code for fitting and assessing models for the growth of trees. In 
    particular for the Bayesian neighborhood competition linear regression model 
    of Allen (2020): methods for model fitting and generating fitted/predicted 
    values, evaluating the effect of competitor species identity using 
    permutation tests, and evaluating model performance using spatial 
    cross-validation. ",2021-03-12,Albert Y. Kim,https://github.com/rudeboybert/forestecology,TRUE,https://github.com/rudeboybert/forestecology,2069,3,2021-09-01T23:27:03Z,689.6666666666666
forestplot,"A forest plot that allows for
    multiple confidence intervals per row,
    custom fonts for each text element,
    custom confidence intervals,
    text mixed with expressions, and more.
    The aim is to extend the use of forest plots beyond meta-analyses.
    This is a more general version of the original 'rmeta' package's forestplot()
    function and relies heavily on the 'grid' package.",2021-09-03,Max Gordon,https://gforge.se/packages/,TRUE,https://github.com/gforge/forestplot,188416,24,2021-08-25T20:01:16Z,7850.666666666667
forestr,"Provides a toolkit for calculating forest and canopy structural complexity metrics from
    terrestrial LiDAR (light detection and ranging). References:  Atkins et al. 2018 <doi:10.1111/2041-210X.13061>; Hardiman et al. 2013 <doi:10.3390/f4030537>;
    Parker et al. 2004 <doi:10.1111/j.0021-8901.2004.00925.x>.",2020-04-14,Jeff Atkins,https://github.com/atkinsjeff/forestr,TRUE,https://github.com/atkinsjeff/forestr,15465,14,2021-07-30T17:58:29Z,1104.642857142857
ForestTools,"Provides tools for analyzing remotely sensed forest data, including functions for detecting treetops from canopy models, outlining tree crowns, calculating textural metrics and generating spatial statistics.",2021-04-18,Andrew Plowright,https://github.com/andrew-plowright/ForestTools,TRUE,https://github.com/andrew-plowright/foresttools,21880,21,2021-04-17T04:29:40Z,1041.904761904762
forImage,"
    The goal of this collection of functions is to provide an easy to use tool for the measurement of foraminifera and other unicellulars organisms size. With functions developed to guide foraminiferal test biovolume calculations and cell biomass estimations. The volume function includes several microalgae models geometric adaptations based on Hillebrand et al. (1999) <doi:10.1046/j.1529-8817.1999.3520403.x>, Sun and Liu (2003) <doi:10.1093/plankt/fbg096> and Vadrucci, Cabrini and Basset (2007) <doi:10.1285/i1825229Xv1n2p83>.",2021-03-02,Thaise R Freitas,https://github.com/ThaiseRF/forImage,TRUE,https://github.com/thaiserf/forimage,1674,0,2021-03-06T15:04:15Z,NA
formatR,"Provides a function tidy_source() to format R source code. Spaces
    and indent will be added to the code automatically, and comments will be
    preserved under certain conditions, so that R code will be more
    human-readable and tidy. There is also a Shiny app as a user interface in
    this package (see tidy_app()).",2021-06-01,Yihui Xie,https://github.com/yihui/formatR,TRUE,https://github.com/yihui/formatr,3154342,206,2021-06-01T18:52:59Z,15312.339805825242
formattable,"Provides functions to create formattable vectors and data frames.
    'Formattable' vectors are printed with text formatting, and formattable
    data frames are printed with multiple types of formatting in HTML
    to improve the readability of data presented in tabular form rendered in
    web pages.",2021-01-07,Kun Ren,"https://renkun-ken.github.io/formattable/,
https://github.com/renkun-ken/formattable",TRUE,https://github.com/renkun-ken/formattable,757333,623,2021-07-29T04:13:59Z,1215.6227929373997
formulaic,"Many statistical models and analyses in R are implemented through formula objects. The formulaic package creates a unified approach for programmatically and dynamically generating formula objects. Users may specify the outcome and inputs of a model directly, search for variables to include based upon naming patterns, incorporate interactions, and identify variables to exclude. A wide range of quality checks are implemented to identify issues such as misspecified variables, duplication, a lack of contrast in the inputs, and a large number of levels in categorical data.  Variables that do not meet these quality checks can be automatically excluded from the model.  These issues are documented and reported in a manner that provides greater accountability and useful information to guide an investigation of the data.",2021-02-15,David Shilane,https://dachosen1.github.io/formulaic/index.html,TRUE,https://github.com/dachosen1/formulaic,13397,6,2021-03-26T04:37:33Z,2232.8333333333335
forrel,"Forensic applications of pedigree analysis, including likelihood ratios 
    for relationship testing, general relatedness inference, marker simulation, and 
    power analysis. General computation of exclusion powers is based on Egeland et 
    al. (2014) <doi:10.1016/j.fsigen.2013.05.001>. Several functions deal 
    specifically with family reunion cases, implementing and developing ideas from 
    Kling et al. (2017) <doi:10.1016/j.fsigen.2017.08.006>. A novelty of 'forrel' 
    is the ability to model background inbreeding in forensic pedigree computations.
    This can have significant impact in applications, as exemplified in Vigeland 
    and Egeland (2019) <doi:10.1016/j.fsigss.2019.10.175>. 'forrel' is part of the
    ped suite, a collection of packages for pedigree analysis. In particular, 
    'forrel' imports 'pedtools' for creating and manipulating pedigrees and markers,
    'pedprobr' for likelihood computations, and 'pedmut' for mutation modelling. 
    Pedigree data may be created from scratch, or loaded from text files. Data 
    import from the 'Familias' software (Egeland et al. (2000) 
    <doi:10.1016/S0379-0738(00)00147-X>) is supported. ",2021-03-14,Magnus Dehli Vigeland,https://github.com/magnusdv/forrel,TRUE,https://github.com/magnusdv/forrel,10388,8,2021-06-25T09:13:30Z,1298.5
FORTLS,"Process automation of Terrestrial Laser Scanner (TLS) point cloud data derived from single scans. 'FORTLS' enables (i) detection of trees and estimation of diameter at breast height (dbh), (ii) estimation of some stand variables (e.g. density, basal area, mean and dominant height), (iii) computation of metrics related to important forest attributes estimated in Forest Inventories (FIs) at stand level and (iv) optimization of plot design for combining TLS data and field measured data. Documentation about 'FORTLS' is described in Molina-Valero et al. (2020, <doi:10.3390/IECF2020-08066>). ",2021-04-21,Juan Alberto Molina-Valero,https://github.com/Molina-Valero/FORTLS,TRUE,https://github.com/molina-valero/fortls,2456,2,2021-05-24T18:24:14Z,1228
foster,"Set of tools to streamline the modeling of the relationship between 
    satellite imagery time series or any other environmental information, 
    such as terrain elevation, with forest structural attributes derived from 
    3D point cloud data and their subsequent imputation over the broader 
    landscape. ",2021-03-30,Martin Queinnec,NA,TRUE,https://github.com/mqueinnec/foster,3830,4,2021-03-23T17:07:58Z,957.5
foto,"The Fourier Transform Textural Ordination method 
  uses a principal component analysis on radially averaged
  two dimensional Fourier spectra to characterize image texture.",2019-01-17,Koen Hufkens,https://github.com/khufkens/foto,TRUE,https://github.com/khufkens/foto,12062,4,2021-06-11T08:59:25Z,3015.5
fpcb,"Functions to represent functional objects under a Reproducing Kernel Hilbert Space (RKHS) framework as described 
  in Muñoz & González (2010). Autoregressive Hilbertian Model for functional time series using RKHS and predictive confidence bands construction 
  as proposed in Hernández et al (2021).",2021-06-07,Nicolás Hernández,NA,TRUE,https://github.com/nicolashernandezb/fpcb,1245,0,2021-06-11T08:49:51Z,NA
fpeek,"Tools to help text files importation. It can return 
 the number of lines; print the first and last lines; convert 
 encoding. Operations are made without reading the entire file 
 before starting, resulting in good performances with large files. 
 This package provides an alternative to a simple use of the 
 'head', 'tail', 'wc' and 'iconv' programs that are not always 
 available on machine where R is installed.",2021-03-31,David Gohel,https://github.com/davidgohel/fpeek,TRUE,https://github.com/davidgohel/fpeek,21694,33,2021-03-31T15:23:26Z,657.3939393939394
fpp2,"All data sets required for the examples and exercises
  in the book ""Forecasting: principles and practice"" (2nd ed, 2018)
  by Rob J Hyndman and George Athanasopoulos <https://otexts.com/fpp2/>.
  All packages required to run the examples are also loaded.",2020-09-09,Rob Hyndman,"https://pkg.robjhyndman.com/fpp2-package/,
https://github.com/robjhyndman/fpp2-package,
https://otexts.com/fpp2/",TRUE,https://github.com/robjhyndman/fpp2-package,347508,90,2020-09-09T05:53:15Z,3861.2
fpp3,"
    All data sets required for the examples and exercises in the book
    ""Forecasting: principles and practice"" by Rob J Hyndman and George Athanasopoulos
    <https://OTexts.com/fpp3/>.  All packages required to run the examples are also
    loaded.",2021-02-06,Rob Hyndman,"https://github.com/robjhyndman/fpp3-package,
https://OTexts.com/fpp3/",TRUE,https://github.com/robjhyndman/fpp3-package,65878,60,2021-03-07T05:18:45Z,1097.9666666666667
fracdist,"Calculate numerical asymptotic distribution functions of likelihood ratio 
    statistics for fractional unit root tests and tests of cointegration rank. 
    For these distributions, the included functions calculate critical values 
    and P-values used in unit root tests, cointegration tests, and rank tests 
    in the Fractionally Cointegrated Vector Autoregression (FCVAR) model.
    The functions implement procedures for tests described in the following articles:
    Johansen, S. and M. Ø. Nielsen (2012) <doi:10.3982/ECTA9299>,
    MacKinnon, J. G. and M. Ø. Nielsen (2014) <doi:10.1002/jae.2295>.",2021-05-25,Lealand Morin,https://github.com/LeeMorinUCF/fracdist,TRUE,https://github.com/leemorinucf/fracdist,1197,0,2021-08-05T20:36:45Z,NA
fracture,"Provides functions for converting decimals to a
    matrix of numerators and denominators or a character vector of
    fractions.  Supports mixed or improper fractions, finding common
    denominators for vectors of fractions, limiting denominators to powers
    of ten, and limiting denominators to a maximum value.  Also includes
    helper functions for finding the least common multiple and greatest
    common divisor for a vector of integers.  Implemented using C++ for
    maximum speed.",2021-05-25,Alexander Rossell Hayes,"https://fracture.rossellhayes.com/,
https://github.com/rossellhayes/fracture",TRUE,https://github.com/rossellhayes/fracture,5487,2,2021-05-25T01:11:30Z,2743.5
frailtySurv,"Simulates and fits semiparametric shared frailty models under a
    wide range of frailty distributions using a consistent and
    asymptotically-normal estimator. Currently supports: gamma, power variance
    function, log-normal, and inverse Gaussian frailty models.",2019-04-19,Vinnie Monaco,https://github.com/vmonaco/frailtySurv/,TRUE,https://github.com/vmonaco/frailtysurv,20514,8,2021-05-05T00:37:11Z,2564.25
frechet,"Provides implementation of statistical methods for random objects 
    lying in various metric spaces, which are not necessarily linear spaces. 
    The core of this package is Fréchet regression for random objects with 
    Euclidean predictors, which allows one to perform regression analysis 
    for non-Euclidean responses under some mild conditions. 
    Examples include distributions in L^2-Wasserstein space, 
    covariance matrices endowed with power metric (with Frobenius metric 
    as a special case), Cholesky and log-Cholesky metrics.  
    References: Petersen, A., & Müller, H.-G. (2019) <doi:10.1214/17-AOS1624>.",2020-12-16,Yaqing Chen,https://github.com/functionaldata/tFrechet,TRUE,https://github.com/functionaldata/tfrechet,5383,2,2020-12-16T07:15:49Z,2691.5
fredr,"An R client for the 'Federal Reserve Economic Data'
    ('FRED') API <https://research.stlouisfed.org/docs/api/>.  Functions
    to retrieve economic time series and other data from 'FRED'.",2021-01-29,Sam Boysel,https://github.com/sboysel/fredr,TRUE,https://github.com/sboysel/fredr,33342,73,2021-08-13T08:47:33Z,456.73972602739724
freealg,The free algebra in R; multivariate polynomials with non-commuting indeterminates.,2021-04-19,Robin K. S. Hankin,https://github.com/RobinHankin/freealg,TRUE,https://github.com/robinhankin/freealg,10464,1,2021-08-13T09:47:16Z,10464
freedom,"Implements the formulae required to calculate freedom
    from disease according to Cameron and Baldock (1998)
    <doi:10.1016/S0167-5877(97)00081-0>. These are the
    methods used at the Swedish national veterinary institute (SVA) to
    evaluate the performance of our nation animal disease
    surveillance programmes.",2020-09-08,Thomas Rosendal,https://github.com/SVA-SE/freedom,TRUE,https://github.com/sva-se/freedom,4389,0,2020-09-05T18:40:47Z,NA
freegroup,"Provides functionality for manipulating elements of the free group (juxtaposition is represented by a plus) including inversion, multiplication by a scalar, group-theoretic power operation, and Tietze forms.  The package is fully vectorized.",2018-09-25,Robin K. S. Hankin,https://github.com/RobinHankin/freegroup.git,TRUE,https://github.com/robinhankin/freegroup,15639,0,2021-08-16T22:38:31Z,NA
freesurfer,"Wrapper functions that interface with 'Freesurfer' 
    <https://surfer.nmr.mgh.harvard.edu/>, a powerful and 
    commonly-used 'neuroimaging'
    software, using system commands. The goal is to be able to interface with
    'Freesurfer' completely in R, where you pass R objects of class 'nifti',
    implemented by package 'oro.nifti', and the function executes an 'Freesurfer'
    command and returns an R object of class 'nifti' or necessary output.",2020-12-08,John Muschelli,NA,TRUE,https://github.com/muschellij2/freesurfer,19786,7,2020-12-08T18:41:34Z,2826.5714285714284
freesurferformats,"Provides functions to read and write neuroimaging data in various file formats, with a focus on 'FreeSurfer' <http://freesurfer.net/> formats. This includes, but is not limited to, the following file formats: 1) MGH/MGZ format files, which can contain multi-dimensional images or other data. Typically they contain time-series of three-dimensional brain scans acquired by magnetic resonance imaging (MRI). They can also contain vertex-wise measures of surface morphometry data. The MGH format is named after the Massachusetts General Hospital, and the MGZ format is a compressed version of the same format. 2) 'FreeSurfer' morphometry data files in binary 'curv' format. These contain vertex-wise surface measures, i.e., one scalar value for each vertex of a brain surface mesh. These are typically values like the cortical thickness or brain surface area at each vertex. 3) Annotation file format. This contains a brain surface parcellation derived from a cortical atlas. 4) Surface file format. Contains a brain surface mesh, given by a list of vertices and a list of faces.",2021-05-25,Tim Schäfer,https://github.com/dfsp-spirit/freesurferformats,TRUE,https://github.com/dfsp-spirit/freesurferformats,22958,14,2021-06-14T15:06:51Z,1639.857142857143
freqtables,"Quickly make tables of descriptive statistics (i.e., counts, 
    percentages, confidence intervals) for categorical variables. This 
    package is designed to work in a Tidyverse pipeline, and consideration
    has been given to get results from R to Microsoft Word ® with minimal pain.",2020-07-20,Brad Cannell,https://github.com/brad-cannell/freqtables,TRUE,https://github.com/brad-cannell/freqtables,6209,6,2021-05-01T04:54:31Z,1034.8333333333333
frequency,"Generate 'SPSS'/'SAS' styled frequency tables. Frequency tables are 
    generated with variable and value label attributes where applicable with optional
    html output to quickly examine datasets.",2021-01-11,Alistair Wilcox,https://github.com/wilcoxa/frequency,TRUE,https://github.com/wilcoxa/frequency,28393,2,2021-01-11T13:02:23Z,14196.5
frequencyConnectedness,"Accompanies a paper (Barunik, Krehlik (2018) <doi:10.1093/jjfinec/nby001>) dedicated to spectral decomposition of connectedness measures and their interpretation. We implement all the developed estimators as well as the historical counterparts. For more information, see the help or GitHub page (<https://github.com/tomaskrehlik/frequencyConnectedness>) for relevant information.",2020-11-10,Tomas Krehlik,https://github.com/tomaskrehlik/frequencyConnectedness,TRUE,https://github.com/tomaskrehlik/frequencyconnectedness,22449,43,2020-11-10T21:49:05Z,522.0697674418604
fresh,"Customize 'Bootstrap' and 'Bootswatch' themes, like colors, fonts, grid layout, 
  to use in 'Shiny' applications, 'rmarkdown' documents and 'flexdashboard'.",2020-05-29,Victor Perrier,https://github.com/dreamRs/fresh,TRUE,https://github.com/dreamrs/fresh,56696,181,2021-02-11T21:34:47Z,313.23756906077347
friends,"The complete scripts from the American sitcom Friends in tibble 
    format. Use this package to practice data wrangling, text analysis and 
    network analysis.",2020-09-03,Emil Hvitfeldt,https://github.com/EmilHvitfeldt/friends,TRUE,https://github.com/emilhvitfeldt/friends,5096,42,2021-06-01T08:17:47Z,121.33333333333333
FRK,"Fixed Rank Kriging is a tool for spatial/spatio-temporal modelling and prediction with large datasets. The approach models the field, and hence the covariance function, using a set of r basis functions, where r is typically much smaller than the number of data points (or polygons) m. This low-rank basis-function representation facilitates the modelling of 'big' spatial/spatio-temporal data. The method naturally allows for non-stationary, anisotropic covariance functions. Discretisation of the spatial domain into so-called basic areal units (BAUs) facilitates the use of observations with varying support (i.e., both point-referenced and areal supports, potentially simultaneously), and prediction over arbitrary user-specified regions. `FRK` also supports inference over various manifolds, including the 2D plane and 3D sphere, and it provides helper functions to model, fit, predict, and plot with relative ease. Version 2.0.0 and above of the package `FRK` also supports modelling of non-Gaussian data, by employing a spatial generalised linear mixed model (GLMM) framework  to cater for Poisson, binomial, negative-binomial, gamma, and inverse-Gaussian distributions.  Zammit-Mangion and Cressie <doi:10.18637/jss.v098.i04> describe `FRK` in a Gaussian setting, and detail its use of basis functions and BAUs.",2021-05-28,Andrew Zammit-Mangion,NA,TRUE,https://github.com/andrewzm/frk,29892,44,2021-05-05T05:31:06Z,679.3636363636364
fromo,"Fast, numerically robust computation of weighted moments via 'Rcpp'. 
   Supports computation on vectors and matrices, and Monoidal append of moments. 
   Moments and cumulants over running fixed length windows can be computed, 
   as well as over time-based windows.
   Moment computations are via a generalization of Welford's method, as described
   by Bennett et. (2009) <doi:10.1109/CLUSTR.2009.5289161>.",2019-01-30,Steven E. Pav,https://github.com/shabbychef/fromo,TRUE,https://github.com/shabbychef/fromo,18292,2,2021-04-03T22:34:53Z,9146
frost,"A compilation of empirical methods used by farmers and agronomic engineers to predict the minimum temperature to detect a frost night. These functions use variables such as environmental temperature, relative humidity, and dew point. See <http://sedici.unlp.edu.ar/handle/10915/72102> <http://www.fao.org/docrep/008/y7223e/y7223e0b.htm#bm11.8> for details.",2019-04-12,Ana Laura Diedrichs,https://github.com/anadiedrichs/frost,TRUE,https://github.com/anadiedrichs/frost,11449,2,2021-03-18T02:57:32Z,5724.5
fs,"A cross-platform interface to file system operations, built on
  top of the 'libuv' C library.",2020-07-31,Jim Hester,"http://fs.r-lib.org, https://github.com/r-lib/fs",TRUE,https://github.com/r-lib/fs,23918256,295,2021-05-19T19:44:54Z,81078.83389830508
FSA,"A variety of simple fish stock assessment methods.
    Detailed vignettes are available on the fishR website <http://derekogle.com/fishR/>.",2021-07-17,Derek Ogle,https://github.com/droglenc/FSA,TRUE,https://github.com/droglenc/fsa,257818,38,2021-07-17T15:11:42Z,6784.684210526316
FSAdata,The datasets to support the Fish Stock Assessment ('FSA') package.,2019-05-18,Derek Ogle,"http://derekogle.com/fishR/, https://github.com/droglenc/FSAdata",TRUE,https://github.com/droglenc/fsadata,47256,6,2021-01-18T12:15:47Z,7876
fsbrain,"Provides high-level access to neuroimaging data from standard software packages like 'FreeSurfer' <http://freesurfer.net/> on the level of subjects and groups. Load morphometry data, surfaces and brain parcellations based on atlases. Mask data using labels, load data for specific atlas regions only, and visualize data and statistical results directly in 'R'.",2021-05-12,Tim Schäfer,https://github.com/dfsp-spirit/fsbrain,TRUE,https://github.com/dfsp-spirit/fsbrain,12776,28,2021-07-26T13:41:46Z,456.2857142857143
FSelector,"Functions for selecting attributes from a given
    dataset. Attribute subset selection is the process of identifying and
    removing as much of the irrelevant and redundant information as
    possible.",2021-02-16,Piotr Romanski,https://github.com/larskotthoff/fselector,TRUE,https://github.com/larskotthoff/fselector,171368,6,2021-02-16T17:51:50Z,28561.333333333332
FSelectorRcpp,"'Rcpp' (free of 'Java'/'Weka') implementation of 'FSelector' entropy-based feature selection 
 algorithms based on an MDL discretization (Fayyad U. M., Irani K. B.: Multi-Interval Discretization of Continuous-Valued Attributes for Classification Learning.
 In 13'th International Joint Conference on Uncertainly in Artificial Intelligence (IJCAI93), pages 1022-1029, Chambery, France, 1993.) <https://www.ijcai.org/Proceedings/93-2/Papers/022.pdf>
 with a sparse matrix support.",2021-01-14,Zygmunt Zawadzki,https://github.com/mi2-warsaw/FSelectorRcpp,TRUE,https://github.com/mi2-warsaw/fselectorrcpp,78834,31,2021-01-14T14:21:51Z,2543.032258064516
fslr,"Wrapper functions that interface with 'FSL' 
    <http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/>, a powerful and commonly-used 'neuroimaging'
    software, using system commands. The goal is to be able to interface with 'FSL'
    completely in R, where you pass R objects of class 'nifti', implemented by
    package 'oro.nifti', and the function executes an 'FSL' command and returns an R
    object of class 'nifti' if desired.",2019-08-05,John Muschelli,NA,TRUE,https://github.com/muschellij2/fslr,26868,33,2021-04-14T14:26:03Z,814.1818181818181
fspe,Estimating the number of factors in Exploratory Factor Analysis (EFA) with out-of-sample prediction errors using a cross-validation scheme. Haslbeck & van Bork (Preprint) <https://psyarxiv.com/qktsd>.,2021-02-24,Jonas Haslbeck,NA,TRUE,https://github.com/jmbh/fspe,2061,0,2021-02-21T14:50:47Z,NA
fssemR,"An optimizer of Fused-Sparse Structural Equation Models, which is 
 the state of the art jointly fused sparse maximum likelihood function 
 for structural equation models proposed by Xin Zhou and Xiaodong Cai (2018 
 <doi:10.1101/466623>).",2019-12-04,Xin Zhou,https://github.com/Ivis4ml/fssemR,TRUE,https://github.com/ivis4ml/fssemr,13038,2,2021-04-09T22:14:46Z,6519
fst,"Multithreaded serialization of compressed data frames using the
    'fst' format. The 'fst' format allows for random access of stored data and
    compression with the LZ4 and ZSTD compressors created by Yann Collet. The ZSTD
    compression library is owned by Facebook Inc.",2020-08-27,Mark Klik,http://www.fstpackage.org,TRUE,https://github.com/fstpackage/fst,392400,539,2021-04-22T21:55:36Z,728.0148423005566
fstcore,"The 'fstlib' library provides multithreaded serialization of compressed data frames using the
    'fst' format. The 'fst' format allows for random access of stored data and compression with the 'LZ4' and 'ZSTD'
    compressors.",2021-01-05,Mark Klik,https://www.fstpackage.org/fstcore/,TRUE,https://github.com/fstpackage/fst,6757,539,2021-04-22T21:55:36Z,12.536178107606679
ftExtra,"Build display tables easily by extending the functionality of the
    'flextable' package. Features include spanning header, grouping rows,
    parsing markdown and so on.",2021-03-28,Atsushi Yasumoto,"https://ftextra.atusy.net, https://github.com/atusy/ftExtra",TRUE,https://github.com/atusy/ftextra,28118,40,2021-06-04T14:13:55Z,702.95
fullROC,"Enable researchers to adjust identification rates using the 1/(lineup size) method, generate the full receiver operating characteristic (ROC) curves, and statistically compare the area under the curves (AUC). 
  References: Yueran Yang & Andrew Smith. (2020). ""fullROC: An R package for generating and analyzing eyewitness-lineup ROC curves"". <doi:10.13140/RG.2.2.20415.94885/1>  ,
              Andrew Smith, Yueran Yang, & Gary Wells. (2020). ""Distinguishing between investigator discriminability and eyewitness discriminability: A method for creating full receiver operating characteristic curves of lineup identification performance"". Perspectives on Psychological Science, 15(3), 589-607. <doi:10.1177/1745691620902426>.",2021-01-13,Yueran Yang,NA,TRUE,https://github.com/yuerany/fullroc,3593,0,2021-08-20T18:29:01Z,NA
fun,"This is a collection of R games and other funny stuff, such as the
    classic Mine sweeper and sliding puzzles.",2020-10-23,Yihui Xie,https://github.com/yihui/fun,TRUE,https://github.com/yihui/fun,47111,46,2020-11-23T14:59:13Z,1024.1521739130435
funcharts,"Provides functional control charts 
    for statistical process monitoring of functional data, 
    using the methods of Capezza et al. (2020) <doi:10.1002/asmb.2507> and 
    Centofanti et al. (2020) <doi:10.1080/00401706.2020.1753581>.",2021-03-15,Christian Capezza,https://github.com/unina-sfere/funcharts,TRUE,https://github.com/unina-sfere/funcharts,1575,0,2021-06-24T11:14:17Z,NA
FuncNN,"A collection of functions which fit functional neural network models. In
            other words, this package will allow users to build deep learning models 
            that have either functional or scalar responses paired with functional and 
            scalar covariates. We implement the theoretical discussion found 
            in Thind, Multani and Cao (2020) <arXiv:2006.09590> through the help of a main fitting and 
            prediction function as well as a number of helper functions to assist with 
            cross-validation, tuning, and the display of estimated functional weights.",2020-09-15,Barinder Thind,"https://arxiv.org/abs/2006.09590, https://github.com/b-thi/FuncNN",TRUE,https://github.com/b-thi/funcnn,4333,3,2020-09-07T22:37:30Z,1444.3333333333333
funData,"S4 classes for univariate and multivariate functional data with
    utility functions. See <doi:10.18637/jss.v093.i05> for a detailed description 
    of the package functionalities and its interplay with the MFPCA package for 
    multivariate functional principal component analysis 
    <https://CRAN.R-project.org/package=MFPCA>. ",2021-08-04,Clara Happ-Kurz,https://github.com/ClaraHapp/funData,TRUE,https://github.com/clarahapp/fundata,24755,9,2021-08-02T17:27:31Z,2750.5555555555557
fundiversity,"Computes 5 alpha-functional diversity indices: Functional
  Divergence (FDiv), Function Evenness (FEve), Functional Richness (FRic),
  Functional Dispersion (FDis) and Rao's entropy (Q) (reviewed in Villéger
  et al. 2008 <doi:10.1890/07-1206.1>). Provides efficient and modular functions
  to compute functional diversity indices.",2021-05-14,Matthias Grenié,"https://bisaloo.github.io/fundiversity/,
https://github.com/bisaloo/fundiversity",TRUE,https://github.com/bisaloo/fundiversity,2578,3,2021-09-02T13:16:46Z,859.3333333333334
FunnelPlotR,"An implementation of methods presented by Spiegelhalter (2005) <doi:10.1002/sim.1970> Funnel plots for comparing institutional performance, for standardised ratios, ratios of counts and proportions with additive overdispersion adjustment.",2020-11-30,Chris Mainey,"https://chrismainey.github.io/FunnelPlotR/,
https://github.com/chrismainey/FunnelPlotR",TRUE,https://github.com/chrismainey/funnelplotr,13899,30,2021-08-26T11:54:52Z,463.3
funr,"A small utility which wraps Rscript and provides access to all R
    functions from the shell.",2016-04-19,Sahil Seth,https://github.com/sahilseth/funr,TRUE,https://github.com/sahilseth/funr,22364,33,2021-02-28T04:43:22Z,677.6969696969697
funrar,"Computes functional rarity indices as proposed by Violle et al.
    (2017) <doi:10.1016/j.tree.2017.02.002>. Various indices can be computed
    using both regional and local information. Functional Rarity combines both
    the functional aspect of rarity as well as the extent aspect of rarity.
    'funrar' is presented in Grenié et al. (2017) <doi:10.1111/ddi.12629>.",2020-04-20,Matthias Grenié,https://rekyt.github.io/funrar/,TRUE,https://github.com/rekyt/funrar,26777,12,2020-12-05T08:17:38Z,2231.4166666666665
furrr,"Implementations of the family of map() functions
    from 'purrr' that can be resolved using any 'future'-supported
    backend, e.g. parallel on the local machine or distributed on a
    compute cluster.",2021-06-25,Davis Vaughan,https://github.com/DavisVaughan/furrr,TRUE,https://github.com/davisvaughan/furrr,1573337,583,2021-06-25T17:52:22Z,2698.6912521440822
fusen,"Use Rmarkdown First method to build your package. Start your
    package with documentation, functions, examples and tests in the same
    unique file. Everything can be set from the Rmarkdown template file
    provided in your project, then inflated as a package. Inflating the
    template copies the relevant chunks and sections in the appropriate
    files required for package development.",2021-08-04,Sebastien Rochette,"https://thinkr-open.github.io/fusen/,
https://github.com/Thinkr-open/fusen",TRUE,https://github.com/thinkr-open/fusen,1003,58,2021-08-04T13:11:53Z,17.29310344827586
fusionclust,"Provides the Big Merge Tracker and COSCI algorithms for convex clustering and 
    feature screening using L1 fusion penalty. See Radchenko, P. and Mukherjee, G. (2017) <doi:10.1111/rssb.12226> and 
    T.Banerjee et al. (2017) <doi:10.1016/j.jmva.2017.08.001> for more details.",2017-09-19,Trambak Banerjee,https://github.com/trambakbanerjee/fusionclust,TRUE,https://github.com/trambakbanerjee/fusionclust,13498,1,2021-06-17T21:56:56Z,13498
futility,"Randomized clinical trials commonly follow participants for a time-to-event efficacy endpoint for a fixed period of time. Consequently, at the time when the last enrolled participant completes their follow-up, the number of observed endpoints is a random variable. Assuming data collected through an interim timepoint, simulation-based estimation and inferential procedures in the standard right-censored failure time analysis framework are conducted for the distribution of the number of endpoints--in total as well as by treatment arm--at the end of the follow-up period. The future (i.e., yet unobserved) enrollment, endpoint, and dropout times are generated according to mechanisms specified in the simTrial() function in the 'seqDesign' package. A Bayesian model for the endpoint rate, offering the option to specify a robust mixture prior distribution, is used for generating future data (see the vignette for details). Inference can be restricted to participants who received treatment according to the protocol and are observed to be at risk for the endpoint at a specified timepoint. Plotting functions are provided for graphical display of results.",2019-04-11,Michal Juraska,https://github.com/mjuraska/futility,TRUE,https://github.com/mjuraska/futility,17832,1,2021-08-11T17:58:06Z,17832
future,"The purpose of this package is to provide a lightweight and
    unified Future API for sequential and parallel processing of R
    expression via futures.  The simplest way to evaluate an expression
    in parallel is to use `x %<-% { expression }` with `plan(multisession)`.
    This package implements sequential, multicore, multisession, and
    cluster futures.  With these, R expressions can be evaluated on the
    local machine, in parallel a set of local machines, or distributed
    on a mix of local and remote machines.
    Extensions to this package implement additional backends for
    processing futures via compute cluster schedulers, etc.
    Because of its unified API, there is no need to modify any code in order
    switch from sequential on the local machine to, say, distributed
    processing on a remote compute cluster.
    Another strength of this package is that global variables and functions
    are automatically identified and exported as needed, making it
    straightforward to tweak existing code to make use of futures.",2021-08-25,Henrik Bengtsson,"https://future.futureverse.org,
https://github.com/HenrikBengtsson/future",TRUE,https://github.com/henrikbengtsson/future,3347408,789,2021-09-03T08:45:15Z,4242.595690747782
future.apply,"Implementations of apply(), by(), eapply(), lapply(), Map(), .mapply(), mapply(), replicate(), sapply(), tapply(), and vapply() that can be resolved using any future-supported backend, e.g. parallel on the local machine or distributed on a compute cluster.  These future_*apply() functions come with the same pros and cons as the corresponding base-R *apply() functions but with the additional feature of being able to be processed via the future framework.",2021-08-10,Henrik Bengtsson,"https://future.apply.futureverse.org,
https://github.com/HenrikBengtsson/future.apply",TRUE,https://github.com/henrikbengtsson/future.apply,962618,169,2021-08-12T13:08:33Z,5695.96449704142
future.batchtools,"Implementation of the Future API on top of the 'batchtools' package.
    This allows you to process futures, as defined by the 'future' package,
    in parallel out of the box, not only on your local machine or ad-hoc
    cluster of machines, but also via high-performance compute ('HPC') job
    schedulers such as 'LSF', 'OpenLava', 'Slurm', 'SGE', and 'TORQUE' / 'PBS',
    e.g. 'y <- future.apply::future_lapply(files, FUN = process)'.",2021-01-04,Henrik Bengtsson,https://github.com/HenrikBengtsson/future.batchtools,TRUE,https://github.com/henrikbengtsson/future.batchtools,73558,72,2021-05-18T02:50:27Z,1021.6388888888889
future.callr,"Implementation of the Future API on top of the 'callr' package.  This allows you to process futures, as defined by the 'future' package, in parallel out of the box, on your local (Linux, macOS, Windows, ...) machine.  Contrary to backends relying on the 'parallel' package (e.g. 'future::multisession') and socket connections, the 'callr' backend provided here can run more than 125 parallel R processes.",2021-05-04,Henrik Bengtsson,"https://future.callr.futureverse.org,
https://github.com/HenrikBengtsson/future.callr",TRUE,https://github.com/henrikbengtsson/future.callr,65264,48,2021-05-18T02:48:29Z,1359.6666666666667
future.tests,"Backends implementing the 'Future' API, as defined by the 'future' package, should use the tests provided by this package to validate that they meet the minimal requirements of the 'Future' API.  The tests can be performed easily from within R or from outside of R from the command line making it easy to include them package tests and in Continuous Integration (CI) pipelines.",2020-03-20,Henrik Bengtsson,https://github.com/HenrikBengtsson/future.tests,TRUE,https://github.com/henrikbengtsson/future.tests,7929,9,2021-08-25T15:10:09Z,881
fuzzywuzzyR,Fuzzy string matching implementation of the 'fuzzywuzzy' <https://github.com/seatgeek/fuzzywuzzy> 'python' package. It uses the Levenshtein Distance <https://en.wikipedia.org/wiki/Levenshtein_distance> to calculate the differences between sequences. ,2021-05-07,Lampros Mouselimis,https://github.com/mlampros/fuzzywuzzyR,TRUE,https://github.com/mlampros/fuzzywuzzyr,34014,29,2021-05-07T12:27:53Z,1172.896551724138
fwildclusterboot,"Implementation of the fast algorithm for wild cluster bootstrap 
             inference developed in Roodman et al (2019, STATA Journal) for 
             linear regression models 
             <https://journals.sagepub.com/doi/full/10.1177/1536867X19830877>, 
             which makes it feasible to quickly calculate bootstrap test 
             statistics based on a large number of bootstrap draws even for 
             large samples - as long as the number of bootstrapping clusters 
             is not too large. Multiway clustering, regression weights, 
             bootstrap weights, fixed effects and subcluster bootstrapping
             are supported. Further, both restricted (WCR) and unrestricted
             (WCU) bootstrap are supported. Methods are provided for a variety 
             of fitted models, including 'lm()', 'feols()' 
             (from package 'fixest') and 'felm()' (from package 'lfe').",2021-08-01,Alexander Fischer,https://s3alfisc.github.io/fwildclusterboot/,TRUE,https://github.com/s3alfisc/fwildclusterboot,3490,8,2021-08-29T16:25:36Z,436.25
fxTWAPLS,"The goal of this package is to provide an improved version of 
    WA-PLS (Weighted Averaging Partial Least Squares) by including the 
    tolerances of taxa and the frequency of the sampled climate variable. 
    This package also provides a way of leave-out cross-validation that 
    removes both the test site and sites that are both geographically 
    close and climatically close for each cycle, to avoid the risk of 
    pseudo-replication.",2021-09-01,Mengmeng Liu,"https://github.com/special-uor/fxTWAPLS/,
https://special-uor.github.io/fxTWAPLS/,
https://research.reading.ac.uk/palaeoclimate/",TRUE,https://github.com/special-uor/fxtwapls,4541,0,2021-08-31T20:12:52Z,NA
g3viz,"R interface for 'g3-lollipop' JavaScript library.
    Visualize genetic mutation data using an interactive lollipop diagram in RStudio or your browser.",2021-03-09,Xin Guo,https://github.com/G3viz/g3viz,TRUE,https://github.com/g3viz/g3viz,21786,17,2021-02-27T21:59:49Z,1281.5294117647059
GA,"Flexible general-purpose toolbox implementing genetic algorithms (GAs) for stochastic optimisation. Binary, real-valued, and permutation representations are available to optimize a fitness function, i.e. a function provided by users depending on their objective function. Several genetic operators are available and can be combined to explore the best settings for the current task. Furthermore, users can define new genetic operators and easily evaluate their performances. Local search using general-purpose optimisation algorithms can be applied stochastically to exploit interesting regions. GAs can be run sequentially or in parallel, using an explicit master-slave parallelisation or a coarse-grain islands approach.",2021-04-21,Luca Scrucca,https://luca-scr.github.io/GA/,TRUE,https://github.com/luca-scr/ga,210752,62,2021-04-26T07:22:30Z,3399.2258064516127
GADMTools,"Manipulate, assemble, export <https://gadm.org/> maps. Create 'choropleth', 'isopleth', dots plot, proportional dots,
   dot-density and more.",2021-08-05,Jean Pierre Decorps,https://github.com/IamKDO/GADMTools,TRUE,https://github.com/iamkdo/gadmtools,33805,2,2021-08-18T19:06:23Z,16902.5
galah,"The Atlas of Living Australia ('ALA') provides tools to enable users
    of biodiversity information to find, access, combine and visualise data on
    Australian plants and animals; these have been made available from
    <https://ala.org.au/>. 'galah' provides a subset of the tools to be
    directly used within R. It enables the R community to directly access data
    and resources hosted by the 'ALA'.",2021-08-21,Martin Westgate,https://github.com/AtlasOfLivingAustralia/galah,TRUE,https://github.com/atlasoflivingaustralia/galah,2761,8,2021-08-19T02:57:37Z,345.125
GALLO,"The accurate annotation of genes and Quantitative Trait Loci (QTLs) located within candidate markers and/or regions (haplotypes, windows, CNVs, etc) is a crucial step the most common genomic analyses performed in livestock, such as Genome-Wide Association Studies or transcriptomics. The Genomic Annotation in Livestock for positional candidate LOci (GALLO) is an R package designed to provide an intuitive and straightforward environment to annotate positional candidate genes and QTLs from high-throughput genetic studies in livestock. Moreover, GALLO allows the graphical visualization of gene and QTL annotation results, data comparison among different grouping factors (e.g., methods, breeds, tissues, statistical models, studies, etc.), and QTL enrichment in different livestock species including cattle, pigs, sheep, and chicken, among others.",2021-04-08,Pablo Fonseca,<https://github.com/pablobio/GALLO>,TRUE,https://github.com/pablobio/gallo,6725,3,2021-02-26T18:59:33Z,2241.6666666666665
galvanizer,An R interface to the Galvanize 'Highbond' API <https://docs-apis.highbond.com>.,2021-05-28,Jonathan Lin,"https://jonlinca.github.io/galvanizer/,
https://github.com/jonlinca/galvanizer",TRUE,https://github.com/jonlinca/galvanizer,2812,0,2021-05-28T16:19:36Z,NA
gambin,"Fits unimodal and multimodal gambin distributions to species-abundance distributions
    from ecological data, as in in Matthews et al. (2014) <DOI:10.1111/ecog.00861>. 
    'gambin' is short for 'gamma-binomial'. The main function is fit_abundances(), which estimates 
    the 'alpha' parameter(s) of the gambin distribution using maximum likelihood. Functions are 
    also provided to generate the gambin distribution and for calculating likelihood statistics.",2021-04-16,Thomas Matthews,https://github.com/txm676/gambin/,TRUE,https://github.com/txm676/gambin,23782,3,2021-04-16T17:52:22Z,7927.333333333333
gamboostLSS,"Boosting models for fitting generalized additive models for
  location, shape and scale ('GAMLSS') to potentially high dimensional
  data.",2021-01-20,Benjamin Hofner,"For source code, development versions and issue tracker see
https://github.com/boost-R/gamboostLSS",TRUE,https://github.com/boost-r/gamboostlss,93788,20,2021-07-23T13:11:16Z,4689.4
gamlr,"The gamma lasso algorithm provides regularization paths corresponding to a range of non-convex cost functions between L0 and L1 norms.  As much as possible, usage for this package is analogous to that for the glmnet package (which does the same thing for penalization between L1 and L2 norms).  For details see: Taddy (2017 JCGS), 'One-Step Estimator Paths for Concave Regularization', <arXiv:1308.5623>.",2021-05-07,Matt Taddy,https://github.com/TaddyLab/gamlr,TRUE,https://github.com/taddylab/gamlr,42636,21,2021-05-07T06:09:53Z,2030.2857142857142
gamma,"Process in-situ Gamma-Ray Spectrometry for
    Luminescence Dating. This package allows to import, inspect and
    correct the energy shifts of Gamma-ray spectra. It provides methods
    for estimating the gamma dose rate by the use of a calibration curve.
    The package only supports Canberra CNF and TKA files.",2021-01-13,Nicolas Frerebeau,"https://crp2a.github.io/gamma/, https://github.com/crp2a/gamma",TRUE,https://github.com/crp2a/gamma,4611,6,2021-08-28T12:25:38Z,768.5
gap,"It is designed as an integrated package for genetic data
        analysis of both population and family data. Currently, it
        contains functions for sample size calculations of both
        population-based and family-based designs, probability of
        familial disease aggregation, kinship calculation, statistics
        in linkage analysis, and association analysis involving genetic
        markers including haplotype analysis with or without environmental
        covariates. Over years, the package has been developed in-between
        many projects hence also in line with the name (gap).",2021-04-21,"Jing Hua Zhao and colleagues with inputs from Kurt Hornik and
        Brian Ripley",https://github.com/jinghuazhao/R,TRUE,https://github.com/jinghuazhao/r,150881,6,2021-08-27T21:09:36Z,25146.833333333332
GapAnalysis,"Supports the assessment of the degree of conservation of taxa in conservation systems, 
  both in ex situ [in genebanks, botanical gardens, and other repositories] and in situ [in protected natural areas]. Methods are
  described in Carver et al. [2021] <doi:10.1111/ecog.05430>, building on Khoury et al. [2020] <doi:10.1073/pnas.2007029117>, 
  Khoury et al. [2019] <doi:10.1016/j.ecolind.2018.11.016>, Khoury et al. [2019] <doi:10.1111/DDI.13008>, 
  Castaneda-Alvarez et al. [2016] <doi:10.1038/nplants.2016.22>, and Ramirez-Villegas et al. [2010] <doi:10.1371/journal.pone.0013497>.",2021-06-14,Dan Carver,https://github.com/CIAT-DAPA/GapAnalysis,TRUE,https://github.com/ciat-dapa/gapanalysis,3543,3,2021-04-28T18:40:52Z,1181
gapfill,"Tools to fill missing values in satellite data and to develop new
    gap-fill algorithms. The methods are tailored to data (images) observed
    at equally-spaced points in time. The package is illustrated with MODIS
    NDVI data.",2021-02-12,Florian Gerber,https://github.com/florafauna/gapfill,TRUE,https://github.com/florafauna/gapfill,20738,0,2021-02-11T17:45:06Z,NA
gapmap,"The gap encodes the distance between clusters and improves
    interpretation of cluster heatmaps. The gaps can be of the same
    distance based on a height threshold to cut the dendrogram. Another
    option is to vary the size of gaps based on the distance between
    clusters.",2021-04-19,Evan Biederstedt,https://github.com/evanbiederstedt/gapmap,TRUE,https://github.com/evanbiederstedt/gapmap,11926,1,2021-04-20T01:18:20Z,11926
garchx,"Flexible and robust estimation and inference of generalised autoregressive conditional heteroscedasticity (GARCH) models with covariates ('X') based on the results by Francq and Thieu (2018) <doi:10.1017/S0266466617000512>. Coefficients can straightforwardly be set to zero by omission, and quasi maximum likelihood methods ensure estimates are generally consistent and inference valid, even when the standardised innovations are non-normal and/or dependent over time.",2021-07-15,Genaro Sucarrat,http://www.sucarrat.net/,TRUE,https://github.com/gsucarrat/garchx,10937,1,2021-07-20T15:51:30Z,10937
gargle,"Provides utilities for working with Google APIs
    <https://developers.google.com/apis-explorer>.  This includes
    functions and classes for handling common credential types and for
    preparing, executing, and processing HTTP requests.",2021-07-02,Jennifer Bryan,"https://gargle.r-lib.org, https://github.com/r-lib/gargle",TRUE,https://github.com/r-lib/gargle,8641955,94,2021-07-23T04:51:58Z,91935.6914893617
garma,"Methods for estimating univariate long memory-seasonal/cyclical
             Gegenbauer time series processes. See for example (2018) <doi:10.1214/18-STS649>.
             Refer to the vignette for details of fitting these processes.",2021-07-22,Richard Hunt,https://github.com/rlph50/garma,TRUE,https://github.com/rlph50/garma,7125,1,2021-07-21T20:57:52Z,7125
gasper,"Provides the standard operations for signal processing on graphs: 
    graph Fourier transform, spectral graph wavelet transform, 
    visualization tools. It also implements a data driven method
    for graph signal denoising/regression, for details see 
    De Loynes, Navarro, Olivier (2019) <arxiv:1906.01882>. 
    The package also provides an interface to the SuiteSparse Matrix Collection, 
    <https://sparse.tamu.edu/>, a large and widely used set of sparse matrix 
    benchmarks collected from a wide range of applications.",2021-02-16,Fabien Navarro,https://github.com/fabnavarro/gasper,TRUE,https://github.com/fabnavarro/gasper,5560,3,2021-02-17T21:21:18Z,1853.3333333333333
gastempt,"Fits gastric emptying time series from MRI or 'scintigraphic' measurements
   using nonlinear mixed-model population fits with 'nlme' and Bayesian methods with 
   Stan; computes derived parameters such as t50 and AUC.",2021-03-09,Dieter Menne,https://github.com/dmenne/gastempt,TRUE,https://github.com/dmenne/gastempt,15541,3,2021-05-25T09:04:42Z,5180.333333333333
gateR,"Estimates statistically significant marker combination values within
        which one immunologically distinctive group (i.e., disease case) is more associated than
        another group (i.e., healthy control), successively, using various combinations (i.e.,
        ""gates"") of markers to examine features of cells that may be different between
        groups. For a two-group comparison, the 'gateR' package uses the spatial relative risk
        function that is estimated using the 'sparr' package. Details about the 'sparr' package
        methods can be found in the tutorial: Davies et al. (2018) <doi:10.1002/sim.7577>. Details
        about kernel density estimation can be found in J. F. Bithell (1990) <doi:10.1002/sim.4780090616>.
        More information about relative risk functions using kernel density estimation can be
        found in J. F. Bithell (1991) <doi:10.1002/sim.4780101112>.",2021-06-25,Ian D. Buller,https://github.com/Waller-SUSAN/gateR,TRUE,https://github.com/waller-susan/gater,4789,1,2021-08-31T17:22:48Z,4789
GauPro,"Fits a Gaussian process model to data. Gaussian processes
 are commonly used in computer experiments to fit an interpolating model.
 The model is stored as an 'R6' object and can be easily updated with new 
 data. There are options to run in parallel (not for Windows), and 'Rcpp'
 has been used to speed up calculations. Other R packages that perform
 similar calculations include 'laGP', 'DiceKriging', 'GPfit', and 'mlegp'.",2021-04-11,Collin Erickson,https://github.com/CollinErickson/GauPro,TRUE,https://github.com/collinerickson/gaupro,21473,6,2021-07-20T01:37:31Z,3578.8333333333335
gaussfacts,"Display a random fact about Carl Friedrich Gauss
 based the on collection curated by Mike Cavers via the
 <http://gaussfacts.com> site.",2016-08-03,Dirk Eddelbuettel,NA,TRUE,https://github.com/eddelbuettel/gaussfacts,19467,4,2021-08-07T13:07:36Z,4866.75
gaussplotR,"
    Functions to fit two-dimensional Gaussian functions, predict values from
    fits, and produce plots of predicted data via either 'ggplot2' or base R 
    plotting.",2021-05-02,Vikram B. Baliga,https://github.com/vbaliga/gaussplotR,TRUE,https://github.com/vbaliga/gaussplotr,4575,2,2021-07-29T20:06:02Z,2287.5
gawdis,"R function gawdis() produces multi-trait dissimilarity with more uniform contributions of different traits. de Bello et al. (2021) <doi:10.1111/2041-210X.13537> presented the approach based on minimizing the differences in the correlation between the dissimilarity of each trait, or groups of traits, and the multi-trait dissimilarity. This is done using either an analytic or a numerical solution, both available in the function.",2021-05-12,Pavel Fibich,https://github.com/pavel-fibich/gawdis,TRUE,https://github.com/pavel-fibich/gawdis,4613,2,2021-05-05T13:23:09Z,2306.5
gbeta,"Density, distribution function, quantile function, and random generation for the generalized Beta and Beta prime distributions. The family of generalized Beta distributions is conjugate for the Bayesian binomial model, and the generalized Beta prime distribution is the posterior distribution of the relative risk in the Bayesian 'two Poisson samples' model when a Gamma prior is assigned to the Poisson rate of the reference group and a Beta prime prior is assigned to the relative risk. References: Laurent (2012) <doi:10.1214/11-BJPS139>, Hamza & Vallois (2016) <doi:10.1016/j.spl.2016.03.014>, Chen & Novick (1984) <doi:10.3102/10769986009002163>.",2020-11-19,Stéphane Laurent,https://github.com/stla/gbeta,TRUE,https://github.com/stla/gbeta,3226,0,2020-11-14T08:57:52Z,NA
gbfs,"Supplies a set of functions to interface with bikeshare data
    following the General Bikeshare Feed Specification, allowing users to query
    and accumulate tidy datasets for specified cities/bikeshare programs.",2021-04-03,Simon P. Couch,https://github.com/simonpcouch/gbfs,TRUE,https://github.com/simonpcouch/gbfs,20571,28,2021-04-03T14:51:36Z,734.6785714285714
GCalignR,"Aligns peak based on peak retention times and matches homologous peaks
    across samples. The underlying alignment procedure comprises three sequential steps.
    (1) Full alignment of samples by linear transformation of retention times to 
    maximise similarity among homologous peaks (2) Partial alignment of peaks within 
    a user-defined retention time window to cluster homologous peaks (3) Merging rows
    that are likely representing homologous substances (i.e. no sample shows peaks in 
    both rows and the rows have similar retention time means). The algorithm is described in detail
    in Ottensmann et al., 2018 <doi:10.1371/journal.pone.0198311>. ",2020-08-26,Meinolf Ottensmann,https://github.com/mottensmann/GCalignR,TRUE,https://github.com/mottensmann/gcalignr,18615,3,2021-08-23T15:54:44Z,6205
gclm,"Estimation of covariance matrices as solutions of 
             continuous time Lyapunov equations. 
             Sparse coefficient matrix and diagonal noise are estimated 
             with a proximal gradient 
             method for an l1-penalized loss minimization problem.
             Varando G, Hansen NR (2020) <arXiv:2005.10483>.",2020-06-04,Gherardo Varando,https://github.com/gherardovarando/gclm,TRUE,https://github.com/gherardovarando/gclm,4601,0,2020-10-30T07:17:25Z,NA
GCSM,"Provides implementation of the generic composite similarity measure
    (GCSM) described in Liu et al. (2020) <doi:10.1016/j.ecoinf.2020.101169>. The
    implementation is in C++ and uses 'RcppArmadillo'. Additionally, implementations
    of the structural similarity (SSIM) and the composite similarity measure based
    on means, standard deviations, and correlation coefficient (CMSC), are included.",2021-03-27,Yadong Liu,https://github.com/liuyadong/GCSM,TRUE,https://github.com/liuyadong/gcsm,3668,0,2021-04-10T05:35:34Z,NA
gdalcubes,"Processing collections of Earth observation images as on-demand multispectral, multitemporal raster data cubes. Users
    define cubes by spatiotemporal extent, resolution, and spatial reference system and let 'gdalcubes' automatically apply cropping, reprojection, and 
    resampling using the 'Geospatial Data Abstraction Library' ('GDAL'). Implemented functions on data cubes include reduction over space and time, 
    applying arithmetic expressions on pixel band values, moving window aggregates over time, filtering by space, time, bands, and predicates on pixel values, 
    exporting data cubes as 'netCDF' or 'GeoTIFF' files, and plotting.  The package implements lazy evaluation and 
    multithreading. All computational parts are implemented in C++, linking to the 'GDAL', 'netCDF', 'CURL', and 'SQLite' libraries. 
    See Appel and Pebesma (2019) <doi:10.3390/data4030092> for further details.",2021-07-29,Marius Appel,https://github.com/appelmar/gdalcubes_R,TRUE,https://github.com/appelmar/gdalcubes_r,18680,74,2021-08-02T08:24:29Z,252.43243243243242
gdalUtilities,"R's 'sf' package ships with self-contained 'GDAL'
    executables, including a bare bones interface to several
    'GDAL'-related utility programs collectively known as the 'GDAL
    utilities'. For each of those utilities, this package provides an
    R wrapper whose formal arguments closely mirror those of the
    'GDAL' command line interface. The utilities operate on data
    stored in files and typically write their output to other
    files. Therefore, to process data stored in any of R's more common
    spatial formats (i.e. those supported by the 'sp', 'sf', and
    'raster' packages), first write them to disk, then process them
    with the package's wrapper functions before reading the outputted
    results back into R. GDAL function arguments introduced in GDAL
    version 3.2.1 or earlier are supported.",2021-04-05,Joshua OBrien,https://github.com/JoshOBrien/gdalUtilities/,TRUE,https://github.com/joshobrien/gdalutilities,24508,22,2021-04-05T18:43:46Z,1114
GDAtools,"Contains functions for 'specific' Multiple Correspondence Analysis, 
	Class Specific Analysis, Multiple Factor Analysis, 'standardized' MCA, computing and plotting structuring factors and concentration ellipses, 
	inductive tests and others tools for Geometric Data Analysis (Le Roux & Rouanet (2005) <doi:10.1007/1-4020-2236-0>). It also provides functions
	for the translation of logit models coefficients into percentages (Deauvieau (2010) <doi:10.1177/0759106309352586>), weighted contingency tables, an association 
  measure for contingency tables (""Percentages of Maximum Deviation from Independence"", aka PEM, see Cibois (1993) <doi:10.1177/075910639304000103>) and some tools to measure 
  and plot bivariate associations between variables
  (phi, Cramér V, correlation coefficient, eta-squared...).",2021-05-31,Nicolas Robette,"https://github.com/nicolas-robette/GDAtools,
https://nicolas-robette.github.io/GDAtools/",TRUE,https://github.com/nicolas-robette/gdatools,26224,4,2021-07-14T20:46:59Z,6556
GDINA,"A set of psychometric tools for cognitive diagnosis modeling based on the generalized deterministic inputs, noisy and gate (G-DINA) model by de la Torre (2011) <DOI:10.1007/s11336-011-9207-7> and its extensions, including the sequential G-DINA model by Ma and de la Torre (2016) <DOI:10.1111/bmsp.12070> for polytomous responses, and the polytomous G-DINA model by Chen and de la Torre <DOI:10.1177/0146621613479818> for polytomous attributes. Joint attribute distribution can be independent, saturated, higher-order, loglinear smoothed or structured. Q-matrix validation, item and model fit statistics, model comparison at test and item level and differential item functioning can also be conducted. A graphical user interface is also provided. For tutorials, please check Ma and de la Torre (2020) <DOI:10.18637/jss.v093.i14>, Ma and de la Torre (2019) <DOI:10.1111/emip.12262>, Ma (2019) <DOI:10.1007/978-3-030-05584-4_29> and de la Torre and Akbay (2019) <DOI:10.14689/ejer.2019.80.9>. ",2021-05-31,Wenchao Ma,"https://github.com/Wenchao-Ma/GDINA,
https://wenchao-ma.github.io/GDINA/",TRUE,https://github.com/wenchao-ma/gdina,58855,21,2021-05-28T11:45:49Z,2802.6190476190477
gdtools,Useful tools for writing vector graphics devices.,2021-01-06,David Gohel,NA,TRUE,https://github.com/davidgohel/gdtools,1948311,23,2021-06-24T09:15:06Z,84709.17391304347
geckor,"Collect the current and historical cryptocurrency market data using 
    the public 'CoinGecko' API (<https://www.coingecko.com/en/api>).",2021-07-13,Sergey Mastitsky,https://github.com/next-game-solutions/geckor,TRUE,https://github.com/next-game-solutions/geckor,1037,8,2021-07-17T17:54:22Z,129.625
geex,"Provides a general, flexible framework for estimating parameters
    and empirical sandwich variance estimator from a set of unbiased estimating
    equations (i.e., M-estimation in the vein of Stefanski & Boos (2002)
    <doi:10.1198/000313002753631330>). All examples from Stefanski & Boos (2002)
    are published in the corresponding Journal of Statistical Software paper 
    <doi:10.18637/jss.v092.i02>. Also provides an API to compute finite-sample 
    variance corrections.",2020-02-17,Bradley Saul,"https://github.com/bsaul/geex, https://bsaul.github.io/geex/",TRUE,https://github.com/bsaul/geex,16944,5,2020-12-25T14:30:47Z,3388.8
gemma2,"Fits a multivariate linear mixed effects model that uses a polygenic term, after Zhou & Stephens (2014) (<https://www.nature.com/articles/nmeth.2848>). Of particular interest is the estimation of variance components with restricted maximum likelihood (REML) methods. Genome-wide efficient mixed-model association (GEMMA), as implemented in the package 'gemma2', uses an expectation-maximization algorithm for variance components inference for use in quantitative trait locus studies.",2020-10-24,Frederick Boehm,https://github.com/fboehm/gemma2,TRUE,https://github.com/fboehm/gemma2,10942,3,2020-10-24T14:46:58Z,3647.3333333333335
gen3sis,Contains an engine for spatially-explicit eco-evolutionary mechanistic models with a modular implementation and several support functions. It allows exploring the consequences of ecological and macroevolutionary processes across realistic or theoretical spatio-temporal landscapes on biodiversity patterns as a general term.,2021-07-12,Oskar Hagen  (Landscape Ecology,https://github.com/project-Gen3sis/R-package,TRUE,https://github.com/project-gen3sis/r-package,6890,7,2021-08-23T12:31:22Z,984.2857142857143
gender,"Infers state-recorded gender categories from first names and dates of birth using historical
    datasets. By using these datasets instead of lists of male and female names,
    this package is able to more accurately infer the gender of a name, and it
    is able to report the probability that a name was male or female. GUIDELINES:
    This method must be used cautiously and responsibly. Please be sure to see the
    guidelines and warnings about usage in the 'README' or the package documentation.
    See Blevins and Mullen (2015) <http://www.digitalhumanities.org/dhq/vol/9/3/000223/000223.html>.",2020-05-15,Lincoln Mullen,"https://docs.ropensci.org/gender/,
https://github.com/ropensci/gender",TRUE,https://github.com/ropensci/gender,171824,157,2021-03-21T01:18:56Z,1094.4203821656051
genderBR,"A method to predict and report gender from Brazilian first names
    using the Brazilian Institute of Geography and Statistics' Census data (<https://censo2010.ibge.gov.br/nomes/>).",2021-05-02,Fernando Meireles,https://github.com/meirelesff/genderBR,TRUE,https://github.com/meirelesff/genderbr,19418,45,2021-04-30T13:09:52Z,431.5111111111111
geneExpressionFromGEO,"A function that reads in the GEO code of a gene expression dataset, retrieves its data from GEO, (optional) retrieves the gene symbols of the dataset, and returns a simple dataframe table containing all the data. Platforms available: GPL11532, GPL23126, GPL6244, GPL80, GPL8300, GPL80, GPL96, GPL570, GPL571, GPL20115, GPL1293,  GPL6102, GPL6104, GPL6883, GPL6884, GPL13497, GPL14550, GPL17077, GPL6480. GEO: Gene Expression Omnibus. ID: identifier code. The GEO datasets are downloaded from the URL <https://ftp.ncbi.nlm.nih.gov/geo/series/>.",2021-04-24,Davide Chicco,https://github.com/davidechicco/geneExpressionFromGEO,TRUE,https://github.com/davidechicco/geneexpressionfromgeo,3964,1,2021-06-23T18:32:40Z,3964
GeneralizedUmatrix,"Projections are common dimensionality reduction methods, which represent high-dimensional data in a two-dimensional space. However, when restricting the output space to two dimensions, which results in a two dimensional scatter plot (projection) of the data, low dimensional similarities do not represent high dimensional distances coercively [Thrun, 2018] <DOI: 10.1007/978-3-658-20540-9>. This could lead to a misleading interpretation of the underlying structures [Thrun, 2018]. By means of the 3D topographic map the generalized Umatrix is able to depict errors of these two-dimensional scatter plots. The package is derived from the book of Thrun, M.C.: ""Projection Based Clustering through Self-Organization and Swarm Intelligence"" (2018) <DOI:10.1007/978-3-658-20540-9> and the main algorithm called simplified self-organizing map for dimensionality reduction methods is published in <DOI: 10.1016/j.mex.2020.101093>.",2021-01-12,Michael Thrun,http://www.deepbionics.org,TRUE,https://github.com/mthrun/generalizedumatrix,24658,0,2021-06-23T14:02:33Z,NA
generics,"In order to reduce potential package dependencies
    and conflicts, generics provides a number of commonly used S3
    generics.",2020-10-31,Hadley Wickham,"https://generics.r-lib.org, https://github.com/r-lib/generics",TRUE,https://github.com/r-lib/generics,14010787,53,2021-06-28T19:48:46Z,264354.4716981132
GeNetIt,"Implementation of spatial graph-theoretic genetic gravity models.
    The model framework is applicable for other types of spatial flow questions.
    Includes functions for constructing spatial graphs, sampling and summarizing
    associated raster variables and building unconstrained and singly constrained
    gravity models.",2020-04-01,Jeffrey S. Evans,https://github.com/jeffreyevans/GeNetIt,TRUE,https://github.com/jeffreyevans/genetit,18327,1,2021-08-06T19:15:37Z,18327
genieclust,"A retake on the Genie algorithm - a robust
    hierarchical clustering method
    (Gagolewski, Bartoszuk, Cena, 2016 <DOI:10.1016/j.ins.2016.05.003>).
    Now faster and more memory efficient; determining the whole hierarchy
    for datasets of 10M points in low dimensional Euclidean spaces or
    100K points in high-dimensional ones takes only 1-2 minutes.
    Allows clustering with respect to mutual reachability distances
    so that it can act as a noise point detector or a robustified version of
    'HDBSCAN*' (that is able to detect a predefined number of
    clusters and hence it does not dependent on the somewhat
    fragile 'eps' parameter).
    The package also features an implementation of economic inequity indices
    (the Gini, Bonferroni index) and external cluster validity measures
    (partition similarity scores; e.g., the adjusted Rand, Fowlkes-Mallows,
    adjusted mutual information, pair sets index).
    See also the 'Python' version of 'genieclust' available on 'PyPI', which
    supports sparse data, more metrics, and even larger datasets.",2021-04-22,Marek Gagolewski,https://genieclust.gagolewski.com/,TRUE,https://github.com/gagolews/genieclust,8749,33,2021-08-21T07:24:57Z,265.1212121212121
genio,"Implements readers and writers for file formats associated with genetics data.  Reading and writing Plink BED/BIM/FAM and GCTA binary GRM formats is fully supported, including a lightning-fast BED reader and writer implementations.  Other functions are 'readr' wrappers that are more constrained, user-friendly, and efficient for these particular applications; handles Plink and Eigenstrat tables (FAM, BIM, IND, and SNP files).  There are also make functions for FAM and BIM tables with default values to go with simulated genotype data.",2021-07-26,Alejandro Ochoa,https://github.com/OchoaLab/genio,TRUE,https://github.com/ochoalab/genio,14125,8,2021-08-18T02:49:19Z,1765.625
genius,Easily access song lyrics in a tidy way.,2021-07-24,Josiah Parry,https://github.com/josiahparry/genius,TRUE,https://github.com/josiahparry/genius,25002,117,2021-07-24T16:31:29Z,213.69230769230768
GenomeAdmixR,"Individual-based simulations forward in time,
    simulating how patterns in ancestry along the genome change after
    admixture. Full description can be found in Janzen (2020)
    <doi:10.1101/2020.10.19.343491>.",2021-03-30,Thijs Janzen,https://github.com/thijsjanzen/GenomeAdmixR,TRUE,https://github.com/thijsjanzen/genomeadmixr,3296,3,2021-03-30T14:03:41Z,1098.6666666666667
geobr,"Easy access to official spatial data sets of Brazil as 'sf' objects 
             in R. The package includes a wide range of geospatial data available
             at various geographic scales and for various years with harmonized
             attributes, projection and fixed topology.",2021-07-22,Rafael H. M. Pereira,https://github.com/ipeaGIT/geobr,TRUE,https://github.com/ipeagit/geobr,40032,473,2021-08-18T17:52:10Z,84.63424947145877
geocmeans,"Provides functions to apply spatial fuzzy unsupervised classification, visualize and interpret results. This method is well suited when the user wants to analyze data with a fuzzy clustering algorithm and to account for the spatial dimension of the dataset. In addition, indexes for estimating the spatial consistency and classification quality are proposed.
    The methods were originally proposed in the field of brain imagery (seed Cai and al. 2007 <doi:10.1016/j.patcog.2006.07.011> and Zaho and al. 2013 <doi:10.1016/j.dsp.2012.09.016>) and recently applied in geography (see Gelb and Apparicio <doi:10.4000/cybergeo.36414>).",2021-08-23,Jeremy Gelb,https://github.com/JeremyGelb/geocmeans,TRUE,https://github.com/jeremygelb/geocmeans,1681,12,2021-08-23T12:11:00Z,140.08333333333334
geodaData,"Stores small spatial datasets used to teach basic spatial analysis
    concepts. Datasets are based off of the 'GeoDa' software workbook and data
    site <https://geodacenter.github.io/data-and-lab/> developed by Luc Anselin
    and team at the University of Chicago. Datasets are stored as 'sf' objects.",2020-05-27,Angela Li,https://github.com/spatialanalysis/geodaData,TRUE,https://github.com/spatialanalysis/geodadata,5354,14,2020-10-05T19:06:42Z,382.42857142857144
geodata,"Functions for downloading of geographic data for use in spatial data analysis and mapping. The package facilitates access to climate, elevation, soil, species occurrence, and administrative boundary data.",2021-05-31,Robert J. Hijmans,NA,TRUE,https://github.com/rspatial/geodata,1368,7,2021-08-28T12:48:39Z,195.42857142857142
GeodesiCL,"Geometric geodesy functions applied to most common ellipsoids. This package was created to streamline and facilitate their work for surveyors, geographers, and everything related to geosciences.",2021-05-25,Diego Alonso Alarcon Diaz,https://github.com/diegoalarc/GeodesiCL,TRUE,https://github.com/diegoalarc/geodesicl,1374,6,2021-05-28T22:08:58Z,229
geodist,"Dependency-free, ultra fast calculation of geodesic
    distances.  Includes the reference nanometre-accuracy geodesic
    distances of Karney (2013) <doi:10.1007/s00190-012-0578-z>, as used by
    the 'sf' package, as well as Haversine and Vincenty distances. Default
    distance measure is the ""Mapbox cheap ruler"" which is generally more
    accurate than Haversine or Vincenty for distances out to a few hundred
    kilometres, and is considerably faster. The main function accepts one
    or two inputs in almost any generic rectangular form, and returns
    either matrices of pairwise distances, or vectors of sequential
    distances.",2021-01-27,Mark Padgham,https://github.com/hypertidy/geodist,TRUE,https://github.com/hypertidy/geodist,46402,78,2021-05-25T12:07:11Z,594.8974358974359
geodiv,"Methods for calculating gradient surface metrics for
    continuous analysis of landscape features. ",2021-09-03,Annie C. Smith,https://github.com/bioXgeo/geodiv,TRUE,https://github.com/bioxgeo/geodiv,10675,6,2021-09-01T21:27:28Z,1779.1666666666667
geodrawr,"Draw geospatial objects by clicks on the map.
    This packages can help data analyst who want to check
    their own geospatial hypothesis but has no ready-made geospatial objects.",2020-11-08,Heoncheol Ha,https://github.com/Curycu/geodrawr,TRUE,https://github.com/curycu/geodrawr,5370,2,2021-08-21T09:40:11Z,2685
GeodRegr,"Provides a gradient descent algorithm to find a geodesic relationship between real-valued independent variables and a manifold-valued dependent variable (i.e. geodesic regression). Available manifolds are Euclidean space, the sphere, hyperbolic space, and Kendall's 2-dimensional shape space. Besides the standard least-squares loss, the least absolute deviations, Huber, and Tukey biweight loss functions can also be used to perform robust geodesic regression. Functions to help choose appropriate cutoff parameters to maintain high efficiency for the Huber and Tukey biweight estimators are included, as are functions for generating random tangent vectors from the Riemannian normal distributions on the sphere and hyperbolic space. The n-sphere is a n-dimensional manifold: we represent it as a sphere of radius 1 and center 0 embedded in (n+1)-dimensional space. Using the hyperboloid model of hyperbolic space, n-dimensional hyperbolic space is embedded in (n+1)-dimensional Minkowski space as the upper sheet of a hyperboloid of two sheets. Kendall's 2D shape space with K landmarks is of real dimension 2K-4; preshapes are represented as complex K-vectors with mean 0 and magnitude 1. Details are described in Shin, H.-Y. and Oh, H.-S. (2020) <arXiv:2007.04518>. Also see Fletcher, P. T. (2013) <doi:10.1007/s11263-012-0591-y>.",2021-09-03,Ha-Young Shin,https://github.com/hayoungshin1/GeodRegr,TRUE,https://github.com/hayoungshin1/geodregr,3597,0,2021-08-28T04:19:56Z,NA
geoFKF,"A Kriging method for  functional datasets with spatial dependency.
    This functional Kriging method avoids the need to estimate the
    trace-variogram, and the curve is estimated by minimizing a quadratic
    form. The curves in the functional dataset are smoothed using Fourier
    series. The functional Kriging of this package is a modification of the
    method proposed by Giraldo (2011) <doi:10.1007/s10651-010-0143-y>.",2020-11-02,Gilberto Sassi,https://github.com/gilberto-sassi/geoFKF,TRUE,https://github.com/gilberto-sassi/geofkf,3421,0,2020-10-23T15:06:02Z,NA
geojsonio,"Convert data to 'GeoJSON' or 'TopoJSON' from various R classes,
    including vectors, lists, data frames, shape files, and spatial classes.
    'geojsonio' does not aim to replace packages like 'sp', 'rgdal', 'rgeos',
    but rather aims to be a high level client to simplify conversions of data
    from and to 'GeoJSON' and 'TopoJSON'.",2021-01-13,Scott Chamberlain,"https://github.com/ropensci/geojsonio (devel),
https://docs.ropensci.org/geojsonio/ (docs)",TRUE,https://github.com/ropensci/geojsonio,339393,134,2021-01-13T16:20:12Z,2532.783582089552
geojsonlint,"Tools for linting 'GeoJSON'. Includes tools for interacting with the
    online tool <http://geojsonlint.com>, the 'Javascript' library 'geojsonhint'
    (<https://www.npmjs.com/package/geojsonhint>), and validating against a
    'GeoJSON' schema via the 'Javascript' library
    (<https://www.npmjs.com/package/is-my-json-valid>). Some tools work locally
    while others require an internet connection.",2020-02-13,Scott Chamberlain,"https://github.com/ropensci/geojsonlint (devel)
https://docs.ropensci.org/geojsonlint (docs)",TRUE,https://github.com/ropensci/geojsonlint,148855,11,2020-11-26T05:31:43Z,13532.272727272728
geojsonR,"Includes functions for processing GeoJson objects <https://en.wikipedia.org/wiki/GeoJSON> relying on 'RFC 7946' <https://tools.ietf.org/pdf/rfc7946.pdf>. The geojson encoding is based on 'json11', a tiny JSON library for 'C++11' <https://github.com/dropbox/json11>. Furthermore, the source code is exported in R through the 'Rcpp' and 'RcppArmadillo' packages.",2021-05-04,Lampros Mouselimis,https://github.com/mlampros/geojsonR,TRUE,https://github.com/mlampros/geojsonr,41370,8,2021-05-04T10:19:39Z,5171.25
geojsonsf,Converts Between GeoJSON and simple feature objects. ,2020-10-02,David Cooley,https://github.com/SymbolixAU/geojsonsf,TRUE,https://github.com/symbolixau/geojsonsf,319004,62,2020-10-02T04:43:13Z,5145.225806451613
geoknife,"Processes gridded datasets found on the U.S. Geological Survey
    Geo Data Portal web application or elsewhere, using a web-enabled workflow
    that eliminates the need to download and store large datasets that are reliably
    hosted on the Internet. The package provides access to several data subset and
    summarization algorithms that are available on remote web processing servers (Read et al. (2015) <doi:10.1111/ecog.01880>).",2021-04-27,Jordan Read,https://github.com/USGS-R/geoknife,TRUE,https://github.com/usgs-r/geoknife,58181,63,2021-04-27T14:50:26Z,923.5079365079365
geomander,"A compilation of tools to complete common tasks for studying gerrymandering. This focuses on the geographic tool side of common problems, such as linking different levels of spatial units or estimating how to break up units. Functions exist for creating redistricting-focused data for the US.",2021-06-16,Christopher T. Kenny,https://christopherkenny.github.io/geomander/,TRUE,https://github.com/christopherkenny/geomander,1189,8,2021-08-18T21:24:31Z,148.625
geometries,"Geometry shapes in 'R' are typically represented by matrices (points, lines), with more complex 
  shapes being lists of matrices (polygons). 'Geometries' will convert various 'R' objects into these shapes. 
  Conversion functions are available at both the 'R' level, and through 'Rcpp'.",2020-11-26,David Cooley,https://dcooley.github.io/geometries/,TRUE,https://github.com/dcooley/geometries,172117,25,2021-05-10T03:00:23Z,6884.68
GeoMongo,"Utilizes methods of the 'PyMongo' 'Python' library to initialize, insert and query 'GeoJson' data (see <https://github.com/mongodb/mongo-python-driver> for more information on 'PyMongo'). Furthermore, it allows the user to validate 'GeoJson' objects and to use the console for 'MongoDB' (bulk) commands. The 'reticulate' package provides the 'R' interface to 'Python' modules, classes and functions.",2021-05-07,Lampros Mouselimis,https://github.com/mlampros/GeoMongo,TRUE,https://github.com/mlampros/geomongo,16261,1,2021-05-08T05:24:04Z,16261
geomorph,"Read, manipulate, and digitize landmark data, generate shape
    variables via Procrustes analysis for points, curves and surfaces, perform
    shape analyses, and provide graphical depictions of shapes and patterns of
    shape variation.",2021-04-02,Dean Adams,https://github.com/geomorphR/geomorph,TRUE,https://github.com/geomorphr/geomorph,90017,56,2021-09-01T15:38:20Z,1607.4464285714287
geos,"Provides an R API to the Open Source Geometry Engine
  ('GEOS') library (<https://trac.osgeo.org/geos/>) and a vector format 
  with which to efficiently store 'GEOS' geometries. High-performance functions 
  to extract information from, calculate relationships between, and
  transform geometries are provided. Finally, facilities to import 
  and export geometry vectors to other spatial formats are provided.",2021-05-18,Dewey Dunnington,"https://paleolimbot.github.io/geos/,
https://github.com/paleolimbot/geos/",TRUE,https://github.com/paleolimbot/geos,7322,25,2021-05-17T15:30:29Z,292.88
geoSAE,"This function is an extension of the Small Area Estimation (SAE) model. Geoadditive Small Area Model is a combination of the geoadditive model with the Small Area Estimation (SAE) model, by adding geospatial information to the SAE model. This package refers to J.N.K Rao and Isabel Molina (2015, ISBN: 978-1-118-73578-7), Bocci, C., & Petrucci, A. (2016)<doi:10.1002/9781118814963.ch13>, and Ardiansyah, M., Djuraidah, A., & Kurnia, A. (2018)<doi:10.21082/jpptp.v2n2.2018.p101-110>.",2021-06-14,Ketut Karang Pradnyadika,https://github.com/ketutdika/geoSAE,TRUE,https://github.com/ketutdika/geosae,970,0,2021-06-15T09:25:47Z,NA
geospark,"R binds 'GeoSpark' <http://geospark.datasyslab.org/> extending 'sparklyr' 
    <https://spark.rstudio.com/> R package to make distributed 'geocomputing' easier. Sf is a
    package that provides [simple features] <https://en.wikipedia.org/wiki/Simple_Features> access
    for R and which is a leading 'geospatial' data processing tool. 'Geospark' R package bring 
    the same simple features access like sf but running on Spark distributed system.",2020-03-02,Harry Zhu,NA,TRUE,https://github.com/harryprince/geospark,117637,45,2021-04-19T08:50:47Z,2614.1555555555556
geostats,"A collection of datasets and simplified functions for an introductory (geo)statistics module at University College London. Provides functionality for compositional, directional and spatial data, including ternary diagrams, Wulff and Schmidt stereonets, and ordinary kriging interpolation. Implements logistic and (additive and centred) logratio transformations. Computes vector averages and concentration parameters for the von-Mises distribution. Includes a collection of natural and synthetic fractals, and a simulator for deterministic chaos using a magnetic pendulum example. The main purpose of these functions is pedagogical. Researchers can find more complete alternatives for these tools in other packages such as 'compositions', 'robCompositions', 'sp', 'gstat' and 'RFOC'. All the functions are written in plain R, with no compiled code and a minimal number of dependencies. Theoretical background and worked examples are available at <https://tinyurl.com/UCLgeostats/>.",2021-04-06,Pieter Vermeesch,https://github.com/pvermees/geostats/,TRUE,https://github.com/pvermees/geostats,6132,6,2021-04-27T15:17:12Z,1022
geotopbricks,"It analyzes raster maps and other information as input/output
    files from the Hydrological Distributed Model GEOtop. It contains functions
    and methods to import maps and other keywords from geotop.inpts file. Some
    examples with simulation cases of GEOtop 2.x/3.x are presented in the package.
    Any information about the GEOtop Distributed Hydrological Model source code
    is available on www.geotop.org. Technical details about the model are
    available in Endrizzi et al, 2014
    (<http://www.geosci-model-dev.net/7/2831/2014/gmd-7-2831-2014.html>).",2020-02-11,Emanuele Cordano,"http://www.geotop.org, https://github.com/ecor/geotopbricks",TRUE,https://github.com/ecor/geotopbricks,21140,3,2020-11-03T08:53:38Z,7046.666666666667
geouy,"The toolbox have functions to load and process geographic information for Uruguay. 
        And extra-function to get address coordinates and orthophotos through the uruguayan 'IDE' API <https://www.gub.uy/infraestructura-datos-espaciales/tramites-y-servicios/servicios/servicio-direcciones-geograficas>.",2021-08-17,Richard Detomasi,NA,TRUE,https://github.com/richdeto/geouy,12488,9,2021-08-17T04:00:50Z,1387.5555555555557
GerminaR,A collection of different indices and visualization techniques for evaluate the seed germination process in ecophysiological studies (Lozano-Isla et al. 2019) <doi:10.1111/1440-1703.1275>.,2021-06-11,Flavio Lozano-Isla,"https://germinar.inkaverse.com/,
https://github.com/flavjack/germinar",TRUE,https://github.com/flavjack/germinar,21294,1,2021-06-11T09:19:24Z,21294
germinationmetrics,"Provides functions to compute various germination indices such as
    germinability, median germination time, mean germination time, mean
    germination rate, speed of germination, Timson's index, germination value,
    coefficient of uniformity of germination, uncertainty of germination
    process, synchrony of germination etc. from germination count data. Includes
    functions for fitting cumulative seed germination curves using
    four-parameter hill function and computation of associated parameters. See
    the vignette for more, including full list of citations for the methods
    implemented.",2021-02-17,J. Aravind,"https://github.com/aravind-j/germinationmetrics,
https://aravind-j.github.io/germinationmetrics/
https://CRAN.R-project.org/package=germinationmetrics
https://doi.org/10.5281/zenodo.1219630",TRUE,https://github.com/aravind-j/germinationmetrics,20027,1,2021-03-06T16:28:58Z,20027
gert,"Simple git client for R based on 'libgit2' with support for SSH and 
    HTTPS remotes. All functions in 'gert' use basic R data types (such as vectors
    and data-frames) for their arguments and return values. User credentials are
    shared with command line 'git' through the git-credential store and ssh keys
    stored on disk or ssh-agent.",2021-08-16,Jeroen Ooms,"https://docs.ropensci.org/gert/ (website),
https://github.com/r-lib/gert (devel), https://libgit2.org
(upstream)",TRUE,https://github.com/r-lib/gert,2547037,112,2021-09-01T17:00:35Z,22741.401785714286
gesisdata,"Reproducible, programmatic retrieval of datasets from the
    GESIS Data Archive.  The GESIS Data Archive <https://search.gesis.org>  
    makes available thousands of invaluable datasets, but researchers using
    these datasets are caught in a bind.  The archive's terms and conditions
    bar dissemination of downloaded datasets to third parties, but to ensure 
    that one's work can be reproduced, assessed, and built upon by others, one
    must provide access to the raw data one has employed.  The 'gesisdata'
    package cuts this knot by providing registered users with programmatic,
    reproducible access to GESIS datasets from within 'R'.",2020-06-26,Frederick Solt,https://github.com/fsolt/gesisdata,TRUE,https://github.com/fsolt/gesisdata,4423,5,2020-11-27T00:22:17Z,884.6
gesttools,"Provides a series of general purpose tools to perform g-estimation using the methods described in Sjolander and Vansteelandt (2016) <doi:10.1515/em-2015-0005> and Dukes and Vansteelandt <doi:10.1093/aje/kwx347>. The package allows for g-estimation in a wide variety of circumstances, including an end of study or time-varying outcome, and an exposure that is a binary, continuous, or a categorical variable with three or more categories. The package also supports g-estimation with time-varying causal effects and effect modification by a confounding variable.",2021-06-10,Daniel Tompsett,https://github.com/danieltompsett/gesttools,TRUE,https://github.com/danieltompsett/gesttools,3373,0,2020-11-27T17:40:38Z,NA
getable,"Dynamically retrieve data from the web to render HTML tables
    on inspection in R Markdown HTML documents.",2020-10-02,Yongfu Liao,"https://yongfu.name/getable/,
https://github.com/liao961120/getable/",TRUE,https://github.com/liao961120/getable,4700,14,2020-10-14T09:40:20Z,335.7142857142857
GetBCBData,"Downloads and organizes datasets using BCB's API <https://www.bcb.gov.br/>. Offers options for caching with the 'memoise' package and
    , multicore/multisession with 'furrr' and format of output data (long/wide). ",2021-01-21,Marcelo Perlin,https://github.com/msperlin/GetBCBData/,TRUE,https://github.com/msperlin/getbcbdata,14903,10,2021-03-15T20:42:04Z,1490.3
getCRUCLdata,"Provides functions that automate downloading and importing
    University of East Anglia Climate Research Unit ('CRU') 'CL' v. 2.0
    climatology data, facilitates the calculation of minimum temperature and
    maximum temperature and formats the data into a tidy data frame as a
    'tibble' or a list of 'raster' 'stack' objects for use.  'CRU' 'CL' v. 2.0
    data are a gridded climatology of 1961-1990 monthly means released in 2002
    and cover all land areas (excluding Antarctica) at 10 arcminutes
    (0.1666667 degree) resolution.  For more information see the description of
    the data provided by the University of East Anglia Climate Research Unit,
    <https://crudata.uea.ac.uk/cru/data/hrg/tmc/readme.txt>.",2020-10-26,Adam H. Sparks,https://docs.ropensci.org/getCRUCLdata/,TRUE,https://github.com/ropensci/getcrucldata,27145,16,2020-12-16T04:24:19Z,1696.5625
GetDFPData,"Reads annual financial reports including assets, liabilities, dividends history, stockholder composition and much more from Bovespa's DFP, FRE and FCA systems <http://www.b3.com.br/pt_br/produtos-e-servicos/negociacao/renda-variavel/empresas-listadas.htm>.
 These are web based interfaces for all financial reports of companies traded at Bovespa. The package is specially designed for large scale data importation, keeping a tabular (long) structure for easier processing.  ",2021-04-01,Marcelo Perlin,https://github.com/msperlin/GetDFPData/,TRUE,https://github.com/msperlin/getdfpdata,30690,33,2021-04-01T13:05:18Z,930
GetDFPData2,"Reads annual and quarterly financial reports from companies traded at B3, the Brazilian exchange 
            <http://www.b3.com.br/>. 
            All data is downloaded and imported from CVM's public ftp site <http://dados.cvm.gov.br/dados/CIA_ABERTA/>.",2021-03-24,Marcelo Perlin,https://github.com/msperlin/GetDFPData2/,TRUE,https://github.com/msperlin/getdfpdata2,2306,11,2021-04-02T16:20:54Z,209.63636363636363
GetFREData,"Reads corporate data such as board composition and compensation for companies traded at B3, 
             the Brazilian exchange <http://www.b3.com.br/>. All data is downloaded and imported from the ftp site <http://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/FRE/>.",2021-09-03,Marcelo Perlin,https://github.com/msperlin/GetFREData/,TRUE,https://github.com/msperlin/getfredata,1879,1,2021-09-03T10:49:59Z,1879
getlandsat,"Get Landsat 8 Data from Amazon Web Services ('AWS')
    public data sets (<https://registry.opendata.aws/landsat-8/>).
    Includes functions for listing images and fetching them, and handles
    caching to prevent unnecessary additional requests.",2018-04-30,Scott Chamberlain,https://github.com/ropensci/getlandsat,TRUE,https://github.com/ropensci/getlandsat,17257,56,2020-12-30T18:12:06Z,308.1607142857143
getLattes,"Tool for import and process data from 'Lattes' curriculum platform (<http://lattes.cnpq.br/>). The Brazilian government keeps an extensive base of curricula for academics from all over the country, with over 5 million registrations. The academic life of the Brazilian researcher, or related to Brazilian universities, is documented in 'Lattes'. Some information that can be obtained: professional formation, research area, publications, academics advisories, projects, etc. 'getLattes' package allows work with 'Lattes' data exported to XML format.",2021-06-11,Roney Fraga Souza,https://github.com/roneyfraga/getLattes,TRUE,https://github.com/roneyfraga/getlattes,4025,1,2021-06-11T15:38:02Z,4025
getmstatistic,"Quantifying systematic heterogeneity in meta-analysis using R.
    The M statistic aggregates heterogeneity information across multiple
    variants to, identify systematic heterogeneity patterns and their direction
    of effect in meta-analysis. It's primary use is to identify outlier studies,
    which either show ""null"" effects or consistently show stronger or weaker
    genetic effects than average across, the panel of variants examined in a
    GWAS meta-analysis. In contrast to conventional heterogeneity metrics
    (Q-statistic, I-squared and tau-squared) which measure random heterogeneity
    at individual variants, M measures systematic (non-random)
    heterogeneity across multiple independently associated variants. Systematic
    heterogeneity can arise in a meta-analysis due to differences in the study
    characteristics of participating studies. Some of the differences may
    include: ancestry, allele frequencies, phenotype definition, age-of-disease
    onset, family-history, gender, linkage disequilibrium and quality control
    thresholds. See <https://magosil86.github.io/getmstatistic/> for statistical
    statistical theory, documentation and examples.",2021-05-09,Lerato E Magosi,https://magosil86.github.io/getmstatistic/,TRUE,https://github.com/magosil86/getmstatistic,18134,3,2021-05-08T22:10:29Z,6044.666666666667
GetoptLong,"This is a command-line argument parser which wraps the 
    powerful Perl module Getopt::Long and with some adaptations for easier use
	in R. It also provides a simple way for variable interpolation in R.",2020-12-15,Zuguang Gu,https://github.com/jokergoo/GetoptLong,TRUE,https://github.com/jokergoo/getoptlong,322029,10,2021-07-01T12:47:39Z,32202.9
getProxy,"Allows get address and port 
	of the free proxy server, from one of two services
	<http://gimmeproxy.com/> or <https://getproxylist.com/>. 
	And it's easy to redirect your Internet connection through
	a proxy server.",2018-08-20,Alexey Seleznev,http://selesnow.github.io/getProxy,TRUE,https://github.com/selesnow/getproxy,13677,7,2021-05-25T14:07:55Z,1953.857142857143
gets,"Automated General-to-Specific (GETS) modelling of the mean and variance of a regression, and indicator saturation methods for detecting and testing for structural breaks in the mean, see Pretis, Reade and Sucarrat (2018) <doi:10.18637/jss.v086.i03>.",2021-09-02,Genaro Sucarrat,"https://CRAN.R-project.org/package=gets,
http://www.sucarrat.net/R/gets/",TRUE,https://github.com/gsucarrat/gets,48635,5,2021-09-03T12:26:37Z,9727
getspres,"An implementation of SPRE (standardised predicted random-effects)
    statistics in R to explore heterogeneity in genetic association meta-
    analyses, as described by Magosi et al. (2019) 
    <doi:10.1093/bioinformatics/btz590>. SPRE statistics are precision 
    weighted residuals that indicate the direction and extent with which 
    individual study-effects in a meta-analysis deviate from the average 
    genetic effect. Overly influential positive outliers have the potential 
    to inflate average genetic effects in a meta-analysis whilst negative 
    outliers might lower or change the direction of effect. See the 'getspres' 
    website for documentation and examples 
    <https://magosil86.github.io/getspres/>.",2021-05-09,Lerato E Magosi,https://magosil86.github.io/getspres/,TRUE,https://github.com/magosil86/getspres,11078,0,2021-05-09T04:39:06Z,NA
gettz,"A function to retrieve the system timezone on Unix systems
 which has been found to find an answer when 'Sys.timezone()' has failed.
 It is based on an answer by Duane McCully posted on 'StackOverflow', and
 adapted to be callable from R. The package also builds on Windows, but
 just returns NULL.",2020-04-14,Dirk Eddelbuettel,http://dirk.eddelbuettel.com/code/gettz.html,TRUE,https://github.com/eddelbuettel/gettz,30935,1,2021-06-25T21:47:57Z,30935
gexp,"Generates experiments - simulating structured or experimental data as: 
             completely randomized design, randomized block design, latin square design, 
             factorial and split-plot experiments (Ferreira, 2008, ISBN:8587692526; 
             Naes et al., 2007 <doi:10.1002/qre.841>; Rencher et al., 2007, ISBN:9780471754985; 
             Montgomery, 2001, ISBN:0471316490).",2020-04-02,Ivan Bezerra Allaman,https://github.com/ivanalaman/gexp,TRUE,https://github.com/ivanalaman/gexp,12636,1,2020-10-30T14:36:10Z,12636
geysertimes,"Download geyser eruption and observation data from the GeyserTimes
  site (<https://geysertimes.org>) and optionally store it locally. The vignette
  shows a simple analysis of downloading, accessing, and summarizing the data.",2021-05-19,Stephen Kaluzny,https://github.com/geysertimes/geysertimes-r-package,TRUE,https://github.com/geysertimes/geysertimes-r-package,2977,0,2021-05-19T01:55:43Z,NA
gfcanalysis,"Supports analyses using the Global Forest Change dataset released
    by Hansen et al. gfcanalysis was originally written for the Tropical Ecology 
    Assessment and Monitoring (TEAM) Network. For additional details on the 
    Global Forest Change dataset, see: Hansen, M. et al. 2013. ""High-Resolution 
    Global Maps of 21st-Century Forest Cover Change."" Science 342 (15 
    November): 850-53. The forest change data and more information on the 
    product is available at <http://earthenginepartners.appspot.com>.",2020-06-16,Matthew Cooper,https://github.com/azvoleff/gfcanalysis,TRUE,https://github.com/azvoleff/gfcanalysis,19950,12,2021-04-13T22:50:32Z,1662.5
GFDsurv,"Implemented are three Wald-type statistic and respective
  permuted versions for null hypotheses formulated in terms of cumulative hazard rate functions, medians and the concordance measure, respectively, in the general framework of survival factorial designs with possibly heterogeneous survival and/or censoring distributions, for crossed designs with an arbitrary number of factors and nested designs with up to three factors.
	Ditzhaus, Dobler and Pauly (2020) <doi:10.1177/0962280220980784> 
	Ditzhaus, Janssen, Pauly (2020) <arXiv: 2004.10818v2>
	Dobler and Pauly (2019) <doi:10.1177/0962280219831316>.",2021-07-14,Philipp Steinhauer,https://github.com/PhilippSteinhauer/GFDsurv,TRUE,https://github.com/philippsteinhauer/gfdsurv,1703,0,2021-07-14T08:33:24Z,NA
gfer,"Focuses on data collecting, analyzing and visualization in green finance and environmental 
  risk research and analysis. Main function includes environmental data collecting from 
  official websites such as MEP (Ministry of Environmental Protection of China, <https://www.mee.gov.cn>), water 
  related projects identification and environmental data visualization.",2021-05-14,Yuanchao Xu,https://yuanchao-xu.github.io/gfer/,TRUE,https://github.com/yuanchao-xu/gfer,17750,5,2021-05-14T09:15:04Z,3550
gfiExtremes,"Fiducial framework to perform inference on the quantiles for a generalized Pareto distribution model and on the parameters of the Pareto exceedance distribution, assuming the exceedance threshold is a known or unknown parameter. Reference: Damian V. Wandler & Jan Hannig (2012) <doi:10.1007/s10687-011-0127-9>.",2020-11-26,Stéphane Laurent,https://github.com/stla/gfiExtremes,TRUE,https://github.com/stla/gfiextremes,3091,0,2020-11-27T16:09:46Z,NA
gfilmm,"Simulation of the generalized fiducial distribution for normal linear mixed models with interval data. Fiducial inference is somehow similar to Bayesian inference, in the sense that it is based on a distribution that represents the uncertainty about the parameters, like the posterior distribution in Bayesian statistics. It does not require a prior distribution, and it yields results close to frequentist results. Reference: Cisewski and Hannig (2012) <doi:10.1214/12-AOS1030>.",2021-06-25,Stéphane Laurent,https://github.com/stla/gfilmm,TRUE,https://github.com/stla/gfilmm,4757,0,2021-06-25T09:56:09Z,NA
gfiUltra,"Variable selection for ultrahigh-dimensional (""large p small n"") linear Gaussian models using a fiducial framework allowing to draw inference on the parameters. Reference: Lai, Hannig & Lee (2015) <doi:10.1080/01621459.2014.931237>.",2020-12-09,Stéphane Laurent,https://github.com/stla/gfiUltra,TRUE,https://github.com/stla/gfiultra,2920,1,2020-12-07T11:46:39Z,2920
gfonts,"Download 'Google' fonts and generate 'CSS' to use in 'rmarkdown' documents and 
  'shiny' applications. Some popular fonts are included and ready to use.",2021-01-11,Victor Perrier,https://github.com/dreamRs/gfonts,TRUE,https://github.com/dreamrs/gfonts,7100,91,2021-06-08T11:43:01Z,78.02197802197803
gfoRmula,"Implements the parametric g-formula algorithm of Robins (1986) 
    <doi:10.1016/0270-0255(86)90088-6>. The g-formula can be used to estimate 
    the causal effects of hypothetical time-varying treatment interventions on 
    the mean or risk of an outcome from longitudinal data with time-varying 
    confounding. This package allows: 1) binary or continuous/multi-level 
    time-varying treatments; 2) different types of outcomes (survival or 
    continuous/binary end of follow-up); 3) data with competing events or 
    truncation by death and loss to follow-up and other types of censoring 
    events; 4) different options for handling competing events in the case of 
    survival outcomes; 5) a random measurement/visit process; 6) joint 
    interventions on multiple treatments; and 7) general incorporation of a 
    priori knowledge of the data structure.",2021-07-13,Victoria Lin,"https://github.com/CausalInference/gfoRmula,
https://www.cell.com/patterns/fulltext/S2666-3899(20)30008-8",TRUE,https://github.com/causalinference/gformula,12882,66,2021-07-13T05:53:35Z,195.1818181818182
ggallin,"Extra geoms and scales for 'ggplot2', including geom_cloud(),
  a Normal density cloud replacement for errorbars;
  transforms ssqrt_trans and pseudolog10_trans, which are loglike but 
  appropriate for negative data; interp_trans() and warp_trans() which
  provide scale transforms based on interpolation;
  and an infix compose operator for scale transforms.",2017-10-02,Steven E. Pav,https://github.com/shabbychef/ggallin,TRUE,https://github.com/shabbychef/ggallin,17833,13,2021-04-03T23:11:25Z,1371.7692307692307
ggalluvial,"Alluvial plots use variable-width ribbons and stacked bar plots to
    represent multi-dimensional or repeated-measures data with categorical or
    ordinal variables; see Riehmann, Hanfler, and Froehlich (2005)
    <doi:10.1109/INFVIS.2005.1532152> and Rosvall and Bergstrom (2010)
    <doi:10.1371/journal.pone.0008694>.
    Alluvial plots are statistical graphics in the sense of Wilkinson (2006)
    <doi:10.1007/0-387-28695-0>; they share elements with Sankey diagrams and
    parallel sets plots but are uniquely determined from the data and a small
    set of parameters. This package extends Wickham's (2010)
    <doi:10.1198/jcgs.2009.07098> layered grammar of graphics to generate
    alluvial plots from tidy data.",2020-12-05,Jason Cory Brunson,http://corybrunson.github.io/ggalluvial/,TRUE,https://github.com/corybrunson/ggalluvial,148615,344,2021-08-30T18:43:29Z,432.0203488372093
GGally,"
    The R package 'ggplot2' is a plotting system based on the grammar of graphics.
    'GGally' extends 'ggplot2' by adding several functions
    to reduce the complexity of combining geometric objects with transformed data.
    Some of these functions include a pairwise plot matrix, a two group pairwise plot
    matrix, a parallel coordinates plot, a survival plot, and several functions to
    plot networks.",2021-06-21,Barret Schloerke,"https://ggobi.github.io/ggally/, https://github.com/ggobi/ggally",TRUE,https://github.com/ggobi/ggally,2305123,441,2021-06-21T21:52:19Z,5227.036281179138
gganimate,"The grammar of graphics as implemented in the 'ggplot2' package has
    been successful in providing a powerful API for creating static 
    visualisation. In order to extend the API for animated graphics this package
    provides a completely new set of grammar, fully compatible with 'ggplot2' 
    for specifying transitions and animations in a flexible and extensible way.",2020-10-15,Thomas Lin Pedersen,"https://gganimate.com, https://github.com/thomasp85/gganimate",TRUE,https://github.com/thomasp85/gganimate,460127,1704,2021-03-23T11:23:29Z,270.0275821596244
ggbeeswarm,"Provides two methods of plotting categorical scatter plots such
    that the arrangement of points within a category reflects the density of
    data at that region, and avoids over-plotting.",2017-08-07,Erik Clarke,https://github.com/eclarke/ggbeeswarm,TRUE,https://github.com/eclarke/ggbeeswarm,338981,389,2021-07-02T17:42:15Z,871.4164524421594
ggborderline,"A set of geometries to make line plots a little bit nicer. Use 
    along with 'ggplot2' to:
    - Improve the clarity of line plots with many overlapping lines
    - Draw more realistic worms.",2021-08-09,Jacob Scott,"https://github.com/wurli/ggborderline,
https://wurli.github.io/ggborderline/",TRUE,https://github.com/wurli/ggborderline,331,9,2021-08-12T19:13:35Z,36.77777777777778
ggbreak,An implementation of scale functions for setting axis breaks of a 'gg' plot.,2021-08-14,Guangchuang Yu,https://github.com/YuLab-SMU/ggbreak,TRUE,https://github.com/yulab-smu/ggbreak,3276,54,2021-09-01T08:26:26Z,60.666666666666664
ggcharts,"Streamline the creation of common charts by taking care of a lot of
    data preprocessing and plot customization for the user. Provides a
    high-level interface to create plots using 'ggplot2'.",2020-05-20,Thomas Neitmann,https://github.com/thomas-neitmann/ggcharts,TRUE,https://github.com/thomas-neitmann/ggcharts,13105,223,2021-01-28T16:27:42Z,58.766816143497756
ggcleveland,"William S. Cleveland's book 'Visualizing Data' is a classic piece 
	of literature on Exploratory Data Analysis. Although it was written 
	several decades ago, its content is still relevant as it proposes several 
	tools which are useful to discover patterns and relationships among the data 
	under study, and also to assess the goodness of fit o a model.  This package 
	provides functions to produce the 'ggplot2' versions of the visualization tools 
	described in this book and is thought to be used in the context of courses on 
	Exploratory Data Analysis.",2021-08-16,Marcos Prunello,https://github.com/mpru/ggcleveland,TRUE,https://github.com/mpru/ggcleveland,253,3,2021-08-17T21:44:37Z,84.33333333333333
ggcorrplot,"The 'ggcorrplot' package can be used to visualize easily a
    correlation matrix using 'ggplot2'. It provides a solution for reordering the
    correlation matrix and displays the significance level on the plot. It also
    includes a function for computing a matrix of correlation p-values.",2019-05-19,Alboukadel Kassambara,http://www.sthda.com/english/wiki/ggcorrplot,TRUE,https://github.com/kassambara/ggcorrplot,484644,145,2020-12-10T06:07:35Z,3342.3724137931035
ggdag,"Tidy, analyze, and plot directed acyclic graphs
    (DAGs). 'ggdag' is built on top of 'dagitty', an R package that uses
    the 'DAGitty' web tool (<http://dagitty.net>) for creating and
    analyzing DAGs. 'ggdag' makes it easy to tidy and plot 'dagitty'
    objects using 'ggplot2' and 'ggraph', as well as common analytic and
    graphical functions, such as determining adjustment sets and node
    relationships.",2021-01-12,Malcolm Barrett,https://github.com/malcolmbarrett/ggdag,TRUE,https://github.com/malcolmbarrett/ggdag,49079,326,2021-01-12T16:22:25Z,150.54907975460122
ggdemetra,"Provides 'ggplot2' functions to return the results of seasonal and trading day adjustment 
    made by 'RJDemetra'. 'RJDemetra' is an 'R' interface around 'JDemetra+' (<https://github.com/jdemetra/jdemetra-app>),
    the seasonal adjustment software officially recommended to the members of the European Statistical System and
    the European System of Central Banks.",2020-12-02,Alain Quartier-la-Tente,https://github.com/AQLT/ggdemetra,TRUE,https://github.com/aqlt/ggdemetra,15456,9,2021-07-15T20:16:40Z,1717.3333333333333
ggdendro,"This is a set of tools for dendrograms and
    tree plots using 'ggplot2'.  The 'ggplot2' philosophy is to
    clearly separate data from the presentation.
    Unfortunately the plot method for dendrograms plots
    directly to a plot device without exposing the data.
    The 'ggdendro' package resolves this by making available
    functions that extract the dendrogram plot data. The package
    provides implementations for 'tree', 'rpart', as well as diana and agnes
    (from 'cluster') diagrams.",2020-09-13,Andrie de Vries,https://github.com/andrie/ggdendro,TRUE,https://github.com/andrie/ggdendro,706499,66,2020-09-13T20:02:13Z,10704.530303030304
ggdist,"Provides primitives for visualizing distributions using 'ggplot2' that are particularly tuned for
    visualizing uncertainty in either a frequentist or Bayesian mode. Both analytical distributions (such as 
    frequentist confidence distributions or Bayesian priors) and distributions represented as samples (such as 
    bootstrap distributions or Bayesian posterior samples) are easily visualized. Visualization primitives include
    but are not limited to: points with multiple uncertainty intervals, 
    eye plots (Spiegelhalter D., 1999) <doi:10.1111/1467-985X.00120>,
    density plots, gradient plots, dot plots (Wilkinson L., 1999) <doi:10.1080/00031305.1999.10474474>,
    quantile dot plots (Kay M., Kola T., Hullman J., Munson S., 2016) <doi:10.1145/2858036.2858558>,
    complementary cumulative distribution function 
    barplots (Fernandes M., Walls L., Munson S., Hullman J., Kay M., 2018) <doi:10.1145/3173574.3173718>,
    and fit curves with multiple uncertainty ribbons.",2021-07-19,Matthew Kay,"https://mjskay.github.io/ggdist/,
https://github.com/mjskay/ggdist/",TRUE,https://github.com/mjskay/ggdist,78281,283,2021-07-19T21:29:31Z,276.6113074204947
ggdmc,"Hierarchical Bayesian models. The package provides tools to fit two response time models, using the population-based Markov Chain Monte Carlo. ",2019-04-29,Yi-Shin Lin,https://github.com/yxlin/ggdmc,TRUE,https://github.com/yxlin/ggdmc,18027,13,2020-10-13T10:09:18Z,1386.6923076923076
gge,"Create biplots for GGE (genotype plus genotype-by-environment) and
    GGB (genotype plus genotype-by-block-of-environments) models. 
    See Laffont et al. (2013) <doi:10.2135/cropsci2013.03.0178>.",2020-12-16,Kevin Wright,http://kwstat.github.io/gge/,TRUE,https://github.com/kwstat/gge,26231,6,2020-12-16T14:58:56Z,4371.833333333333
ggeasy,"Provides a series of aliases to commonly used but difficult 
    to remember 'ggplot2' sequences.",2021-01-07,Jonathan Carroll,https://github.com/jonocarroll/ggeasy,TRUE,https://github.com/jonocarroll/ggeasy,18490,214,2021-01-07T06:46:18Z,86.40186915887851
ggeffects,"Compute marginal effects and adjusted predictions from statistical
    models and returns the result as tidy data frames. These data frames are 
    ready to use with the 'ggplot2'-package. Effects and predictions can be 
    calculated for many different models. Interaction terms, splines and 
    polynomial terms are also supported. The main functions are ggpredict(), 
    ggemmeans() and ggeffect(). There is a generic plot()-method to plot the 
    results using 'ggplot2'.",2021-07-29,Daniel Lüdecke,https://strengejacke.github.io/ggeffects/,TRUE,https://github.com/strengejacke/ggeffects,552783,356,2021-07-29T14:13:45Z,1552.7612359550562
ggetho,"Extension of 'ggplot2' providing layers, scales and preprocessing functions
    useful to represent behavioural variables that are recorded over multiple animals and days.
    This package is part of the 'rethomics' framework <http://rethomics.github.io/>.",2020-04-29,Quentin Geissmann,https://github.com/rethomics/ggetho,TRUE,https://github.com/rethomics/ggetho,19404,7,2021-06-11T08:52:59Z,2772
ggExtra,"Collection of functions and layers to enhance 'ggplot2'. The 
    flagship function is 'ggMarginal()', which can be used to add marginal
    histograms/boxplots/density plots to 'ggplot2' scatterplots.",2019-08-27,Dean Attali,https://github.com/daattali/ggExtra,TRUE,https://github.com/daattali/ggextra,339471,318,2021-07-05T18:35:09Z,1067.5188679245282
ggfittext,"Provides 'ggplot2' geoms to fit text into a box by growing, shrinking
    or wrapping the text.",2021-01-30,David Wilkins,https://wilkox.org/ggfittext/,TRUE,https://github.com/wilkox/ggfittext,663016,246,2021-08-29T18:49:34Z,2695.1869918699185
ggforce,"The aim of 'ggplot2' is to aid in visual data investigations. This
    focus has led to a lack of facilities for composing specialised plots.
    'ggforce' aims to be a collection of mainly new stats and geoms that fills
    this gap. All additional functionality is aimed to come through the official
    extension system so using 'ggforce' should be a stable experience.",2021-03-05,Thomas Lin Pedersen,"https://ggforce.data-imaginist.com,
https://github.com/thomasp85/ggforce",TRUE,https://github.com/thomasp85/ggforce,1733855,684,2021-02-22T08:41:53Z,2534.875730994152
ggformula,Provides a formula interface to 'ggplot2' graphics.,2021-01-13,Randall Pruim,https://github.com/ProjectMOSAIC/ggformula,TRUE,https://github.com/projectmosaic/ggformula,513458,31,2021-09-01T18:38:33Z,16563.16129032258
ggfortify,"Unified plotting tools for statistics commonly used, such as GLM,
    time series, PCA families, clustering and survival analysis. The package offers
    a single plotting interface for these analysis results and plots in a unified
    style using 'ggplot2'.",2021-07-07,Yuan Tang,https://github.com/sinhrks/ggfortify,TRUE,https://github.com/sinhrks/ggfortify,1149462,470,2021-08-06T18:24:56Z,2445.663829787234
ggfx,"Provides a range of filters that can be applied to layers from the 
    'ggplot2' package and its extensions, along with other graphic elements such 
    as guides and theme elements. The filters are applied at render time and 
    thus uses the exact pixel dimensions needed.",2021-03-31,Thomas Lin Pedersen,"https://ggfx.data-imaginist.com, https://github.com/thomasp85/ggfx",TRUE,https://github.com/thomasp85/ggfx,6542,141,2021-04-06T20:37:36Z,46.39716312056738
gggap,"The function gggap() streamlines the creation of segments on
    the y-axis of 'ggplot2' plots which is otherwise not a trivial task to
    accomplish.",2020-11-20,Carlos Morales,https://github.com/cmoralesmx/gggap,TRUE,https://github.com/cmoralesmx/gggap,3292,2,2020-11-16T20:37:36Z,1646
gggenes,"Provides a 'ggplot2' geom and helper functions for drawing gene
  arrow maps.",2020-12-10,David Wilkins,https://wilkox.org/gggenes/,TRUE,https://github.com/wilkox/gggenes,27314,238,2021-06-01T08:07:28Z,114.76470588235294
gggibbous,"Moon charts are like pie charts except that the proportions are
    shown as crescent or gibbous portions of a circle, like the lit and unlit
    portions of the moon. As such, they work best with only one or two groups.
    'gggibbous' extends 'ggplot2' to allow for plotting multiple moon charts in
    a single panel and does not require a square coordinate system.",2021-01-06,Michael Bramson,https://github.com/mnbram/gggibbous,TRUE,https://github.com/mnbram/gggibbous,11873,53,2020-12-24T16:26:42Z,224.0188679245283
ggh4x,"A 'ggplot2' extension that does a variety of little
    helpful things.  The package extends 'ggplot2' facets through
    customisation, by setting individual scales per panel, resizing panels
    and providing nested facets.  Also allows multiple colour and fill
    scales per plot. Also hosts a smaller collection of stats, geoms and axis 
    guides.",2021-08-21,Teun van den Brand,https://github.com/teunbrand/ggh4x,TRUE,https://github.com/teunbrand/ggh4x,6883,206,2021-09-03T06:56:00Z,33.4126213592233
gghalves,"A 'ggplot2' extension for easy plotting of half-half geom combinations. Think half boxplot and half jitterplot, or half violinplot and half dotplot.",2020-11-08,Frederik Tiedemann,https://github.com/erocoar/gghalves,TRUE,https://github.com/erocoar/gghalves,23569,191,2021-03-06T12:13:58Z,123.3979057591623
gghighlight,Make it easier to explore data with highlights.,2021-06-05,Hiroaki Yutani,https://github.com/yutannihilation/gghighlight/,TRUE,https://github.com/yutannihilation/gghighlight,100394,446,2021-06-12T12:05:40Z,225.09865470852017
gghilbertstrings,"A set of functions that help to create plots based on Hilbert curves. 
    Hilbert curves are used to map one dimensional data into the 2D plane.
    The package provides a function that generate a 2D coordinate from an 
    integer position. As a specific use case the package provides a function 
    that allows mapping a character column in a data frame into 2D space 
    using 'ggplot2'. This allows visually comparing long lists of URLs, words,
    genes or other data that has a fixed order and position.",2021-04-07,André Calero Valdez,https://github.com/Sumidu/gghilbertstrings,TRUE,https://github.com/sumidu/gghilbertstrings,2041,16,2021-04-05T11:25:27Z,127.5625
ggimage,"Supports image files and graphic objects to be visualized in
    'ggplot2' graphic system.",2021-08-20,Guangchuang Yu,"https://github.com/GuangchuangYu/ggimage (devel),
https://yulab-smu.top/pkgdocs/ggimage.html (vignette)",TRUE,https://github.com/guangchuangyu/ggimage,116413,124,2021-08-20T08:38:39Z,938.8145161290323
ggimg,"Provides two new layer types for displaying image data as layers
  within the Grammar of Graphics framework. Displays images using either a
  rectangle interface, with a fixed bounding box, or a point interface using a
  central point and general size parameter. Images can be given as local
  JPEG or PNG files, external resources, or as a list column containing
  raster image data.",2021-05-06,Taylor B. Arnold,https://github.com/statsmaths/ggimg,TRUE,https://github.com/statsmaths/ggimg,8780,40,2021-05-06T19:16:08Z,219.5
gginference,"Visualise the results of F test to compare two variances, Student's t-test, test of equal or given proportions, Pearson's chi-squared test for count data and test for association/correlation between paired samples.",2020-10-31,Kleanthis Koupidis,https://github.com/okgreece/gginference,TRUE,https://github.com/okgreece/gginference,18988,11,2021-09-02T13:39:03Z,1726.1818181818182
gginnards,"Extensions to 'ggplot2' providing low-level debug tools: statistics
    and geometries echoing their data argument. Layer manipulation: deletion,
    insertion, extraction and reordering of layers. Deletion of unused variables
    from the data object embedded in ""ggplot"" objects.",2021-07-30,Pedro J. Aphalo,"https://www.r4photobiology.info,
https://github.com/aphalo/gginnards",TRUE,https://github.com/aphalo/gginnards,25490,5,2021-07-29T16:11:51Z,5098
ggip,"A 'ggplot2' extension that enables visualization of
    IP (Internet Protocol) addresses and networks. The address space is
    mapped onto the Cartesian coordinate system using a space-filling
    curve. Offers full support for both IPv4 and IPv6 (Internet Protocol
    versions 4 and 6) address spaces.",2020-10-11,David Hall,"https://davidchall.github.io/ggip/,
https://github.com/davidchall/ggip",TRUE,https://github.com/davidchall/ggip,4394,10,2020-10-11T04:59:53Z,439.4
GGIR,"A tool to process and analyse data collected with wearable raw acceleration sensors as described in Migueles and colleagues (JMPB 2019), and van Hees and colleagues (JApplPhysiol 2014; PLoSONE 2015). The package has been developed and tested for binary data from 'GENEActiv' <https://www.activinsights.com/> and GENEA devices (not for sale), .csv-export data from  'Actigraph' <https://actigraphcorp.com> devices, and .cwa and .wav-format data from 'Axivity' <https://axivity.com>. These devices are currently widely used in research on human daily physical activity. Further, the package can handle accelerometer data file from any other sensor brand providing that the data is stored in csv format and has either no header or a two column header. Also the package allows for external function embedding.",2021-06-03,Vincent T van Hees,"https://github.com/wadpac/GGIR/,
https://groups.google.com/forum/#!forum/RpackageGGIR",TRUE,https://github.com/wadpac/ggir,57943,46,2021-07-27T12:57:08Z,1259.6304347826087
ggiraph,Create interactive 'ggplot2' graphics using 'htmlwidgets'.,2021-05-19,David Gohel,https://davidgohel.github.io/ggiraph/,TRUE,https://github.com/davidgohel/ggiraph,275641,529,2021-06-23T22:04:47Z,521.0604914933838
ggiraphExtra,"Collection of functions to enhance 'ggplot2' and 'ggiraph'. Provides functions for exploratory plots.
    All plot can be a 'static' plot or an 'interactive' plot using 'ggiraph'.",2020-10-06,Keon-Woong Moon,https://github.com/cardiomoon/ggiraphExtra,TRUE,https://github.com/cardiomoon/ggiraphextra,115387,31,2020-10-06T04:02:53Z,3722.1612903225805
gglm,"Allows for easy creation of diagnostic plots for linear models using the Grammar of Graphics.  
  Provides functionality for both individual diagnostic plots and an array of four standard diagnostic plots.",2020-10-08,Grayson White,https://github.com/graysonwhite/gglm,TRUE,https://github.com/graysonwhite/gglm,5254,59,2020-10-31T18:42:34Z,89.05084745762711
ggmap,"A collection of functions to visualize spatial data and models
    on top of static maps from various online sources (e.g Google Maps and Stamen
    Maps). It includes tools common to those tasks, including functions for
    geolocation and routing.",2019-02-05,David Kahle,https://github.com/dkahle/ggmap,TRUE,https://github.com/dkahle/ggmap,1868030,670,2020-12-02T02:27:06Z,2788.10447761194
ggmcmc,"Tools for assessing and diagnosing convergence of
    Markov Chain Monte Carlo simulations, as well as for graphically display
    results from full MCMC analysis. The package also facilitates the graphical
    interpretation of models by providing flexible functions to plot the
    results against observed variables, and functions to work with
    hierarchical/multilevel batches of parameters
    (Fernández-i-Marín, 2016 <doi:10.18637/jss.v070.i09>).",2021-02-10,Xavier Fernández i Marín,"http://xavier-fim.net/packages/ggmcmc/,
https://github.com/xfim/ggmcmc/",TRUE,https://github.com/xfim/ggmcmc,75101,96,2021-02-10T15:41:19Z,782.3020833333334
ggmix,"Fit penalized multivariable linear mixed models with a single 
    random effect to control for population structure in genetic association 
    studies. The goal is to simultaneously fit many genetic variants at the 
    same time, in order to select markers that are independently associated 
    with the response. Can also handle prior annotation information, 
    for example, rare variants, in the form of variable weights. For more 
    information, see the website below and the accompanying paper: 
    Bhatnagar et al., ""Simultaneous SNP selection and adjustment for 
    population structure in high dimensional prediction models"", 2020, 
    <DOI:10.1371/journal.pgen.1008766>.",2021-04-13,Sahir Bhatnagar,https://github.com/sahirbhatnagar/ggmix,TRUE,https://github.com/sahirbhatnagar/ggmix,6615,9,2021-04-14T00:13:54Z,735
GGMncv,"Estimate Gaussian graphical models with nonconvex penalties <doi:10.31234/osf.io/ad57p>, 
  including the atan Wang and Zhu (2016) <doi:10.1155/2016/6495417>, 
  seamless L0 Dicker, Huang, and Lin (2013) <doi:10.5705/ss.2011.074>,
  exponential Wang, Fan, and Zhu <doi:10.1007/s10463-016-0588-3>, 
  smooth integration of counting and absolute deviation Lv and Fan (2009) <doi:10.1214/09-AOS683>,
  logarithm Mazumder, Friedman, and Hastie (2011) <doi:10.1198/jasa.2011.tm09738>,
  Lq, smoothly clipped absolute deviation Fan and Li (2001) <doi:10.1198/016214501753382273>,
  and minimax concave penalty Zhang (2010) <doi:10.1214/09-AOS729>. There are also extensions
  for computing variable inclusion probabilities, multiple regression coefficients, and 
  statistical inference <doi:10.1214/15-EJS1031>.",2020-11-16,Donald Williams,NA,TRUE,https://github.com/donaldrwilliams/ggmncv,4016,1,2020-11-28T16:05:39Z,4016
ggmosaic,"Mosaic plots in the 'ggplot2' framework. Mosaic
    plot functionality is provided in a single 'ggplot2' layer by calling
    the geom 'mosaic'.",2021-02-23,Haley Jeppson,https://github.com/haleyjeppson/ggmosaic,TRUE,https://github.com/haleyjeppson/ggmosaic,132981,149,2021-03-05T15:51:54Z,892.489932885906
ggnetwork,Geometries to plot network objects with 'ggplot2'.,2021-07-06,François Briatte,https://github.com/briatte/ggnetwork,TRUE,https://github.com/briatte/ggnetwork,101170,112,2021-07-07T14:48:51Z,903.3035714285714
ggnormalviolin,"Uses 'ggplot2' to create normally distributed
    violin plots with specified means and standard deviations. This
    function can be useful in showing hypothetically normal distributions
    and confidence intervals.",2019-05-27,W. Joel Schneider,https://github.com/wjschne/ggnormalviolin,TRUE,https://github.com/wjschne/ggnormalviolin,13680,4,2021-05-10T20:49:18Z,3420
ggpacman,"A funny coding challenge to reproduce the game Pac-Man using 'ggplot2' and 'gganimate'.
    It provides a pre-defined moves set for Pac-Man and the ghosts for the first level of the
    game Pac-Man as well as polygon datasets to draw ghosts in 'ggplot2'.",2020-05-16,Mickaël Canouil,https://github.com/mcanouil/pacman,TRUE,https://github.com/mcanouil/pacman,5104,55,2021-04-24T07:48:01Z,92.8
ggparliament,Simple parliament plots using 'ggplot2'. Visualize election results as points in the architectural layout of the legislative chamber.,2018-09-30,Robert Hickman,https://github.com/robwhickman/ggparliament,TRUE,https://github.com/robwhickman/ggparliament,15361,131,2021-03-07T23:06:56Z,117.25954198473282
ggperiodic,Implements methods to plot periodic data in any arbitrary range on the fly.,2020-12-08,Elio Campitelli,https://github.com/eliocamp/ggperiodic,TRUE,https://github.com/eliocamp/ggperiodic,14908,20,2020-12-08T19:35:36Z,745.4
ggplot2,"A system for 'declaratively' creating graphics,
    based on ""The Grammar of Graphics"". You provide the data, tell 'ggplot2'
    how to map variables to aesthetics, what graphical primitives to use,
    and it takes care of the details.",2021-06-25,Thomas Lin Pedersen,"https://ggplot2.tidyverse.org,
https://github.com/tidyverse/ggplot2",TRUE,https://github.com/tidyverse/ggplot2,44273833,5042,2021-08-24T14:05:01Z,8781.006148353828
ggplotify,"Convert plot function call (using expression or formula) to 'grob' or 'ggplot' object that compatible to the 'grid' and 'ggplot2' ecosystem. With this package, we are able to e.g. using 'cowplot' to align plots produced by 'base' graphics, 'ComplexHeatmap', 'eulerr', 'grid', 'lattice', 'magick', 'pheatmap', 'vcd' etc. by converting them to 'ggplot' objects.",2021-09-02,Guangchuang Yu,https://github.com/GuangchuangYu/ggplotify,TRUE,https://github.com/guangchuangyu/ggplotify,369069,77,2021-09-02T15:47:05Z,4793.103896103896
ggpmisc,"Extensions to 'ggplot2' respecting the grammar of graphics
    paradigm. Statistics: locate and tag peaks and valleys; label plot with the
    equation of a fitted polynomial or other types of models; labels
    with P-value, R^2 or adjusted R^2 or information criteria for fitted models;
    label with ANOVA table for fitted models; label with summary for fitted
    models. Model fit classes for which suitable methods are provided by package
    'broom' and 'broom.mixed' are supported. Scales and stats to build volcano
    and quadrant plots based on outcomes, fold changes, p-values and false 
    discovery rates.",2021-09-03,Pedro J. Aphalo,"https://docs.r4photobiology.info/ggpmisc/,
https://github.com/aphalo/ggpmisc",TRUE,https://github.com/aphalo/ggpmisc,191689,17,2021-09-03T00:05:29Z,11275.823529411764
ggPMX,"At Novartis, we aimed at standardizing the set of diagnostic plots used for modeling 
  activities in order to reduce the overall effort required for generating such plots. 
  For this, we developed a guidance that proposes an adequate set of diagnostics and a toolbox, 
  called 'ggPMX' to execute them. 'ggPMX' is a toolbox that can generate all diagnostic plots at a quality sufficient 
  for publication and submissions using few lines of code. ",2021-05-29,Matthew Fidler,https://github.com/ggPMXdevelopment/ggPMX,TRUE,https://github.com/ggpmxdevelopment/ggpmx,17322,22,2021-06-15T13:02:02Z,787.3636363636364
ggpol,A 'ggplot2' extension for implementing parliament charts and several other useful visualizations. ,2020-11-08,Frederik Tiedemann,https://github.com/erocoar/ggpol,TRUE,https://github.com/erocoar/ggpol,54973,76,2020-11-08T13:24:05Z,723.328947368421
ggpp,"Extensions to 'ggplot2' respecting the grammar of graphics 
    paradigm. Geomas: geom_table(), geom_plot() and geom_grob() add insets to 
    plots using native data coordinates, while geom_table_npc(), geom_plot_npc()
    and geom_grob_npc() do the same using ""npc"" coordinates through new 
    aesthetics ""npcx"" and ""npcy"". Statistics: select observations based on 2D 
    density. Positions: radial nudging away from a center point and nudging away
    from a line or curve.",2021-07-31,Pedro J. Aphalo,"https://docs.r4photobiology.info/ggpp/,
https://github.com/aphalo/ggpp",TRUE,https://github.com/aphalo/ggpp,23268,17,2021-07-30T21:04:45Z,1368.7058823529412
ggprism,"Provides various themes, palettes, and other functions
    that are used to customise ggplots to look like they were made in 'GraphPad 
    Prism'. The 'Prism'-look is achieved with theme_prism() and 
    scale_fill|colour_prism(), axes can be changed with custom guides like 
    guide_prism_minor(), and significance indicators added with add_pvalue().",2021-06-08,Charlotte Dawson,"https://csdaw.github.io/ggprism/, https://github.com/csdaw/ggprism",TRUE,https://github.com/csdaw/ggprism,5821,68,2021-06-24T13:48:27Z,85.6029411764706
ggpubr,"The 'ggplot2' package is excellent and flexible for elegant data
    visualization in R. However the default generated plots requires some formatting
    before we can send them for publication. Furthermore, to customize a 'ggplot',
    the syntax is opaque and this raises the level of difficulty for researchers
    with no advanced R programming skills. 'ggpubr' provides some easy-to-use
    functions for creating and customizing 'ggplot2'- based publication ready plots.",2020-06-27,Alboukadel Kassambara,https://rpkgs.datanovia.com/ggpubr/,TRUE,https://github.com/kassambara/ggpubr,5806210,770,2020-10-20T21:53:23Z,7540.532467532467
ggpval,"Automatically performs desired statistical tests (e.g. wilcox.test(), t.test()) to compare between groups, 
    and adds the resulting p-values to the plot with an annotation bar.
    Visualizing group differences are frequently performed by boxplots, bar plots, etc.
    Statistical test results are often needed to be annotated on these plots. 
    This package provides a convenient function that works on 'ggplot2' objects, 
    performs the desired statistical test between groups of interest and annotates the test results on the plot.",2020-11-22,Jun Cheng,https://github.com/s6juncheng/ggpval,TRUE,https://github.com/s6juncheng/ggpval,22634,30,2020-11-25T20:55:43Z,754.4666666666667
ggquickeda,"Quickly and easily perform exploratory data analysis by uploading your
     data as a 'csv' file. Start generating insights using 'ggplot2' plots and
     'table1' tables with descriptive stats, all using an easy-to-use point and click 
     'Shiny' interface.",2021-06-22,Samer Mouksassi,https://github.com/smouksassi/ggquickeda,TRUE,https://github.com/smouksassi/ggquickeda,24177,49,2021-06-22T20:12:13Z,493.40816326530614
ggraph,"The grammar of graphics as implemented in ggplot2 is a poor fit for
    graph and network visualizations due to its reliance on tabular data input.
    ggraph is an extension of the ggplot2 API tailored to graph visualizations
    and provides the same flexible approach to building up plots layer by layer.",2021-02-23,Thomas Lin Pedersen,"https://ggraph.data-imaginist.com,
https://github.com/thomasp85/ggraph",TRUE,https://github.com/thomasp85/ggraph,1156257,862,2021-02-23T12:17:54Z,1341.3654292343388
ggrastr,Rasterize only specific layers of a 'ggplot2' plot while simultaneously keeping all labels and text in vector format. This allows users to keep plots within the reasonable size limit without loosing vector properties of the scale-sensitive information. ,2021-03-01,Evan Biederstedt,https://github.com/VPetukhov/ggrastr,TRUE,https://github.com/vpetukhov/ggrastr,53199,160,2021-02-28T12:56:50Z,332.49375
ggrepel,"Provides text and label geoms for 'ggplot2' that help to avoid
    overlapping text labels. Labels repel away from each other and away from the
    data points.",2021-01-15,Kamil Slowikowski,https://github.com/slowkow/ggrepel,TRUE,https://github.com/slowkow/ggrepel,8112620,901,2021-05-03T19:28:24Z,9004.017758046615
ggridges,Ridgeline plots provide a convenient way of visualizing changes in distributions over time or space. This package enables the creation of such plots in 'ggplot2'.,2021-01-08,Claus O. Wilke,https://wilkelab.org/ggridges/,TRUE,https://github.com/wilkelab/ggridges,1624296,345,2021-05-18T15:53:51Z,4708.104347826087
ggroups,"Calculates additive and dominance genetic relationship matrices and their inverses, in matrix and tabular-sparse formats. It includes functions for checking and processing pedigree, calculating inbreeding coefficients (Meuwissen & Luo, 1992 <doi:10.1186/1297-9686-24-4-305>), as well as functions to calculate the matrix of genetic group contributions (Q), and adding those contributions to the genetic merit of animals (Quaas (1988) <doi:10.3168/jds.S0022-0302(88)79691-5>). Calculation of Q is computationally extensive. There are computationally optimized functions to calculate Q.",2021-01-06,Mohammad Ali Nilforooshan,https://github.com/nilforooshan/ggroups,TRUE,https://github.com/nilforooshan/ggroups,14763,0,2020-12-18T23:32:08Z,NA
ggsci,"A collection of 'ggplot2' color palettes inspired by
    plots in scientific journals, data visualization libraries,
    science fiction movies, and TV shows.",2018-05-14,Nan Xiao,"https://nanx.me/ggsci/, https://github.com/road2stat/ggsci",TRUE,https://github.com/road2stat/ggsci,4378174,426,2021-07-17T02:52:44Z,10277.403755868545
ggseg,"Contains 'ggplot2' geom for plotting brain atlases using 
    simple features. The largest component of the package is the data 
    for the two built-in atlases. Mowinckel & Vidal-Piñeiro (2020)
    <doi:10.1177/2515245920928009>.",2021-09-03,Athanasia Mo Mowinckel,https://github.com/ggseg/ggseg,TRUE,https://github.com/ggseg/ggseg,1156,102,2021-09-02T08:21:16Z,11.333333333333334
ggseg3d,"Mainly contains a plotting function ggseg3d(), 
    and data of two standard brain atlases (Desikan-Killiany and aseg). 
    By far, the largest bit of the package is the data for each of the atlases.
    The functions and data enable users to plot tri-surface mesh plots of
    brain atlases, and customise these by projecting colours onto the brain
    segments based on values in their own data sets. Functions are wrappers
    for 'plotly'. Mowinckel & Vidal-Piñeiro (2020)
    <doi:10.1177/2515245920928009>.",2021-06-01,Athanasia Mo Mowinckel,https://github.com/ggseg/ggseg3d/,TRUE,https://github.com/ggseg/ggseg3d,1290,15,2021-06-01T07:30:20Z,86
ggshadow,"A collection of Geoms for R's 'ggplot2' library. geom_shadowpath(), geom_shadowline(), 
    geom_shadowstep() and geom_shadowpoint() functions draw a shadow below lines to make busy plots more 
    aesthetically pleasing. geom_glowpath(), geom_glowline(), geom_glowstep() and geom_glowpoint() add a 
    neon glow around lines to get a steampunk style.",2021-01-22,Marc Menem,https://github.com/marcmenem/ggshadow/,TRUE,https://github.com/marcmenem/ggshadow,2829,45,2021-01-15T14:02:19Z,62.86666666666667
ggsignif,"Enrich your 'ggplots' with group-wise comparisons.
    This package provides an easy way to indicate if two groups are
    significantly different. Commonly this is shown by a bracket on top
    connecting the groups of interest which itself is annotated with the
    level of significance (NS, *, **, ***).  The package provides a single
    layer (geom_signif()) that takes the groups for comparison and the
    test (t.test(), wilcox.text() etc.) as arguments and adds the
    annotation to the plot.",2021-06-14,Constantin Ahlmann-Eltze,"https://const-ae.github.io/ggsignif/,
https://github.com/const-ae/ggsignif",TRUE,https://github.com/const-ae/ggsignif,4612263,387,2021-08-27T10:08:08Z,11917.992248062015
ggsoccer,"The 'ggplot2' package provides a powerful set of tools 
  for visualising and investigating data. The 'ggsoccer' package provides a 
  set of functions for elegantly displaying and exploring soccer event data
  with 'ggplot2'. Providing extensible layers and themes, it is designed to
  work smoothly with a variety of popular sports data providers.",2020-06-21,Ben Torvaney,"https://torvaney.github.io/ggsoccer/,
https://github.com/Torvaney/ggsoccer",TRUE,https://github.com/torvaney/ggsoccer,22409,121,2021-01-09T23:09:59Z,185.19834710743802
ggspatial,"Spatial data plus the power of the ggplot2 framework means easier mapping when input 
  data are already in the form of spatial objects.",2021-01-04,Dewey Dunnington,"https://paleolimbot.github.io/ggspatial/,
https://github.com/paleolimbot/ggspatial",TRUE,https://github.com/paleolimbot/ggspatial,168317,288,2021-02-26T00:04:27Z,584.4340277777778
ggstance,"A 'ggplot2' extension that provides flipped components:
    horizontal versions of 'Stats' and 'Geoms', and vertical versions
    of 'Positions'. This package is now superseded by 'ggplot2' itself
    which now has full native support for horizontal layouts. It
    remains available for backward compatibility.",2020-12-17,Lionel Henry,https://github.com/lionel-/ggstance,TRUE,https://github.com/lionel-/ggstance,668100,190,2020-12-17T21:03:18Z,3516.315789473684
ggstar,"To create the multiple polygonal point layer for easily discernible shapes, 
             we developed the package, it is like the 'geom_point' of 'ggplot2'.
             It can be used to draw the scatter plot.",2021-04-07,Shuangbin Xu,https://github.com/xiangpin/ggstar/,TRUE,https://github.com/xiangpin/ggstar,12696,38,2021-06-28T03:04:48Z,334.10526315789474
ggstatsplot,"Extension of 'ggplot2', 'ggstatsplot' creates graphics with
    details from statistical tests included in the plots themselves. It
    provides an easier syntax to generate information-rich plots for
    statistical analysis of continuous (violin plots, scatterplots,
    histograms, dot plots, dot-and-whisker plots) or categorical (pie and
    bar charts) data. Currently, it supports the most common types of
    statistical approaches and tests: parametric, nonparametric, robust,
    and Bayesian versions of t-test/ANOVA, correlation analyses,
    contingency table analysis, meta-analysis, and regression analyses.",2021-06-09,Indrajeet Patil,"https://indrajeetpatil.github.io/ggstatsplot/,
https://github.com/IndrajeetPatil/ggstatsplot",TRUE,https://github.com/indrajeetpatil/ggstatsplot,151130,1268,2021-08-14T06:38:43Z,119.18769716088327
ggtext,"A 'ggplot2' extension that enables the rendering of
    complex formatted plot labels (titles, subtitles, facet labels,
    axis labels, etc.). Text boxes with automatic word wrap are also
    supported.",2020-12-17,Claus O. Wilke,https://wilkelab.org/ggtext/,TRUE,https://github.com/wilkelab/ggtext,612831,506,2020-12-15T02:24:38Z,1211.1284584980237
ggthemes,"Some extra themes, geoms, and scales for 'ggplot2'.
    Provides 'ggplot2' themes and scales that replicate the look of plots
    by Edward Tufte, Stephen Few, 'Fivethirtyeight', 'The Economist', 'Stata',
    'Excel', and 'The Wall Street Journal', among others.
    Provides 'geoms' for Tufte's box plot and range frame.",2021-01-20,Jeffrey B. Arnold,https://github.com/jrnold/ggthemes,TRUE,https://github.com/jrnold/ggthemes,2194363,1141,2021-01-18T22:06:13Z,1923.1928133216477
GGUM,"An implementation of the generalized graded unfolding model (GGUM) in R, see Roberts, Donoghue, and Laughlin (2000) <doi:10.1177/01466216000241001>). It allows to simulate data sets based on the GGUM. It fits the GGUM and the GUM, and it retrieves item and person parameter estimates. Several plotting functions are available (item and test information functions; item and test characteristic curves; item category response curves). Additionally, there are some functions that facilitate the communication between R and 'GGUM2004'. Finally, a model-fit checking utility, MODFIT(), is also available.",2021-02-21,Jorge N. Tendeiro,https://github.com/jorgetendeiro/GGUM/,TRUE,https://github.com/jorgetendeiro/ggum,17734,4,2021-02-18T23:33:54Z,4433.5
ggupset,"Replace the standard x-axis in 'ggplots' with a combination matrix
  to visualize complex set overlaps. 'UpSet' has introduced a new way to visualize
  the overlap of sets as an alternative to Venn diagrams. 
  This package provides a simple way to produce such plots using 'ggplot2'. 
  In addition it can convert any categorical axis into a combination
  matrix axis.",2020-05-05,Constantin Ahlmann-Eltze,https://github.com/const-ae/ggupset,TRUE,https://github.com/const-ae/ggupset,30407,237,2021-08-24T09:26:31Z,128.29957805907173
ggVennDiagram,"Easy-to-use functions to generate 2-7 sets Venn plot in publication quality. 
  'ggVennDiagram' plot Venn using well-defined geometry dataset and 'ggplot2'. The shapes of 2-4 sets 
  Venn use circles and ellipses, while the shapes of 4-7 sets Venn use irregular polygons (4 has both forms), which 
  are developed and imported from another package 'venn', authored by Adrian Dusa. We provided internal functions to 
  integrate shape data with user provided sets data, and calculated the geometry of every regions/intersections 
  of them, then separately plot Venn in three components: set edges, set labels, and regions.
  From version 1.0, it is possible to customize these components as you demand in ordinary 'ggplot2' grammar.",2021-07-07,Chun-Hui Gao,https://github.com/gaospecial/ggVennDiagram,TRUE,https://github.com/gaospecial/ggvenndiagram,33088,155,2021-07-17T01:51:35Z,213.47096774193548
ggvoronoi,Easy creation and manipulation of Voronoi diagrams using 'deldir' with visualization in 'ggplot2'.,2021-01-19,Robert C. Garrett,https://github.com/garretrc/ggvoronoi/,TRUE,https://github.com/garretrc/ggvoronoi,27696,21,2021-01-12T01:08:17Z,1318.857142857143
ghcm,"A statistical hypothesis test for conditional independence.
    Given residuals from a sufficiently powerful regression, it tests whether 
    the covariance of the residuals is vanishing. It can be applied to both
    discretely-observed functional data and multivariate data. 
    Details of the method can be found in Anton Rask Lundborg, Rajen D. Shah and Jonas
    Peters (2020) <arXiv:2101.07108>.",2021-01-25,Anton Rask Lundborg,https://github.com/arlundborg/ghcm,TRUE,https://github.com/arlundborg/ghcm,2002,0,2021-08-25T07:48:26Z,NA
ghee,"Provides a user friendly wrapper for the 'gh' package facilitating easy
  access to the REST API for 'GitHub'. Includes support for common tasks such as 
  creating and commenting on issues, inviting collaborators, and more.",2021-03-11,Jonathan Trattner,https://github.com/jdtrat/ghee,TRUE,https://github.com/jdtrat/ghee,1926,4,2021-08-11T18:13:02Z,481.5
ghibli,"Colour palettes inspired by Studio Ghibli <https://en.wikipedia.org/wiki/Studio_Ghibli> 
    films, ported to R for your enjoyment.",2020-04-16,Ewen Henderson,"https://ewenme.github.io/ghibli/, https://github.com/ewenme/ghibli",TRUE,https://github.com/ewenme/ghibli,23364,260,2021-09-03T10:35:39Z,89.86153846153846
ghql,"A 'GraphQL' client, with an R6 interface for initializing
    a connection to a 'GraphQL' instance, and methods for constructing
    queries, including fragments and parameterized queries. Queries
    are checked with the 'libgraphqlparser' C++ parser via the
    'gaphql' package.",2020-03-04,Scott Chamberlain,"https://github.com/ropensci/ghql (devel)
https://docs.ropensci.org/ghql (docs)",TRUE,https://github.com/ropensci/ghql,13889,117,2021-08-30T08:13:52Z,118.7094017094017
gifski,"Multi-threaded GIF encoder written in Rust: <https://gif.ski/>. 
    Converts images to GIF animations using pngquant's efficient cross-frame 
    palettes and temporal dithering with thousands of colors per frame.",2021-05-02,Jeroen Ooms,"https://gif.ski/ (upstream), https://github.com/r-rust/gifski
(devel)",TRUE,https://github.com/r-rust/gifski,674920,49,2021-05-02T12:02:58Z,13773.877551020409
gifti,"Functions to read in the geometry format under the 
    'Neuroimaging' 'Informatics' Technology Initiative ('NIfTI'), called 
    'GIFTI' <https://www.nitrc.org/projects/gifti/>. 
    These files contain surfaces of brain imaging data.",2020-11-11,John Muschelli,NA,TRUE,https://github.com/muschellij2/gifti,21721,3,2021-03-31T17:46:18Z,7240.333333333333
gigg,"A Gibbs sampler corresponding to a Group 
    Inverse-Gamma Gamma (GIGG) regression model with adjustment covariates. 
    Hyperparameters in the GIGG prior specification can either be fixed by the 
    user or can be estimated via Marginal Maximum Likelihood Estimation.
    Jonathan Boss, Jyotishka Datta, Xin Wang, Sung Kyun Park, Jian Kang, Bhramar 
    Mukherjee (2021) <arXiv:2102.10670>.",2021-03-09,Michael Kleinsasser,https://github.com/umich-cphds/gigg,TRUE,https://github.com/umich-cphds/gigg,1941,0,2021-03-05T14:07:10Z,NA
GillespieSSA2,"A fast, scalable, and versatile framework for
    simulating large systems with Gillespie's Stochastic Simulation
    Algorithm ('SSA').  This package is the spiritual successor to the
    'GillespieSSA' package originally written by Mario Pineda-Krch.
    Benefits of this package include major speed improvements (>100x),
    easier to understand documentation, and many unit tests that try to
    ensure the package works as intended. Cannoodt and Saelens et al. (2020) 
    <doi:10.1101/2020.02.06.936971>.",2021-05-18,Robrecht Cannoodt,https://github.com/rcannood/GillespieSSA2,TRUE,https://github.com/rcannood/gillespiessa2,13993,5,2021-05-19T06:15:57Z,2798.6
gim,"Implements the generalized integration model, which integrates individual-level data and summary statistics under a generalized linear model framework. It supports continuous and binary outcomes to be modeled by the linear and logistic regression models. For binary outcome, data can be sampled in prospective cohort studies or case-control studies. Described in Zhang et al. (2020)<doi:10.1093/biomet/asaa014>. ",2020-06-12,Han Zhang,https://github.com/zhangh12/gim,TRUE,https://github.com/zhangh12/gim,16175,0,2021-08-26T05:39:20Z,NA
gimme,"Automated identification and estimation of group- and
    individual-level relations in time series data.",2021-08-19,Kathleen Gates,https://github.com/GatesLab/gimme/,TRUE,https://github.com/gateslab/gimme,33877,15,2021-08-26T18:38:56Z,2258.4666666666667
gimms,"This is a set of functions to retrieve information about GIMMS
    NDVI3g files currently available online; download (and re-arrange, in the 
    case of NDVI3g.v0) the half-monthly data sets; import downloaded files from 
    ENVI binary (NDVI3g.v0) or NetCDF format (NDVI3g.v1) directly into R based 
    on the widespread 'raster' package; conduct quality control; and generate 
    monthly composites (e.g., maximum values) from the half-monthly input data. 
    As a special gimmick, a method is included to conveniently apply the 
    Mann-Kendall trend test upon 'Raster*' images, optionally featuring 
    trend-free pre-whitening to account for lag-1 autocorrelation.",2021-07-30,Florian Detsch,https://github.com/environmentalinformatics-marburg/gimms,TRUE,https://github.com/environmentalinformatics-marburg/gimms,24598,14,2021-07-30T09:35:27Z,1757
giscoR,"Tools to download data from the GISCO 
    (Geographic Information System of the Commission) Eurostat database
    <https://ec.europa.eu/eurostat/web/gisco>. Global and European map data available.
    This package is in no way officially related to or endorsed by Eurostat.",2021-04-13,Diego Hernangómez,"https://dieghernan.github.io/giscoR/,
https://github.com/dieghernan/giscoR",TRUE,https://github.com/dieghernan/giscor,8392,23,2021-09-01T08:17:19Z,364.8695652173913
git2r,"Interface to the 'libgit2' library, which is a pure C
    implementation of the 'Git' core methods. Provides access to 'Git'
    repositories to extract data and running some basic 'Git'
    commands.",2021-01-10,See AUTHORS file.,"https://docs.ropensci.org/git2r/ (website),
https://github.com/ropensci/git2r",TRUE,https://github.com/ropensci/git2r,6359296,172,2021-07-22T14:25:22Z,36972.651162790695
git2rdata,"The git2rdata package is an R package for writing and reading
    dataframes as plain text files.  A metadata file stores important
    information.  1) Storing metadata allows to maintain the classes of
    variables.  By default, git2rdata optimizes the data for file storage.
    The optimization is most effective on data containing factors.  The
    optimization makes the data less human readable.  The user can turn
    this off when they prefer a human readable format over smaller files.
    Details on the implementation are available in vignette(""plain_text"",
    package = ""git2rdata"").  2) Storing metadata also allows smaller row
    based diffs between two consecutive commits.  This is a useful feature
    when storing data as plain text files under version control.  Details
    on this part of the implementation are available in
    vignette(""version_control"", package = ""git2rdata"").  Although we
    envisioned git2rdata with a git workflow in mind, you can use it in
    combination with other version control systems like subversion or
    mercurial.  3) git2rdata is a useful tool in a reproducible and
    traceable workflow.  vignette(""workflow"", package = ""git2rdata"") gives
    a toy example.  4) vignette(""efficiency"", package = ""git2rdata"")
    provides some insight into the efficiency of file storage, git
    repository size and speed for writing and reading.  Please cite using
    <doi:10.5281/zenodo.1485309>.",2021-01-21,Thierry Onkelinx,https://ropensci.github.io/git2rdata/,TRUE,https://github.com/ropensci/git2rdata,13674,88,2021-01-20T18:47:50Z,155.38636363636363
gitcreds,"Query, set, delete credentials from the 'git' credential
    store. Manage 'GitHub' tokens and other 'git' credentials. This package
    is to be used by other packages that need to authenticate to 'GitHub'
    and/or other 'git' repositories.",2020-12-04,Gábor Csárdi,https://github.com/r-lib/gitcreds,TRUE,https://github.com/r-lib/gitcreds,1838944,13,2021-01-28T20:11:29Z,141457.23076923078
gitdown,"Read all commit messages of your local git repository and
    sort them according to tags or specific text pattern into chapters of
    a HTML book using 'bookdown'. The git history book presentation helps
    organisms required to testify for every changes in their source code,
    in relation to features requests.",2021-07-22,Sébastien Rochette,"https://thinkr-open.github.io/gitdown/,
https://github.com/Thinkr-open/gitdown",TRUE,https://github.com/thinkr-open/gitdown,1653,27,2021-07-21T12:37:37Z,61.22222222222222
gitear,"'Gitea' is a community managed, lightweight code hosting solution 
  were projects and their respective git repositories can be managed 
  <https://gitea.io>. This package gives an interface to the 'Gitea' API to 
  access and manage repositories, issues and organizations directly in R. ",2020-09-18,Frans van Dunné,https://ixpantia.github.io/gitear/,TRUE,https://github.com/ixpantia/gitear,4108,1,2021-01-28T18:06:57Z,4108
gitgadget,"An 'Rstudio' addin for version control that allows users to clone
    repositories, create and delete branches, and sync forks on GitHub, GitLab, etc.
    Furthermore, the addin uses the GitLab API to allow instructors to create
    forks and merge requests for all students/teams with one click of a button.",2021-08-11,Vincent Nijs,URL: https://github.com/vnijs/gitgadget,TRUE,https://github.com/vnijs/gitgadget,20158,17,2021-08-22T05:38:59Z,1185.764705882353
gitignore,"Simple interface to query gitignore.io to fetch
    gitignore templates that can be included in the .gitignore file. More
    than 450 templates are currently available.",2020-11-04,Philippe Massicotte,"https://docs.ropensci.org/gitignore/,
https://github.com/ropensci/gitignore",TRUE,https://github.com/ropensci/gitignore,13728,32,2020-11-20T18:29:22Z,429
gitlabr,"Provides R functions to access the API of the project and
    repository management web application 'GitLab'. For many common tasks
    (repository file access, issue assignment and status, commenting)
    convenience wrappers are provided, and in addition the full API can be
    used by specifying request locations. 'GitLab' is open-source software
    and can be self-hosted or used on <https://about.gitlab.com>.",2021-08-05,Sébastien Rochette,https://statnmap.github.io/gitlabr/,TRUE,https://github.com/statnmap/gitlabr,25836,20,2021-08-11T14:33:15Z,1291.8
gkgraphR,"A simple way to interact with and extract data from
  the official 'Google Knowledge Graph' API <https://developers.google.com/knowledge-graph/>.",2021-03-01,Ricardo Correia,https://github.com/racorreia/gkgraphR,TRUE,https://github.com/racorreia/gkgraphr,5008,1,2021-03-01T13:12:32Z,5008
glca,"Fits latent class analysis (LCA) including group variable and covariates. 
   The group variable can be handled either by multilevel LCA described in Vermunt (2003) <DOI:10.1111/j.0081-1750.2003.t01-1-00131.x> or standard LCA at each level of group variable.
   The covariates can be incorporated in the form of logistic regression (Bandeen-Roche et al. (1997) <DOI:10.1080/01621459.1997.10473658>).",2021-03-29,Youngsun Kim,https://kim0sun.github.io/glca/,TRUE,https://github.com/kim0sun/glca,6900,3,2021-09-01T03:57:34Z,2300
gllvm,"Analysis of multivariate data using generalized linear latent variable models (gllvm). 
      Estimation is performed using either Laplace approximation method or variational approximation method implemented via TMB (Kristensen et al., (2016), <doi:10.18637/jss.v070.i05>). 
      For details see Niku et al. (2019a) <doi:10.1371/journal.pone.0216129> and 
      Niku et al. (2019b) <doi:10.1111/2041-210X.13303>.",2021-07-28,Jenni Niku,https://github.com/JenniNiku/gllvm,TRUE,https://github.com/jenniniku/gllvm,24858,23,2021-09-02T11:07:57Z,1080.7826086956522
glmbb,"Find all hierarchical models of specified generalized linear
    model with information criterion (AIC, BIC, or AICc) within specified
    cutoff of minimum value.  Alternatively, find all such graphical models.
    Use branch and bound algorithm so we do not have to fit all models.",2020-11-21,Charles J. Geyer <charlie@stat.umn.edu>.,https://github.com/cjgeyer/glmbb,TRUE,https://github.com/cjgeyer/glmbb,21033,0,2021-01-26T22:43:48Z,NA
glmdisc,"A Stochastic-Expectation-Maximization (SEM) algorithm (Celeux et al. (1995) <https://hal.inria.fr/inria-00074164>) associated with a Gibbs sampler which purpose is to learn a constrained representation for logistic regression that is called quantization (Ehrhardt et al. (2019) <arXiv:1903.08920>). Continuous features are discretized and categorical features' values are grouped to produce a better logistic regression model. Pairwise interactions between quantized features are dynamically added to the model through a Metropolis-Hastings algorithm (Hastings, W. K. (1970) <doi:10.1093/biomet/57.1.97>).",2020-09-30,Adrien Ehrhardt,https://adimajo.github.io,TRUE,https://github.com/adimajo/glmdisc,14560,3,2021-02-04T18:39:07Z,4853.333333333333
GLMMadaptive,"Fits generalized linear mixed models for a single grouping factor under
    maximum likelihood approximating the integrals over the random effects with an 
    adaptive Gaussian quadrature rule; Jose C. Pinheiro and Douglas M. Bates (1995) 
    <doi:10.1080/10618600.1995.10474663>.  ",2021-07-05,Dimitris Rizopoulos,"https://drizopoulos.github.io/GLMMadaptive/,
https://github.com/drizopoulos/GLMMadaptive",TRUE,https://github.com/drizopoulos/glmmadaptive,97331,38,2021-07-05T17:54:22Z,2561.342105263158
glmmfields,"Implements Bayesian spatial and spatiotemporal
    models that optionally allow for extreme spatial deviations through
    time. 'glmmfields' uses a predictive process approach with random
    fields implemented through a multivariate-t distribution instead of
    the usual multivariate normal.  Sampling is conducted with 'Stan'.
    References: Anderson and Ward (2019) <doi:10.1002/ecy.2403>.",2020-07-09,Sean C. Anderson,https://github.com/seananderson/glmmfields,TRUE,https://github.com/seananderson/glmmfields,18594,35,2021-08-31T18:01:09Z,531.2571428571429
glmmSeq,"Using random and fixed effects to model expression at an individual gene level can highlight differences between sample groups over time. The most widely used differential gene expression tools are unable to fit linear mixed effect models, therefore do not capture interaction terms. This package uses negative binomial mixed effects models to fit gene expression with matched samples. This is particularly useful for investigating  changes in gene expression between groups of individuals over time, as seen in: Rivellese F., Surace A.E.A.,  Goldmann K., Sciacca E., Giorli G., Cubuk C., John C.R.,  Nerviani A., Fossati-Jimack L., Thorborn G., Humby F., Bombardieri M.,  Lewis M.J.,  Pitzalis C. (2021) ""Molecular Pathology Profiling of Synovial Tissue Predicts Response to Biologic Treatment in Rheumatoid Arthritis"" [Manuscript in preparation].",2021-03-30,Myles Lewis,https://github.com/KatrionaGoldmann/glmmSeq,TRUE,https://github.com/katrionagoldmann/glmmseq,2116,4,2021-05-18T13:38:52Z,529
glmmTMB,"Fit linear and generalized linear mixed models with various
    extensions, including zero-inflation. The models are fitted using maximum
    likelihood estimation via 'TMB' (Template Model Builder). Random effects are
    assumed to be Gaussian on the scale of the linear predictor and are integrated
    out using the Laplace approximation. Gradients are calculated using automatic
    differentiation.",2021-09-01,Ben Bolker,https://github.com/glmmTMB/glmmTMB,TRUE,https://github.com/glmmtmb/glmmtmb,446785,146,2021-09-02T23:21:19Z,3060.171232876712
glmnetUtils,"Provides a formula interface for the 'glmnet' package for
    elasticnet regression, a method for cross-validating the alpha parameter,
    and other quality-of-life tools.",2021-02-25,Hong Ooi,https://github.com/hongooi73/glmnetUtils,TRUE,https://github.com/hongooi73/glmnetutils,128743,54,2021-02-25T14:33:26Z,2384.1296296296296
glmtree,"A logistic regression tree is a decision tree with logistic regressions at its leaves. A particular stochastic expectation maximization algorithm is used to draw a few good trees, that are then assessed via the user's criterion of choice among BIC / AIC / test set Gini. The formal development is given in a PhD chapter, see Ehrhardt (2019) <https://github.com/adimajo/manuscrit_these/releases/>.",2020-10-03,Adrien Ehrhardt,https://adimajo.github.io,TRUE,https://github.com/adimajo/glmtree,11116,5,2020-11-04T13:53:59Z,2223.2
GlobalOptions,"It provides more configurations on the option values such as validation
    and filtering on the values, making options invisible or private.",2020-06-10,Zuguang Gu,https://github.com/jokergoo/GlobalOptions,TRUE,https://github.com/jokergoo/globaloptions,500482,7,2021-07-01T12:46:55Z,71497.42857142857
globals,"Identifies global (""unknown"" or ""free"") objects in R expressions
    by code inspection using various strategies (ordered, liberal, or
    conservative).  The objective of this package is to make it as simple as
    possible to identify global objects for the purpose of exporting them in
    parallel, distributed compute environments.",2020-11-22,Henrik Bengtsson,https://github.com/HenrikBengtsson/globals,TRUE,https://github.com/henrikbengtsson/globals,4796268,25,2021-04-07T18:46:53Z,191850.72
glow,Provides a framework for creating plots with glowing points.,2020-08-31,Travers Ching,https://github.com/traversc/glow,TRUE,https://github.com/traversc/glow,4542,15,2020-09-25T00:19:23Z,302.8
glue,"An implementation of interpreted string literals, inspired by
  Python's Literal String Interpolation <https://www.python.org/dev/peps/pep-0498/> and Docstrings
  <https://www.python.org/dev/peps/pep-0257/> and Julia's Triple-Quoted String Literals
  <https://docs.julialang.org/en/v1.3/manual/strings/#Triple-Quoted-String-Literals-1>.",2020-08-27,Jim Hester,"https://github.com/tidyverse/glue, https://glue.tidyverse.org/",TRUE,https://github.com/tidyverse/glue,32833092,544,2021-01-06T14:00:35Z,60354.94852941176
gluedown,"Ease the transition between R vectors and markdown
    text. With 'gluedown' and 'rmarkdown', users can create traditional
    vectors in R, glue those strings together with the markdown syntax,
    and print those formatted vectors directly to the document. This
    package primarily uses GitHub Flavored Markdown (GFM), an offshoot of
    the unambiguous CommonMark specification by John MacFarlane (2019)
    <https://spec.commonmark.org/>.",2021-05-10,Kiernan Nicholls,"https://kiernann.com/gluedown/,
https://github.com/kiernann/gluedown/",TRUE,https://github.com/kiernann/gluedown,11458,88,2021-05-15T01:59:46Z,130.20454545454547
glycanr,"Useful utilities in N-glycan data analysis. This package tries
    to fill the gap in N-glycan data analysis by providing easy
    to use functions for basic operations on data
    (see <https://en.wikipedia.org/wiki/Glycomics> for more
    details on Glycomics). At the moment 'glycanr' is mostly oriented
    to data obtained by UPLC (Ultra Performance Liquid Chromatography)
    and LCMS (Liquid chromatography–mass spectrometry)
    analysis of Plasma and IgG glycome.",2021-03-29,Ivo Ugrina,https://github.com/iugrina/glycanr,TRUE,https://github.com/iugrina/glycanr,4331,2,2021-03-29T15:56:54Z,2165.5
gm,"Provides a simple and intuitive high-level language, with which
    you can create music easily. Takes care of all the dirty technical
    details in converting your music to musical scores and audio files.
    Works in 'R Markdown' documents <https://rmarkdown.rstudio.com/>,
    R 'Jupyter Notebooks' <https://jupyter.org/>, and 'RStudio'
    <https://www.rstudio.com/>, so you can embed generated music
    anywhere. Internally, uses 'MusicXML' <https://www.musicxml.com/> to
    represent musical scores, and 'MuseScore' <https://musescore.org/> to
    convert 'MusicXML'.",2021-04-17,Renfei Mao,"https://github.com/flujoo/gm, https://flujoo.github.io/gm/",TRUE,https://github.com/flujoo/gm,2704,82,2021-05-09T16:40:34Z,32.97560975609756
gmailr,"An interface to the 'Gmail' 'RESTful' API.  Allows
    access to your 'Gmail' messages, threads, drafts and labels.",2019-08-23,Jim Hester,https://github.com/r-lib/gmailr,TRUE,https://github.com/r-lib/gmailr,5817712,211,2021-06-29T14:00:26Z,27572.094786729856
GMCM,"Unsupervised Clustering and Meta-analysis using Gaussian Mixture
    Copula Models.",2019-11-05,Anders Ellern Bilgrau,https://github.com/AEBilgrau/GMCM,TRUE,https://github.com/aebilgrau/gmcm,29001,6,2020-11-18T21:58:38Z,4833.5
Gmisc,"Tools for making the descriptive ""Table 1"" used in medical
    articles, a transition plot for showing changes between categories
    (also known as a Sankey diagram), flow charts by extending the grid package,
    a method for variable selection based on the SVD, Bézier lines with arrows
    complementing the ones in the 'grid' package, and more.",2021-07-23,Max Gordon,https://gforge.se,TRUE,https://github.com/gforge/gmisc,76572,43,2021-07-22T19:16:52Z,1780.7441860465117
gMOIP,"Make 2D and 3D plots of linear programming (LP), 
    integer linear programming (ILP), or mixed integer linear programming (MILP) models 
    with up to three objectives. Plots of both the solution and criterion space are possible.
    For instance the non-dominated (Pareto) set for bi-objective LP/ILP/MILP programming models 
    (see vignettes for an overview). The package also contains an function for checking if a point
    is inside the convex hull.",2021-08-23,Lars Relund Nielsen,"https://relund.github.io/gMOIP/, https://github.com/relund/gMOIP/",TRUE,https://github.com/relund/gmoip,19425,2,2021-08-23T11:16:27Z,9712.5
gms,"A collection of tools to create, use and maintain modularized model code written in the modeling 
    language 'GAMS' (<https://www.gams.com/>). Out-of-the-box 'GAMS' does not come with support for modularized
    model code. This package provides the tools necessary to convert a standard 'GAMS' model to a modularized one
    by introducing a modularized code structure together with a naming convention which emulates local
    environments. In addition, this package provides tools to monitor the compliance of the model code with
    modular coding guidelines.",2020-07-01,Jan Philipp Dietrich,https://github.com/pik-piam/gms,TRUE,https://github.com/pik-piam/gms,4471,0,2021-08-19T12:01:20Z,NA
GMSE,"Integrates game theory and ecological theory to construct 
    social-ecological models that simulate the management of populations and 
    stakeholder actions. These models build off of a previously developed 
    management strategy evaluation (MSE) framework to simulate all aspects of 
    management: population dynamics, manager observation of populations, manager
    decision making, and stakeholder responses to management decisions. The 
    newly developed generalised management strategy evaluation (GMSE) 
    framework uses genetic algorithms to mimic the decision-making process of 
    managers and stakeholders under conditions of change, uncertainty, and 
    conflict. Simulations can be run using gmse(), gmse_apply(), and
    gmse_gui() functions.",2021-08-02,A. Bradley Duthie,https://confoobio.github.io/gmse/,TRUE,https://github.com/confoobio/gmse,17214,7,2021-08-03T10:35:30Z,2459.1428571428573
goeveg,"A collection of functions useful in (vegetation) community analyses and ordinations. Includes automatic species selection for ordination diagrams, NMDS stress plots, species response curves and rank-abundance curves.",2021-05-10,Friedemann Goral,https://github.com/fgoral/goeveg/,TRUE,https://github.com/fgoral/goeveg,28443,3,2021-05-10T18:28:35Z,9481
goffda,"Implementation of several goodness-of-fit tests for functional
    data. Currently, mostly related with the functional linear model with
    functional/scalar response and functional/scalar predictor. The package
    allows for the replication of the data applications considered in
    García-Portugués, Álvarez-Liébana, Álvarez-Pérez and González-Manteiga
    (2021) <doi:10.1111/sjos.12486>.",2021-08-19,Eduardo García-Portugués,https://github.com/egarpor/goffda,TRUE,https://github.com/egarpor/goffda,11498,8,2021-08-26T00:58:37Z,1437.25
golem,"An opinionated framework for building a
    production-ready 'Shiny' application. This package contains a series
    of tools for building a robust 'Shiny' application from start to
    finish.",2021-04-17,Colin Fay,https://github.com/ThinkR-open/golem,TRUE,https://github.com/thinkr-open/golem,91906,602,2021-09-01T08:04:12Z,152.66777408637873
GomoGomonoMi,"Allows the user to animate text within 'rmarkdown' documents and 'shiny' applications. 
    The animations are activated using the 'Animate.css' library. See <https://animate.style/> for more information.",2020-06-16,Mohamed El Fodil Ihaddaden,https://github.com/feddelegrand7/GomoGomonoMi,TRUE,https://github.com/feddelegrand7/gomogomonomi,4409,7,2020-12-10T20:52:50Z,629.8571428571429
goodpractice,"Give advice about good practices when building R packages.
    Advice includes functions and syntax to avoid, package structure,
    code complexity, code formatting, etc.",2018-05-02,Hannah Frick,https://github.com/mangothecat/goodpractice,TRUE,https://github.com/mangothecat/goodpractice,33814,339,2021-06-28T14:30:37Z,99.74631268436578
googleAnalyticsR,"Interact with the Google Analytics 
  APIs <https://developers.google.com/analytics/>, including 
  the Core Reporting API (v3 and v4), Management API, User Activity API
  GA4's Data API and Admin API and Multi-Channel Funnel API.",2021-04-17,Mark Edmondson,https://code.markedmondson.me/googleAnalyticsR/,TRUE,https://github.com/markedmondson1234/googleanalyticsr,191998,230,2021-08-24T05:56:06Z,834.7739130434783
googleAuthR,"Create R functions that interact with OAuth2 Google APIs 
    <https://developers.google.com/apis-explorer/> easily,
    with auto-refresh and Shiny compatibility.",2021-04-02,Mark Edmondson,https://code.markedmondson.me/googleAuthR/,TRUE,https://github.com/markedmondson1234/googleauthr,376402,153,2021-04-02T15:58:59Z,2460.143790849673
googleCloudRunner,"Tools to easily enable R scripts in the Google Cloud Platform.
  Utilise cloud services such as Cloud Run <https://cloud.google.com/run/> for R over HTTP, 
  Cloud Build <https://cloud.google.com/cloud-build/> for Continuous Delivery 
  and Integration services and 
  Cloud Scheduler <https://cloud.google.com/scheduler/> for scheduled scripts.",2021-01-30,Mark Edmondson,https://code.markedmondson.me/googleCloudRunner/,TRUE,https://github.com/markedmondson1234/googlecloudrunner,11376,54,2021-09-01T15:31:58Z,210.66666666666666
googleCloudStorageR,"Interact with Google Cloud Storage <https://cloud.google.com/storage/> 
  API in R. Part of the 'cloudyr' <https://cloudyr.github.io/> project.",2021-01-05,Mark Edmondson,https://code.markedmondson.me/googleCloudStorageR/,TRUE,https://github.com/cloudyr/googlecloudstorager,139421,84,2021-04-23T13:09:04Z,1659.7738095238096
googleComputeEngineR,"Interact with the 'Google Compute Engine' API in R. Lets you create, 
  start and stop instances in the 'Google Cloud'.  Support for preconfigured instances, 
  with templates for common R needs. ",2019-05-04,Mark Edmondson,https://cloudyr.github.io/googleComputeEngineR/,TRUE,https://github.com/cloudyr/googlecomputeenginer,46129,133,2021-08-19T06:50:35Z,346.83458646616543
googledrive,Manage Google Drive files from R.,2021-07-08,Jennifer Bryan,"https://googledrive.tidyverse.org,
https://github.com/tidyverse/googledrive",TRUE,https://github.com/tidyverse/googledrive,2901359,232,2021-08-25T19:24:45Z,12505.85775862069
googlePublicData,"Provides a collection of functions to set up 'Google Public Data Explorer'
  <https://www.google.com/publicdata/> data visualization tool with your own data,
  building automatically the corresponding DataSet Publishing Language file, or
  DSPL (XML), metadata file jointly with the CSV files. All zip-up and ready to
  be published in 'Public Data Explorer'.",2017-11-06,George Vega Yon,http://github.com/gvegayon/googlePublicData/,TRUE,https://github.com/gvegayon/googlepublicdata,17810,8,2021-06-06T07:25:52Z,2226.25
googlesheets,Interact with Google Sheets from R.,2018-06-29,Jennifer Bryan,https://github.com/jennybc/googlesheets,TRUE,https://github.com/jennybc/googlesheets,585143,779,2020-09-07T03:53:41Z,751.1463414634146
googlesheets4,"Interact with Google Sheets through the Sheets API v4
    <https://developers.google.com/sheets/api>. ""API"" is an acronym for
    ""application programming interface""; the Sheets API allows users to
    interact with Google Sheets programmatically, instead of via a web
    browser. The ""v4"" refers to the fact that the Sheets API is currently
    at version 4. This package can read and write both the metadata and
    the cell data in a Sheet.",2021-07-21,Jennifer Bryan,"https://googlesheets4.tidyverse.org,
https://github.com/tidyverse/googlesheets4",TRUE,https://github.com/tidyverse/googlesheets4,2665019,266,2021-08-17T18:18:17Z,10018.868421052632
googleway,"Provides a mechanism to plot a 'Google Map' from 'R' and overlay
    it with shapes and markers. Also provides access to 'Google Maps' APIs,
    including places, directions, roads, distances, geocoding, elevation and
    timezone.",2020-11-16,David Cooley,NA,TRUE,https://github.com/symbolixau/googleway,138159,198,2021-02-04T07:53:18Z,697.7727272727273
gotop,"Add a scroll back to top 'Font Awesome' icon 
    <https://fontawesome.com/> in 'rmarkdown' documents and 'shiny'
    apps thanks to 'jQuery GoTop' <https://scottdorman.blog/jquery-gotop/>.",2020-10-31,Félix Luginbuhl,"https://felixluginbuhl.com/gotop, https://github.com/lgnbhl/gotop",TRUE,https://github.com/lgnbhl/gotop,8373,10,2021-02-27T11:17:10Z,837.3
govdown,"A suite of custom R Markdown formats and templates
    for authoring web pages styled with the GOV.UK Design System.",2021-03-10,Duncan Garmonsway,https://ukgovdatascience.github.io/govdown/,TRUE,https://github.com/ukgovdatascience/govdown,11720,38,2021-03-11T10:56:10Z,308.42105263157896
goxygen,"A collection of tools which extract a model documentation from 'GAMS' code and comments. 
             In order to use the package you need to install 'pandoc' and 'pandoc-citeproc' 
             first (<https://pandoc.org/>).",2020-08-16,Jan Philipp Dietrich,"https://github.com/pik-piam/goxygen,
https://doi.org/10.5281/zenodo.1411404",TRUE,https://github.com/pik-piam/goxygen,5061,3,2020-12-23T13:16:51Z,1687
GPareto,"Gaussian process regression models, a.k.a. Kriging models, are
    applied to global multi-objective optimization of black-box functions.
    Multi-objective Expected Improvement and Step-wise Uncertainty Reduction
    sequential infill criteria are available. A quantification of uncertainty
    on Pareto fronts is provided using conditional simulations.",2021-05-31,Mickael Binois,https://github.com/mbinois/GPareto,TRUE,https://github.com/mbinois/gpareto,67602,12,2021-06-21T10:47:17Z,5633.5
gpboost,An R package that allows for combining tree-boosting with Gaussian process and mixed effects models. It also allows for independently doing tree-boosting as well as inference and prediction for Gaussian process and mixed effects models. See <https://github.com/fabsig/GPBoost> for more information on the software and Sigrist (2020) <arXiv:2004.02653> and Sigrist (2021) <arXiv:2105.08966> for more information on the methodology.,2021-08-17,Fabio Sigrist,https://github.com/fabsig/GPBoost,TRUE,https://github.com/fabsig/gpboost,10209,237,2021-08-24T08:09:49Z,43.075949367088604
gpbStat,"Performs statistical data analysis of various Plant Breeding experiments. Contains functions for Line by Tester analysis as per Arunachalam, V.(1974) <http://repository.ias.ac.in/89299/> and Diallel analysis as per Griffing, B. (1956) <https://www.publish.csiro.au/bi/pdf/BI9560463>.  ",2021-08-10,Nandan Patil,https://github.com/nandp1/gpbStat/,TRUE,https://github.com/nandp1/gpbstat,5312,0,2021-08-14T08:01:42Z,NA
gpg,"Bindings to GnuPG for working with OpenGPG (RFC4880) cryptographic methods.
    Includes utilities for public key encryption, creating and verifying digital signatures,
    and managing your local keyring. Note that some functionality depends on the version of 
    GnuPG that is installed on the system. On Windows this package can be used together with
    'GPG4Win' which provides a GUI for managing keys and entering passphrases.",2021-07-26,Jeroen Ooms,https://github.com/jeroen/gpg,TRUE,https://github.com/jeroen/gpg,24295,15,2021-07-26T19:54:26Z,1619.6666666666667
gpindex,"A small package for calculating lots of different price indexes, and by extension quantity indexes. Provides tools to build and work with any type of bilateral generalized-mean index (of which most price indexes are), along with a few important indexes that don't belong to the generalized-mean family. Implements and extends many of the methods in Balk (2008, ISBN:978-1-107-40496-0) and ILO, IMF, OECD, Eurostat, UN, and World Bank (2020, ISBN:978-1-51354-298-0) for bilateral price indexes.",2021-08-04,Steve Martin,https://github.com/marberts/gpindex,TRUE,https://github.com/marberts/gpindex,8523,2,2021-08-19T10:50:22Z,4261.5
gplots,"Various R programming tools for plotting data, including:
  - calculating and plotting locally smoothed summary function as
    ('bandplot', 'wapply'),
  - enhanced versions of standard plots ('barplot2', 'boxplot2',
    'heatmap.2', 'smartlegend'),
  - manipulating colors ('col2hex', 'colorpanel', 'redgreen',
    'greenred', 'bluered', 'redblue', 'rich.colors'),
  - calculating and plotting two-dimensional data summaries ('ci2d',
    'hist2d'),
  - enhanced regression diagnostic plots ('lmplot2', 'residplot'),
  - formula-enabled interface to 'stats::lowess' function ('lowess'),
  - displaying textual data in plots ('textplot', 'sinkplot'),
  - plotting a matrix where each cell contains a dot whose size
    reflects the relative magnitude of the elements ('balloonplot'),
  - plotting ""Venn"" diagrams ('venn'),
  - displaying Open-Office style plots ('ooplot'),
  - plotting multiple data on same region, with separate axes
    ('overplot'),
  - plotting means and confidence intervals ('plotCI', 'plotmeans'),
  - spacing points in an x-y plot so they don't overlap ('space').",2020-11-28,Gregory R. Warnes,https://github.com/talgalili/gplots,TRUE,https://github.com/talgalili/gplots,4832163,10,2021-01-09T15:46:06Z,483216.3
grainchanger,"Data aggregation via moving window or direct methods. Aggregate a 
    fine-resolution raster to a grid. The moving window method smooths the surface 
    using a specified function within a moving window of a specified size and shape 
    prior to aggregation. The direct method simply aggregates to the grid using the 
    specified function.",2021-02-01,Laura Graham,"https://docs.ropensci.org/grainchanger/,
https://github.com/ropensci/grainchanger",TRUE,https://github.com/ropensci/grainchanger,5381,47,2021-02-01T13:44:11Z,114.48936170212765
gramEvol,"A native R implementation of grammatical evolution (GE).
    GE facilitates the discovery of programs that can achieve a desired goal.
    This is done by performing an evolutionary optimisation over a population
    of R expressions generated via a user-defined context-free grammar (CFG)
    and cost function.",2020-07-18,Farzad Noorian,https://github.com/fnoorian/gramEvol/,TRUE,https://github.com/fnoorian/gramevol,29967,22,2021-08-31T11:36:06Z,1362.1363636363637
graph3d,"Create interactive visualization charts to draw data in three dimensional graphs. The graphs can be included in Shiny apps and R markdown documents, or viewed from the R console and 'RStudio' Viewer. Based on the 'vis.js' Graph3d module and the 'htmlwidgets' R package.",2020-11-12,Stéphane Laurent,https://github.com/stla/graph3d,TRUE,https://github.com/stla/graph3d,5903,3,2020-11-12T19:44:05Z,1967.6666666666667
grapherator,"Set of functions for step-wise generation of (weighted) graphs. Aimed for research in the field of single- and multi-objective combinatorial optimization. Graphs are generated adding nodes, edges and weights. Each step may be repeated multiple times with different predefined and custom generators resulting in high flexibility regarding the graph topology and structure of edge weights.",2017-12-21,Jakob Bossek,https://github.com/jakobbossek/grapherator,TRUE,https://github.com/jakobbossek/grapherator,15356,7,2021-06-29T15:30:18Z,2193.714285714286
graphhopper,"Provides a quick and easy access to the 'GraphHopper' Directions API.
  'GraphHopper' <https://www.graphhopper.com/> itself is a routing engine based on 'OpenStreetMap' data.
  API responses can be converted to simple feature (sf) objects in a convenient way. ",2021-02-06,Stefan Kuethe,https://github.com/crazycapivara/graphhopper-r,TRUE,https://github.com/crazycapivara/graphhopper-r,3664,16,2021-07-17T10:38:55Z,229
graphlayouts,"Several new layout algorithms to visualize networks are provided which are not part of 'igraph'. 
    Most are based on the concept of stress majorization by Gansner et al. (2004) <doi:10.1007/978-3-540-31843-9_25>. 
    Some more specific algorithms allow to emphasize hidden group structures in networks or focus on specific nodes.",2020-10-26,David Schoch,"http://graphlayouts.schochastics.net/,
https://github.com/schochastics/graphlayouts",TRUE,https://github.com/schochastics/graphlayouts,832211,185,2020-10-26T15:13:16Z,4498.437837837838
graphsim,"Functions to develop simulated continuous data (e.g., gene expression) from a sigma covariance matrix derived from a graph structure in 'igraph' objects. Intended to extend 'mvtnorm' to take 'igraph'  structures rather than sigma matrices as input. This allows the use of simulated data that correctly accounts for pathway relationships and correlations. This allows the use of simulated data that correctly accounts for pathway relationships and correlations. Here we present a versatile statistical framework to simulate  correlated gene expression data from biological pathways, by sampling from a multivariate normal distribution derived from a graph structure. This package allows the simulation of biological pathways from a graph structure based on a statistical model of gene expression. For example methods to infer biological pathways and gene regulatory networks from gene expression data can be tested on simulated datasets using this framework. This also allows for pathway structures to be considered as a confounding variable when simulating gene expression data to test the performance of genomic analyses.",2021-07-30,S. Thomas Kelly,https://github.com/TomKellyGenetics/graphsim/,TRUE,https://github.com/tomkellygenetics/graphsim,10603,14,2021-07-29T01:13:08Z,757.3571428571429
grates,"Provides a coherent interface and implementation for creating
  grouped date classes. This package is part of the RECON
  (<https://www.repidemicsconsortium.org/>) toolkit for outbreak analysis.",2021-05-28,Tim Taylor,"https://www.reconverse.org/grates/,
https://github.com/reconverse/grates",TRUE,https://github.com/reconverse/grates,6001,11,2021-05-29T08:18:57Z,545.5454545454545
gratia,"Graceful 'ggplot'-based graphics and utility functions for working with generalized additive models (GAMs) fitted using the 'mgcv' package. Provides a reimplementation of the plot() method for GAMs that 'mgcv' provides, as well as 'tidyverse' compatible representations of estimated smooths.",2021-04-18,Gavin L. Simpson,https://gavinsimpson.github.io/gratia/,TRUE,https://github.com/gavinsimpson/gratia,25848,108,2021-08-24T11:57:20Z,239.33333333333334
graticule,"Create graticule lines and labels for maps. Control the creation
    of lines by setting their placement (at particular meridians and parallels)
    and extent (along parallels and meridians). Labels are created independently of
    lines.",2021-05-04,Michael D. Sumner,https://github.com/mdsumner/graticule,TRUE,https://github.com/mdsumner/graticule,19506,18,2021-08-28T11:00:39Z,1083.6666666666667
gratis,"Generates time series based on mixture autoregressive models. Kang,Y.,Hyndman,R.,Li,F.(2020)<doi:10.1002/sam.11461>.",2020-10-08,Yanfei Kang,https://github.com/ykang/gratis,TRUE,https://github.com/ykang/gratis,6389,53,2020-11-16T02:39:44Z,120.54716981132076
grattan,"Utilities to cost and evaluate Australian tax policy, including fast
    projections of personal income tax collections, high-performance tax and 
    transfer calculators, and an interface to common indices from the Australian
    Bureau of Statistics.  Written to support Grattan Institute's Australian 
    Perspectives program, and related projects. Access to the Australian Taxation
    Office's sample files of personal income tax returns is assumed. ",2021-07-16,Hugh Parsonage,"https://github.com/HughParsonage/grattan,
https://hughparsonage.github.io/grattan/",TRUE,https://github.com/hughparsonage/grattan,30247,18,2021-02-05T11:35:48Z,1680.388888888889
gravitas,"Provides tools for systematically exploring large quantities of 
             temporal data across cyclic temporal granularities
             (deconstructions of time) by visualizing probability distributions.
             Cyclic time granularities can be circular, quasi-circular or 
             aperiodic. 'gravitas' computes cyclic
             single-order-up or multiple-order-up granularities, check the
             feasibility of creating plots for any two cyclic granularities
             and recommend probability distributions plots for exploring
             periodicity in the data.",2020-06-25,Sayani Gupta,https://github.com/Sayani07/gravitas/,TRUE,https://github.com/sayani07/gravitas,11831,13,2021-07-12T08:46:52Z,910.0769230769231
gravity,"A wrapper of different standard estimation methods for gravity models. 
  This package provides estimation methods for log-log models and multiplicative models.",2020-07-27,Mauricio Vargas,http://pachamaltese.github.io/gravity,TRUE,https://github.com/pachamaltese/gravity,29386,19,2021-02-18T04:51:51Z,1546.6315789473683
greed,"An ensemble of algorithms that enable the clustering of networks and data matrix such as counts matrix with different type of generative models. Model selection and clustering is performed in combination by optimizing the Integrated Classification Likelihood (which is equivalent to minimizing the description length). Several models are available such as: Stochastic Block Model, degree corrected Stochastic Block Model, Mixtures of Multinomial, Latent Block Model. The optimization is performed thanks to a combination of greedy local search and a genetic algorithm (see <arXiv:2002:11577> for more details).",2021-05-10,Etienne Côme,"https://comeetie.github.io/greed/,
https://github.com/comeetie/greed",TRUE,https://github.com/comeetie/greed,1835,5,2021-05-11T12:48:24Z,367
gremlin,"Fit linear mixed-effects models using restricted (or residual)
    maximum likelihood (REML) and with generalized inverse matrices to specify
    covariance structures for random effects. In particular, the package is
    suited to fit quantitative genetic mixed models, often referred to as
    'animal models'. Implements the average information algorithm as the main
    tool to maximize the restricted log-likelihood, but with other algorithms
    available.",2020-06-25,Matthew Wolak,http://github.com/matthewwolak/gremlin,TRUE,https://github.com/matthewwolak/gremlin,13106,2,2021-01-25T16:15:52Z,6553
GREMLINS,"We define generalized multipartite networks  as the joint observation of several networks implying some common pre-specified groups of individuals. The aim is to fit an adapted version of  the popular stochastic block model to multipartite networks, as described in Bar-hen, Barbillon and Donnet (2020) <arXiv:1807.10138>.  ",2020-11-25,Sophie Donnet,https://demiperimetre.github.io/GREMLINS/,TRUE,https://github.com/demiperimetre/gremlins,3628,0,2021-05-28T14:52:51Z,NA
greta,"Write statistical models in R and fit them by MCMC and optimisation on CPUs and GPUs, using Google 'TensorFlow'.
  greta lets you write your own model like in BUGS, JAGS and Stan, except that you write models right in R, it scales well to massive datasets, and it’s easy to extend and build on.
  See the website for more information, including tutorials, examples, package documentation, and the greta forum.",2019-08-09,Nick Golding,https://greta-stats.org,TRUE,https://github.com/greta-dev/greta,33842,457,2021-07-08T08:08:25Z,74.05251641137856
grex,"Convert 'Ensembl' gene identifiers from Genotype-Tissue
    Expression (GTEx) data to identifiers in other annotation systems,
    including 'Entrez', 'HGNC', and 'UniProt'.",2019-05-17,Nan Xiao,"https://nanx.me/grex/, https://github.com/nanxstats/grex",TRUE,https://github.com/nanxstats/grex,17992,6,2021-03-04T02:37:38Z,2998.6666666666665
greybox,"Implements functions and instruments for regression model building and its
             application to forecasting. The main scope of the package is in variables selection
             and models specification for cases of time series data. This includes promotional
             modelling, selection between different dynamic regressions with non-standard
             distributions of errors, selection based on cross validation, solutions to the fat
             regression model problem and more. Models developed in the package are tailored
             specifically for forecasting purposes. So as a results there are several methods
             that allow producing forecasts from these models and visualising them.",2021-06-27,"Ivan Svetunkov  (Lecturer at Centre for Marketing Analytics
    and Forecasting",https://github.com/config-i1/greybox,TRUE,https://github.com/config-i1/greybox,393184,20,2021-09-01T21:36:36Z,19659.2
grf,"A pluggable package for forest-based statistical estimation and inference.
    GRF currently provides methods for non-parametric least-squares regression,
    quantile regression, survival regression and treatment effect estimation (optionally using instrumental
    variables), with support for missing values.",2021-07-14,Julie Tibshirani,https://github.com/grf-labs/grf,TRUE,https://github.com/grf-labs/grf,113956,599,2021-08-27T00:12:36Z,190.24373956594323
gridpattern,"Provides 'grid' grobs that fill in a user-defined area with various patterns.  Includes enhanced versions of the geometric and image-based patterns originally contained in the 'ggpattern' package as well as original 'pch', 'polygon_tiling', 'regular_polygon', 'rose', 'text', 'wave', and 'weave' patterns plus support for custom user-defined patterns.",2021-07-17,Mike FC,"https://trevorldavis.com/R/gridpattern/,
https://github.com/trevorld/gridpattern",TRUE,https://github.com/trevorld/gridpattern,2613,23,2021-07-16T17:20:13Z,113.6086956521739
gridtext,"Provides support for rendering of formatted text using 'grid' graphics. Text can be
    formatted via a minimal subset of 'Markdown', 'HTML', and inline 'CSS' directives, and it can be
    rendered both with and without word wrap.",2020-12-10,Claus O. Wilke,https://wilkelab.org/gridtext/,TRUE,https://github.com/wilkelab/gridtext,628571,83,2021-07-28T20:14:44Z,7573.144578313253
groundhog,"Make R scripts that rely on packages reproducible, by ensuring that
    every time a given script is run, the same version of the used packages are
    loaded (instead of whichever version the user running the script happens to
    have installed). This is achieved by using the new command
    groundhog.library() instead of the base command library(), and including a
    date in the call. The date is used to call on the same version of the
    package every time (the most recent version available on CRAN at that date).",2021-05-03,Uri Simonsohn,"https://groundhogr.com/,
https://github.com/CredibilityLab/groundhog",TRUE,https://github.com/credibilitylab/groundhog,8146,31,2021-08-27T22:35:23Z,262.7741935483871
groupdata2,"Methods for dividing data into groups. 
    Create balanced partitions and cross-validation folds. 
    Perform time series windowing and general grouping and splitting of data. 
    Balance existing groups with up- and downsampling.",2021-07-03,Ludvig Renbo Olsen,https://github.com/ludvigolsen/groupdata2,TRUE,https://github.com/ludvigolsen/groupdata2,51486,20,2021-07-06T11:14:41Z,2574.3
groupr,"The 'groupr' package provides a more powerful version of grouped
  tibbles from 'dplyr'. It allows groups to be marked inapplicable,
  which is a simple but widely useful way to express structure in a dataset.
  It also provides powerful pivoting and other group manipulation functions.",2020-10-14,Nicholas Griffiths,https://github.com/ngriffiths21/groupr,TRUE,https://github.com/ngriffiths21/groupr,3731,0,2020-10-14T14:38:40Z,NA
growthcurver,"Fits the logistic equation to
    microbial growth curve data (e.g., repeated absorbance measurements
    taken from a plate reader over time). From this fit, a variety of
    metrics are provided, including the maximum growth rate,
    the doubling time, the carrying capacity, the area under the logistic
    curve, and the time to the inflection point. Method described in 
    Sprouffske and Wagner (2016) <doi:10.1186/s12859-016-1016-7>.",2020-10-19,Kathleen Sprouffske,https://github.com/sprouffske/growthcurver,TRUE,https://github.com/sprouffske/growthcurver,25119,19,2020-10-15T15:10:49Z,1322.0526315789473
growthrates,"A collection of methods to determine growth rates from
    experimental data, in particular from batch experiments and
    plate reader trials.",2020-11-02,Thomas Petzoldt,https://github.com/tpetzoldt/growthrates,TRUE,https://github.com/tpetzoldt/growthrates,28753,19,2020-11-02T12:32:29Z,1513.3157894736842
grpreg,"Efficient algorithms for fitting the regularization path of linear
  regression, GLM, and Cox regression models with grouped penalties.  This
  includes group selection methods such as group lasso, group MCP, and
  group SCAD as well as bi-level selection methods such as the group
  exponential lasso, the composite MCP, and the group bridge.  For more
  information, see Breheny and Huang (2009) <doi:10.4310/sii.2009.v2.n3.a10>,
  Huang, Breheny, and Ma (2012) <doi:10.1214/12-sts392>, Breheny and Huang
  (2015) <doi:10.1007/s11222-013-9424-2>, and Breheny (2015)
  <doi:10.1111/biom.12300>, or visit the package homepage
  <https://pbreheny.github.io/grpreg/>.",2021-07-26,Patrick Breheny,"https://pbreheny.github.io/grpreg/,
https://github.com/pbreheny/grpreg",TRUE,https://github.com/pbreheny/grpreg,501080,23,2021-08-11T14:49:51Z,21786.08695652174
grpsel,"Provides tools for sparse regression modelling with grouped predictors using the group 
    subset selection penalty. Uses coordinate descent and local search algorithms to rapidly deliver 
    near optimal estimates. The group subset penalty can be combined with a group lasso or ridge 
    penalty for added shrinkage. Linear and logistic regression are supported, as are overlapping 
    groups.",2021-07-13,Ryan Thompson,https://github.com/ryan-thompson/grpsel,TRUE,https://github.com/ryan-thompson/grpsel,1656,2,2021-07-13T12:22:26Z,828
gsignal,"R implementation of the 'Octave' package 'signal', containing
    a variety of signal processing tools, such as signal generation and
    measurement, correlation and convolution, filtering, filter design,
    filter analysis and conversion, power spectrum analysis, system
    identification, decimation and sample rate change, and windowing.",2021-05-18,Geert van Boxtel,https://github.com/gjmvanboxtel/gsignal,TRUE,https://github.com/gjmvanboxtel/gsignal,1982,7,2021-07-23T23:03:54Z,283.14285714285717
gsisdecoder,"A set of high efficient functions to decode identifiers of National 
  Football League players.",2020-10-13,Sebastian Carl,https://github.com/mrcaseb/gsisdecoder,TRUE,https://github.com/mrcaseb/gsisdecoder,7994,0,2021-06-30T16:04:10Z,NA
gsl,"
 An R wrapper for some of the functionality of the
 Gnu Scientific Library.",2019-03-25,Robin K. S. Hankin,https://github.com/RobinHankin/gsl.git,TRUE,https://github.com/robinhankin/gsl,647498,8,2021-08-15T22:31:29Z,80937.25
GSODR,"Provides automated downloading, parsing, cleaning, unit conversion
    and formatting of Global Surface Summary of the Day ('GSOD') weather data
    from the from the USA National Centers for Environmental Information
    ('NCEI').  Units are converted from from United States Customary System
    ('USCS') units to International System of Units ('SI').  Stations may be
    individually checked for number of missing days defined by the user, where
    stations with too many missing observations are omitted.  Only stations with
    valid reported latitude and longitude values are permitted in the final
    data.  Additional useful elements, saturation vapour pressure ('es'), actual
    vapour pressure ('ea') and relative humidity ('RH') are calculated from the
    original data using the improved August-Roche-Magnus approximation (Alduchov
    & Eskridge 1996) and included in the final data set.  The resulting metadata
    include station identification information, country, state, latitude,
    longitude, elevation, weather observations and associated flags.  For
    information on the 'GSOD' data from 'NCEI', please see the 'GSOD'
    'readme.txt' file available from,
    <https://www1.ncdc.noaa.gov/pub/data/gsod/readme.txt>.",2021-07-04,Adam H. Sparks,https://docs.ropensci.org/GSODR/,TRUE,https://github.com/ropensci/gsodr,40752,73,2021-07-08T10:30:15Z,558.2465753424658
gstat,"Variogram modelling; simple, ordinary and universal point or block (co)kriging; spatio-temporal kriging; sequential Gaussian or indicator (co)simulation; variogram and variogram map plotting utility functions; supports sf and stars.",2021-03-19,Edzer Pebesma,https://github.com/r-spatial/gstat/,TRUE,https://github.com/r-spatial/gstat,694150,142,2021-05-25T16:43:53Z,4888.380281690141
gsw,"Provides an interface to the Gibbs 'SeaWater' ('TEOS-10') C library, version 3.05 (commit 'f7bfebf44f686034636facb09852f1d5760c27f5', dated 2021-03-27, available at <https://github.com/TEOS-10/GSW-C>, which stems from 'Matlab' and other code written by members of Working Group 127 of 'SCOR'/'IAPSO' (Scientific Committee on Oceanic Research / International Association for the Physical Sciences of the Oceans).",2021-07-07,Dan Kelley,"http://teos-10.github.io/GSW-R/,
https://teos-10.github.io/GSW-R/index.html",TRUE,https://github.com/teos-10/gsw-r,70371,6,2021-07-07T22:09:58Z,11728.5
gt,"Build display tables from tabular data with an easy-to-use set of
    functions. With its progressive approach, we can construct display tables
    with a cohesive set of table parts. Table values can be formatted using any
    of the included formatting functions. Footnotes and cell styles can be 
    precisely added through a location targeting system. The way in which 'gt'
    handles things for you means that you don't often have to worry about the
    fine details.",2021-08-07,Richard Iannone,"https://gt.rstudio.com/, https://github.com/rstudio/gt",TRUE,https://github.com/rstudio/gt,311372,1376,2021-08-09T16:07:56Z,226.2877906976744
gtable,"Tools to make it easier to work with ""tables"" of
    'grobs'. The 'gtable' package defines a 'gtable' grob class that specifies a
    grid along with a list of grobs and their placement in the grid. Further the
    package makes it easy to manipulate and combine 'gtable' objects so that 
    complex compositions can be build up sequentially.",2019-03-25,Hadley Wickham,https://github.com/r-lib/gtable,TRUE,https://github.com/r-lib/gtable,15879075,68,2021-08-02T10:29:59Z,233515.8088235294
gtfs2gps,Convert general transit feed specification (GTFS) data to global positioning system (GPS) records in 'data.table' format. It also has some functions to subset GTFS data in time and space and to convert both representations to simple feature format.,2021-08-13,Rafael H. M. Pereira,https://github.com/ipeaGIT/gtfs2gps,TRUE,https://github.com/ipeagit/gtfs2gps,10673,50,2021-09-02T16:51:39Z,213.46
gtfsio,"Tools for the development of packages related to General
    Transit Feed Specification (GTFS) files. Establishes a standard for
    representing GTFS feeds using R data types. Provides fast and flexible
    functions to read and write GTFS feeds while sticking to this
    standard. Defines a basic 'gtfs' class which is meant to be extended
    by packages that depend on it. And offers utility functions that
    support checking the structure of GTFS objects.",2021-06-07,Daniel Herszenhut,"https://r-transit.github.io/gtfsio/,
https://github.com/r-transit/gtfsio",TRUE,https://github.com/r-transit/gtfsio,2946,8,2021-08-30T17:15:15Z,368.25
gtfsrouter,"Use GTFS (General Transit Feed Specification) data for
    routing from nominated start and end stations, and for extracting
    isochrones from nominated start station.",2021-06-11,Mark Padgham,https://github.com/ATFutures/gtfs-router,TRUE,https://github.com/atfutures/gtfs-router,13827,47,2021-07-13T12:02:58Z,294.1914893617021
gtfstools,"Utility functions to read, manipulate, analyse and
    write transit feeds in the General Transit Feed Specification (GTFS)
    data format.",2021-02-23,Daniel Herszenhut,"https://ipeagit.github.io/gtfstools/,
https://github.com/ipeaGIT/gtfstools",TRUE,https://github.com/ipeagit/gtfstools,2222,11,2021-08-02T16:03:02Z,202
gtools,"Functions to assist in R programming, including:
  - assist in developing, updating, and maintaining R and R packages ('ask', 'checkRVersion',
    'getDependencies', 'keywords', 'scat'),
  - calculate the logit and inverse logit transformations ('logit', 'inv.logit'),
  - test if a value is missing, empty or contains only NA and NULL values ('invalid'),
  - manipulate R's .Last function ('addLast'),
  - define macros ('defmacro'),
  - detect odd and even integers ('odd', 'even'),
  - convert strings containing non-ASCII characters (like single quotes) to plain ASCII ('ASCIIfy'),
  - perform a binary search ('binsearch'),
  - sort strings containing both numeric and character components ('mixedsort'),
  - create a factor variable from the quantiles of a continuous variable ('quantcut'),
  - enumerate permutations and combinations ('combinations', 'permutation'),
  - calculate and convert between fold-change and log-ratio ('foldchange',
    'logratio2foldchange', 'foldchange2logratio'),
  - calculate probabilities and generate random numbers from Dirichlet distributions
    ('rdirichlet', 'ddirichlet'),
  - apply a function over adjacent subsets of a vector ('running'),
  - modify the TCP\_NODELAY ('de-Nagle') flag for socket objects,
  - efficient 'rbind' of data frames, even if the column names don't match ('smartbind'),
  - generate significance stars from p-values ('stars.pval'),
  - convert characters to/from ASCII codes ('asc', 'chr'),
  - convert character vector to ASCII representation ('ASCIIfy'),
  - apply title capitalization rules to a character vector ('capwords').",2021-06-06,Gregory R. Warnes,https://github.com/r-gregmisc/gtools,TRUE,https://github.com/r-gregmisc/gtools,7802355,6,2021-06-03T15:32:22Z,1300392.5
gtrendsR,"An interface for retrieving and displaying the information
        returned online by Google Trends is provided. Trends (number of
        hits) over the time as well as geographic representation of the
        results can be displayed.",2021-02-23,Philippe Massicotte,https://github.com/PMassicotte/gtrendsR,TRUE,https://github.com/pmassicotte/gtrendsr,1116286,279,2021-06-16T15:29:39Z,4001.0250896057346
gtsummary,"Creates presentation-ready tables summarizing data
    sets, regression models, and more. The code to create the tables is
    concise and highly customizable. Data frames can be summarized with
    any function, e.g. mean(), median(), even user-written functions.
    Regression models are summarized and include the reference rows for
    categorical variables. Common regression models, such as logistic
    regression and Cox proportional hazards regression, are automatically
    identified and the tables are pre-filled with appropriate column
    headers.",2021-07-13,Daniel D. Sjoberg,"https://github.com/ddsjoberg/gtsummary,
http://www.danieldsjoberg.com/gtsummary/",TRUE,https://github.com/ddsjoberg/gtsummary,124015,546,2021-08-31T12:26:47Z,227.13369963369962
guaguas,"Datos de nombres inscritos en Chile
    entre 1920 y 2020, de acuerdo al Servicio de Registro Civil.
    English: Chilean baby names registered from 1920 to 2020
    by the Civil Registry Service.",2021-07-06,Riva Quiroga,https://github.com/rivaquiroga/guaguas,TRUE,https://github.com/rivaquiroga/guaguas,5771,29,2021-07-23T16:07:52Z,199
guardianapi,"Access to 'The Guardian' newspaper's open API
  <https://open-platform.theguardian.com/>, containing all articles published 
  in 'The Guardian' from 1999 to the present, including article text, metadata,
  tags and contributor information. An API key and registration is required.",2019-06-23,Evan Odell,https://docs.evanodell.com/guardianapi,TRUE,https://github.com/evanodell/guardianapi,13768,4,2020-11-22T13:32:00Z,3442
Guerry,"Maps of France in 1830, multivariate datasets from A.-M. Guerry and others, and statistical and 
	graphic methods related to Guerry's ""Moral Statistics of France"". The goal is to facilitate the exploration and
	development of statistical and graphic methods for multivariate data in a geo-spatial context of historical interest.",2021-07-29,Michael Friendly,https://github.com/friendly/Guerry,TRUE,https://github.com/friendly/guerry,67091,0,2021-08-25T10:29:37Z,NA
GUILDS,"A collection of sampling formulas for the unified neutral model of biogeography and biodiversity. Alongside the sampling formulas, it includes methods to perform maximum likelihood optimization of the sampling formulas, methods to generate data given the neutral model, and methods to estimate the expected species abundance distribution. Sampling formulas included in the GUILDS package are the Etienne Sampling Formula (Etienne 2005), the guild sampling formula, where guilds are assumed to differ in dispersal ability (Janzen et al. 2015), and  the guilds sampling formula conditioned on guild size (Janzen et al. 2015).",2016-09-26,Thijs Janzen,https://github.com/thijsjanzen/GUILDS,TRUE,https://github.com/thijsjanzen/guilds,24559,1,2020-12-16T12:36:05Z,24559
gustave,"Provides a toolkit for analytical variance estimation in survey sampling. Apart from the implementation of standard variance estimators, its main feature is to help the sampling expert produce easy-to-use variance estimation ""wrappers"", where systematic operations (linearization, domain estimation) are handled in a consistent and transparent way.",2019-12-16,Martin Chevalier,https://github.com/martinchevalier/gustave,TRUE,https://github.com/martinchevalier/gustave,13969,4,2021-08-24T07:53:34Z,3492.25
gutenbergr,"Download and process public domain works in the Project
    Gutenberg collection <http://www.gutenberg.org/>. Includes metadata for
    all Project Gutenberg works, so that they can be searched and retrieved.",2021-06-01,David Robinson,"https://docs.ropensci.org/gutenbergr/,
https://github.com/ropensci/gutenbergr",TRUE,https://github.com/ropensci/gutenbergr,163188,83,2020-09-20T23:14:16Z,1966.120481927711
gvc,"Several tools for Global Value Chain ('GVC') analysis are
    implemented.",2021-05-10,Bastiaan Quast,"https://qua.st/gvc, https://github.com/bquast/gvc",TRUE,https://github.com/bquast/gvc,24405,9,2021-05-09T12:25:38Z,2711.6666666666665
gwaRs,"Generate Manhattan, Q-Q, and PCA plots from GWAS and PCA results using 'ggplot2'.",2021-04-19,Lindokuhle Nkambule,https://github.com/LindoNkambule/gwaRs,TRUE,https://github.com/lindonkambule/gwars,3008,2,2021-04-25T17:49:49Z,1504
gwasforest,"Extract and reform data from GWAS (genome-wide association study) results, and then make a single integrated forest plot containing multiple windows of which each shows the result of individual SNPs (or other items of interest).",2020-11-24,Yili Xu,https://github.com/yilixu/gwasforest,TRUE,https://github.com/yilixu/gwasforest,3195,1,2021-01-27T22:19:35Z,3195
gwpcormapper,"An interactive mapping tool for geographically weighted correlation and partial correlation. Geographically weighted partial correlation coefficients are calculated following (Percival and Tsutsumida, 2017)<doi:10.1553/giscience2017_01_s36> and are described in greater detail in (Tsutsumida et al., 2019)<doi:10.5194/ica-abs-1-372-2019> and (Percival et al., 2021)<arXiv:2101.03491>.",2021-04-06,Joseph Emile Honour Percival,https://github.com/gwpcor/gwpcormapper,TRUE,https://github.com/gwpcor/gwpcormapper,1715,1,2021-04-12T12:17:25Z,1715
gwsem,"Melds genome-wide association tests with structural
    equation modeling (SEM) using 'OpenMx'. This package contains
    low-level C/C++ code to rapidly read genetic data encoded in U.K.
    Biobank or 'plink' formats. Prebuilt modeling options include one and
    two factor models. Alternately, analyses may utilize arbitrary,
    user-provided SEMs.  See Verhulst, Maes, & Neale (2017)
    <doi:10.1007/s10519-017-9842-6> for details. An updated manuscript is
    in preparation.",2021-08-05,Joshua N. Pritikin,"https://github.com/jpritikin/gwsem,
https://jpritikin.github.io/gwsem/",TRUE,https://github.com/jpritikin/gwsem,11358,3,2021-09-01T22:19:43Z,3786
h2o4gpu,"Interface to 'H2O4GPU' <https://github.com/h2oai/h2o4gpu>, a collection of 'GPU' solvers for machine learning algorithms.",2021-05-17,Yuan Tang,https://github.com/h2oai/h2o4gpu,TRUE,https://github.com/h2oai/h2o4gpu,18197,427,2021-05-17T21:42:00Z,42.61592505854801
hackeRnews,"Use the <https://hacker-news.firebaseio.com/v0/> API through R. Retrieve
    posts, articles and other items in form of convenient R objects.",2019-12-13,Ryszard Szymanski,https://github.com/szymanskir/hackeRnews,TRUE,https://github.com/szymanskir/hackernews,8921,22,2020-09-17T21:51:21Z,405.5
hadron,"Toolkit to perform statistical analyses of correlation
   functions generated from Lattice Monte Carlo simulations. In
   particular, a class 'cf' for correlation functions and
   methods to analyse those are defined. This includes (blocked)
   bootstrap (based on the 'boot' package) and jackknife, but also an
   automatic determination of integrated autocorrelation
   times. 'hadron' also provides a very  general function
   bootstrap.nlsfit() to bootstrap a non-linear least squares fit.
   More specific functions are provided to extract hadronic quantities
   from Lattice Quantum Chromodynamics simulations, a particular Monte
   Carlo simulation,(see e.g. European Twisted Mass Collaboration, P. Boucaud et
   al. (2008) <doi:10.1016/j.cpc.2008.06.013>). Here, to determine
   energy eigenvalues of hadronic states, specific fitting routines
   and in particular the generalised eigenvalue method (see
   e.g. B. Blossier et al. (2009) <doi:10.1088/1126-6708/2009/04/094>
   and M. Fischer et al. (2020)
   <https://inspirehep.net/literature/1792113>) are implemented.
   In addition, input/output and plotting routines are available.",2020-09-10,Carsten Urbach,https://github.com/HISKP-LQCD/hadron,TRUE,https://github.com/hiskp-lqcd/hadron,5299,14,2021-04-09T09:51:12Z,378.5
hagis,"Analysis of plant pathogen pathotype survey data.  Functions
  provided calculate distribution of susceptibilities, distribution of
  complexities with statistics, pathotype frequency distribution, as well as
  diversity indices for pathotypes.  This package is meant to be a direct
  replacement for Herrmann, Löwer and Schachtel's (1999)
  <doi:10.1046/j.1365-3059.1999.00325.x> Habgood-Gilmour Spreadsheet, 'HaGiS',
  previously used for pathotype analysis.",2020-12-09,Austin G. McCoy,"https://github.com/openplantpathology/hagis,
https://openplantpathology.github.io/hagis/",TRUE,https://github.com/openplantpathology/hagis,12296,2,2020-12-13T05:57:02Z,6148
hakaiApi,"Initializes a class that obtains API credentials and provides
    a method to use those credentials to make GET requests to the 'Hakai'
    API server. Usage instructions are documented at
    <https://hakaiinstitute.github.io/hakai-api/>.",2021-05-07,Taylor Denouden,https://github.com/HakaiInstitute/hakai-api-client-r,TRUE,https://github.com/hakaiinstitute/hakai-api-client-r,1357,2,2021-05-07T16:04:29Z,678.5
hal9001,"A scalable implementation of the highly adaptive lasso algorithm,
  including routines for constructing sparse matrices of basis functions of the
  observed data, as well as a custom implementation of Lasso regression tailored
  to enhance efficiency when the matrix of predictors is composed exclusively of
  indicator functions. For ease of use and increased flexibility, the Lasso
  fitting routines invoke code from the 'glmnet' package by default. The highly
  adaptive lasso was first formulated and described by MJ van der Laan (2017)
  <doi:10.1515/ijb-2015-0097>, with practical demonstrations of its performance
  given by Benkeser and van der Laan (2016) <doi:10.1109/DSAA.2016.93>.",2021-01-22,Jeremy Coyle,https://github.com/tlverse/hal9001,TRUE,https://github.com/tlverse/hal9001,10983,30,2021-04-30T21:15:20Z,366.1
haldensify,"Conditional density estimation is a longstanding and challenging
    problem in statistical theory, and numerous proposals exist for optimally
    estimating such complex functions. Algorithms for nonparametric estimation
    of conditional densities based on a pooled hazard regression formulation and
    semiparametric estimation via conditional hazards modeling are implemented
    based on the highly adaptive lasso, a nonparametric regression function for
    efficient estimation with fast convergence under mild assumptions. The
    pooled hazards formulation implemented was first described by Díaz and
    van der Laan (2011) <doi:10.2202/1557-4679.1356>.",2020-09-16,Nima Hejazi,https://github.com/nhejazi/haldensify,TRUE,https://github.com/nhejazi/haldensify,9805,5,2021-07-29T17:45:13Z,1961
handlr,"Converts among many citation formats, including 'BibTeX',
    'Citeproc', 'Codemeta', 'RDF XML', 'RIS', 'Schema.org', and
    'Citation File Format'. A low level 'R6' class is provided, as well
    as stand-alone functions for each citation format for both read
    and write.",2020-10-15,Scott Chamberlain,"https://github.com/ropensci/handlr (devel),
https://docs.ropensci.org/handlr/ (docs)",TRUE,https://github.com/ropensci/handlr,14332,33,2020-10-29T22:26:36Z,434.3030303030303
hansard,"Provides functions to download data from the 
  <http://www.data.parliament.uk/> APIs. Because of the structure of the API, 
  there is a named function for each type of available data for ease of use, 
  as well as some functions designed to retrieve specific pieces of commonly 
  used data. Functions for each new API will be added as and when they become
  available.",2019-11-13,Evan Odell,https://docs.evanodell.com/hansard,TRUE,https://github.com/evanodell/hansard,23922,24,2020-12-17T12:46:53Z,996.75
hardhat,"Building modeling packages is hard. A large amount
    of effort generally goes into providing an implementation for a new
    method that is efficient, fast, and correct, but often less emphasis
    is put on the user interface. A good interface requires specialized
    knowledge about S3 methods and formulas, which the average package
    developer might not have.  The goal of 'hardhat' is to reduce the
    burden around building new modeling packages by providing
    functionality for preprocessing, predicting, and validating input.",2021-07-14,Davis Vaughan,https://github.com/tidymodels/hardhat,TRUE,https://github.com/tidymodels/hardhat,353317,79,2021-08-27T13:58:25Z,4472.367088607595
hashr,"Apply an adaptation of the SuperFastHash algorithm to any R
    object. Hash whole R objects or, for vectors or lists, hash R objects to obtain
    a set of hash values that is stored in a structure equivalent to the input. See
    <http://www.azillionmonkeys.com/qed/hash.html> for a description of the hash
    algorithm.",2021-09-02,Mark van der Loo,https://github.com/markvanderloo/hashr,TRUE,https://github.com/markvanderloo/hashr,20827,7,2021-09-01T14:42:11Z,2975.285714285714
hasseDiagram,Drawing Hasse diagram - visualization of transitive reduction of a finite partially ordered set.,2021-06-10,Krzysztof Ciomek,https://github.com/kciomek/hasseDiagram,TRUE,https://github.com/kciomek/hassediagram,23664,7,2021-06-10T06:00:38Z,3380.5714285714284
haven,"Import foreign statistical formats into R via the embedded
    'ReadStat' C library, <https://github.com/WizardMac/ReadStat>.",2021-08-04,Hadley Wickham,"https://haven.tidyverse.org, https://github.com/tidyverse/haven,
https://github.com/WizardMac/ReadStat",TRUE,https://github.com/tidyverse/haven,16000471,347,2021-08-18T13:36:12Z,46110.8674351585
hBayesDM,"
    Fit an array of decision-making tasks with computational models in
    a hierarchical Bayesian framework. Can perform hierarchical Bayesian analysis of
    various computational models with a single line of coding
    (Ahn et al., 2017) <doi:10.1162/CPSY_a_00002>.",2021-05-03,Woo-Young Ahn,https://github.com/CCS-Lab/hBayesDM,TRUE,https://github.com/ccs-lab/hbayesdm,28915,140,2021-07-29T04:01:19Z,206.53571428571428
HCmodelSets,"Software for performing the reduction, exploratory and model selection phases of the procedure proposed by Cox, D.R. and Battey, H.S. (2017) <doi:10.1073/pnas.1703764114> for sparse regression when the number of potential explanatory variables far exceeds the sample size. The software supports linear regression, likelihood-based fitting of generalized linear regression models and the proportional hazards model fitted by partial likelihood.",2021-06-01,H. H. Hoeltgebaum,NA,TRUE,https://github.com/hhhelfer/hcmodelsets,17730,0,2021-05-31T13:21:54Z,NA
hddtools,"Tools to discover hydrological data, accessing catalogues and databases from various data providers. The package is described in Vitolo (2017) ""hddtools: Hydrological Data Discovery Tools"" <doi:10.21105/joss.00056>.",2021-02-14,Claudia Vitolo,"https://docs.ropensci.org/hddtools/,
https://github.com/ropensci/hddtools",TRUE,https://github.com/ropensci/hddtools,19774,39,2021-02-14T15:53:32Z,507.02564102564105
hdf5r,"'HDF5' is a data model, library and file format for storing
    and managing large amounts of data. This package provides a nearly
    feature complete, object oriented  wrapper for the 'HDF5' API
    <https://support.hdfgroup.org/HDF5/doc/RM/RM_H5Front.html> using R6 classes.
    Additionally, functionality is added so that 'HDF5' objects behave very
    similar to their corresponding R counterparts.",2021-08-25,Holger Hoefling,"https://hhoeflin.github.io/hdf5r/,
https://github.com/hhoeflin/hdf5r/",TRUE,https://github.com/hhoeflin/hdf5r,269514,47,2021-08-30T06:17:58Z,5734.340425531915
hdme,"Penalized regression for generalized linear models for
  measurement error problems (aka. errors-in-variables). The package
  contains a version of the lasso (L1-penalization) which corrects
  for measurement error (Sorensen et al. (2015) <doi:10.5705/ss.2013.180>). 
  It also contains an implementation of the Generalized Matrix Uncertainty 
  Selector, which is a version the (Generalized) Dantzig Selector for the 
  case of measurement error (Sorensen et al. (2018) <doi:10.1080/10618600.2018.1425626>).",2021-09-02,Oystein Sorensen,https://github.com/osorensen/hdme,TRUE,https://github.com/osorensen/hdme,18646,6,2021-08-26T12:24:24Z,3107.6666666666665
hdnom,"Creates nomogram visualizations for penalized Cox regression
    models, with the support of reproducible survival model building,
    validation, calibration, and comparison for high-dimensional data.",2019-06-23,Nan Xiao,"https://nanx.me/hdnom/, https://github.com/nanxstats/hdnom,
http://hdnom.io",TRUE,https://github.com/nanxstats/hdnom,23200,35,2021-07-19T05:26:42Z,662.8571428571429
HDPenReg,"Algorithms for lasso and fused-lasso problems: implementation of
    the lars algorithm for lasso and fusion penalization and EM-based
    algorithms for (logistic) lasso  and fused-lasso penalization.",2020-10-05,Quentin Grimonprez,NA,TRUE,https://github.com/modal-inria/hdpenreg,21594,1,2020-12-21T13:51:01Z,21594
hdpGLM,"Implementation of MCMC algorithms to estimate the Hierarchical Dirichlet Process Generalized Linear Model (hdpGLM) presented in the paper Ferrari (2020) Modeling Context-Dependent Latent Heterogeneity, Political Analysis <DOI:10.1017/pan.2019.13>.",2020-11-09,Diogo Ferrari,https://github.com/DiogoFerrari/hdpGLM,TRUE,https://github.com/diogoferrari/hdpglm,3416,5,2020-11-11T00:54:06Z,683.2
hdrcde,"Computation of highest density regions in one and two dimensions, kernel estimation of univariate density functions conditional on one covariate,and multimodal regression.",2021-01-18,Rob Hyndman,"https://pkg.robjhyndman.com/hdrcde/,
https://github.com/robjhyndman/hdrcde",TRUE,https://github.com/robjhyndman/hdrcde,253015,15,2021-01-18T05:12:10Z,16867.666666666668
HDShOP,"Constructs shrinkage estimators of high-dimensional mean-variance portfolios and performs 
    high-dimensional tests on optimality of a given portfolio. The techniques developed in 
    Bodnar et al. (2018) <doi:10.1016/j.ejor.2017.09.028>, Bodnar et al. (2019) 
    <doi:10.1109/TSP.2019.2929964>, Bodnar et al. (2020) <doi:10.1109/TSP.2020.3037369> 
    are central to the package. They provide simple and feasible estimators and tests for optimal 
    portfolio weights, which are applicable for 'large p and large n' situations where p is the 
    portfolio dimension (number of stocks) and n is the sample size. The package also includes tools
    for constructing portfolios based on shrinkage estimators of the mean vector and covariance matrix
    as well as a new Bayesian estimator for the Markowitz efficient frontier recently developed by 
    Bauder et al. (2021) <doi:10.1080/14697688.2020.1748214>.",2021-09-03,Taras Bodnar,https://github.com/Otryakhin-Dmitry/global-minimum-variance-portfolio,TRUE,https://github.com/otryakhin-dmitry/global-minimum-variance-portfolio,405,1,2021-09-02T21:13:13Z,405
HDTSA,"Procedures for high-dimensional time series analysis including factor analysis proposed by Lam and Yao (2012) <doi:10.1214/12-AOS970> and Chang, Guo and Yao (2015) <doi:10.1016/j.jeconom.2015.03.024>, martingale difference test proposed by Chang, Jiang and Shao (2021) preprint, principal component analysis proposed by Chang, Guo and Yao (2018) <doi:10.1214/17-AOS1613>, unit root test proposed by Chang, Cheng and Yao (2021) <arXiv:2006.07551> and white noise test proposed by Chang, Yao and Zhou (2017) <doi:10.1093/biomet/asw066>.",2021-06-04,Chen Lin,https://github.com/Linc2021/HDTSA,TRUE,https://github.com/linc2021/hdtsa,1245,0,2021-06-29T01:50:15Z,NA
healthfinance,"Provides a shiny interface for a free, open-source managerial
    accounting-like system for health care practices. This package allows
    health care administrators to project revenue with monthly adjustments
    and procedure-specific boosts up to a 3-year period. Granular data
    (patient-level) to aggregated data (department- or hospital-level) can
    all be used as valid inputs provided historical volume and revenue
    data is available. For more details on managerial accounting
    techniques, see Brewer et al. (2015, ISBN:9780078025792).",2020-10-24,Raoul Wadhwa,https://rrrlw.github.io/healthfinance/,TRUE,https://github.com/rrrlw/healthfinance,3618,2,2020-10-15T20:19:12Z,1809
healthyR,"
    Hospital data analysis workflow tools, modeling, and automations. This library
    provides many useful tools to review common administrative hospital data. Some 
    of these include average length of stay, readmission rates, average net pay
    amounts by service lines just to name a few. The aim is to provide a simple
    and consistent verb framework that takes the guesswork out of everything.",2021-08-20,Steven Sanderson,https://github.com/spsanderson/healthyR,TRUE,https://github.com/spsanderson/healthyr,4233,14,2021-08-23T13:39:54Z,302.35714285714283
healthyR.ai,"
    Hospital machine learning and ai data analysis workflow tools, modeling, and automations. 
    This library provides many useful tools to review common administrative 
    hospital data. Some of these include predicting length of stay, and readmits.
    The aim is to provide a simple and consistent verb framework that takes the 
    guesswork out of everything.",2021-09-03,Steven Sanderson,https://github.com/spsanderson/healthyR.ai,TRUE,https://github.com/spsanderson/healthyr.ai,201,1,2021-09-03T12:40:06Z,201
healthyR.data,Provides data for functions typically used in the 'healthyR' package.,2021-04-09,Steven Sanderson,https://github.com/spsanderson/healthyR.data,TRUE,https://github.com/spsanderson/healthyr.data,5157,2,2021-07-14T02:03:30Z,2578.5
healthyR.ts,"
    Hospital time series data analysis workflow tools, modeling, and automations. 
    This library provides many useful tools to review common administrative time 
    series hospital data. Some of these include average length of stay, and 
    readmission rates. The aim is to provide a simple and consistent verb 
    framework that takes the guesswork out of everything.",2021-08-23,Steven Sanderson,https://github.com/spsanderson/healthyR.ts,TRUE,https://github.com/spsanderson/healthyr.ts,3244,5,2021-08-23T13:36:42Z,648.8
heapsofpapers,"Makes it easy to download a large number of files such as PDF files 
    and CSV files, while automatically slowing down requests, letting you know 
    where it is up to, and adjusting for files that have already been downloaded.",2021-08-23,Rohan Alexander,https://github.com/RohanAlexander/heapsofpapers,TRUE,https://github.com/rohanalexander/heapsofpapers,140,0,2021-08-23T20:36:35Z,NA
heatmaply,"Create interactive cluster 'heatmaps' that can be saved as a stand-
    alone HTML file, embedded in 'R Markdown' documents or in a 'Shiny' app, and
    available in the 'RStudio' viewer pane. Hover the mouse pointer over a cell to
    show details or drag a rectangle to zoom. A 'heatmap' is a popular graphical
    method for visualizing high-dimensional data, in which a table of numbers
    are encoded as a grid of colored cells. The rows and columns of the matrix
    are ordered to highlight patterns and are often accompanied by 'dendrograms'.
    'Heatmaps' are used in many fields for visualizing observations, correlations,
    missing values patterns, and more. Interactive 'heatmaps' allow the inspection
    of specific value by hovering the mouse over a cell, as well as zooming into
    a region of the 'heatmap' by dragging a rectangle around the relevant area.
    This work is based on the 'ggplot2' and 'plotly.js' engine. It produces
    similar 'heatmaps' to 'heatmap.2' with the advantage of speed
    ('plotly.js' is able to handle larger size matrix), the ability to zoom from
    the 'dendrogram' panes, and the placing of factor variables in the sides of the
    'heatmap'.",2021-02-02,Tal Galili,"https://talgalili.github.io/heatmaply/,
https://cran.r-project.org/package=heatmaply,
https://github.com/talgalili/heatmaply/,
https://www.r-statistics.com/tag/heatmaply/",TRUE,https://github.com/talgalili/heatmaply,226804,295,2021-08-04T12:12:45Z,768.8271186440678
heatwaveR,"The different methods of defining and detecting extreme events, known as heatwaves or cold-spells in both air and water temperature data are encompassed within this package. These detection algorithms may be used on non-temperature data as well however, this is not catered for explicitly here as no use of this technique in the literature currently exists.",2021-01-07,Robert W. Schlegel,"https://robwschlegel.github.io/heatwaveR/index.html,
https://github.com/robwschlegel/heatwaveR",TRUE,https://github.com/robwschlegel/heatwaver,19995,21,2021-08-26T08:33:26Z,952.1428571428571
heddlr,"Helper functions designed to make
    dynamically generating R Markdown documents easier by providing a
    simple and tidy way to create report pieces, shape them to your data,
    and combine them for exporting into a single R Markdown document.",2020-03-24,Michael Mahoney,"https://github.com/mikemahoney218/heddlr,
https://mikemahoney218.github.io/heddlr/",TRUE,https://github.com/mikemahoney218/heddlr,9076,14,2021-06-19T13:51:46Z,648.2857142857143
hedgehog,"Hedgehog will eat all your bugs.
  'Hedgehog' is a property-based testing package in the spirit
  of 'QuickCheck'. With 'Hedgehog', one can test properties
  of their programs against randomly generated input, providing
  far superior test coverage compared to unit testing. One of the
  key benefits of 'Hedgehog' is integrated shrinking of
  counterexamples, which allows one to quickly find the cause of
  bugs, given salient examples when incorrect behaviour occurs.",2018-08-22,Huw Campbell,https://hedgehog.qa,TRUE,https://github.com/hedgehogqa/r-hedgehog,13822,37,2021-07-13T03:45:15Z,373.56756756756755
heemod,"An implementation of the modelling and reporting features described 
    in reference textbook and guidelines (Briggs, Andrew, et al. Decision 
    Modelling for Health Economic Evaluation. Oxford Univ. Press, 2011;
    Siebert, U. et al. State-Transition Modeling. Medical Decision Making 
    32, 690-700 (2012).): deterministic and probabilistic sensitivity analysis, 
    heterogeneity analysis, time dependency on state-time and model-time 
    (semi-Markov and non-homogeneous Markov models), etc.",2021-01-22,Kevin Zarca,NA,TRUE,https://github.com/pierucci/heemod,30200,34,2021-06-28T12:20:41Z,888.2352941176471
helda,"The main focus is on preprocessing and data visualization of machine learning models performances. 
  Some functions allow to fill in gaps in time series using linear interpolation on panel data, some functions 
  permit to draw lift effect and lift curve in order to benchmark machine learning models or you can even find 
  the optimal number of clusters in agglomerative clustering algorithm.",2021-01-06,Simon Corde,https://github.com/Redcart/helda,TRUE,https://github.com/redcart/helda,11199,0,2021-03-08T09:59:21Z,NA
helminthR,"Access to large host-parasite data is often hampered by the 
  availability of data and difficulty in obtaining it in a programmatic way
  to encourage analyses. 'helminthR' provides a programmatic interface to the 
  London Natural History Museum's host-parasite database, one of the largest 
  host-parasite databases existing currently <https://www.nhm.ac.uk/research-curation/scientific-resources/taxonomy-systematics/host-parasites/>. The package allows the user
  to query by host species, parasite species, and geographic location.",2021-08-16,Tad Dallas,"https://docs.ropensci.org/helminthR/,
https://github.com/rOpenSci/helminthR/",TRUE,https://github.com/ropensci/helminthr,14751,7,2021-08-16T19:39:47Z,2107.285714285714
helsinki,"Tools for accessing various open data sources in the Helsinki
    region in Finland. Current data sources include
    the Real Estate Department (<http://ptp.hel.fi/avoindata/>),
    Service Map API (<http://api.hel.fi/servicemap/v2/>),
    Linked Events API (<http://api.hel.fi/linkedevents/v1/>),
    Helsinki Region Infoshare statistics API (<https://dev.hel.fi/stats/>).",2021-09-03,Juuso Parkkinen,"http://ropengov.github.io/helsinki/,
https://github.com/rOpenGov/helsinki",TRUE,https://github.com/ropengov/helsinki,18617,7,2021-09-03T10:48:42Z,2659.5714285714284
heplots,"Provides HE plot and other functions for visualizing hypothesis
    tests in multivariate linear models. HE plots represent sums-of-squares-and-
    products matrices for linear hypotheses and for error using ellipses (in two
    dimensions) and ellipsoids (in three dimensions). The related 'candisc' package
    provides visualizations in a reduced-rank canonical discriminant space when
    there are more than a few response variables.",2021-01-21,Michael Friendly,http://friendly.github.io/heplots/,TRUE,https://github.com/friendly/heplots,241555,3,2021-01-20T20:03:13Z,80518.33333333333
here,"Constructs paths to your project's files.
    Declare the relative path of a file within your project with 'i_am()'.
    Use the 'here()' function as a drop-in replacement for 'file.path()',
    it will always locate the files relative to your project root.",2020-12-13,Kirill Müller,"https://here.r-lib.org/, https://github.com/r-lib/here",TRUE,https://github.com/r-lib/here,957592,299,2021-07-29T04:14:03Z,3202.648829431438
hereR,"Interface to the 'HERE' REST APIs <https://developer.here.com/develop/rest-apis>:
  (1) geocode and autosuggest addresses or reverse geocode POIs using the 'Geocoder' API;
  (2) route directions, travel distance or time matrices and isolines using the 'Routing', 'Matrix Routing' and 'Isoline Routing' APIs;
  (3) request real-time traffic flow and incident information from the 'Traffic' API;
  (4) find request public transport connections and nearby stations from the 'Public Transit' API;
  (5) request intermodal routes using the 'Intermodal Routing' API;
  (6) get weather forecasts, reports on current weather conditions, astronomical
  information and alerts at a specific location from the 'Destination Weather' API.
  Locations, routes and isolines are returned as 'sf' objects.",2021-07-21,Merlin Unterfinger,"https://munterfinger.github.io/hereR/,
https://github.com/munterfinger/hereR/",TRUE,https://github.com/munterfinger/herer,19599,65,2021-07-21T16:06:24Z,301.52307692307693
hermiter,"Facilitates estimation of full univariate and bivariate 
  probability density functions and cumulative distribution functions along with
  full quantile functions (univariate) and Spearman's rank correlation 
  (bivariate) using Hermite series based estimators. These estimators are 
  particularly useful in the sequential setting (both stationary and 
  non-stationary) and one-pass batch estimation setting for large data sets. 
  Based on: Stephanou, Michael, Varughese, Melvin and Macdonald, Iain. ""Sequential quantiles via Hermite series density estimation."" Electronic Journal of Statistics 11.1 (2017): 570-607 <doi:10.1214/17-EJS1245>, 
  Stephanou, Michael and Varughese, Melvin. ""On the properties of Hermite series based distribution function estimators."" Metrika (2020) <doi:10.1007/s00184-020-00785-z> and Stephanou, Michael and Varughese, Melvin. ""Sequential Estimation of Nonparametric Correlation using Hermite Series Estimators."" arXiv Preprint (2020) <arXiv:2012.06287>.",2021-02-17,Michael Stephanou,https://github.com/MikeJaredS/hermiter,TRUE,https://github.com/mikejareds/hermiter,4984,9,2021-02-16T12:47:54Z,553.7777777777778
hesim,"A modular and computationally efficient R package for  
  parameterizing, simulating, and analyzing health economic simulation 
  models. The package supports cohort discrete time state transition models 
  (Briggs et al. 1998) <doi:10.2165/00019053-199813040-00003>,
  N-state partitioned survival models (Glasziou et al. 1990)
  <doi:10.1002/sim.4780091106>, and individual-level continuous 
  time state transition models (Siebert et al. 2012) <doi:10.1016/j.jval.2012.06.014>,
  encompassing both Markov (time-homogeneous and time-inhomogeneous) and 
  semi-Markov processes. Decision uncertainty from a cost-effectiveness analysis is 
  quantified with standard graphical and tabular summaries of a probabilistic 
  sensitivity analysis (Claxton et al. 2005, Barton et al. 2008) <doi:10.1002/hec.985>, 
  <doi:10.1111/j.1524-4733.2008.00358.x>. Use of C++ and data.table
  make individual-patient simulation, probabilistic sensitivity analysis, 
  and incorporation of patient heterogeneity fast.",2021-07-26,Devin Incerti,"https://hesim-dev.github.io/hesim/,
https://github.com/hesim-dev/hesim",TRUE,https://github.com/hesim-dev/hesim,23229,40,2021-08-13T15:09:39Z,580.725
hetu,"Structural handling of Finnish identity numbers (persons and companies); extract information, check ID validity and diagnostics.",2020-10-24,Pyry Kantanen,https://github.com/ropengov/hetu,TRUE,https://github.com/ropengov/hetu,2645,1,2021-08-31T14:04:26Z,2645
hexbin,Binning and plotting functions for hexagonal bins.,2021-01-08,Dan Carr,https://github.com/edzer/hexbin,TRUE,https://github.com/edzer/hexbin,4885637,26,2021-01-07T23:15:52Z,187909.11538461538
hexSticker,Helper functions for creating reproducible hexagon sticker purely in R.,2020-12-05,Guangchuang Yu,https://github.com/GuangchuangYu/hexSticker,TRUE,https://github.com/guangchuangyu/hexsticker,31242,516,2021-06-28T09:29:32Z,60.54651162790697
HGNChelper,"Contains functions for
 identifying and correcting HGNC human gene symbols and MGI mouse gene symbols 
 which have been converted to date format by Excel, withdrawn, or aliased.
 Also contains functions for reversibly converting between HGNC
 symbols and valid R names.",2019-10-24,Levi Waldron and Markus Riester,https://github.com/waldronlab/HGNChelper,TRUE,https://github.com/waldronlab/hgnchelper,34031,29,2021-04-26T14:32:28Z,1173.4827586206898
HHG,"Heller-Heller-Gorfine tests are a set of powerful statistical
    tests of multivariate k-sample homogeneity and independence (Heller et. al., 2013, <doi:10.1093/biomet/ass070>). For the univariate
    case, the package also offers implementations of the 'MinP DDP' and 'MinP ADP'
    tests by Heller et. al. (2016), which are consistent against all continuous alternatives but are
    distribution-free, and are thus much faster to apply.",2021-05-15,Barak Brill & Shachar Kaufman,https://github.com/barakbri/HHG,TRUE,https://github.com/barakbri/hhg,22551,3,2021-05-13T10:37:36Z,7517
HiClimR,"A tool for Hierarchical Climate Regionalization applicable to any correlation-based clustering.
             It adds several features and a new clustering method (called, 'regional' linkage) to hierarchical
             clustering in R ('hclust' function in 'stats' library): data regridding, coarsening spatial resolution,
             geographic masking, contiguity-constrained clustering, data filtering by mean and/or variance
             thresholds, data preprocessing (detrending, standardization, and PCA), faster correlation function
             with preliminary big data support, different clustering methods, hybrid hierarchical clustering,
             multivariate clustering (MVC), cluster validation, visualization of regionalization results, and
             exporting region map and mean timeseries into NetCDF-4 file.
             The technical details are described in Badr et al. (2015) <doi:10.1007/s12145-015-0221-7>.",2021-05-31,Hamada S. Badr,"https://hsbadr.github.io/HiClimR/,
https://github.com/hsbadr/HiClimR",TRUE,https://github.com/hsbadr/hiclimr,38357,10,2021-08-12T17:52:11Z,3835.7
hidetify,"Efficient tool for identifying influential observations in
    high dimensional linear regression. The tool implements two detection
    techniques single detection (Barry et al. (2020)
    <doi:10.1080/03610926.2020.1841793>) and multiple detection (Barry et
    al. (2021) <arXiv:2105.12286>).  The single detection is an adaptation
    of Cook's measure for high dimensional data. The method relies on the
    concept of expectile to construct an influence measure based on
    asymmetric correlations. The multiple detection technique applies a
    group deletion procedure to build the algorithm on three main steps.
    The first stage applies an ultra conservative score to mitigate the
    swamping effect, the second stage uses the clean sample generated in
    the previous stage and applies an aggressive score to attenuate the
    masking phenomenon. Finally, the last step is concerned with the
    validation of the influential set generated by the two previous steps.
    The main functions take a response variable and a design matrix as
    input and output a set of potential influential observations.",2021-08-20,Amadou Barry,"https://doi.org/10.1080/03610926.2020.1841793,
https://arxiv.org/abs/2105.12286",TRUE,https://github.com/ambarry/hidetify,169,2,2021-08-16T15:33:08Z,84.5
hier.part,"Partitioning of the independent and joint contributions of each
    variable in a multivariate data set, to a linear regression by hierarchical
    decomposition of goodness-of-fit measures of regressions using all subsets 
    of predictors in the data set. (i.e., model (1), (2), ..., (N), (1,2), ...,
    (1,N), ..., (1,2,3,...,N)). A Z-score based estimate of the 'importance' of
    each predictor is provided by using a randomisation test.",2020-03-03,Chris Walsh,NA,TRUE,https://github.com/cjbwalsh/hier.part,37419,0,2020-11-30T21:29:13Z,NA
HierDpart,"Miscellaneous R functions for calculating and decomposing hierarchical diversity metrics, including hierarchical allele richness, hierarchical exponential Shannon entropy (true diversity of order q=1), hierarchical heterozygosity and genetic differentiation (Jaccard dissimilarity, Delta D, Fst and Jost's D). In addition,a new approach to identify population structure based on the homogeneity of multivariate variances of Shannon differentiation is presented. This package allows users to analyse spatial structured genetic data or species data under a unifying framework (Gaggiotti, O. E. et al, 2018, Evol Appl, 11:1176-1193; <DOI:10.1111/eva.12593>), which partitions diversity and differentiation into any hierarchical levels. It helps you easily structure and format your data. In summary,it implements the analyses of true diversity profiles (q=0, 1, 2), hierarchical diversities and differentiation decomposition, visualization of population structure, as well as the estimation of correlation between geographic distance and genetic differentiation.",2021-03-31,Xinghu Qin,https://github.com/xinghuq/HierDpart,TRUE,https://github.com/xinghuq/hierdpart,15081,3,2021-06-03T03:20:16Z,5027
hierfstat,"Estimates hierarchical F-statistics from haploid or
    diploid genetic data with any numbers of levels in the hierarchy, following the
    algorithm of Yang (Evolution(1998), 52:950).
    Tests via randomisations the significance
    of each F and variance components, using the likelihood-ratio statistics G
    (Goudet et al. (1996) <https://www.genetics.org/content/144/4/1933>).
    Estimates genetic diversity statistics
    for haploid and diploid genetic datasets in various formats, including inbreeding and
    coancestry coefficients, and population specific F-statistics following
    Weir and Goudet (2017) <https://www.genetics.org/content/206/4/2085>.",2020-07-20,Jerome Goudet,"https://www.r-project.org, https://github.com/jgx65/hierfstat",TRUE,https://github.com/jgx65/hierfstat,80121,15,2021-05-10T16:22:13Z,5341.4
highcharter,"A wrapper for the 'Highcharts' library including
    shortcut functions to plot R objects. 'Highcharts'
    <http://www.highcharts.com/> is a charting library offering
    numerous chart types with a simple configuration syntax.",2020-07-26,Joshua Kunst,"http://jkunst.com/highcharter,
https://github.com/jbkunst/highcharter",TRUE,https://github.com/jbkunst/highcharter,370983,614,2021-08-27T21:20:42Z,604.2068403908795
highfrequency,"Provide functionality to manage, clean and match highfrequency
    trades and quotes data, calculate various liquidity measures, estimate and
    forecast volatility, detect price jumps and investigate microstructure noise and intraday
    periodicity.",2021-06-11,Kris Boudt,https://github.com/jonathancornelissen/highfrequency,TRUE,https://github.com/jonathancornelissen/highfrequency,44556,71,2021-08-11T14:26:35Z,627.5492957746479
highlightHTML,"A tool to format R markdown with CSS ids for HTML output. 
    The tool may be most helpful for those using markdown to create reproducible
    documents. The biggest limitations in formatting is the knowledge of CSS
    by the document authors.",2020-04-21,Brandon LeBeau,https://github.com/lebebr01/highlightHTML,TRUE,https://github.com/lebebr01/highlighthtml,17309,4,2021-05-27T16:21:03Z,4327.25
highr,"Provides syntax highlighting for R source code. Currently it
    supports LaTeX and HTML output. Source code of other languages is supported
    via Andre Simon's highlight package (<http://www.andre-simon.de>).",2021-04-16,Yihui Xie,https://github.com/yihui/highr,TRUE,https://github.com/yihui/highr,21127556,41,2021-04-16T21:42:38Z,515306.243902439
hilldiv,"Tools for analysing, comparing, visualising and partitioning diversity based on Hill numbers.
  'hilldiv' is an R package that provides a set of functions to assist analysis of diversity for
  diet reconstruction, microbial community profiling or more general ecosystem characterisation
  analyses based on Hill numbers, using OTU/ASV tables and associated phylogenetic trees as
  inputs. The package includes functions for (phylo)diversity measurement, (phylo)diversity
  profile plotting, (phylo)diversity comparison between samples and groups, (phylo)diversity
  partitioning and (dis)similarity measurement. All of these grounded in abundance-based and
  incidence-based Hill numbers.
  The statistical framework developed around Hill numbers encompasses many of the most
  broadly employed diversity (e.g. richness, Shannon index, Simpson index),
  phylogenetic diversity (e.g. Faith's PD, Allen's H, Rao's quadratic entropy) and
  dissimilarity (e.g. Sorensen index, Unifrac distances) metrics. This enables the most
  common analyses of diversity to be performed while grounded in a single statistical
  framework. The methods are described in Jost et al. (2007) <DOI:10.1890/06-1736.1>,
  Chao et al. (2010) <DOI:10.1098/rstb.2010.0272> and Chiu et al. (2014)
  <DOI:10.1890/12-0960.1>; and reviewed in the framework of molecularly characterised
  biological systems in Alberdi & Gilbert (2019) <DOI:10.1111/1755-0998.13014>.",2019-10-01,Antton Alberdi,https://github.com/anttonalberdi/hilldiv,TRUE,https://github.com/anttonalberdi/hilldiv,10554,2,2021-02-08T08:35:16Z,5277
hillR,"Calculate taxonomic, functional and phylogenetic diversity measures 
    through Hill Numbers proposed by Chao, Chiu and Jost (2014) 
    <doi:10.1146/annurev-ecolsys-120213-091540>.",2021-03-02,Daijiang Li,https://github.com/daijiang/hillR,TRUE,https://github.com/daijiang/hillr,16801,19,2021-03-22T16:01:48Z,884.2631578947369
HIMA,"Allows to estimate and test high-dimensional mediation effects based on advanced mediator screening and penalized regression techniques. Methods used in the package refer to Haixiang Zhang, Yinan Zheng, Zhou Zhang, Tao Gao, Brian Joyce, Grace Yoon, Wei Zhang, Joel Schwartz, Allan Just, Elena Colicino, Pantel Vokonas, Lihui Zhao, Jinchi Lv, Andrea Baccarelli, Lifang Hou, Lei Liu (2016) <doi:10.1093/bioinformatics/btw351>.",2021-05-15,Yinan Zheng,https://github.com/YinanZheng/HIMA/,TRUE,https://github.com/yinanzheng/hima,16075,6,2021-05-18T00:57:45Z,2679.1666666666665
himach,"For supersonic aircraft, flying subsonic over land,
    High Mach finds the best route between airports. Allows for coastal buffer and
    potentially closed regions. Uses a minimal model of aircraft
    performance: the focus is on time saved versus subsonic flight, rather
    than on vertical flight profile. For modelling and forecasting, not for planning your
    flight!",2021-06-17,David Marsh,https://github.com/david6marsh/himach,TRUE,https://github.com/david6marsh/himach,2220,1,2021-08-25T13:53:33Z,2220
hJAM,"Provides functions to implement a hierarchical approach which is designed to perform joint analysis of summary statistics using the framework of Mendelian Randomization or transcriptome analysis. Reference: Lai Jiang, Shujing Xu, Nicholas Mancuso, Paul J. Newcombe, David V. Conti (2020). ""A Hierarchical Approach Using Marginal Summary Statistics for Multiple Intermediates in a Mendelian Randomization or Transcriptome Analysis."" <bioRxiv><doi:10.1101/2020.02.03.924241>.",2020-02-20,Lai Jiang,https://github.com/lailylajiang/hJAM,TRUE,https://github.com/lailylajiang/hjam,8105,6,2021-02-24T01:01:46Z,1350.8333333333333
hkdatasets,"Datasets related to Hong Kong, including information on the 2019 elected District Councillors (<https://www.districtcouncils.gov.hk> and <https://dce2019.hk01.com/>) and traffic collision data from the Hong Kong Department of Transport (<https://www.td.gov.hk/>). All
    of the data in this package is available in the public domain.",2020-10-17,Martin Chan,https://hong-kong-districts-info.github.io/hkdatasets/,TRUE,https://github.com/hong-kong-districts-info/hkdatasets,3635,9,2021-08-15T12:45:13Z,403.8888888888889
hlidacr,"Provides access to datasets published by 'Hlídač státu' <https://www.hlidacstatu.cz/>, 
    a Czech watchdog, via their API. ",2021-03-04,Michael Škvrňák,https://github.com/skvrnami/hlidacr,TRUE,https://github.com/skvrnami/hlidacr,1999,6,2021-03-04T11:44:31Z,333.1666666666667
HLMdiag,"A suite of diagnostic tools for hierarchical
    (multilevel) linear models. The tools include
    not only leverage and traditional deletion diagnostics (Cook's
    distance, covratio, covtrace, and MDFFITS) but also 
    convenience functions and graphics for residual analysis. Models
    can be fit using either lmer in the 'lme4' package or lme in the 'nlme' package.",2021-05-02,Adam Loy,https://github.com/aloy/HLMdiag,TRUE,https://github.com/aloy/hlmdiag,37776,14,2021-05-01T14:22:34Z,2698.285714285714
HMDHFDplus,"Utilities for reading data from the Human Mortality Database (<https://www.mortality.org>), Human Fertility Database (<https://www.humanfertility.org>), and similar databases from the web or locally into an R session as data.frame objects. These are the two most widely used sources of demographic data to study basic demographic change, trends, and develop new demographic methods. Other supported databases at this time include the Human Fertility Collection (<http://www.fertilitydata.org/>), The Japanese Mortality Database (<http://www.ipss.go.jp/p-toukei/JMD>), and the Canadian Human Mortality Database (<http://www.bdlc.umontreal.ca/chmd/>). Arguments and data are standardized.",2020-02-20,Tim Riffe,https://github.com/timriffe/TR1,TRUE,https://github.com/timriffe/tr1,20946,3,2021-04-07T09:58:40Z,6982
Hmisc,"Contains many functions useful for data
	analysis, high-level graphics, utility operations, functions for
	computing sample size and power, simulation, importing and annotating datasets,
	imputing missing values, advanced table making, variable clustering,
	character string manipulation, conversion of R objects to LaTeX and html code,
	and recoding variables.",2021-02-28,Frank E Harrell Jr,"https://hbiostat.org/R/Hmisc/, https://github.com/harrelfe/Hmisc/",TRUE,https://github.com/harrelfe/hmisc,16155952,165,2021-08-29T21:10:43Z,97914.86060606061
hms,"Implements an S3 class for storing and formatting time-of-day
    values, based on the 'difftime' class.",2021-05-17,Kirill Müller,"https://hms.tidyverse.org/, https://github.com/tidyverse/hms",TRUE,https://github.com/tidyverse/hms,21954371,120,2021-07-29T04:14:00Z,182953.09166666667
Hmsc,"Hierarchical Modelling of Species Communities (HMSC) is
   a model-based approach for analyzing community ecological data. 
   This package implements it in the Bayesian framework with Gibbs
   Markov chain Monte Carlo (MCMC) sampling (Tikhonov et al. (2020)
   <doi:10.1111/2041-210X.13345>).",2021-02-24,Otso Ovaskainen,https://www.helsinki.fi/en/researchgroups/statistical-ecology/hmsc,TRUE,https://github.com/hmsc-r/hmsc,18547,66,2021-09-03T10:46:48Z,281.0151515151515
hoardr,"Suite of tools for managing cached files, targeting
    use in other R packages. Uses 'rappdirs' for cross-platform paths.
    Provides utilities to manage cache directories, including targeting
    files by path or by key; cached directories can be compressed and
    uncompressed easily to save disk space.",2018-12-02,Scott Chamberlain,https://github.com/ropensci/hoardr,TRUE,https://github.com/ropensci/hoardr,94415,18,2020-11-04T22:05:11Z,5245.277777777777
hockeystick,"Provides easy access to essential climate change datasets to non-climate experts. Users can download the latest raw data from authoritative sources and view it via pre-defined 'ggplot2' charts. Datasets include atmospheric CO2, instrumental and proxy temperature records, sea levels, Arctic/Antarctic sea-ice, Hurricanes, and Paleoclimate data. Sources include: NOAA Mauna Loa Laboratory <https://gml.noaa.gov/ccgg/trends/data.html>, NASA GISTEMP <https://data.giss.nasa.gov/gistemp/>, National Snow and Sea Ice Data Center <https://nsidc.org/data/seaice_index/archives>, CSIRO <https://research.csiro.au/slrwavescoast/sea-level/measurements-and-data/sea-level-data/>, NOAA Laboratory for Satellite Altimetry <https://www.star.nesdis.noaa.gov/socd/lsa/SeaLevelRise/> and HURDAT Atlantic Hurricane Database <https://www.aoml.noaa.gov/hrd/hurdat/Data_Storm.html>, Vostok Paleo carbon dioxide and temperature data: <https://cdiac.ess-dive.lbl.gov/trends/co2/vostok.html>.",2021-05-18,Hernando Cortina,"https://cortinah.github.io/hockeystick/,
https://github.com/cortinah/hockeystick",TRUE,https://github.com/cortinah/hockeystick,3074,30,2021-08-24T01:40:47Z,102.46666666666667
homologene,"A wrapper for the homologene database by the National Center for
    Biotechnology Information ('NCBI'). It allows searching for gene homologs across 
    species. Data in this package can be found at <ftp://ftp.ncbi.nih.gov/pub/HomoloGene/build68/>.
    The package also includes an updated version of the homologene database where 
    gene identifiers and symbols are replaced with their latest (at the time of
    submission) version and functions to fetch latest annotation data to keep updated.",2019-03-28,Ogan Mancarci,https://github.com/oganm/homologene,TRUE,https://github.com/oganm/homologene,15597,25,2021-05-23T14:06:37Z,623.88
HoRM,"Supplement for the book ""Handbook of Regression Methods"" by D. S. Young.  Some datasets used in the book are included and documented.  Wrapper functions are included that simplify the examples in the textbook, such as code for constructing a regressogram and expanding ANOVA tables to reflect the total sum of squares.",2021-03-11,Derek S. Young,https://github.com/dsy109/HoRM,TRUE,https://github.com/dsy109/horm,18951,0,2021-03-11T04:06:59Z,NA
HotellingEllipse,Functions to compute the semi-axes lengths and coordinate points of Hotelling ellipse. Bro and Smilde (2014) <DOI:10.1039/c3ay41907j>. Brereton (2016) <DOI:10.1002/cem.2763>.,2021-05-18,Christian L. Goueguel,https://github.com/ChristianGoueguel/HotellingEllipse,TRUE,https://github.com/christiangoueguel/hotellingellipse,1165,2,2021-06-09T15:46:53Z,582.5
hover,"A wrapper around a CSS library called 'Hover.css', intended for use 
  in 'shiny' applications.",2021-03-20,Tyler Littlefield,https://github.com/r4fun/hover,TRUE,https://github.com/r4fun/hover,4874,14,2021-03-19T18:25:31Z,348.14285714285717
hR,Transform and analyze workforce data in meaningful ways for human resources (HR) analytics. Get started with workforce planning using a simple Shiny app.,2021-01-14,Dale Kube,NA,TRUE,https://github.com/dalekube/hr,21483,13,2021-01-17T14:36:19Z,1652.5384615384614
hrbrthemes,"A compilation of extra 'ggplot2' themes, scales and utilities, including a 
    spell check function for plot label fields and an overall emphasis on typography. 
    A copy of the 'Google' font 'Roboto Condensed' <https://github.com/google/roboto/> 
    is also included along with a copy of the 'IBM' 'Plex Sans' <https://github.com/IBM/type>,
    'Titillium Web' <https://fonts.google.com/specimen/Titillium+Web>, and
    'Public Sans' <https://github.com/uswds/public-sans/> fonts
    are also included to support their respective typography-oriented themes.",2020-03-06,Bob Rudis,http://github.com/hrbrmstr/hrbrthemes,TRUE,https://github.com/hrbrmstr/hrbrthemes,587543,956,2021-07-21T11:46:10Z,614.5847280334729
hrqglas,"A program that conducts group variable selection for quantile and robust mean 
              regression (Sherwood and Li, 2021). The group lasso penalty (Yuan and Lin, 2006) is used for
              group-wise variable selection. Both of the quantile and mean regression models are based on the Huber loss.
              Specifically, with the tuning parameter in the Huber loss approaching to 0, the quantile check 
              function can be approximated by the Huber loss for the median and the tilted version of 
              Huber loss at other quantiles. Such approximation provides computational efficiency and stability, and
              has also been shown to be statistical consistent.",2021-08-16,Shaobo Li,GitHub: https://github.com/shaobo-li/hrqglas,TRUE,https://github.com/shaobo-li/hrqglas,1012,0,2021-08-16T21:45:28Z,NA
hsstan,"Linear and logistic regression models penalized with hierarchical
  shrinkage priors for selection of biomarkers (or more general variable
  selection), which can be fitted using Stan (Carpenter et al. (2017)
  <doi:10.18637/jss.v076.i01>). It implements the horseshoe and regularized
  horseshoe priors (Piironen and Vehtari (2017) <doi:10.1214/17-EJS1337SI>),
  as well as the projection predictive selection approach to recover a sparse
  set of predictive biomarkers (Piironen, Paasiniemi and Vehtari (2020)
  <doi:10.1214/20-EJS1711>).",2020-06-29,Marco Colombo,https://github.com/mcol/hsstan,TRUE,https://github.com/mcol/hsstan,8673,2,2021-09-03T11:51:18Z,4336.5
HTLR,"Efficient Bayesian multinomial logistic regression based on heavy-tailed
  (hyper-LASSO, non-convex) priors. The posterior of coefficients and hyper-parameters
  is sampled with restricted Gibbs sampling for leveraging the high-dimensionality and
  Hamiltonian Monte Carlo for handling the high-correlation among coefficients. A detailed
  description of the method: Li and Yao (2018), 
  Journal of Statistical Computation and Simulation, 88:14, 2827-2851, <arXiv:1405.3319>.",2020-09-09,Longhai Li,https://longhaisk.github.io/HTLR/,TRUE,https://github.com/longhaisk/htlr,11676,6,2021-05-12T04:41:06Z,1946
html2R,"Provides a 'Shiny' app allowing to convert 'HTML' code to 'R' code (e.g. '<span>Hello</span>' to 'tags$span(""Hello"")'), for usage in a 'Shiny' UI.",2020-09-15,Stéphane Laurent,https://github.com/stla/html2R,TRUE,https://github.com/stla/html2r,4428,2,2020-09-08T01:03:23Z,2214
htmldf,"Simple tools for scraping webpages, extracting common html tags and parsing contents to a tidy, tabular format.  Tools help with extraction of page titles, links, images, rss feeds, social media handles and page metadata.",2021-08-17,Alastair Rushworth,https://github.com/alastairrushworth/htmldf/,TRUE,https://github.com/alastairrushworth/htmldf,4971,67,2021-08-17T08:47:47Z,74.19402985074628
htmltab,"HTML tables are a valuable data source but extracting and recasting
    these data into a useful format can be tedious. This package allows to collect
    structured information from HTML tables. It is similar to `readHTMLTable()`
    of the XML package but provides three major advantages. First, the function
    automatically expands row and column spans in the header and body cells.
    Second, users are given more control over the identification of header and body
    rows which will end up in the R table, including semantic header information
    that appear throughout the body. Third, the function preprocesses table code,
    corrects common types of malformations, removes unneeded parts and so helps to
    alleviate the need for tedious post-processing.",2021-03-08,Christian Rubba,https://github.com/htmltab/htmltab,TRUE,https://github.com/htmltab/htmltab,119530,1,2021-03-08T21:42:11Z,119530
htmlTable,"Tables with state-of-the-art layout elements such as row spanners,
    column spanners, table spanners, zebra striping, and more. While allowing
    advanced layout, the underlying css-structure is simple in order to maximize
    compatibility with word processors such as 'MS Word' or 'LibreOffice'. The package
    also contains a few text formatting functions that help outputting text
    compatible with HTML/LaTeX.",2021-05-18,Max Gordon,https://gforge.se/packages/,TRUE,https://github.com/gforge/htmltable,6200149,65,2021-05-18T20:32:05Z,95386.9076923077
htmltools,Tools for HTML generation and output.,2021-08-25,Carson Sievert,https://github.com/rstudio/htmltools,TRUE,https://github.com/rstudio/htmltools,21742067,132,2021-08-26T16:14:52Z,164712.62878787878
htmlwidgets,"A framework for creating HTML widgets that render in various
    contexts including the R console, 'R Markdown' documents, and 'Shiny'
    web applications.",2020-12-10,Carson Sievert,https://github.com/ramnathv/htmlwidgets,TRUE,https://github.com/ramnathv/htmlwidgets,13297826,699,2021-05-19T19:08:35Z,19024.071530758225
hts,"Provides methods for analysing and forecasting hierarchical and 
    grouped time series. The available forecast methods include bottom-up,
    top-down, optimal combination reconciliation (Hyndman et al. 2011) 
    <doi:10.1016/j.csda.2011.03.006>, and trace minimization reconciliation
    (Wickramasuriya et al. 2018) <doi:10.1080/01621459.2018.1448825>.",2021-05-30,Rob Hyndman,https://pkg.earo.me/hts/,TRUE,https://github.com/earowang/hts,355574,98,2021-06-01T07:44:15Z,3628.3061224489797
httpcache,"In order to improve performance for HTTP API clients, 'httpcache'
    provides simple tools for caching and invalidating cache. It includes the
    HTTP verb functions GET, PUT, PATCH, POST, and DELETE, which are drop-in
    replacements for those in the 'httr' package. These functions are cache-aware
    and provide default settings for cache invalidation suitable for RESTful
    APIs; the package also enables custom cache-management strategies.
    Finally, 'httpcache' includes a basic logging framework to facilitate the
    measurement of HTTP request time and cache performance.",2021-01-10,Neal Richardson,"https://enpiar.com/r/httpcache/,
https://github.com/nealrichardson/httpcache/",TRUE,https://github.com/nealrichardson/httpcache,33891,11,2021-01-10T21:18:48Z,3081
httpgd,"A graphics device for R that is accessible via network protocols.
    This package was created to make it easier to embed live R graphics in 
    integrated development environments and other applications.
    The included 'HTML/JavaScript' client (plot viewer) aims to provide a better overall user experience when dealing with R graphics.
    The device asynchronously serves 'SVG' graphics via 'HTTP' and 'WebSockets'.",2021-04-02,Florian Rupprecht,https://github.com/nx10/httpgd,TRUE,https://github.com/nx10/httpgd,3962,125,2021-08-31T13:59:46Z,31.696
httpproblems,"Tools for emitting the 'Problem Details' structure defined in
  'RFC' 7807 <https://tools.ietf.org/html/rfc7807> for reporting errors from
  'HTTP' servers in a standard way.",2021-06-16,Aaron Jacobs,https://github.com/atheriel/httpproblems,TRUE,https://github.com/atheriel/httpproblems,3576,4,2021-07-06T16:44:33Z,894
httptest,"Testing and documenting code that communicates with remote servers
    can be painful. Dealing with authentication, server state,
    and other complications can make testing seem too costly to
    bother with. But it doesn't need to be that hard. This package enables one
    to test all of the logic on the R sides of the API in your package without
    requiring access to the remote service. Importantly, it provides three
    contexts that mock the network connection in different ways, as well as
    testing functions to assert that HTTP requests were---or were
    not---made. It also allows one to safely record real API responses to use as
    test fixtures. The ability to save responses and load them offline also
    enables one to write vignettes and other dynamic documents that can be
    distributed without access to a live server.",2021-02-01,Neal Richardson,"https://enpiar.com/r/httptest/,
https://github.com/nealrichardson/httptest",TRUE,https://github.com/nealrichardson/httptest,58381,70,2021-06-16T23:15:07Z,834.0142857142857
httpuv,"Provides low-level socket and protocol support for handling
    HTTP and WebSocket requests directly from within R. It is primarily
    intended as a building block for other packages, rather than making it
    particularly easy to create complete web applications using httpuv alone.
    httpuv is built on top of the libuv and http-parser C libraries, both of
    which were developed by Joyent, Inc. (See LICENSE file for libuv and
    http-parser license information.)",2021-08-18,Winston Chang,https://github.com/rstudio/httpuv,TRUE,https://github.com/rstudio/httpuv,13297170,183,2021-08-19T15:24:15Z,72662.13114754099
httr,"Useful tools for working with HTTP organised by
    HTTP verbs (GET(), POST(), etc). Configuration functions make it easy
    to control additional request components (authenticate(),
    add_headers() and so on).",2020-07-20,Hadley Wickham,"https://httr.r-lib.org/, https://github.com/r-lib/httr",TRUE,https://github.com/r-lib/httr,21876756,917,2021-03-29T17:10:47Z,23856.87677208288
hunspell,"Low level spell checker and morphological analyzer based on the 
    famous 'hunspell' library <https://hunspell.github.io>. The package can analyze
    or check individual words as well as parse text, latex, html or xml documents.
    For a more user-friendly interface use the 'spelling' package which builds on
    this package to automate checking of files, documentation and vignettes in all
    common formats.",2020-12-09,Jeroen Ooms,"https://docs.ropensci.org/hunspell/ (docs),
https://github.com/ropensci/hunspell (devel)
https://hunspell.github.io (upstream)",TRUE,https://github.com/ropensci/hunspell,1736580,91,2021-02-04T14:00:45Z,19083.296703296703
hutils,"Provides utility functions for, and drawing on, the 'data.table' package. The package also collates useful miscellaneous functions extending base R not available elsewhere. The name is a portmanteau of 'utils' and the author.",2021-07-22,Hugh Parsonage,"https://github.com/hughparsonage/hutils,
https://hughparsonage.github.io/hutils/",TRUE,https://github.com/hughparsonage/hutils,54493,7,2021-07-22T06:33:19Z,7784.714285714285
hutilscpp,"Provides utility functions that are simply, frequently used, 
    but may require higher performance that what can be obtained from base R.
    Incidentally provides support for 'reverse geocoding', such as matching a point
    with its nearest neighbour in another array. Used as a complement to package
    'hutils' by sacrificing compilation or installation time for higher running 
    speeds. The name is a portmanteau of the author and 'Rcpp'.",2021-07-24,Hugh Parsonage,https://github.com/hughparsonage/hutilscpp,TRUE,https://github.com/hughparsonage/hutilscpp,16524,5,2021-08-19T15:33:23Z,3304.8
huxtable,"Creates styled tables for data presentation. Export to HTML, LaTeX,
  RTF, 'Word', 'Excel', and 'PowerPoint'. Simple, modern interface to manipulate 
  borders, size, position, captions, colours, text styles and number formatting.
  Table cells can span multiple rows and/or columns.
  Includes  a 'huxreg' function for creation of regression tables, and 'quick_*' 
  one-liners to print data to a new document.",2021-05-14,David Hugh-Jones,https://hughjonesd.github.io/huxtable/,TRUE,https://github.com/hughjonesd/huxtable,191419,281,2021-07-20T10:36:58Z,681.2064056939502
hwsdr,"Programmatic interface to the Harmonized World Soil Database 
    'HWSD' web services (<https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1247>).
    Allows for easy downloads of 'HWSD' soil data directly to your R workspace 
    or your computer. Routines for both single pixel data downloads and
    gridded data are provided.",2021-06-30,Koen Hufkens,https://github.com/bluegreen-labs/hwsdr,TRUE,https://github.com/bluegreen-labs/hwsdr,837,5,2021-06-30T13:48:02Z,167.4
hydraulics,"Functions for basic hydraulic calculations related to 
    water flow in circular pipes both flowing full (under pressure), and 
    partially full (gravity flow), and trapezoidal open channels. For 
    pressure flow this includes friction loss calculations by solving 
    the Darcy-Weisbach equation for head loss, flow or diameter, and 
    plotting a Moody diagram. The Darcy-Weisbach friction factor is 
    calculated using the Colebrook (or Colebrook-White equation), 
    the basis of the Moody diagram, the original citation being 
    Colebrook (1939) <doi:10.1680/ijoti.1939.13150>. For gravity flow, the
    Manning equation is used, again solving for missing parameters. The 
    derivation of and solutions using the Darcy-Weisbach equation and the
    Manning equation are outlined in many fluid mechanics texts such as 
    Finnemore and Franzini (2002, ISBN:978-0072432022). For the Manning equation
    solutions, this package uses modifications of original code from the 'iemisc' 
    package by Irucka Embry.",2021-05-19,Ed Maurer,https://github.com/EdM44/hydraulics,TRUE,https://github.com/edm44/hydraulics,7243,2,2021-05-19T01:00:07Z,3621.5
hydroPSO,"State-of-the-art version of the Particle Swarm Optimisation (PSO) algorithm (SPSO-2011 and SPSO-2007 capable). hydroPSO can be used as a replacement of the 'optim' R function for (global) optimization of non-smooth and non-linear functions. However, the main focus of hydroPSO is the calibration of environmental and other real-world models that need to be executed from the system console. hydroPSO is model-independent, allowing the user to easily interface any computer simulation model with the calibration engine (PSO). hydroPSO  communicates with the model through the model's own input and output files, without requiring access to the model's source code. Several PSO variants and controlling options are included to fine-tune the performance of the calibration engine to different calibration problems. An advanced sensitivity analysis function together with user-friendly plotting summaries facilitate the interpretation and assessment of the calibration results. hydroPSO is parallel-capable, to alleviate the computational burden of complex models with ""long"" execution time. Bugs reports/comments/questions are very welcomed (in English, Spanish or Italian). See Zambrano-Bigiarini and Rojas (2013) <doi:10.1016/j.envsoft.2013.01.004> for more details.",2020-04-29,Mauricio Zambrano-Bigiarini,https://github.com/hzambran/hydroPSO,TRUE,https://github.com/hzambran/hydropso,27565,22,2021-01-04T14:17:22Z,1252.9545454545455
hydroscoper,"R interface to the Greek National Data Bank for Hydrological and 
    Meteorological Information. It covers 
    Hydroscope's data sources and provides functions to transliterate, 
    translate and download them into tidy dataframes.",2021-05-14,Konstantinos Vantas,"https://github.com/ropensci/hydroscoper,
https://docs.ropensci.org/hydroscoper/",TRUE,https://github.com/ropensci/hydroscoper,16715,11,2021-05-14T16:17:49Z,1519.5454545454545
hydrostats,Calculates a suite of hydrologic indices for daily time series data that are widely used in hydrology and stream ecology.,2021-08-12,Nick Bond,https://github.com/nickbond/hydrostats,TRUE,https://github.com/nickbond/hydrostats,24038,16,2021-08-04T23:42:57Z,1502.375
hydroTSM,"S3 functions for management, analysis, interpolation and plotting of time series used in hydrology and related environmental sciences. In particular, this package is highly oriented to hydrological modelling tasks. The focus of this package has been put in providing a collection of tools useful for the daily work of hydrologists (although an effort was made to optimise each function as much as possible, functionality has had priority over speed). Bugs / comments / questions / collaboration of any kind are very welcomed, and in particular, datasets that can be included in this package for academic purposes.",2020-03-11,Mauricio Zambrano-Bigiarini,https://github.com/hzambran/hydroTSM,TRUE,https://github.com/hzambran/hydrotsm,109283,26,2021-06-30T22:56:42Z,4203.192307692308
hyper2,A suite of routines for the hyperdirichlet distribution; supersedes the 'hyperdirichlet' package.,2021-03-04,Robin K. S. Hankin,https://github.com/RobinHankin/hyper2,TRUE,https://github.com/robinhankin/hyper2,19763,3,2021-08-26T05:16:47Z,6587.666666666667
HypergeoMat,"Evaluates the hypergeometric functions of a matrix argument, which appear in random matrix theory. This is an implementation of Koev & Edelman's algorithm (2006) <doi:10.1090/S0025-5718-06-01824-2>. ",2020-10-24,Stéphane Laurent,https://github.com/stla/HypergeoMat,TRUE,https://github.com/stla/hypergeomat,11714,0,2020-11-15T02:18:51Z,NA
hyperSpec,"Comfortable ways to work with hyperspectral data sets.
    I.e. spatially or time-resolved spectra, or spectra with any other kind
    of information associated with each of the spectra. The spectra can be data
    as obtained in XRF, UV/VIS, Fluorescence, AES, NIR, IR, Raman, NMR, MS,
    etc. More generally, any data that is recorded over a discretized variable,
    e.g. absorbance = f (wavelength), stored as a vector of absorbance values
    for discrete wavelengths is suitable.",2020-11-29,Claudia Beleites,https://github.com/cbeleites/hyperSpec,TRUE,https://github.com/cbeleites/hyperspec,40799,24,2021-07-14T21:20:21Z,1699.9583333333333
hypr,"Translation between experimental null hypotheses, hypothesis matrices, and contrast matrices as used in linear regression models. The package is based on the method described in Schad, Vasishth, Hohenstein, and Kliegl (2019) <doi:10.1016/j.jml.2019.104038> and Rabe, Vasishth, Hohenstein, Kliegl, and Schad (2020) <doi:10.21105/joss.02134>.",2021-07-19,Maximilian M. Rabe,https://maxrabe.com/hypr,TRUE,https://github.com/mmrabe/hypr,14105,6,2021-08-18T10:04:27Z,2350.8333333333335
hystReet,An R API wrapper for the 'Hystreet' project <https://hystreet.com>. 'Hystreet' provides pedestrian counts in different cities in Germany.,2020-06-14,Johannes Friedrich,https://github.com/JohannesFriedrich/hystReet,TRUE,https://github.com/johannesfriedrich/hystreet,7603,10,2021-07-31T11:07:04Z,760.3
i2dash,"Create customized, web-based dashboards for data presentation, exploration and sharing. 'i2dash' integrates easily into existing data analysis pipelines and can organize scientific findings thematically across different pages and layouts.",2021-03-29,Arsenij Ustjanzew,https://loosolab.github.io/i2dash/,TRUE,https://github.com/loosolab/i2dash,8695,5,2021-03-30T13:52:31Z,1739
i2extras,"Provides functions to work with 'incidence2' objects, including a
  simplified interface for trend fitting and peak estimation. This package is
  part of the RECON (<https://www.repidemicsconsortium.org/>) toolkit for 
  outbreak analysis (<https://www.reconverse.org/).",2021-07-08,Tim Taylor,https://www.reconverse.org/i2extras/,TRUE,https://github.com/reconverse/i2extras,6305,2,2021-08-27T21:59:09Z,3152.5
ib,"An implementation of the iterative bootstrap procedure of 
    Kuk (1995) <doi:10.1111/j.2517-6161.1995.tb02035.x> to correct the estimation bias of a fitted model object. This
    procedure has better bias correction properties than the 
    bootstrap bias correction technique.",2020-12-16,Samuel Orso,https://github.com/SMAC-Group/ib/,TRUE,https://github.com/smac-group/ib,2133,0,2020-12-14T13:36:23Z,NA
ibawds,"A collection of useful functions and datasets for the Data Science
  Course at IBAW in Lucerne.",2021-05-29,Stefan Lanz,https://github.com/stibu81/ibawds,TRUE,https://github.com/stibu81/ibawds,2245,0,2021-05-29T16:08:32Z,NA
ibb,"Call wrappers for Istanbul Metropolitan
    Municipality's Open Data Portal (Turkish: İstanbul Büyükşehir
    Belediyesi Açık Veri Portalı) at <https://data.ibb.gov.tr/en/>.",2021-02-17,Berk Orbay,https://github.com/berkorbay/ibb,TRUE,https://github.com/berkorbay/ibb,5654,5,2021-04-24T09:02:47Z,1130.8
ibdsim2,"Simulation of segments shared identical-by-descent (IBD) by
    pedigree members. Using sex specific recombination rates along the
    human genome (Halldorsson et al. (2019)
    <doi:10.1126/science.aau1043>), phased chromosomes are simulated for
    all pedigree members. Applications include calculation of realised
    relatedness coefficients and IBD segment distributions. 'ibdsim2' is
    part of the 'ped suite' collection of packages for pedigree analysis.
    A detailed presentation of the 'ped suite', including a separate
    chapter on 'ibdsim2', is available in the book 'Pedigree analysis in
    R' (Vigeland, 2021, ISBN:9780128244302). A 'shiny' app for visualising
    and comparing IBD distributions is available at
    <https://magnusdv.shinyapps.io/ibdsim2-shiny/>.",2021-07-24,Magnus Dehli Vigeland,"https://github.com/magnusdv/ibdsim2,
https://magnusdv.github.io/pedsuite/,
https://magnusdv.shinyapps.io/ibdsim2-shiny/",TRUE,https://github.com/magnusdv/ibdsim2,5929,4,2021-08-17T17:31:50Z,1482.25
IBMPopSim,"Simulation of the random evolution of structured population dynamics, called stochastic Individual Based Models (IBMs) (see e.g. Ferrière and Tran (2009) <doi:10.1051/proc/2009033>, Bansaye and Méléard (2015) <doi:10.1007/978-3-319-21711-6>, Boumezoued (2016)).
    The package allows users to simulate the random evolution of a population in which individuals are characterised by their date of birth, a set of attributes, and their potential date of death.",2020-11-10,Daphné Giorgi,"https://github.com/DaphneGiorgi/IBMPopSim,
https://DaphneGiorgi.github.io/IBMPopSim/",TRUE,https://github.com/daphnegiorgi/ibmpopsim,3882,2,2020-11-10T14:46:56Z,1941
iBreakDown,"Model agnostic tool for decomposition of predictions from black boxes.
    Supports additive attributions and attributions with interactions.
    The Break Down Table shows contributions of every variable to a final prediction. 
    The Break Down Plot presents variable contributions in a concise graphical way. 
    This package works for classification and regression models. 
    It is an extension of the 'breakDown' package (Staniak and Biecek 2018) <doi:10.32614/RJ-2018-072>,
    with new and faster strategies for orderings. 
    It supports interactions in explanations and has interactive visuals (implemented with 'D3.js' library). 
    The methodology behind is described in the 'iBreakDown' article (Gosiewska and Biecek 2019) <arXiv:1903.11420>
    This package is a part of the 'DrWhy.AI' universe (Biecek 2018) <arXiv:1806.08915>.",2021-05-07,Przemyslaw Biecek,"https://ModelOriented.github.io/iBreakDown/,
https://github.com/ModelOriented/iBreakDown",TRUE,https://github.com/modeloriented/ibreakdown,74681,62,2021-06-30T08:56:56Z,1204.532258064516
iCAMP,"To implement a general framework to quantitatively infer Community Assembly Mechanisms by Phylogenetic-bin-based null model analysis, abbreviated as 'iCAMP' (Ning et al 2020) <doi:10.1038/s41467-020-18560-z>. It can quantitatively assess the relative importance of different community assembly processes, such as selection, dispersal, and drift, for both communities and each phylogenetic group ('bin'). Each bin usually consists of different taxa from a family or an order. The package also provides functions to implement some other published methods, including neutral taxa percentage (Burns et al 2016) <doi:10.1038/ismej.2015.142> based on neutral theory model (Sloan et al 2006) <doi:10.1111/j.1462-2920.2005.00956.x> and quantifying assembly processes based on entire-community null models ('QPEN', Stegen et al 2013) <doi:10.1038/ismej.2013.93>. It also includes some handy functions, particularly for big datasets, such as phylogenetic and taxonomic null model analysis at both community and bin levels, between-taxa niche difference and phylogenetic distance calculation, phylogenetic signal test within phylogenetic groups, midpoint root of big trees, etc. Version 1.3.x mainly improved the function for 'QPEN' and added function 'icamp.cate()' to summarize 'iCAMP' results for different categories of taxa (e.g. core versus rare taxa).",2021-01-09,Daliang Ning,https://github.com/DaliangNing/iCAMP1,TRUE,https://github.com/daliangning/icamp1,6179,18,2021-08-05T00:23:22Z,343.27777777777777
ICAMS,"Analysis and visualization of experimentally elucidated mutational
    signatures -- the kind of analysis and visualization in Boot et al.,
    ""In-depth characterization of the cisplatin mutational signature in 
    human cell lines and in esophageal and liver tumors"", Genome Research 2018, 
    <doi:10.1101/gr.230219.117> and
    ""Characterization of colibactin-associated mutational signature in an 
    Asian oral squamous cell carcinoma and in other mucosal tumor types"",
    Genome Research 2020 <doi:10.1101/gr.255620.119>.
    'ICAMS' stands for In-depth Characterization 
    and Analysis of Mutational Signatures. 'ICAMS' has functions to read in 
    variant call files (VCFs) and to collate the corresponding catalogs of 
    mutational spectra and to analyze and plot catalogs of mutational spectra
    and signatures. Handles both ""counts-based"" and ""density-based"" catalogs
    of mutational spectra or signatures.",2021-04-03,Steve Rozen,https://github.com/steverozen/ICAMS,TRUE,https://github.com/steverozen/icams,11980,7,2021-08-02T09:36:29Z,1711.4285714285713
ICD10gm,"Provides convenient access to the German modification of the International Classification of Diagnoses, 10th revision (ICD-10-GM). It provides functionality to aid in the identification, specification and historisation of ICD-10 codes. Its intended use is the analysis of routinely collected data in the context of epidemiology, medical research and health services research. The underlying metadata are released by the German Institute for Medical Documentation and Information <https://www.dimdi.de>, and are redistributed in accordance with their license.",2021-04-09,Ewan Donnachie,"https://edonnachie.github.io/ICD10gm/,
https://doi.org/10.5281/zenodo.2542833",TRUE,https://github.com/edonnachie/icd10gm,14074,3,2021-07-16T21:27:23Z,4691.333333333333
icecream,"Provides user-friendly and configurable print debugging via a
    single function, ic(). Wrap an expression in ic() to print the
    expression, its value and (where available) its source location.
    Debugging output can be toggled globally without modifying code.",2021-05-31,Lewin Appleton-Fox,"https://lewinfox.github.io/icecream/,
https://github.com/lewinfox/icecream",TRUE,https://github.com/lewinfox/icecream,1121,10,2021-05-31T21:13:05Z,112.1
iCellR,"A toolkit that allows scientists to work with data from single cell sequencing technologies such as scRNA-seq, scVDJ-seq and CITE-Seq. Single (i) Cell R package ('iCellR') provides unprecedented flexibility at every step of the analysis pipeline, including normalization, clustering, dimensionality reduction, imputation, visualization, and so on. Users can design both unsupervised and supervised models to best suit their research. In addition, the toolkit provides 2D and 3D interactive visualizations, differential expression analysis, filters based on cells, genes and clusters, data merging, normalizing for dropouts, data imputation methods, correcting for batch differences, pathway analysis, tools to find marker genes for clusters and conditions, predict cell types and pseudotime analysis. See Khodadadi-Jamayran, et al (2020) <doi:10.1101/2020.05.05.078550>  and Khodadadi-Jamayran, et al (2020) <doi:10.1101/2020.03.31.019109> for more details.",2021-04-27,Alireza Khodadadi-Jamayran,https://github.com/rezakj/iCellR,TRUE,https://github.com/rezakj/icellr,20725,85,2021-08-26T19:42:02Z,243.8235294117647
icesVocab,"R interface to access the RECO POX web services of the ICES
  (International Council for the Exploration of the Sea) Vocabularies database
  <https://vocab.ices.dk/services/POX.aspx>.",2021-01-15,Colin Millar,https://vocab.ices.dk/services/POX.aspx,TRUE,https://github.com/ices-tools-prod/icesvocab,16828,3,2021-01-09T11:58:25Z,5609.333333333333
ichimoku,"An implementation of 'Ichimoku Kinko Hyo', also commonly known as
    'cloud charts'. Static and interactive visualizations with tools for
    creating, backtesting and development of quantitative 'ichimoku' strategies.
    As described in Sasaki (1996, ISBN:4925152009), the technique is a refinement
    on candlestick charting originating from Japan, now in widespread use in
    technical analysis worldwide. Translating as 'one-glance equilibrium chart',
    it allows the price action and market structure of financial securities to
    be determined 'at-a-glance'. Incorporates an interface with the OANDA
    fxTrade API <https://developer.oanda.com/> for retrieving historical and
    live streaming price data for major currencies, metals, commodities,
    government bonds and stock indices.",2021-08-23,Charlie Gao,"https://shikokuchuo.net/ichimoku/,
https://github.com/shikokuchuo/ichimoku/",TRUE,https://github.com/shikokuchuo/ichimoku,2310,6,2021-09-03T09:53:09Z,385
iconr,"Set of formal methods for studying archaeological iconographic datasets (rock-art, pottery decoration, stelae, etc.) using network and spatial analysis (Alexander 2008 <doi:10.11588/propylaeumdok.00000512>; Huet 2018 <https://hal.archives-ouvertes.fr/hal-02913656>).",2021-02-16,Thomas Huet,https://zoometh.github.io/iconr/,TRUE,https://github.com/zoometh/iconr,2415,8,2021-09-02T15:46:39Z,301.875
ICvectorfields,"Functions for converting time series of spatial abundance or density 
    data in raster format to vector fields of population movement using the digital 
    image correlation technique. More specifically, the functions in the package 
    compute cross-covariance using discrete fast Fourier transforms for computational 
    efficiency. Vectors in vector fields point in the direction of highest two 
    dimensional cross-covariance. The package has a novel implementation of the 
    digital image correlation algorithm that is designed to detect persistent 
    directional movement when image time series extend beyond a sequence of 
    two raster images. ",2021-06-21,Devin Goodsman,NA,TRUE,https://github.com/goodsman/icvectorfields,1040,0,2021-09-02T15:00:08Z,NA
idem,"In randomized studies involving severely ill patients, functional
    outcomes are often unobserved due to missed clinic visits, premature
    withdrawal or death. It is well known that if these unobserved functional
    outcomes are not handled properly, biased treatment comparisons can be
    produced. In this package, we implement a procedure for comparing treatments
    that is based on the composite endpoint of both the functional outcome and
    survival. The procedure was proposed in Wang et al. (2016) <DOI:10.1111/biom.12594>
    and Wang et al. (2020) <DOI:10.18637/jss.v093.i12>. It considers missing data
    imputation with different sensitivity
    analysis strategies to handle the unobserved functional outcomes not due to
    death.",2021-01-27,Chenguang Wang,https://github.com/olssol/idem/,TRUE,https://github.com/olssol/idem,18670,0,2021-01-26T20:13:38Z,NA
idendr0,"Interactive dendrogram that enables the user to select and
    color clusters, to zoom and pan the dendrogram, and to visualize
    the clustered data not only in a built-in heat map, but also in
    'GGobi' interactive plots and user-supplied plots. 
    This is a backport of Qt-based 'idendro' 
    (<https://github.com/tsieger/idendro>) to base R graphics and 
    Tcl/Tk GUI.",2017-02-22,Tomas Sieger,http://github.com/tsieger/idendr0,TRUE,https://github.com/tsieger/idendr0,22277,5,2021-06-01T12:09:29Z,4455.4
IDSL.IPA,"A sophisticated pipeline for processing
    high-resolution LC/MS data to extract signals of organic compounds. The
    package performs isotope pairing, peak detection, alignment, RT correction,
    gap filling, peak annotation and visualization of extracted ion chromatograms and total ion chromatograms. ",2021-06-30,Sadjad Fakouri-Baygi,"https://ipa.idsl.me, https://github.com/idslme/idsl.ipa",TRUE,https://github.com/idslme/idsl.ipa,1119,0,2021-06-25T19:53:03Z,NA
IFAA,A novel approach to make inference on the association of covariates with the absolute abundance (AA) of 'microbiome' in an ecosystem. It can be also directly applied to relative abundance (RA) data to make inference on AA (even if AA data is not available) because the ratio of two RA is equal ratio of their AA. This algorithm can estimate and test the associations of interest while adjusting for potential 'confounders'. The estimates of this method have easy interpretation like a typical regression analysis. High-dimensional covariates are handled with regularization and it is implemented by parallel computing. This algorithm finds optimal reference 'taxa/OTU (Operational Taxonomic Unit)/ASV (Amplicon Sequence Bariant)' and uses permutation to control FDR (False Discovery Rate).,2021-07-20,Zhigang Li,"https://github.com/gitlzg/IFAA,
https://arxiv.org/abs/1909.10101v3,
https://link.springer.com/article/10.1007/s12561-018-9219-2",TRUE,https://github.com/gitlzg/ifaa,3824,1,2021-01-05T01:32:12Z,3824
IFC,"Contains several tools to treat imaging flow cytometry data from 'ImageStream®' and 'FlowSight®' cytometers ('Amnis®', part of 'Luminex®'). Provides an easy and simple way to read, write and subset .rif, .cif and .daf files. Information such as masks, features, regions and populations set within these files can be retrieved. In addition, raw data such as images stored can also be accessed. Users, may hopefully increase their productivity thanks to dedicated functions to extract, visualize and export 'IFC' data. Toy data example can be installed through the 'IFCdata' package of approximately 32 MB, which is available in a 'drat' repository <https://gitdemont.github.io/IFCdata/>. See file 'COPYRIGHTS' and file 'AUTHORS' for a list of copyright holders and authors.",2020-12-18,Yohann Demont,NA,TRUE,https://github.com/gitdemont/ifc,5553,2,2021-08-31T09:08:46Z,2776.5
ifctools,"Provides utility functions to deal with Italian fiscal
    code ('codice fiscale').",2021-06-25,Luca Braglia,https://github.com/lbraglia/ifctools,TRUE,https://github.com/lbraglia/ifctools,16138,0,2021-06-25T07:37:23Z,NA
ifultools,"Insightful Research Tools is a collection of signal processing,
  image processing, and time series modeling routines written in the C
  language for speed and efficency. The routines were originally developed at
  Insightful for use in S-PLUS. ",2020-12-07,Stephen Kaluzny,https://github.com/spkaluzny/ifultools,TRUE,https://github.com/spkaluzny/ifultools,95363,0,2020-12-03T23:27:52Z,NA
igate,"An implementation of the initial guided analytics for parameter testing and
    controlband extraction framework. Functions are available for continuous and 
    categorical target variables as well as for generating standardized reports of the
    conducted analysis. See <https://github.com/stefan-stein/igate> for more information
    on the technology.",2019-09-10,Stefan Stein,https://github.com/stefan-stein/igate,TRUE,https://github.com/stefan-stein/igate,9829,1,2020-10-30T12:52:29Z,9829
igoR,"Tools to extract information from the Intergovernmental
    Organizations ('IGO') Database , version 3, provided by the Correlates
    of War Project <https://correlatesofwar.org/>. See also Pevehouse, J.
    C. et al. (2020), <doi:10.1177/0022343319881175>.  Version 3 includes
    information from 1815 to 2014.",2021-08-04,Diego Hernangómez,"https://dieghernan.github.io/igoR/,
https://github.com/dieghernan/igoR",TRUE,https://github.com/dieghernan/igor,2730,5,2021-09-01T08:33:43Z,546
IGP,"Creates a Gaussian process model using the specified package. 
    Makes it easy to try different packages in same code, only the
    package argument needs to be changed.
    It is essentially a wrapper for the other Gaussian process
    software packages.",2021-04-26,Collin Erickson,https://github.com/CollinErickson/IGP,TRUE,https://github.com/collinerickson/igp,15416,1,2021-04-27T00:12:25Z,15416
igraph,"Routines for simple graphs and network analysis. It can
  handle large graphs very well and provides functions for generating random
  and regular graphs, graph visualization, centrality methods and much more.",2020-10-06,See AUTHORS file.,https://igraph.org,TRUE,https://github.com/igraph/igraph,8733302,1222,2021-06-06T16:11:53Z,7146.728314238952
iheiddown,"A set of tools to support writing various documents
    according to the Graduate Institute of International and Development Studies 
    conventions and regulations.
    The most common use will be for writing and compiling theses or thesis chapters,
    as drafts or for examination with all the correct preamble content.
    However, the package also offers the creation of html presentation slides via 'xaringan',
    and, for course instructors, also the ability to create a syllabus as a PDF.
    The package includes additional functions for institutional color palettes
    and an institutional 'ggplot' theme, as well as a function for word counts.",2021-06-24,James Hollway,https://github.com/jhollway/iheiddown,TRUE,https://github.com/jhollway/iheiddown,895,7,2021-08-24T14:43:51Z,127.85714285714286
ihpdr,"Web scraping the <https://www.dallasfed.org> for
    up-to-date data on international house prices and exuberance
    indicators. Download data in tidy format.",2020-07-13,Kostas Vasilopoulos,https://github.com/kvasilopoulos/ihpdr,TRUE,https://github.com/kvasilopoulos/ihpdr,11747,0,2021-01-15T17:36:57Z,NA
ijtiff,"General purpose TIFF file I/O for R users.  Currently the
    only such package with read and write support for TIFF files with
    floating point (real-numbered) pixels, and the only package that can
    correctly import TIFF files that were saved from 'ImageJ' and write
    TIFF files than can be correctly read by 'ImageJ'
    <https://imagej.nih.gov/ij/>.  Also supports text image I/O.",2021-06-28,Rory Nolan,"https://docs.ropensci.org/ijtiff/,
https://github.com/ropensci/ijtiff",TRUE,https://github.com/ropensci/ijtiff,38225,11,2021-07-03T23:41:16Z,3475
imabc,"Provides functionality to perform a likelihood-free method for estimating the parameters of complex models
    that results in a simulated sample from the posterior distribution of model parameters given targets. The method begins
    with a accept/reject approximate bayes computation (ABC) step applied to a sample of points from the prior distribution
    of model parameters. Accepted points result in model predictions that are within the initially specified tolerance
    intervals around the target points. The sample is iteratively updated by drawing additional points from a mixture of
    multivariate normal distributions, accepting points within tolerance intervals. As the algorithm proceeds, the
    acceptance intervals are narrowed. The algorithm returns a set of points and sampling weights that account for the
    adaptive sampling scheme. For more details see Rutter, Ozik, DeYoreo, and Collier (2018) <arXiv:1804.02090>.",2021-04-12,Christopher,https://github.com/carolyner/imabc,TRUE,https://github.com/carolyner/imabc,1294,2,2021-07-02T22:32:51Z,647
image.binarization,"Improve optical character recognition by binarizing images. The package focuses primarily on local adaptive thresholding algorithms. 
    In English, this means that it has the ability to turn a color or gray scale image into a black and white image. This is particularly useful
    as a preprocessing step for optical character recognition or handwritten text recognition.",2021-07-05,Jan Wijffels,https://github.com/DIGI-VUB/image.binarization,TRUE,https://github.com/digi-vub/image.binarization,4282,11,2021-07-05T08:45:22Z,389.27272727272725
image.textlinedetector,"Find text lines in scanned images and segment the lines into words.
    Includes implementations of the paper 'Novel A* Path Planning Algorithm for Line Segmentation of Handwritten Documents' by Surinta O. et al (2014) <doi:10.1109/ICFHR.2014.37> available at <https://github.com/smeucci/LineSegm>,
    an implementation of 'A Statistical approach to line segmentation in handwritten documents' by Arivazhagan M. et al (2007) <doi:10.1117/12.704538>, 
    and a wrapper for an image segmentation technique to detect words in text lines as described in the paper 'Scale Space Technique for Word Segmentation in Handwritten Documents' by Manmatha R. and Srimal N. (1999) paper at <doi:10.1007/3-540-48236-9_3>, wrapper for code available at <https://github.com/arthurflor23/text-segmentation>.",2021-09-03,Jan Wijffels,https://github.com/DIGI-VUB/image.textlinedetector,TRUE,https://github.com/digi-vub/image.textlinedetector,6067,5,2021-07-26T08:27:50Z,1213.4
imagefluency,"Get image statistics based on processing fluency theory. The
    functions provide scores for several basic aesthetic principles that
    facilitate fluent cognitive processing of images: contrast,
    complexity / simplicity, self-similarity, symmetry, and typicality.
    See Mayer & Landwehr (2018) <doi:10.1037/aca0000187> and Mayer & Landwehr
    (2018) <doi:10.31219/osf.io/gtbhw> for the theoretical background of the methods.",2020-01-09,Stefan Mayer,https://stm.github.io/imagefluency,TRUE,https://github.com/stm/imagefluency,12759,2,2021-04-29T17:44:11Z,6379.5
imager,"Fast image processing for images in up to 4 dimensions (two spatial
    dimensions, one time/depth dimension, one colour dimension). Provides most
    traditional image processing tools (filtering, morphology, transformations,
    etc.) as well as various functions for easily analysing image data using R. The
    package wraps 'CImg', <http://cimg.eu>, a simple, modern C++ library for image
    processing.",2021-06-10,Shota Ochi,"http://dahtah.github.io/imager/, https://github.com/dahtah/imager/",TRUE,https://github.com/dahtah/imager,345845,171,2021-01-29T13:36:31Z,2022.485380116959
imagerExtra,Provides advanced functions for image processing based on the package 'imager'.,2019-01-25,Shota Ochi,https://github.com/ShotaOchi/imagerExtra,TRUE,https://github.com/shotaochi/imagerextra,19848,10,2021-03-06T04:15:24Z,1984.8
ImaginR,"The pearl oyster, Pinctada margaritifera (Linnaeus, 1758), represents the second economic resource of French Polynesia. It is one of the only bivalves expressing a large varied range of inner shell color, & by correlation, of pearl color. This phenotypic variability is partly under genetic control, but also under environmental influence. With ImaginR, it's now possible to delimit the color phenotype of the pearl oyster's inner shell and to characterize their color variations (by the HSV color code system) with pictures.",2017-05-31,Pierre-Louis Stenger,NA,TRUE,https://github.com/plstenger/imaginr,15270,0,2021-05-06T23:47:06Z,NA
imbibe,"Provides a set of fast, chainable image-processing operations
             which are applicable to images of two, three or four dimensions,
             particularly medical images.",2020-10-26,Jon Clayden,https://github.com/jonclayden/imbibe,TRUE,https://github.com/jonclayden/imbibe,3503,9,2020-12-04T11:05:18Z,389.22222222222223
IMD,"Index of Multiple Deprivation for UK nations at various 
    geographical levels. In England, deprivation data is for Lower Layer Super
    Output Areas, Middle Layer Super Output Areas, Wards, and Local Authorities
    based on data from <https://www.gov.uk/government/statistics/english-indices-of-deprivation-2019>.
    In Wales, deprivation data is for Lower Layer Super Output Areas, Middle 
    Layer Super Output Areas, Wards, and Local Authorities based on data from
    <https://gov.wales/welsh-index-multiple-deprivation-full-index-update-ranks-2019>.
    In Scotland, deprivation data is for Data Zones, Intermediate Zones, and 
    Council Areas based on data from <https://simd.scot>. In Northern Ireland,
    deprivation data is for Super Output Areas and Local Government Districts
    based on data from <https://www.nisra.gov.uk/statistics/deprivation/northern-ireland-multiple-deprivation-measure-2017-nimdm2017>.
    The 'IMD' package also provides the composite UK index developed by
    <https://github.com/mysociety/composite_uk_imd>.",2021-08-10,Matthew Gwynfryn Thomas,https://github.com/matthewgthomas/IMD,TRUE,https://github.com/matthewgthomas/imd,247,3,2021-08-19T10:59:34Z,82.33333333333333
imfr,"Explore and download data from the International Monetary Fund's
    data API <http://data.imf.org/>.",2020-10-03,Christopher Gandrud,https://CRAN.R-project.org/package=imfr,TRUE,https://github.com/christophergandrud/imfr,27379,28,2020-10-03T06:03:11Z,977.8214285714286
imgpalr,"Provides ability to create color palettes from image files. 
    It offers control over the type of color palette to derive from an image (qualitative, sequential or divergent) and other palette properties.
    Quantiles of an image color distribution can be trimmed. 
    Near-black or near-white colors can be trimmed in RGB color space independent of trimming brightness or saturation distributions in HSV color space.
    Creating sequential palettes also offers control over the order of HSV color dimensions to sort by.
    This package differs from other related packages like 'RImagePalette' in approaches to quantizing and extracting colors in images to assemble color palettes 
    and the level of user control over palettes construction.",2021-02-21,Matthew Leonawicz,https://github.com/leonawicz/imgpalr,TRUE,https://github.com/leonawicz/imgpalr,11257,36,2021-02-20T23:09:06Z,312.69444444444446
imgrec,Provides an interface for image recognition using the 'Google Vision API' <https://cloud.google.com/vision/> .  Converts API data for features such as object detection and optical character recognition to data frames. The package also includes functions for analyzing image annotations.,2021-03-29,Carsten Schwemmer,https://github.com/cschwem2er/imgrec,TRUE,https://github.com/cschwem2er/imgrec,10751,11,2021-03-29T12:06:04Z,977.3636363636364
IMIFA,"Provides flexible Bayesian estimation of Infinite Mixtures of Infinite Factor Analysers and related models, for nonparametrically clustering high-dimensional data, introduced by Murphy et al. (2020) <doi:10.1214/19-BA1179>. The IMIFA model conducts Bayesian nonparametric model-based clustering with factor analytic covariance structures without recourse to model selection criteria to choose the number of clusters or cluster-specific latent factors, mostly via efficient Gibbs updates. Model-specific diagnostic tools are also provided, as well as many options for plotting results, conducting posterior inference on parameters of interest, posterior predictive checking, and quantifying uncertainty.",2021-05-24,Keefe Murphy,https://cran.r-project.org/package=IMIFA,TRUE,https://github.com/keefe-murphy/imifa,23632,4,2021-05-24T16:26:48Z,5908
IMIX,"A multivariate Gaussian mixture model framework to integrate multiple types of genomic data and allow modeling of inter-data-type correlations for association analysis. 'IMIX' can be implemented to test whether a disease is associated with genes in multiple genomic data types, such as DNA methylation, copy number variation, gene expression, etc. It can also study the integration of multiple pathways. 'IMIX' uses the summary statistics of association test outputs and conduct integration analysis for two or three types of genomics data. 'IMIX' features statistically-principled model selection, global FDR control and computational efficiency. Details are described in Ziqiao Wang and Peng Wei (2020) <doi:10.1093/bioinformatics/btaa1001>. ",2021-02-10,Ziqiao Wang,https://github.com/ziqiaow/IMIX,TRUE,https://github.com/ziqiaow/imix,3058,5,2021-02-08T19:29:06Z,611.6
iml,"Interpretability methods to analyze the behavior
    and predictions of any machine learning model.  Implemented methods
    are: Feature importance described by Fisher et al. (2018)
    <arXiv:1801.01489>, accumulated local effects plots described by Apley
    (2018) <arXiv:1612.08468>, partial dependence plots described by
    Friedman (2001) <www.jstor.org/stable/2699986>, individual
    conditional expectation ('ice') plots described by Goldstein et al.
    (2013) <doi:10.1080/10618600.2014.907095>, local models (variant of
    'lime') described by Ribeiro et. al (2016) <arXiv:1602.04938>, the
    Shapley Value described by Strumbelj et. al (2014)
    <doi:10.1007/s10115-013-0679-x>, feature interactions described by
    Friedman et. al <doi:10.1214/07-AOAS148> and tree surrogate models.",2020-09-24,Christoph Molnar,"https://christophm.github.io/iml/,
https://github.com/christophM/iml/",TRUE,https://github.com/christophm/iml,141732,413,2021-06-28T06:42:43Z,343.1767554479419
immcp,"The pathway fingerprint is a method to indicate the profile of significant pathways being influenced by drugs, which may hint drug functions. Through the similarity of pathway fingerprints, the potential relationship between disease and prescription can be found. Ye (2012) <doi: 10.1007/s13238-012-2011-z>.",2020-11-14,Yuanlong Hu,https://github.com/YuanlongHu/immcp,TRUE,https://github.com/yuanlonghu/immcp,3187,1,2021-08-26T06:35:14Z,3187
immuneSIM,"Simulate full B-cell and T-cell receptor repertoires using an in silico 
    recombination process that includes a wide variety of tunable parameters to introduce noise and biases. 
    Additional post-simulation modification functions allow the user to implant motifs or codon biases as 
    well as remodeling sequence similarity architecture. The output repertoires contain records of all 
    relevant repertoire dimensions and can be analyzed using provided repertoire analysis functions.
    Preprint is available at bioRxiv (Weber et al., 2019 <doi:10.1101/759795>).",2019-09-27,Cédric R. Weber,https://immuneSIM.readthedocs.io,TRUE,https://github.com/greifflab/immunesim,9529,22,2021-03-05T11:49:53Z,433.1363636363636
impactr,"Functions to read, process and analyse accelerometer
    data related to mechanical loading variables. This package is
    developed and tested for use with raw accelerometer data from
    triaxial 'ActiGraph' <https://actigraphcorp.com> accelerometers.",2021-09-03,Lucas Veras,https://lveras.com/impactr/,TRUE,https://github.com/verasls/impactr,735,0,2021-09-03T06:06:35Z,NA
implyr,"'SQL' back-end to 'dplyr' for Apache Impala, the massively
    parallel processing query engine for Apache 'Hadoop'. Impala enables
    low-latency 'SQL' queries on data stored in the 'Hadoop' Distributed
    File System '(HDFS)', Apache 'HBase', Apache 'Kudu', Amazon Simple 
    Storage Service '(S3)', Microsoft Azure Data Lake Store '(ADLS)', 
    and Dell 'EMC' 'Isilon'. See <https://impala.apache.org> for more
    information about Impala.",2021-03-29,Ian Cook,https://github.com/ianmcook/implyr,TRUE,https://github.com/ianmcook/implyr,28759,74,2021-03-29T14:28:57Z,388.63513513513516
import,"Alternative mechanism for importing objects from packages
    and R modules. The syntax allows for importing multiple objects with a single
    command in an expressive way. The import package bridges some of the gap
    between using library (or require) and direct (single-object) imports.
    Furthermore the imported objects are not placed in the current environment.",2020-09-24,Magnus Thor Torfason,https://github.com/rticulate/import,TRUE,https://github.com/rticulate/import,233968,171,2020-09-24T22:48:09Z,1368.233918128655
importinegi,Download and manage data sets of statistical projects and geographic data created by Instituto Nacional de Estadistica y Geografia (INEGI). See <https://www.inegi.org.mx/>.,2021-01-13,Cesar Renteria,NA,TRUE,https://github.com/crenteriam/importinegi,12759,2,2021-01-01T15:42:02Z,6379.5
imputeFin,"Missing values often occur in financial data due to a variety 
    of reasons (errors in the collection process or in the processing stage, 
    lack of asset liquidity, lack of reporting of funds, etc.). However, 
    most data analysis methods expect complete data and cannot be employed 
    with missing values. One convenient way to deal with this issue without 
    having to redesign the data analysis method is to impute the missing 
    values. This package provides an efficient way to impute the missing 
    values based on modeling the time series with a random walk or an 
    autoregressive (AR) model, convenient to model log-prices and log-volumes 
    in financial data. In the current version, the imputation is 
    univariate-based (so no asset correlation is used). In addition,
    outliers can be detected and removed.
    The package is based on the paper:
    J. Liu, S. Kumar, and D. P. Palomar (2019). Parameter Estimation of 
    Heavy-Tailed AR Model With Missing Data Via Stochastic EM. IEEE Trans. on 
    Signal Processing, vol. 67, no. 8, pp. 2159-2172. <doi:10.1109/TSP.2019.2899816>.",2021-02-20,Daniel P. Palomar,"https://CRAN.R-project.org/package=imputeFin,
https://github.com/dppalomar/imputeFin,
https://www.danielppalomar.com,
https://doi.org/10.1109/TSP.2019.2899816,
https://doi.org/10.1109/TSP.2020.3033378",TRUE,https://github.com/dppalomar/imputefin,11650,14,2021-02-20T06:48:10Z,832.1428571428571
imputeR,"Multivariate Expectation-Maximization (EM) based imputation framework that offers several different algorithms. These include regularisation methods like Lasso and Ridge regression, tree-based models and dimensionality reduction methods like PCA and PLS.",2020-01-20,Steffen Moritz,http://github.com/SteffenMoritz/imputeR,TRUE,https://github.com/steffenmoritz/imputer,39636,11,2021-07-21T14:08:33Z,3603.2727272727275
imputeTS,"Imputation (replacement) of missing values 
             in univariate time series. 
             Offers several imputation functions
             and missing data plots. 
             Available imputation algorithms include: 
            'Mean', 'LOCF', 'Interpolation', 
            'Moving Average', 'Seasonal Decomposition', 
            'Kalman Smoothing on Structural Time Series models',
            'Kalman Smoothing on ARIMA models'. Published in Moritz and Bartz-Beielstein (2017) 
            <doi: 10.32614/RJ-2017-009>.",2021-01-16,Steffen Moritz,"https://github.com/SteffenMoritz/imputeTS,
https://steffenmoritz.github.io/imputeTS/",TRUE,https://github.com/steffenmoritz/imputets,764489,130,2021-07-12T15:03:44Z,5880.684615384615
imsig,Estimate the relative abundance of tissue-infiltrating immune subpopulations abundances using gene expression data. ,2021-01-10,Ajit Johnson Nirmal,https://github.com/ajitjohnson/imsig/,TRUE,https://github.com/ajitjohnson/imsig,14984,20,2021-01-09T03:35:25Z,749.2
incase,"Offers a pipe-friendly alternative to the 'dplyr' functions
    case_when() and if_else(), as well as a number of user-friendly
    simplifications for common use cases.  These functions accept a vector
    as an optional first argument, allowing conditional statements to be
    built using the 'magrittr' dot operator.  The functions also coerce
    all outputs to the same type, meaning you no longer have to worry
    about using specific typed variants of NA or explicitly declaring
    integer outputs, and evaluate outputs somewhat lazily, so you don't
    waste time on long operations that won't be used.",2021-06-06,Alexander Rossell Hayes,"https://incase.rossellhayes.com,
https://github.com/rossellhayes/incase",TRUE,https://github.com/rossellhayes/incase,5075,6,2021-06-05T23:24:41Z,845.8333333333334
incidence,"Provides functions and classes to compute, handle and visualise
  incidence from dated events for a defined time interval. Dates can be provided
  in various standard formats. The class 'incidence' is used to store computed
  incidence and can be easily manipulated, subsetted, and plotted. In addition,
  log-linear models can be fitted to 'incidence' objects using 'fit'. This
  package is part of the RECON (<https://www.repidemicsconsortium.org/>) toolkit
  for outbreak analysis.",2020-11-04,Tim Taylor,https://www.repidemicsconsortium.org/incidence/,TRUE,https://github.com/reconhub/incidence,106719,56,2020-11-03T17:18:51Z,1905.6964285714287
incidence2,"Provides functions and classes to compute, handle and visualise 
  incidence from dated events for a defined time interval. Dates can be 
  provided in various standard formats. The class 'incidence2' is used to store
  computed incidence and can be easily manipulated, subsetted, and plotted.
  This package is part of the RECON (<https://www.repidemicsconsortium.org/>) 
  toolkit for outbreak analysis (<https://www.reconverse.org>).",2021-08-23,Tim Taylor,https://github.com/reconverse/incidence2,TRUE,https://github.com/reconverse/incidence2,9136,14,2021-08-23T12:30:34Z,652.5714285714286
inctools,"Tools for estimating incidence from biomarker data in cross-
    sectional surveys, and for calibrating tests for recent infection. 
    Implements and extends the method of Kassanjee et al. (2012)
    <doi:10.1097/EDE.0b013e3182576c07>.",2019-11-07,Eduard Grebe,http://www.incidence-estimation.org/page/inctools,TRUE,https://github.com/sacema/inctools,18294,5,2021-03-29T18:10:05Z,3658.8
IndexNumR,"Computes bilateral and multilateral index numbers. 
    It has support for many standard bilateral indexes as well as
    multilateral index number methods such as GEKS, GEKS-Tornqvist 
    (or CCDI), Geary-Khamis and the weighted time product dummy
    (for details on these methods see Diewert and Fox (2020) 
    <doi:10.1080/07350015.2020.1816176>). 
    It also supports updating of multilateral indexes using 
    several splicing methods.",2021-07-21,Graham White,https://github.com/grahamjwhite/IndexNumR,TRUE,https://github.com/grahamjwhite/indexnumr,20264,6,2021-07-28T10:57:16Z,3377.3333333333335
indiedown,"Simplifies the generation of customized R Markdown PDF templates.
    A template may include an individual logo, typography, geometry or color
    scheme. The package provides a skeleton with detailed instructions for
    customizations. The skeleton can be modified by changing defaults in the
    'YAML' header, by adding additional 'LaTeX' commands or by applying dynamic
    adjustments in R. Individual corporate design elements, such as a title page, can be added as R functions that produce 'LaTeX' code.",2021-03-22,Christoph Sax,"https://cynkra.github.io/indiedown/,
https://github.com/cynkra/indiedown",TRUE,https://github.com/cynkra/indiedown,3477,15,2021-07-29T04:14:09Z,231.8
industRial,"Companion package to the book ""industRial data science"", 
    J.Ramalho (2021) <https://j-ramalho.github.io/industRial/>. 
    Provides data sets and functions to complete the case studies and contains 
    the book original Rmd files and tutorials.",2021-06-11,Joao Ramalho,https://github.com/J-Ramalho/industRial,TRUE,https://github.com/j-ramalho/industrial,1023,0,2021-06-26T17:41:17Z,NA
infer,"The objective of this package is to perform
    inference using an expressive statistical grammar that coheres with
    the tidy design framework.",2021-08-13,Andrew Bray,"https://github.com/tidymodels/infer, https://infer.tidymodels.org/",TRUE,https://github.com/tidymodels/infer,484204,606,2021-08-21T13:43:50Z,799.016501650165
inferr,"Select set of parametric and non-parametric statistical tests. 'inferr' builds upon the solid set of
    statistical tests provided in 'stats' package by including additional data types as inputs, expanding and
    restructuring the test results. The tests included are t tests, variance tests, proportion tests, chi square tests, Levene's test, McNemar Test, Cochran's Q test and Runs test.",2021-05-28,Aravind Hebbali,"https://rsquaredacademy.github.io/inferr/,
https://github.com/rsquaredacademy/inferr",TRUE,https://github.com/rsquaredacademy/inferr,21567,30,2021-05-29T04:49:33Z,718.9
influenceAUC,"Ke, B. S., Chiang, A. J., & Chang, Y. C. I. (2018) <doi:10.1080/10543406.2017.1377728> provide two theoretical methods (influence function and local influence) based on the area under the receiver operating characteristic curve (AUC) to quantify the numerical impact of each observation to the overall AUC. Alternative graphical tools, cumulative lift charts, are proposed to reveal the existences and approximate locations of those influential observations through data visualization.",2020-05-30,Bo-Shiang Ke,NA,TRUE,https://github.com/boshiangke/influenceauc,9100,0,2021-01-25T15:36:59Z,NA
influential,"Contains functions for the classification and ranking of top candidate features, reconstruction of networks from
    adjacency matrices and data frames, analysis of the topology of the network 
    and calculation of centrality measures, and identification of the most
    influential nodes. Also, a function is provided for running SIRIR model, which 
    is the combination of leave-one-out cross validation technique and the conventional SIR model, on a network to unsupervisedly rank the true influence of vertices. Additionally, some functions have been provided for the assessment 
    of dependence and correlation of two network centrality measures as well as 
    the conditional probability of deviation from their corresponding means in opposite direction.
    Fred Viole and David Nawrocki (2013, ISBN:1490523995).
    Csardi G, Nepusz T (2006). ""The igraph software package for complex network research."" InterJournal, Complex Systems, 1695.
    Adopted algorithms and sources are referenced in function document.",2021-07-17,Adrian (Abbas) Salavaty,https://github.com/asalavaty/influential,TRUE,https://github.com/asalavaty/influential,11936,9,2021-09-03T04:05:05Z,1326.2222222222222
influxdbclient,"
  InfluxDB 2.x time-series database client. Supports both InfluxDB OSS (<https://portal.influxdata.com/downloads/>) and Cloud (<https://cloud2.influxdata.com/>) version.",2021-07-21,Ales Pour,https://github.com/influxdata/influxdb-client-r,TRUE,https://github.com/influxdata/influxdb-client-r,580,3,2021-07-27T12:31:38Z,193.33333333333334
ingredients,"Collection of tools for assessment of feature importance and feature effects.
    Key functions are:
    feature_importance() for assessment of global level feature importance,
    ceteris_paribus() for calculation of the what-if plots,
    partial_dependence() for partial dependence plots,
    conditional_dependence() for conditional dependence plots,
    accumulated_dependence() for accumulated local effects plots,
    aggregate_profiles() and cluster_profiles() for aggregation of ceteris paribus profiles,
    generic print() and plot() for better usability of selected explainers,
    generic plotD3() for interactive, D3 based explanations, and
    generic describe() for explanations in natural language.
    The package 'ingredients' is a part of the 'DrWhy.AI' universe (Biecek 2018) <arXiv:1806.08915>.",2021-04-10,Przemyslaw Biecek,"https://ModelOriented.github.io/ingredients/,
https://github.com/ModelOriented/ingredients",TRUE,https://github.com/modeloriented/ingredients,78036,33,2021-08-21T12:30:33Z,2364.7272727272725
inlabru,"Facilitates spatial and general latent Gaussian modeling using
  integrated nested Laplace approximation via the INLA package (<https://www.r-inla.org>).
  Additionally, extends the GAM-like model class to more general nonlinear predictor
  expressions, and implements a log Gaussian Cox process likelihood for 
  modeling univariate and spatial point processes based on ecological survey data.
  Model components are specified with general inputs and mapping methods to the
  latent variables, and the predictors are specified via general R expressions,
  with separate expressions for each observation likelihood model in
  multi-likelihood models. A prediction method based on fast Monte Carlo sampling
  allows posterior prediction of general expressions of the latent variables.
  Ecology-focused introduction in Bachl, Lindgren, Borchers, and Illian (2019)
  <doi:10.1111/2041-210X.13168>.",2021-03-22,Finn Lindgren,"http://www.inlabru.org, https://inlabru-org.github.io/inlabru/",TRUE,https://github.com/inlabru-org/inlabru,20927,35,2021-09-02T16:22:05Z,597.9142857142857
inline,"Functionality to dynamically define R functions and S4 methods
 with 'inlined' C, C++ or Fortran code supporting the .C and .Call calling
 conventions.",2021-05-31,Oleg Sklyar,"https://github.com/eddelbuettel/inline,
https://dirk.eddelbuettel.com/code/inline.html",TRUE,https://github.com/eddelbuettel/inline,1758345,33,2021-07-14T14:40:54Z,53283.181818181816
inlmisc,"A collection of functions for creating high-level graphics,
    performing raster-based analysis, processing MODFLOW-based models,
    selecting subsets using a genetic algorithm, creating interactive web maps,
    accessing color palettes, etc. Used to support packages and scripts written
    by researchers at the United States Geological Survey (USGS)
    Idaho National Laboratory (INL) Project Office.",2020-09-12,Jason C. Fisher,https://github.com/USGS-R/inlmisc,TRUE,https://github.com/usgs-r/inlmisc,45052,16,2020-09-09T16:31:31Z,2815.75
INQC,Collection of functions for quality control (QC) of climatological daily time series (e.g. the ECA&D station data).,2021-05-24,Enric Aguilar,https://github.com/INDECIS-Project/INQC,TRUE,https://github.com/indecis-project/inqc,2838,5,2021-05-20T15:30:48Z,567.6
insane,"A user-friendly interface, using Shiny, to analyse glucose-stimulated insulin secretion (GSIS) 
    assays in pancreatic beta cells or islets.
    The package allows the user to import several sets of experiments from different spreadsheets 
    and to perform subsequent steps: summarise in a tidy format, visualise data quality 
    and compare experimental conditions without omitting to account for technical confounders 
    such as the date of the experiment or the technician.
    Together, insane is a comprehensive method that optimises pre-processing and analyses of 
    GSIS experiments in a friendly-user interface.
    The Shiny App was initially designed for EndoC-betaH1 cell line following method described 
    in Ndiaye et al., 2017 (<doi:10.1016/j.molmet.2017.03.011>).",2020-11-04,Mickaël Canouil,"https://github.com/mcanouil/insane,
https://mcanouil.github.io/insane/",TRUE,https://github.com/mcanouil/insane,3490,1,2021-01-20T11:32:24Z,3490
insect,Provides tools for probabilistic taxon assignment with informatic sequence classification trees. See Wilkinson et al (2018) <doi:10.7287/peerj.preprints.26812v1>.,2021-08-09,Shaun Wilkinson,https://github.com/shaunpwilkinson/insect/,TRUE,https://github.com/shaunpwilkinson/insect,16064,12,2021-08-06T07:54:33Z,1338.6666666666667
insee,"Using embedded sdmx queries, get the data of more than 150 000 insee series from bdm database. Have a look at the detailed sdmx web service page with the following link : <https://www.insee.fr/en/information/2868055>.",2021-06-04,Hadrien Leclerc,https://InseeFr.github.io/R-Insee-Data/,TRUE,https://github.com/inseefr/r-insee-data,7361,11,2021-06-16T21:36:20Z,669.1818181818181
insight,"A tool to provide an easy, intuitive and consistent
    access to information contained in various R models, like model
    formulas, model terms, information about random effects, data that was
    used to fit the model or data from response variables. 'insight'
    mainly revolves around two types of functions: Functions that find
    (the names of) information, starting with 'find_', and functions that
    get the underlying data, starting with 'get_'.  The package has a
    consistent syntax and works with many different model objects, where
    otherwise functions to access these information are missing.",2021-09-02,Daniel Lüdecke  (<https://orcid.org/0000-0002-8895-3206>,https://easystats.github.io/insight/,TRUE,https://github.com/easystats/insight,1565719,228,2021-09-02T06:52:41Z,6867.188596491228
InSilicoVA,"Computes individual causes of death and population cause-specific mortality fractions using the 'InSilicoVA' algorithm from McCormick et al. (2016) <DOI:10.1080/01621459.2016.1152191>. It uses data derived from verbal autopsy (VA) interviews, in a format similar to the input of the widely used 'InterVA4' method. This package provides general model fitting and customization for 'InSilicoVA' algorithm and basic graphical visualization of the output.",2021-08-02,Zehang Richard Li,https://github.com/verbal-autopsy-software/InSilicoVA,TRUE,https://github.com/verbal-autopsy-software/insilicova,29589,3,2021-08-02T15:50:35Z,9863
inspectdf,"A collection of utilities for columnwise summary, comparison and visualisation of data frames.  Functions report missingness, categorical levels, numeric distribution, correlation, column types and memory usage.",2021-04-02,Alastair Rushworth,https://alastairrushworth.github.io/inspectdf/,TRUE,https://github.com/alastairrushworth/inspectdf,45757,202,2021-04-02T11:12:42Z,226.51980198019803
inspector,"Utility functions that implement and automate common sets of validation tasks. 
    These functions are particularly useful to validate inputs, intermediate objects and output 
    values in user-defined functions, resulting in tidier and less verbose functions.",2021-06-17,Pedro Fonseca,"https://ptfonseca.github.io/inspector/,
https://github.com/ptfonseca/inspector",TRUE,https://github.com/ptfonseca/inspector,4946,0,2021-06-19T16:16:59Z,NA
installr,"R is great for installing software.  Through the 'installr'
    package you can automate the updating of R (on Windows, using updateR())
    and install new software. Software installation is initiated through a
    GUI (just run installr()), or through functions such as: install.Rtools(),
    install.pandoc(), install.git(), and many more. The updateR() command
    performs the following: finding the latest R version, downloading it,
    running the installer, deleting the installation file, copy and updating
    old packages to the new R installation.",2021-05-08,Tal Galili,"https://talgalili.github.io/installr/,
https://github.com/talgalili/installr/,
https://www.r-statistics.com/tag/installr/",TRUE,https://github.com/talgalili/installr,1551244,231,2021-05-21T12:35:31Z,6715.341991341991
insurancerating,"Methods for insurance rating. It helps actuaries to implement GLMs within all relevant steps needed to construct
 a risk premium from raw data. It provides a data driven strategy for the construction of insurance tariff classes.
 This strategy is based on the work by Antonio and Valdez (2012) <doi:10.1007/s10182-011-0152-7>. It also provides recipes
 on how to easily perform one-way, or univariate, analyses on an insurance portfolio. In addition it adds functionality
 to include reference categories in the levels of the coefficients in the output of a generalized linear regression analysis.",2021-07-28,Martin Haringa,"https://github.com/mharinga/insurancerating,
https://mharinga.github.io/insurancerating/",TRUE,https://github.com/mharinga/insurancerating,18534,22,2021-08-10T08:21:13Z,842.4545454545455
intensegRid,"Electricity is not made equal and it vary in its carbon footprint (or carbon intensity) 
    depending on its source. This package enables to access and query data provided by the 
    Carbon Intensity API (<https://carbonintensity.org.uk/>). National Grid’s Carbon Intensity API 
    provides an indicative trend of regional carbon intensity of the electricity system in Great Britain.  ",2021-05-01,Kasia Kulma,"https://github.com/KKulma/intensegRid,
https://kkulma.github.io/intensegRid/articles/intro-to-carbon-intensity.html",TRUE,https://github.com/kkulma/intensegrid,3680,8,2021-06-10T20:47:47Z,460
interactionR,"Produces a publication-ready table that includes all effect estimates necessary for full reporting effect modification and interaction analysis as recommended by Knol and Vanderweele (2012) [<doi:10.1093/ije/dyr218>].
    It also estimates confidence interval for the trio of additive interaction measures using the delta method (see Hosmer and Lemeshow (1992), [<doi:10.1097/00001648-199209000-00012>]), variance recovery method (see Zou (2008), [<doi:10.1093/aje/kwn104>]), or percentile bootstrapping (see Assmann et al. (1996), [<doi:10.1097/00001648-199605000-00012>]). ",2021-09-03,Babatunde Alli,https://github.com/epi-zen/interactionR,TRUE,https://github.com/epi-zen/interactionr,8761,6,2021-08-28T15:26:11Z,1460.1666666666667
interplot,"Plots the conditional coefficients (""marginal effects"") of
    variables included in multiplicative interaction terms.",2021-02-18,Yue Hu,NA,TRUE,https://github.com/sammo3182/interplot,80345,12,2021-03-23T03:40:32Z,6695.416666666667
inTextSummaryTable,"Creation of tables of summary statistics or counts for clinical data (for 'TLFs'). 
  These tables can be exported as in-text table (with the 'flextable' package) for a Clinical Study Report 
  (Word format) or a 'topline' presentation (PowerPoint format), 
  or as interactive table (with the 'DT' package) to an html document for clinical data review.",2021-07-13,Laure Cougnaud,https://github.com/openanalytics/inTextSummaryTable,TRUE,https://github.com/openanalytics/intextsummarytable,975,0,2021-07-13T14:35:44Z,NA
inti,"The 'inti' package is part of the 'inkaverse' project for developing 
    different procedures and tools used in plant science and experimental designs. 
    The mean aim of the package is to support researchers during the planning of
    experiments and data collection (tarpuy()), data analysis and graphics (yupana())
    , and technical writing. 
    Learn more about the 'inkaverse' project at <https://inkaverse.com/>.",2021-08-15,Flavio Lozano-Isla,"https://inkaverse.com/, https://github.com/flavjack/inti",TRUE,https://github.com/flavjack/inti,5751,0,2021-09-02T15:33:39Z,NA
intsurv,"Contains implementations of
    integrative survival analysis routines, including
    regular Cox cure rate model proposed by
    Kuk and Chen (1992) <doi:10.1093/biomet/79.3.531>
    via an EM algorithm proposed by
    Sy and Taylor (2000) <doi:10.1111/j.0006-341X.2000.00227.x>,
    regularized Cox cure rate model with elastic net penalty following
    Masud et al. (2018) <doi:10.1177/0962280216677748>, and
    Zou and Hastie (2005) <doi:10.1111/j.1467-9868.2005.00503.x>, and
    weighted concordance index for cure models proposed by
    Asano and Hirakawa (2017) <doi:10.1080/10543406.2017.1293082>.",2021-01-08,Wenjie Wang,"https://wwenjie.org/intsurv,
https://github.com/wenjie2wang/intsurv",TRUE,https://github.com/wenjie2wang/intsurv,11194,8,2021-04-03T15:51:33Z,1399.25
intsvy,"
  Provides tools for importing, merging, and analysing data from 
  international assessment studies (TIMSS, PIRLS, PISA, ICILS, and PIAAC).",2021-01-23,Daniel Caro,"http://danielcaro.net/r-intsvy/,
https://github.com/eldafani/intsvy",TRUE,https://github.com/eldafani/intsvy,43932,14,2021-08-20T08:40:23Z,3138
iNZightMR,"Interaction and analysis of multiple response data,
    along with other tools for analysing these types of data including
    missing value analysis and calculation of standard errors for
    a range of covariance matrix results (proportions, multinomial,
    independent samples, and multiple response).",2020-05-05,Tom Elliott,https://www.stat.auckland.ac.nz/~wild/iNZight/,TRUE,https://github.com/inzightvit/inzightmr,5565,0,2021-04-28T02:47:28Z,NA
iNZightPlots,"Simple plotting function(s) for exploratory data analysis with flexible options allowing for easy plot customisation. The goal is to make it easy for beginners to start exploring a dataset through simple R function calls, as well as provide a similar interface to summary statistics and inference information. Includes functionality to generate interactive HTML-driven graphs. Used by 'iNZight', a graphical user interface providing easy exploration and visualisation of data for students of statistics, available in both desktop and online versions.",2021-07-01,Tom Elliott,https://inzight.nz,TRUE,https://github.com/inzightvit/inzightplots,5665,1,2021-07-16T01:27:42Z,5665
iNZightRegression,"Provides a suite of functions to use with regression models, including summaries, residual plots, and factor comparisons. Used as part of the Model Fitting module of 'iNZight', a graphical user interface providing easy exploration and visualisation of data for students of statistics, available in both desktop and online versions.",2021-05-06,Tom Elliott,http://inzight.nz,TRUE,https://github.com/inzightvit/inzightregression,3639,0,2021-07-13T00:08:08Z,NA
iNZightTools,"Provides a collection of wrapper functions for common variable and dataset manipulation workflows primarily used by 'iNZight', a graphical user interface providing easy exploration and visualisation of data for students of statistics, available in both desktop and online versions. Additionally, many of the functions return the 'tidyverse' code used to obtain the result in an effort to bridge the gap between GUI and coding.",2021-07-13,Tom Elliott,http://inzight.nz,TRUE,https://github.com/inzightvit/inzighttools,11945,1,2021-09-01T00:25:32Z,11945
iNZightTS,"Provides a collection of functions for working with time series data, including functions for drawing, decomposing, and forecasting. Includes capabilities to compare multiple series and fit both additive and multiplicative models. Used by 'iNZight', a graphical user interface providing easy exploration and visualisation of data for students of statistics, available in both desktop and online versions. Holt (1957) <doi:10.1016/j.ijforecast.2003.09.015>, Winters (1960) <doi:10.1287/mnsc.6.3.324>, Cleveland, Cleveland, & Terpenning (1990) ""STL: A Seasonal-Trend Decomposition Procedure Based on Loess"".",2021-05-21,Tom Elliott,http://inzight.nz,TRUE,https://github.com/inzightvit/inzightts,6741,0,2021-05-20T23:42:36Z,NA
IOHanalyzer,"The data analysis module for the Iterative Optimization Heuristics
    Profiler ('IOHprofiler'). This module provides statistical analysis methods for the 
    benchmark data generated by optimization heuristics, which can be visualized through a 
    web-based interface. The benchmark data is usually generated by the 
    experimentation module, called 'IOHexperimenter'. 'IOHanalyzer' also supports
    the widely used 'COCO' (Comparing Continuous Optimisers) data format for benchmarking.",2021-05-31,Hao Wang,"http://iohprofiler.liacs.nl,
https://github.com/IOHprofiler/IOHAnalyzer",TRUE,https://github.com/iohprofiler/iohanalyzer,12698,10,2021-08-27T09:42:29Z,1269.8
IOHexperimenter,"The benchmarking module for the Iterative Optimization Heuristics
    Profiler ('IOHprofiler'). This module provides benchmarking in the 'IOHprofiler'
    format, which can be visualized using the 'IOHanalyzer' module.",2020-09-01,Diederick Vermetten,https://github.com/IOHprofiler/IOHexperimenter,TRUE,https://github.com/iohprofiler/iohexperimenter,5956,22,2021-07-05T13:15:31Z,270.72727272727275
iotables,"Pre-processing and basic analytical tasks related to working
    with Eurostat's symmetric input-output tables and provide basic
    input-output economics calculations. The package is part of rOpenGov
    <http://ropengov.github.io/> to open source open government initiatives.",2021-08-02,Daniel Antal,https://iotables.dataobservatory.eu/,TRUE,https://github.com/ropengov/iotables,19495,10,2021-08-02T09:00:58Z,1949.5
ipaddress,"Classes and functions for working with IP (Internet Protocol)
    addresses and networks, inspired by the Python 'ipaddress' module.
    Offers full support for both IPv4 and IPv6 (Internet Protocol versions
    4 and 6) address spaces. It is specifically designed to work well with
    the 'tidyverse'.",2021-07-27,David Hall,"https://davidchall.github.io/ipaddress/,
https://github.com/davidchall/ipaddress",TRUE,https://github.com/davidchall/ipaddress,13142,14,2021-07-28T00:50:47Z,938.7142857142857
ipADMIXTURE,"A data clustering package based on admixture ratios (Q matrix) of population structure. The framework is based on iterative Pruning procedure that performs data clustering by splitting a given population into subclusters until meeting the condition of stopping criteria the same as ipPCA, iNJclust, and IPCAPS frameworks. The package also provides a function to retrieve phylogeny tree that construct a neighbor-joining tree based on a similar matrix between clusters. By given multiple Q matrices with varying a number of ancestors (K), the framework define a similar value between clusters i,j as a minimum number K* that makes majority of members of two clusters are in the different clusters. This K* reflexes a minimum number of ancestors we need to splitting cluster i,j into different clusters if we assign K* clusters based on maximum admixture ratio of individuals. The publication of this package is at Chainarong Amornbunchornvej, Pongsakorn Wangkumhang, and Sissades Tongsima (2020) <doi:10.1101/2020.03.21.001206>.",2020-03-26,Chainarong Amornbunchornvej,https://github.com/DarkEyes/ipADMIXTURE,TRUE,https://github.com/darkeyes/ipadmixture,6247,4,2021-03-22T07:24:50Z,1561.75
ipeadatar,"Allows direct access to the macroeconomic, 
             financial and regional database maintained by 
             Brazilian Institute for Applied Economic Research ('Ipea').
             This R package uses the 'Ipeadata' API. For more information, 
             see <http://www.ipeadata.gov.br/>.",2021-07-28,Luiz Eduardo S. Gomes,https://github.com/gomesleduardo/ipeadatar,TRUE,https://github.com/gomesleduardo/ipeadatar,34042,17,2021-07-24T01:32:47Z,2002.4705882352941
IPMbook,Provides functions and data sets to accompany the  book 'Integrated Population Models: Theory and Ecological Applications with R and JAGS' by Michael Schaub and Marc Kéry (ISBN: 9780128205648).,2021-07-29,Mike Meredith,https://www.elsevier.com/books/integrated-population-models/schaub/978-0-12-820564-8,TRUE,https://github.com/mikemeredith/ipmbook,563,4,2021-08-24T08:44:48Z,140.75
ipmisc,"Provides functions needed for data cleaning and
    formatting and forms data cleaning and wrangling backend for the
    following packages: 'ggstatsplot', 'pairwiseComparisons', and
    'statsExpressions'.",2021-05-07,Indrajeet Patil,"https://indrajeetpatil.github.io/ipmisc/,
https://github.com/IndrajeetPatil/ipmisc",TRUE,https://github.com/indrajeetpatil/ipmisc,100230,3,2021-06-27T11:29:31Z,33410
ipmr,"Flexibly implements Integral Projection Models using a
  mathematical(ish) syntax. This package will not help with the vital rate
  modeling process, but will help convert those regression models into an
  IPM. 'ipmr' handles density dependence and environmental stochasticity, with a
  couple of options for implementing the latter. In addition, provides functions
  to avoid unintentional eviction of individuals from models. Additionally, 
  provides model diagnostic tools, plotting functionality, 
  stochastic/deterministic simulations, and analysis tools.
  Integral projection models are described in depth by Easterling et al. (2000) 
  <doi:10.1890/0012-9658(2000)081[0694:SSSAAN]2.0.CO;2>, Merow et al. (2013) 
  <doi:10.1111/2041-210X.12146>, Rees et al. (2014) <doi:10.1111/1365-2656.12178>,
  and Metcalf et al. (2015) <doi:10.1111/2041-210X.12405>. 
  Williams et al. (2012) <doi:10.1890/11-2147.1> discuss the problem of 
  unintentional eviction.",2021-07-13,Sam Levin,"https://levisc8.github.io/ipmr/, https://github.com/levisc8/ipmr",TRUE,https://github.com/levisc8/ipmr,3099,3,2021-08-23T19:40:01Z,1033
iprior,"Provides methods to perform and analyse I-prior regression models.
    Estimation is done either via direct optimisation of the log-likelihood or 
    an EM algorithm.",2019-03-20,Haziq Jamil,https://github.com/haziqj/iprior,TRUE,https://github.com/haziqj/iprior,18761,1,2021-08-27T04:33:53Z,18761
iptools,"A toolkit for manipulating, validating and testing 'IP' addresses and
    ranges, along with datasets relating to 'IP' addresses. Tools are also provided
    to map 'IPv4' blocks to country codes. While it primarily has support for the 'IPv4'
    address space, more extensive 'IPv6' support is intended.",2018-12-09,Bob Rudis,https://github.com/hrbrmstr/iptools,TRUE,https://github.com/hrbrmstr/iptools,29143,46,2021-08-27T16:31:52Z,633.5434782608696
ipumsr,"An easy way to import census, survey and geographic data provided by 'IPUMS'
    into R plus tools to help use the associated metadata to make analysis easier. 'IPUMS'
    data describing 1.4 billion individuals drawn from over 750 censuses and surveys is
    available free of charge from our website <https://ipums.org>.",2020-07-21,Derek Burk,"https://www.ipums.org, https://github.com/mnpopcenter/ipumsr",TRUE,https://github.com/mnpopcenter/ipumsr,86732,84,2021-04-19T15:04:05Z,1032.5238095238096
irace,"Iterated race is an extension of the Iterated F-race method for
             the automatic configuration of optimization algorithms, that is,
             (offline) tuning their parameters by finding the most appropriate
             settings given a set of instances of an optimization problem.
             M. López-Ibáñez, J. Dubois-Lacoste, L. Pérez Cáceres, T. Stützle,
             and M. Birattari (2016) <doi:10.1016/j.orp.2016.09.002>.",2020-03-31,Manuel López-Ibáñez,"http://iridia.ulb.ac.be/irace,
https://github.com/MLopez-Ibanez/irace",TRUE,https://github.com/mlopez-ibanez/irace,87242,17,2021-09-02T15:15:53Z,5131.882352941177
IRdisplay,"
    An interface to the rich display capabilities of 'Jupyter' front-ends (e.g. 'Jupyter Notebook') <https://jupyter.org>.
    Designed to be used from a running 'IRkernel' session <https://irkernel.github.io>.",2021-01-20,Thomas Kluyver,https://github.com/IRkernel/IRdisplay,TRUE,https://github.com/irkernel/irdisplay,801866,37,2021-08-02T11:16:13Z,21672.054054054053
iriR,"Researchers and analysts have access to more than 7,500 innovative companies worldwide, which are or have been part of the top 1,000 innovative companies. They can access the six parameters that compose the global IRI Scoreboard's data on R&D: Country, Year, Company's name, Industry, Indicator and Company's rank. Please cite: Warin, Th. (2020) ""iiriR: An R Package for the EU Industrial R&D Investment Scoreboard"", <doi:10.6084/m9.figshare.11774640.v5>.",2021-04-22,Thierry Warin,https://github.com/warint/iriR/,TRUE,https://github.com/warint/irir,3830,1,2021-08-20T14:11:39Z,3830
IRkernel,"
    The R kernel for the 'Jupyter' environment executes R code which the front-end
    ('Jupyter Notebook' or other front-ends) submits to the kernel via the network.",2021-05-11,Thomas Kluyver,https://irkernel.github.io,TRUE,https://github.com/irkernel/irkernel,432600,1425,2021-08-02T11:47:04Z,303.57894736842104
IRSF,"Builds ensemble survival tree models to reveal variable interactions when the response is a time-to-events outcome. Codes contain randomization, interaction modeling, and prediction subroutines to be used in addition to the following R packages: 'survival' for Kaplan-Meier and Cox regression modeling, 'randomForestSRC' for RSF modeling, and optionally 'ggRandomForests' for Random Forest exploration/visualization. The current version contains additional R codes in folder ""/inst/doc"" for the analysis and generation of results shown in the corresponding article (Dazard et al. (2018) <doi:10.1515/sagmb-2017-0038>). ",2020-11-13,Jean-Eudes Dazard,https://github.com/jedazard/IRSF,TRUE,https://github.com/jedazard/irsf,3216,2,2020-11-15T20:04:52Z,1608
isatabr,"ISA is a metadata framework to manage an increasingly diverse set 
    of life science, environmental and biomedical experiments. In isatabr 
    methods for reading, modifying and writing of files in the ISA-Tab format
    are implemented. It also contains methods for processing assay data.",2021-07-16,Bart-Jan van Rossum,https://github.com/Biometris/isatabr/,TRUE,https://github.com/biometris/isatabr,635,0,2021-07-16T07:12:41Z,NA
isoband,"A fast C++ implementation to generate contour lines (isolines) and
  contour polygons (isobands) from regularly spaced grids containing elevation data.",2021-07-13,Claus O. Wilke,https://wilkelab.org/isoband/,TRUE,https://github.com/wilkelab/isoband,13218538,107,2021-07-12T20:42:31Z,123537.73831775702
isobxr,"A set of functions to run simple and composite box-models to describe the 
    dynamic or static distribution of stable isotopes in open 
    or closed systems. The package also allows the sweeping of many 
    parameters in both static and dynamic conditions. It also comes 
    with a post-run plotting interface built under shiny.
    The mathematical models used in this package are derived from Albarede, 1995, 
    Introduction to Geochemical Modelling, Cambridge University Press,
    Cambridge <doi:10.1017/CBO9780511622960>.",2021-09-02,Theo Tacail,"https://github.com/ttacail/isobxr,
https://ttacail.github.io/isobxr_web/,
https://ttacail.github.io/isobxr/",TRUE,https://github.com/ttacail/isobxr,0,0,2021-09-03T13:14:33Z,NA
isocalcR,"Perform common calculations based on published stable isotope theory, such as calculating carbon isotope discrimination and intrinsic water use efficiency from wood or leaf carbon isotope composition. See Farquhar, O'Leary, and Berry (1982) <doi:10.1071/PP9820121>.  ",2021-07-31,Justin Mathias,https://github.com/justinmathias/isocalcR,TRUE,https://github.com/justinmathias/isocalcr,730,0,2021-07-31T18:16:20Z,NA
isodistrreg,"Distributional regression under stochastic order restrictions for
    numeric and binary response variables and partially ordered covariates. See
    Henzi, Ziegel, Gneiting (2020) <arXiv:1909.03725>.",2021-03-22,Alexander Henzi,https://github.com/AlexanderHenzi/isodistrreg,TRUE,https://github.com/alexanderhenzi/isodistrreg,1706,8,2021-03-22T12:19:56Z,213.25
IsoplotR,"Plots U-Pb data on Wetherill and Tera-Wasserburg concordia diagrams. Calculates concordia and discordia ages. Performs linear regression of measurements with correlated errors using 'York', 'Titterington' and 'Ludwig' approaches. Generates Kernel Density Estimates (KDEs) and Cumulative Age Distributions (CADs). Produces Multidimensional Scaling (MDS) configurations and Shepard plots of multi-sample detrital datasets using the Kolmogorov-Smirnov distance as a dissimilarity measure. Calculates 40Ar/39Ar ages, isochrons, and age spectra. Computes weighted means accounting for overdispersion. Calculates U-Th-He (single grain and central) ages, logratio plots and ternary diagrams. Processes fission track data using the external detector method and LA-ICP-MS, calculates central ages and plots fission track and other data on radial (a.k.a. 'Galbraith') plots. Constructs total Pb-U, Pb-Pb, Th-Pb, K-Ca, Re-Os, Sm-Nd, Lu-Hf, Rb-Sr and 230Th-U isochrons as well as 230Th-U evolution plots.",2021-07-09,Pieter Vermeesch,"https://www.ucl.ac.uk/~ucfbpve/isoplotr/,
https://github.com/pvermees/IsoplotR/",TRUE,https://github.com/pvermees/isoplotr,43029,35,2021-07-22T11:48:40Z,1229.4
IsoplotRgui,"Provides a graphical user interface to the 'IsoplotR' package for radiometric geochronology. The GUI runs in an internet browser and can either be used offline, or hosted on a server to provide online access to the 'IsoplotR' toolbox.",2021-07-10,Pieter Vermeesch,"https://www.ucl.ac.uk/~ucfbpve/isoplotr/,
https://github.com/pvermees/IsoplotRgui/",TRUE,https://github.com/pvermees/isoplotrgui,5273,12,2021-09-03T08:51:59Z,439.4166666666667
isoreader,Interface to the raw data file formats commonly encountered in scientific disciplines that make use of stable isotopes.,2021-02-16,Sebastian Kopf,https://github.com/isoverse/isoreader,TRUE,https://github.com/isoverse/isoreader,3729,3,2021-05-04T19:57:39Z,1243
IsoriX,"Building isoscapes using mixed models and inferring the geographic
  origin of samples based on their isotopic ratios. This package is essentially a
  simplified interface to several other packages which implements a new
  statistical framework based on mixed models. It uses 'spaMM' for fitting and
  predicting isoscapes, and assigning an organism's origin depending on its
  isotopic ratio. 'IsoriX' also relies heavily on the package 'rasterVis' for
  plotting the maps produced with 'raster' using 'lattice'.",2020-09-16,Alexandre Courtiol,https://github.com/courtiol/IsoriX/,TRUE,https://github.com/courtiol/isorix,43554,9,2021-06-26T16:16:14Z,4839.333333333333
isotree,"Fast and multi-threaded implementation of
	isolation forest (Liu, Ting, Zhou (2008) <doi:10.1109/ICDM.2008.17>),
	extended isolation forest (Hariri, Kind, Brunner (2018) <arXiv:1811.02141>),
	SCiForest (Liu, Ting, Zhou (2010) <doi:10.1007/978-3-642-15883-4_18>),
	and fair-cut forest (Cortes (2019) <arXiv:1911.06646>),
	for isolation-based outlier detection, clustered outlier detection, distance or similarity
	approximation (Cortes (2019) <arXiv:1910.12362>),
	and imputation of missing values (Cortes (2019) <arXiv:1911.06646>),
	based on random or guided decision tree splitting. Provides simple heuristics for fitting the model to
	categorical columns and handling missing data, and offers options for varying between random and guided
	splits, and for using different splitting criteria.",2021-08-22,David Cortes,https://github.com/david-cortes/isotree,TRUE,https://github.com/david-cortes/isotree,17238,73,2021-09-02T01:09:20Z,236.13698630136986
iterators,"Support for iterators, which allow a programmer to traverse
    through all the elements of a vector, list, or other collection of data.",2020-10-15,Michelle Wallig,https://github.com/RevolutionAnalytics/iterators,TRUE,https://github.com/revolutionanalytics/iterators,5635843,3,2020-10-11T15:24:47Z,1878614.3333333333
itraxR,"Parse, trim, join, visualise and analyse data from Itrax sediment core multi-parameter 
    scanners manufactured by Cox Analytical Systems, Sweden. Functions are provided for parsing 
    XRF-peak area files, line-scan optical images, and radiographic images, alongside accompanying metadata. 
    A variety of data wrangling tasks like trimming, joining and reducing XRF-peak area data are simplified. 
    Principle component analysis (PCA), cluster analysis and associated multivariate methods are 
    implemented with appropriate data transformation. ",2021-08-17,Thomas Bishop,https://thomasbishop.uk,TRUE,https://github.com/tombishop1/itraxr,2407,3,2021-08-15T20:26:40Z,802.3333333333334
ivdesc,"Estimating the mean and variance of a covariate for the complier, never-taker and always-taker subpopulation in the context of instrumental variable estimation. This package implements the method described in Marbach and Hangartner (2020) <doi:10.1017/pan.2019.48> and Hangartner, Marbach, Henckel, Maathuis, Kelz and Keele (2021) <arXiv:2103.06328>.",2021-03-20,Moritz Marbach,https://github.com/sumtxt/ivdesc/,TRUE,https://github.com/sumtxt/ivdesc,10196,6,2021-03-18T21:22:36Z,1699.3333333333333
ivreg,"Instrumental variable estimation for linear models by two-stage least-squares (2SLS) regression or by robust-regression via M-estimation (2SM) or MM-estimation (2SMM). The main ivreg() model-fitting function is designed to provide a workflow as similar as possible to standard lm() regression. A wide range of methods is provided for fitted ivreg model objects, including extensive functionality for computing and graphing regression diagnostics in addition to other standard model tools. ",2021-05-28,John Fox,https://john-d-fox.github.io/ivreg/,TRUE,https://github.com/john-d-fox/ivreg,27233,6,2021-07-08T14:50:05Z,4538.833333333333
ivx,"Drawing statistical inference on the coefficients
    of a short- or long-horizon predictive regression with persistent
    regressors by using the IVX method of Magdalinos and Phillips (2009)
    <doi:10.1017/S0266466608090154> and Kostakis, Magdalinos and
    Stamatogiannis (2015) <doi:10.1093/rfs/hhu139>.",2020-11-24,Kostas Vasilopoulos,https://github.com/kvasilopoulos/ivx,TRUE,https://github.com/kvasilopoulos/ivx,11522,11,2021-05-08T13:19:43Z,1047.4545454545455
jackalope,"Simply and efficiently
    simulates (i) variants from reference genomes and (ii) reads from both Illumina 
    <https://www.illumina.com/>
    and Pacific Biosciences (PacBio) <https://www.pacb.com/> platforms. 
    It can either read reference genomes from FASTA files or simulate new ones.
    Genomic variants can be simulated using summary statistics, phylogenies, 
    Variant Call Format (VCF) files, and coalescent simulations—the latter of which
    can include selection, recombination, and demographic fluctuations.
    'jackalope' can simulate single, paired-end, or mate-pair Illumina reads, 
    as well as PacBio reads.
    These simulations include sequencing errors, mapping qualities, multiplexing,
    and optical/polymerase chain reaction (PCR) duplicates.
    Simulating Illumina sequencing is based on ART
    by Huang et al. (2012) <doi:10.1093/bioinformatics/btr708>.
    PacBio sequencing simulation is based on 
    SimLoRD  by Stöcker et al. (2016) <doi:10.1093/bioinformatics/btw286>.
    All outputs can be written to standard file formats.",2021-07-06,Lucas A. Nell [cph,https://github.com/lucasnell/jackalope,TRUE,https://github.com/lucasnell/jackalope,13868,7,2021-07-05T22:56:19Z,1981.142857142857
jackstraw,"Test for association between the observed data
	and their systematic patterns of variations.
	The jackstraw enables statistical testing for association between observed variables and latent variables, as captured by principal component analysis (PCA), factor analysis (FA), or other estimates. Similarly, unsupervised clustering, such as K-means clustering, partition around medoids (PAM), and others, finds subpopulations among the observed variables. The jackstraw estimates statistical significance of cluster membership, including unsupervised evaluation of cell identities in single cell RNA-seq. P-values and posterior probabilities allows one to rigorously evaluate the strength of cluster membership assignments. See the GitHub repository for the latest developments and further helps.",2021-06-11,Neo Christopher Chung,https://github.com/ncchung/jackstraw,TRUE,https://github.com/ncchung/jackstraw,24523,10,2021-08-05T13:54:26Z,2452.3
jagsUI,"A set of wrappers around 'rjags' functions to run Bayesian analyses in 'JAGS' (specifically, via 'libjags').  A single function call can control adaptive, burn-in, and sampling MCMC phases, with MCMC chains run in sequence or in parallel. Posterior distributions are automatically summarized (with the ability to exclude some monitored nodes if desired) and functions are available to generate figures based on the posteriors (e.g., predictive check plots, traceplots). Function inputs, argument syntax, and output format are nearly identical to the 'R2WinBUGS'/'R2OpenBUGS' packages to allow easy switching between MCMC samplers.",2021-06-18,Ken Kellner,https://github.com/kenkellner/jagsUI,TRUE,https://github.com/kenkellner/jagsui,99195,23,2021-06-21T21:22:35Z,4312.826086956522
janitor,"The main janitor functions can: perfectly format data.frame column
    names; provide quick counts of variable combinations (i.e., frequency
    tables and crosstabs); and isolate duplicate records. Other janitor functions
    nicely format the tabulation results. These tabulate-and-report functions
    approximate popular features of SPSS and Microsoft Excel. This package
    follows the principles of the ""tidyverse"" and works well with the pipe function
    %>%. janitor was built with beginning-to-intermediate R users in mind and is
    optimized for user-friendliness. Advanced R users can already do everything
    covered here, but with janitor they can do it faster and save their thinking for
    the fun stuff.",2021-01-05,Sam Firke,https://github.com/sfirke/janitor,TRUE,https://github.com/sfirke/janitor,1428247,1044,2021-09-01T19:04:43Z,1368.052681992337
jaod,"Client for the Directory of Open Access Journals ('DOAJ')
    (<https://doaj.org/>). API documentation at
    <https://doaj.org/api/v1/docs>. Methods included for working with
    all 'DOAJ' API routes: fetch article information by identifier,
    search for articles, fetch journal information by identifier,
    and search for journals.",2020-12-02,Scott Chamberlain,"https://docs.ropensci.org/jaod/, https://github.com/ropensci/jaod",TRUE,https://github.com/ropensci/jaod,17194,10,2020-12-02T17:26:33Z,1719.4
JBrowseR,"Provides an R interface to the JBrowse 2 genome browser.
    Enables embedding a JB2 genome browser in a Shiny app or R Markdown
    document. The browser can also be launched from an interactive R console.
    The browser can be loaded with a variety of common genomics data types,
    and can be used with a custom theme.",2021-07-07,Elliot Hershberg,https://gmod.github.io/JBrowseR/ https://github.com/GMOD/JBrowseR,TRUE,https://github.com/gmod/jbrowser,3677,15,2021-07-08T00:10:23Z,245.13333333333333
jetpack,"Manage project dependencies from your DESCRIPTION file. Create a reproducible virtual environment with minimal additional files in your project. Provides tools to add, remove, and update dependencies as well as install existing dependencies with a single function.",2021-04-10,Andrew Kane,https://github.com/ankane/jetpack,TRUE,https://github.com/ankane/jetpack,19183,212,2021-04-10T22:22:07Z,90.48584905660377
jfa,"Implements the audit sampling workflow as discussed in Derks et al. (2019) <doi:10.31234/osf.io/9f6ub>. The package makes it easy for an auditor to plan a statistical sample, select the sample from the population, and evaluate the misstatement in the sample using various methods compliant with the International Standards on Auditing. Furthermore, the package implements Bayesian equivalents of these methods. ",2021-08-12,Koen Derks,"https://koenderks.github.io/jfa/, https://github.com/koenderks/jfa",TRUE,https://github.com/koenderks/jfa,11698,3,2021-09-03T13:47:14Z,3899.3333333333335
jipApprox,"Approximate joint-inclusion probabilities in Unequal Probability Sampling, or compute Monte Carlo approximations of the first and second-order inclusion probabilities of a general sampling design as in Fattorini (2006) <doi:10.1093/biomet/93.2.269>.",2020-10-13,Roberto Sichera,NA,TRUE,https://github.com/rhobis/jipapprox,13580,0,2021-05-23T18:10:39Z,NA
JirAgileR,Allows to interact with the 'JIRA SERVER REST API' to analyze the retrieved data in R. For further information about the API visit <https://docs.atlassian.com/software/jira/docs/api/REST/8.9.1/>.,2021-06-08,Matthias Brenninkmeijer,"https://matbmeijer.github.io/JirAgileR/,
https://github.com/matbmeijer/JirAgileR",TRUE,https://github.com/matbmeijer/jiragiler,1140,18,2021-06-08T19:12:22Z,63.333333333333336
jjb,"Set of common functions used for manipulating colors,
    detecting and interacting with 'RStudio', modeling, formatting, determining
    users' operating system, feature scaling, and more!",2020-01-08,James Balamuta,https://github.com/coatless/jjb,TRUE,https://github.com/coatless/jjb,18861,1,2021-04-09T01:00:53Z,18861
JMbayes,Shared parameter models for the joint modeling of longitudinal and time-to-event data using MCMC; Dimitris Rizopoulos (2016) <doi:10.18637/jss.v072.i07>. ,2020-01-09,Dimitris Rizopoulos,https://github.com/drizopoulos/JMbayes,TRUE,https://github.com/drizopoulos/jmbayes,35666,34,2021-01-28T19:44:29Z,1049
JMbayes2,"Fit joint models for longitudinal and time-to-event data under the Bayesian approach. Multiple longitudinal outcomes of mixed type (continuous/categorical) and multiple event times (competing risks and multi-state processes) are accommodated. Rizopoulos (2012, ISBN:9781439872864).",2021-09-02,Dimitris Rizopoulos,"https://drizopoulos.github.io/JMbayes2/,
https://github.com/drizopoulos/JMbayes2",TRUE,https://github.com/drizopoulos/jmbayes2,4289,17,2021-09-03T12:03:18Z,252.2941176470588
jmcm,"Fit joint mean-covariance models for longitudinal data. The models
    and their components are represented using S4 classes and methods. The core
    computational algorithms are implemented using the 'Armadillo' C++ library
    for numerical linear algebra and 'RcppArmadillo' glue.",2021-01-12,Jianxin Pan,https://github.com/ypan1988/jmcm/,TRUE,https://github.com/ypan1988/jmcm,22073,3,2021-08-24T20:39:42Z,7357.666666666667
JMcmprsk,"Fit joint models of continuous or ordinal longitudinal data and time-to-event data with competing risks. For a detailed information, see Robert Elashoff, Gang Li and Ning Li (2016, ISBN:9781439807828); Robert M. Elashoff,Gang Li and Ning Li (2008) <doi:10.1111/j.1541-0420.2007.00952.x> ; Ning Li, Robert Elashoff, Gang Li and Jeffrey Saver (2010) <doi:10.1002/sim.3798> .",2021-03-22,Hong Wang,https://github.com/whcsu/JMcmprsk,TRUE,https://github.com/whcsu/jmcmprsk,15603,0,2020-11-22T01:56:17Z,NA
jmv,"A suite of common statistical methods such as descriptives,
    t-tests, ANOVAs, regression, correlation matrices, proportion tests,
    contingency tables, and factor analysis. This package is also useable from
    the 'jamovi' statistical spreadsheet (see <https://www.jamovi.org> for more
    information).",2021-07-17,Ravi Selker,NA,TRUE,https://github.com/jamovi/jmv,110554,31,2021-08-17T09:45:56Z,3566.2580645161293
jmvcore,"A framework for creating rich interactive analyses for the jamovi
    platform (see <https://www.jamovi.org> for more information).",2021-07-07,Jonathon Love,https://www.jamovi.org,TRUE,https://github.com/jamovi/jmvcore,137722,3,2021-06-08T08:04:58Z,45907.333333333336
jmvReadWrite,"The free and open a statistical spreadsheet 'jamovi' (www.jamovi.org)
    aims to make statistical analyses easy and intuitive. 'jamovi' produces
    syntax that can directly be used in R (in connection with the R-package 'jmv').
    Having import / export routines for the data files 'jamovi' produces ('.omv')
    permits an easy transfer of analyses between 'jamovi' and R.",2021-08-23,Sebastian Jentschke,NA,TRUE,https://github.com/sjentsch/jmvreadwrite,4327,1,2021-08-23T12:57:37Z,4327
joineR,"Analysis of repeated measurements and time-to-event data via random
    effects joint models. Fits the joint models proposed by Henderson and colleagues
    <doi:10.1093/biostatistics/1.4.465> (single event time) and by Williamson and
    colleagues (2008) <doi:10.1002/sim.3451> (competing risks events time) to a
    single continuous repeated measure. The time-to-event data is modelled using a 
    (cause-specific) Cox proportional hazards regression model with time-varying 
    covariates. The longitudinal outcome is modelled using a linear mixed effects
    model. The association is captured by a latent Gaussian process. The model is 
    estimated using am Expectation Maximization algorithm. Some plotting functions 
    and the variogram are also included. This project is funded by the Medical 
    Research Council (Grant numbers G0400615 and MR/M013227/1).",2021-06-01,Graeme L. Hickey,https://github.com/graemeleehickey/joineR/,TRUE,https://github.com/graemeleehickey/joiner,47898,13,2021-06-01T11:02:57Z,3684.4615384615386
joineRML,"Fits the joint model proposed by Henderson and colleagues (2000) 
    <doi:10.1093/biostatistics/1.4.465>, but extended to the case of multiple 
    continuous longitudinal measures. The time-to-event data is modelled using a 
    Cox proportional hazards regression model with time-varying covariates. The 
    multiple longitudinal outcomes are modelled using a multivariate version of the 
    Laird and Ware linear mixed model. The association is captured by a multivariate
    latent Gaussian process. The model is estimated using a Monte Carlo Expectation 
    Maximization algorithm. This project was funded by the Medical Research Council 
    (Grant number MR/M013227/1).",2021-01-05,Graeme L. Hickey,https://github.com/graemeleehickey/joineRML,TRUE,https://github.com/graemeleehickey/joinerml,71910,19,2021-08-24T16:59:51Z,3784.7368421052633
joinet,"Implements high-dimensional multivariate regression by stacked generalisation (Rauschenberger 2021 <doi:10.1093/bioinformatics/btab576>). For positively correlated outcomes, a single multivariate regression is typically more predictive than multiple univariate regressions. Includes functions for model fitting, extracting coefficients, outcome prediction, and performance measurement. If required, install MRCE or remMap from GitHub (<https://github.com/cran/MRCE>, <https://github.com/cran/remMap>).",2021-08-09,Armin Rauschenberger,https://github.com/rauschenberger/joinet,TRUE,https://github.com/rauschenberger/joinet,12331,3,2021-08-09T06:04:49Z,4110.333333333333
JointAI,"Joint analysis and imputation of incomplete data in the Bayesian
    framework, using (generalized) linear (mixed) models and extensions there of,
    survival models, or joint models for longitudinal and survival data.
    Incomplete covariates, if present, are automatically imputed.
    The package performs some preprocessing of the data and creates a 'JAGS'
    model, which will then automatically be passed to 'JAGS' 
    <http://mcmc-jags.sourceforge.net/> with the help of 
    the package 'rjags'.",2021-01-13,Nicole S. Erler,https://nerler.github.io/JointAI/,TRUE,https://github.com/nerler/jointai,22692,11,2021-08-02T07:27:18Z,2062.909090909091
jointDiag,"Different algorithms to perform approximate joint diagonalization
	of a finite set of square matrices. Depending on the algorithm,
	orthogonal or non-orthogonal diagonalizer is found. These algorithms
	are particularly useful in the context of blind source separation. 
	Original publications of the algorithms can be found in 
	Ziehe et al. (2004), Pham and Cardoso (2001) <doi:10.1109/78.942614>, 
	Souloumiac (2009) <doi:10.1109/TSP.2009.2016997>, 
	Vollgraff and Obermayer <doi:10.1109/TSP.2006.877673>. An example of 
	application in the context of Brain-Computer Interfaces EEG denoising
	can be found in Gouy-Pailler et al (2010) <doi:10.1109/TBME.2009.2032162>.",2020-10-27,Cedric Gouy-Pailler,https://github.com/gouypailler/jointDiag,TRUE,https://github.com/gouypailler/jointdiag,24249,1,2020-10-27T12:29:25Z,24249
jordan,"A Jordan algebra is an algebraic object originally
   designed to study observables in quantum mechanics.  Jordan
   algebras are commutative but non-associative; they satisfy the
   Jordan identity.  The package follows the ideas and notation of
   K. McCrimmon (2004, ISBN:0-387-95447-3) ""A Taste of Jordan
   Algebras"".",2021-04-08,Robin K. S. Hankin,https://github.com/RobinHankin/jordan,TRUE,https://github.com/robinhankin/jordan,1567,0,2021-07-21T09:54:04Z,NA
journalabbr,"Since the reference management software (such as 'Zotero', 'Mendeley') exports Bib file journal abbreviation is not detailed enough, the 'journalabbr' package only abbreviates the journal field of Bib file, and then outputs a new Bib file for generating reference format with journal abbreviation on other software (such as 'texstudio'). The abbreviation table is from 'JabRef'. At the same time, 'Shiny' application is provided to generate 'thebibliography', a reference format that can be directly used for latex paper writing based on 'Rmd' files.",2021-05-01,ShuCai Zou,https://github.com/zoushucai/journalabbr,TRUE,https://github.com/zoushucai/journalabbr,4460,0,2021-05-13T02:13:43Z,NA
joyn,"Tool for diagnosing table joins. It combines the speed
    `data.table`, the flexibility of `dplyr`, and the diagnosis and features of
    the `merge` command in `Stata`.",2021-04-28,R.Andres Castaneda,https://github.com/randrescastaneda/joyn,TRUE,https://github.com/randrescastaneda/joyn,1887,1,2021-04-29T01:07:35Z,1887
jpmesh,"Helpful functions for using mesh code (80km to 100m) data in Japan. Visualize mesh code using 'ggplot2' and 'leaflet', etc.",2021-06-25,Shinya Uryu,https://uribo.github.io/jpmesh/,TRUE,https://github.com/uribo/jpmesh,32437,31,2021-06-26T04:34:10Z,1046.3548387096773
jqr,"Client for 'jq', a 'JSON' processor (<https://stedolan.github.io/jq/>), 
    written in C. 'jq' allows the following with 'JSON' data: index into, parse, 
    do calculations, cut up and filter, change key names and values, perform 
    conditionals and comparisons, and more.",2021-05-06,Scott Chamberlain,"https://docs.ropensci.org/jqr/ (docs),
https://github.com/ropensci/jqr (devel)",TRUE,https://github.com/ropensci/jqr,380629,124,2021-05-06T14:07:29Z,3069.5887096774195
jrc,An 'httpuv' based bridge between R and 'JavaScript'. Provides an easy way to exchange commands and data between a web page and a currently running R session. ,2020-09-25,Svetlana Ovchinnikova,https://github.com/anders-biostat/jrc,TRUE,https://github.com/anders-biostat/jrc,15982,12,2021-06-25T13:07:59Z,1331.8333333333333
jrvFinance,"Implements the basic financial analysis
    functions similar to (but not identical to) what
    is available in most spreadsheet software. This
    includes finding the IRR and NPV of regularly
    spaced cash flows and annuities. Bond pricing and
    YTM calculations are included. In addition, Black
    Scholes option pricing and Greeks are also
    provided.",2021-04-18,Jayanth Varma,https://github.com/jrvarma/jrvFinance,TRUE,https://github.com/jrvarma/jrvfinance,29488,6,2021-04-19T06:11:46Z,4914.666666666667
JSconsole,Provides a 'RStudio' addin to send some 'JavaScript' code to the 'V8' console. The user can send an entire 'JavaScript' file or only some selected lines. This is useful to test the code.,2020-10-15,Stéphane Laurent,https://github.com/stla/JSconsole,TRUE,https://github.com/stla/jsconsole,3638,0,2020-10-08T06:46:02Z,NA
jSDM,"Fits joint species distribution models ('jSDM')
    in a hierarchical Bayesian framework (Warton et al. 2015
    <doi:10.1016/j.tree.2015.09.007>). The Gibbs sampler is written
    in C++. It uses 'Rcpp', 'Armadillo' and 'GSL' to maximize computation
    efficiency.",2019-07-02,Ghislain Vieilledent,"https://ecology.ghislainv.fr/jSDM,
https://github.com/ghislainv/jSDM",TRUE,https://github.com/ghislainv/jsdm,10491,6,2021-08-27T11:41:50Z,1748.5
jskm,The function 'jskm()' creates publication quality Kaplan-Meier plot with at risk tables below. 'svyjskm()' provides plot for weighted Kaplan-Meier estimator. ,2020-11-24,Jinseob Kim,https://github.com/jinseob2kim/jskm,TRUE,https://github.com/jinseob2kim/jskm,19104,5,2021-08-15T13:28:30Z,3820.8
JSmediation,"A set of helper functions to conduct joint-significance tests
    for mediation analysis, as recommended by Yzerbyt, Muller, Batailler,
    & Judd. (2018) <doi:10.1037/pspa0000132>.",2021-09-02,Cédric Batailler,"https://jsmediation.cedricbatailler.me/,
https://github.com/cedricbatailler/JSmediation",TRUE,https://github.com/cedricbatailler/jsmediation,12016,6,2021-09-03T09:50:06Z,2002.6666666666667
jsmodule,"'RStudio' addins and 'Shiny' modules for descriptive statistics, regression and survival analysis.",2021-08-09,Jinseob Kim,https://github.com/jinseob2kim/jsmodule,TRUE,https://github.com/jinseob2kim/jsmodule,21683,12,2021-08-15T13:27:27Z,1806.9166666666667
jsonlite,"A reasonably fast JSON parser and generator, optimized for statistical 
    data and the web. Offers simple, flexible tools for working with JSON in R, and
    is particularly powerful for building pipelines and interacting with a web API. 
    The implementation is based on the mapping described in the vignette (Ooms, 2014).
    In addition to converting JSON data from/to R objects, 'jsonlite' contains 
    functions to stream, validate, and prettify JSON data. The unit tests included 
    with the package verify that all edge cases are encoded and decoded consistently 
    for use with dynamic data in systems and applications.",2020-12-09,Jeroen Ooms,https://arxiv.org/abs/1403.2805 (paper),TRUE,https://github.com/jeroen/jsonlite,37840444,296,2021-08-21T22:35:25Z,127839.33783783784
jsonStrings,"Fast manipulation of JSON strings. Allows to extract or delete an element in a JSON string, merge two JSON strings, and more.",2021-05-25,Stéphane Laurent,https://github.com/stla/jsonStrings,TRUE,https://github.com/stla/jsonstrings,1140,2,2021-05-24T08:14:00Z,570
jsonvalidate,"Uses the node library 'is-my-json-valid' or 'ajv' to
    validate 'JSON' against a 'JSON' schema.  Drafts 04, 06 and 07 of
    'JSON' schema are supported.",2019-06-25,Rich FitzJohn,https://github.com/ropensci/jsonvalidate,TRUE,https://github.com/ropensci/jsonvalidate,182125,39,2021-07-07T12:11:48Z,4669.871794871795
jstable,"Create regression tables from generalized linear model(GLM), generalized estimating equation(GEE), generalized linear mixed-effects model(GLMM), Cox proportional hazards model, survey-weighted generalized linear model(svyglm) and survey-weighted Cox model results for publication.",2021-07-25,Jinseob Kim,https://github.com/jinseob2kim/jstable,TRUE,https://github.com/jinseob2kim/jstable,25956,3,2021-08-15T13:27:53Z,8652
jsTree,"Create and customize interactive trees using the
       'jQuery' 'jsTree' <https://www.jstree.com/> plugin
       library and the 'htmlwidgets' package. These trees can
       be used directly from the R console, from 'RStudio', in
       Shiny apps and R Markdown documents.",2020-12-13,Jonathan Sidi,https://github.com/yonicd/jsTree,TRUE,https://github.com/yonicd/jstree,17075,25,2020-12-12T17:58:15Z,683
jsTreeR,"Creates interactive trees that can be included in 'Shiny' apps and R markdown documents. A tree allows to represent hierarchical data (e.g. the contents of a directory). Similar to the 'shinyTree' package but offers more features and options, such as the grid extension, restricting the drag-and-drop behavior, and settings for the search functionality. It is possible to attach some data to the nodes of a tree and then to get these data in 'Shiny' when a node is selected. Also provides a 'Shiny' gadget allowing to manipulate one or more folders.",2021-08-14,Stéphane Laurent,https://github.com/stla/jsTreeR,TRUE,https://github.com/stla/jstreer,5575,11,2021-08-14T11:17:01Z,506.8181818181818
jti,"Minimal and memory efficient implementation of the junction tree
	     algorithm using the Lauritzen-Spiegelhalter scheme;
	     S. L. Lauritzen and D. J. Spiegelhalter (1988) 
	     <https://www.jstor.org/stable/2345762?seq=1>.",2021-07-05,Mads Lindskou,https://github.com/mlindsk/jti,TRUE,https://github.com/mlindsk/jti,4963,1,2021-07-13T11:25:54Z,4963
jtools,"This is a collection of tools that the author (Jacob) has written
  for the purpose of more efficiently understanding and sharing the results of
  (primarily) regression analyses. There are also a number of miscellaneous
  functions for statistical and programming purposes. Just about everything 
  supports models from the survey package.",2021-09-03,Jacob A. Long,https://jtools.jacob-long.com,TRUE,https://github.com/jacob-long/jtools,374505,125,2021-09-02T20:07:08Z,2996.04
JuliaCall,"Provides an R interface to 'Julia',
    which is a high-level, high-performance dynamic programming language
    for numerical computing, see <https://julialang.org/> for more information.
    It provides a high-level interface as well as a low-level interface.
    Using the high level interface, you could call any 'Julia' function just like
    any R function with automatic type conversion. Using the low level interface,
    you could deal with C-level SEXP directly while enjoying the convenience of
    using a high-level programming language like 'Julia'.",2021-05-16,Changcheng Li,https://github.com/Non-Contradiction/JuliaCall,TRUE,https://github.com/non-contradiction/juliacall,193583,186,2021-07-14T03:57:49Z,1040.768817204301
junctions,"Individual based simulations of hybridizing populations,
    where the accumulation of junctions is tracked. Furthermore,
    mathematical equations are provided to verify simulation outcomes.
    Both simulations and mathematical equations are based on Janzen
    (2018, <doi:10.1101/058107>) and Janzen (2020,
    <doi:10.1101/2020.09.10.292441>).",2021-05-26,Thijs Janzen,https//github.com/thijsjanzen/junctions,TRUE,https://github.com/thijsjanzen/junctions,14772,0,2021-05-27T08:46:29Z,NA
jvcoords,"
  Provides functions to standardize and whiten data, and to perform
  Principal Component Analysis (PCA).  The main advantage of this
  package over alternatives like prcomp() is, that jvcoords makes it
  easy to convert (additional) data between the original and the
  transformed coordinates.  The package also provides a class coords,
  which can represent affine coordinate transformations.  This class
  forms the basis of the transformations provided by the package, but
  can also be used independently.  The implementation has been
  optimized to be of comparable speed (and sometimes even faster) than
  existing alternatives.",2021-06-05,Jochen Voss,https://github.com/seehuhn/jvcoords,TRUE,https://github.com/seehuhn/jvcoords,16057,0,2021-06-05T16:43:02Z,NA
JWileymisc,"Miscellaneous tools and functions,
    including: generate descriptive statistics tables,
    format output, visualize relations among variables or check
    distributions, and generic functions for residual and
    model diagnostics. ",2020-08-31,Joshua F. Wiley,"http://joshuawiley.com/JWileymisc,
https://github.com/JWiley/JWileymisc",TRUE,https://github.com/jwiley/jwileymisc,25336,3,2021-09-01T04:16:46Z,8445.333333333334
kableExtra,"Build complex HTML or 'LaTeX' tables using 'kable()' from 'knitr' 
    and the piping syntax from 'magrittr'. Function 'kable()' is a light weight 
    table generator coming from 'knitr'. This package simplifies the way to 
    manipulate the HTML or 'LaTeX' codes generated by 'kable()' and allows 
    users to construct complex tables and customize styles using a readable 
    syntax. ",2021-02-20,Hao Zhu,"http://haozhu233.github.io/kableExtra/,
https://github.com/haozhu233/kableExtra",TRUE,https://github.com/haozhu233/kableextra,2030582,503,2021-05-25T19:51:26Z,4036.942345924453
kantorovich,Computes the Kantorovich distance between two probability measures on a finite set. The Kantorovich distance is also known as the Monge-Kantorovich distance or the first Wasserstein distance.,2020-10-28,Stéphane Laurent,https://github.com/stla/kantorovich,TRUE,https://github.com/stla/kantorovich,13053,0,2020-10-28T16:00:34Z,NA
karel,"This is the R implementation of Karel the robot, a programming 
  language created by Dr. R. E. Pattis at Stanford University in 1981. Karel is 
  an useful tool to teach introductory concepts about general programming, such 
  as algorithmic decomposition, conditional statements, loops, etc., in an 
  interactive and fun way, by writing programs to make Karel the robot achieve 
  certaing tasks in the world she lives in. Originally based on Pascal, Karel 
  was implemented in many languages through these decades, including 'Java', 'C++', 
  'Ruby' and 'Python'. This is the first package implementing Karel in R.",2021-08-06,Marcos Prunello,https://mpru.github.io/karel/,TRUE,https://github.com/mpru/karel,370,2,2021-08-08T16:18:54Z,185
kayadata,"Provides data for Kaya identity variables (population, gross 
             domestic product, primary energy consumption, and energy-related 
             CO2 emissions) for the world and for individual nations, and 
             utility functions for looking up data,  plotting trends of 
             Kaya variables, and plotting the fuel mix for a given country
             or region. The Kaya identity (Yoichi Kaya and Keiichi Yokobori, 
             ""Environment, Energy, and Economy: Strategies for Sustainability"" 
             (United Nations University Press, 1998) and 
             <https://en.wikipedia.org/wiki/Kaya_identity>) expresses a nation's 
             or region's greenhouse gas emissions in terms of its population, 
             per-capita Gross Domestic Product, the energy intensity of its 
             economy, and the carbon-intensity of its energy supply.",2021-03-28,Jonathan Gilligan,"https://jonathan-g.github.io/kayadata/,
https://github.com/jonathan-g/kayadata",TRUE,https://github.com/jonathan-g/kayadata,12850,0,2021-03-24T13:43:40Z,NA
kcopula,"Provides the density and distribution function of the bivariate 
    K-copula by Wollschläger and Schäfer (2016) <doi:10.21314/JOR.2016.342>.",2020-04-07,Marcel Kremer,https://github.com/mlkremer/kcopula,TRUE,https://github.com/mlkremer/kcopula,5771,1,2021-04-30T13:22:17Z,5771
kde1d,"Provides an efficient implementation of univariate local polynomial
    kernel density estimators that can handle bounded and discrete data. See 
    Geenens (2014) <arXiv:1303.4121>, 
    Geenens and Wang (2018) <arXiv:1602.04862>, 
    Nagler (2018a) <arXiv:1704.07457>, 
    Nagler (2018b) <arXiv:1705.05431>.",2020-10-26,Thomas Nagler,https://github.com/tnagler/kde1d,TRUE,https://github.com/tnagler/kde1d,22687,8,2020-10-26T21:18:07Z,2835.875
kdensity,"Handles univariate non-parametric density estimation with 
    parametric starts and asymmetric kernels in a simple and flexible way. 
    Kernel density estimation with parametric starts involves fitting a
    parametric density to the data before making a correction with kernel 
    density estimation, see Hjort & Glad (1995) <doi:10.1214/aos/1176324627>.
    Asymmetric kernels make kernel density estimation more efficient on bounded
    intervals such as (0, 1) and the positive half-line. Supported asymmetric 
    kernels are the gamma kernel of Chen (2000) <doi:10.1023/A:1004165218295>,
    the beta kernel of Chen (1999) <doi:10.1016/S0167-9473(99)00010-9>, and the
    copula kernel of Jones & Henderson (2007) <doi:10.1093/biomet/asm068>.
    User-supplied kernels, parametric starts, and bandwidths are supported.",2020-09-30,Jonas Moss,https://github.com/JonasMoss/kdensity,TRUE,https://github.com/jonasmoss/kdensity,35814,11,2020-09-30T11:55:33Z,3255.818181818182
kdevine,"Implements the vine copula based kernel density estimator of
    Nagler and Czado (2016) <doi:10.1016/j.jmva.2016.07.003>. The estimator does
    not suffer from the curse of dimensionality and is therefore well suited for
    high-dimensional applications.",2021-05-12,Thomas Nagler,https://github.com/tnagler/kdevine,TRUE,https://github.com/tnagler/kdevine,17874,10,2021-05-11T11:40:22Z,1787.4
kdtools,"Provides various tools for working with multidimensional
  data in R and C++, including extremely fast nearest-neighbor- and range-
  queries without the overhead of linked tree nodes.",2021-08-25,Timothy Keitt,https://github.com/thk686/kdtools,TRUE,https://github.com/thk686/kdtools,12865,7,2021-08-27T03:16:29Z,1837.857142857143
kelvin,"Uses Bessel functions to calculate the
    fundamental and complementary analytic solutions to the
    Kelvin differential equation.",2020-06-18,Andrew J Barbour,https://github.com/abarbour/kelvin,TRUE,https://github.com/abarbour/kelvin,18875,0,2020-09-13T21:21:19Z,NA
keras,"Interface to 'Keras' <https://keras.io>, a high-level neural
  networks 'API'. 'Keras' was developed with a focus on enabling fast experimentation,
  supports both convolution based networks and recurrent networks (as well as
  combinations of the two), and runs seamlessly on both 'CPU' and 'GPU' devices.",2021-08-21,Tomasz Kalinowski [ctb,https://keras.rstudio.com,TRUE,https://github.com/rstudio/keras,925088,714,2021-08-31T20:21:25Z,1295.641456582633
kerastuneR,"'Keras Tuner' <https://keras-team.github.io/keras-tuner/> is a hypertuning framework made for humans. 
             It aims at making the life of AI practitioners, hypertuner 
             algorithm creators and model designers as simple as possible by 
             providing them with a clean and easy to use API for hypertuning. 
             'Keras Tuner' makes moving from a base model to a hypertuned one quick and 
             easy by only requiring you to change a few lines of code.",2020-10-04,Turgut Abdullayev,https://github.com/henry090/kerastuneR,TRUE,https://github.com/henry090/kerastuner,9803,28,2020-10-04T12:42:32Z,350.10714285714283
KernelKnn,Extends the simple k-nearest neighbors algorithm by incorporating numerous kernel functions and a variety of distance metrics. The package takes advantage of 'RcppArmadillo' to speed up the calculation of distances between observations.,2021-05-04,Lampros Mouselimis,https://github.com/mlampros/KernelKnn,TRUE,https://github.com/mlampros/kernelknn,79247,12,2021-08-18T05:30:55Z,6603.916666666667
keyATM,"Fits keyword assisted topic models (keyATM) using collapsed Gibbs samplers. The keyATM combines the latent dirichlet allocation (LDA) models with a small number of keywords selected by researchers in order to improve the interpretability and topic classification of the LDA. The keyATM can also incorporate covariates and directly model time trends. The keyATM is proposed in Eshima, Imai, and Sasaki (2020) <arXiv:2004.05964>.",2021-02-14,Shusei Eshima,https://keyatm.github.io/keyATM/,TRUE,https://github.com/keyatm/keyatm,8811,44,2021-07-06T01:33:34Z,200.25
KeyboardSimulator,Control your keyboard and mouse with R code by simulating key presses and mouse clicks. The input simulation is implemented with the Windows API.,2021-01-13,Jim Chen,https://github.com/ChiHangChen/KeyboardSimulator,TRUE,https://github.com/chihangchen/keyboardsimulator,21928,16,2021-01-13T12:48:23Z,1370.5
keys,"Assign and listen to keyboard shortcuts in 'shiny' using the 
  'Mousetrap' Javascript library.",2021-07-11,Tyler Littlefield,https://github.com/r4fun/keys,TRUE,https://github.com/r4fun/keys,4347,23,2021-07-11T18:13:16Z,189
keyToEnglish,"Convert keys and other values to memorable phrases. 
  Includes some methods to build lists of words.",2021-02-13,Max Candocia,https://github.com/mcandocia/keyToEnglish,TRUE,https://github.com/mcandocia/keytoenglish,4481,0,2021-01-24T21:25:49Z,NA
KFAS,"State space modelling is an efficient and flexible framework for 
    statistical inference of a broad class of time series and other data. KFAS 
    includes computationally efficient functions for Kalman filtering, smoothing, 
    forecasting, and simulation of multivariate exponential family state space models, 
    with observations from Gaussian, Poisson, binomial, negative binomial, and gamma 
    distributions. See the paper by Helske (2017) <doi:10.18637/jss.v078.i10> for details.",2021-06-07,Jouni Helske,https://github.com/helske/KFAS,TRUE,https://github.com/helske/kfas,434054,47,2021-06-07T10:05:42Z,9235.191489361701
kfigr,"A streamlined cross-referencing system for R Markdown documents
    generated with 'knitr'. R Markdown is  an authoring format for generating
    dynamic content from R. 'kfigr' provides a hook for anchoring code
    chunks and a function to cross-reference document elements generated from
    said chunks, e.g. figures and tables.",2021-06-10,Michael C Koohafkan,https://github.com/mkoohafkan/kfigr,TRUE,https://github.com/mkoohafkan/kfigr,21687,25,2021-06-10T03:20:56Z,867.48
kgschart,"Restore underlining numeric data from rating history graph of 
    KGS (an online platform of the game of go, <http://www.gokgs.com/>). 
    A shiny application is also provided.",2017-07-02,Kota Mori,https://github.com/kota7/kgschart,TRUE,https://github.com/kota7/kgschart,15119,1,2020-10-27T22:50:14Z,15119
KHQ,"The King's Health Questionnaire (KHQ) is a disease-specific, 
  self-administered questionnaire designed specific to assess the impact of 
  Urinary Incontinence (UI) on Quality of Life. The questionnaire was developed 
  by Kelleher and collaborators (1997) <doi:10.1111/j.1471-0528.1997.tb11006.x>. 
  It is a simple, acceptable and reliable measure to use in the clinical setting 
  and a research tool that is useful in evaluating UI treatment outcomes. 
  The KHQ five dimensions (KHQ5D) is a condition-specific preference-based 
  measure developed by Brazier and collaborators (2008) <doi:10.1177/0272989X07301820>. 
  Although not as popular as the SF6D <doi:10.1016/S0895-4356(98)00103-6> and 
  EQ-5D <https://euroqol.org/>, the KHQ5D measures health-related quality of 
  life (HRQoL) specifically for UI, not general conditions like the others 
  two instruments mentioned. The KHQ5D ca be used in the clinical and economic 
  evaluation of health care. The subject self-rates their health in terms of 
  five dimensions: Role Limitation (RL), Physical Limitations (PL), Social 
  Limitations (SL), Emotions (E), and Sleep (S). Frequently the states on these 
  five dimensions are converted to a single utility index using country specific 
  value sets, which can be used in the clinical and economic evaluation of 
  health care as well as in population health surveys. This package provides 
  methods to calculate scores for each dimension of the KHQ; converts KHQ item 
  scores to KHQ5D scores; and also calculates the utility index of the KHQ5D.",2021-08-06,Luiz Augusto Brusaca,https://github.com/augustobrusaca/KHQ,TRUE,https://github.com/augustobrusaca/khq,677,0,2021-08-09T01:31:21Z,NA
khroma,"Colour schemes ready for each type of data (qualitative,
    diverging or sequential), with colours that are distinct for all
    people, including colour-blind readers. This package provides an
    implementation of Paul Tol (2018) and Fabio Crameri (2018) <doi:
    10.5194/gmd-11-2541-2018> colour schemes for use with 'graphics' or
    'ggplot2'. It provides tools to simulate colour-blindness and to test
    how well the colours of any palette are identifiable. Several
    scientific thematic schemes (geologic timescale, land cover, FAO
    soils, etc.) are also implemented.",2021-09-02,Nicolas Frerebeau,"https://packages.tesselle.org/khroma/,
https://github.com/tesselle/khroma",TRUE,https://github.com/tesselle/khroma,20226,44,2021-09-02T15:32:20Z,459.6818181818182
kibior,"An interface to store, retrieve, search, join and share datasets, based on Elasticsearch (ES) API. As a decentralized, FAIR and collaborative search engine and database effort, it proposes a simple push/pull/search mechanism only based on ES, a tool which can be deployed on nearly any hardware. It is a high-level R-ES binding to ease data usage using 'elastic' package (S. Chamberlain (2020)) <https://docs.ropensci.org/elastic/>, extends joins from 'dplyr' package (H. Wickham et al. (2020)) <https://dplyr.tidyverse.org/> and integrates specific biological format importation with Bioconductor packages such as 'rtracklayer' (M. Lawrence and al. (2009) <doi:10.1093/bioinformatics/btp328>) <http://bioconductor.org/packages/rtracklayer>, 'Biostrings' (H. Pagès and al. (2020) <doi:10.18129/B9.bioc.Biostrings>) <http://bioconductor.org/packages/Biostrings>, and 'Rsamtools' (M. Morgan and al. (2020) <doi:10.18129/B9.bioc.Rsamtools>) <http://bioconductor.org/packages/Rsamtools>, but also a long list of more common ones with 'rio' (C-h. Chan and al. (2018)) <https://cran.r-project.org/package=rio>.",2021-01-28,Régis Ongaro-Carcy,https://github.com/regisoc/kibior,TRUE,https://github.com/regisoc/kibior,2561,1,2021-08-10T14:47:30Z,2561
kim,"Miscellaneous functions to simplify and expedite
    analyses of experimental data. Examples include a function
    that plots sample means of groups in a factorial experimental design,
    a function that conducts robust regressions with bootstrapped samples,
    and a function that conducts robust two-way analysis of variance.
    Many of the functions will require installing package(s) listed 
    in the Selected References.
    Selected References:
    Canty & Ripley (2021) <https://CRAN.R-project.org/package=boot>.
    Cohen (1988) <doi:10.4324/9780203771587>.
    DeCarlo (1997) <doi:10.1037/1082-989X.2.3.292>.
    Dinno (2018) <https://CRAN.R-project.org/package=paran>.
    Doane & Seward (2011) <doi:10.1080/10691898.2011.11889611>.
    Dowle et al. (2021) <https://CRAN.R-project.org/package=data.table>.
    Edwards et al. (2020) <https://CRAN.R-project.org/package=lemon>.
    Fox et al. (2020) <https://CRAN.R-project.org/package=car>.
    Hester et al. (2020) <https://CRAN.R-project.org/package=remotes>.
    Ioannidis (2005) <doi:10.1371/journal.pmed.0020124>
    Kim (2021) <doi:10.5281/zenodo.4445388>.
    Kim (2020) <https://CRAN.R-project.org/package=ezr>.
    Long (2020) <https://CRAN.R-project.org/package=interactions>.
    Mair & Wilcox (2021) <https://CRAN.R-project.org/package=WRS2>.
    Pasek et al. (2020) <https://CRAN.R-project.org/package=weights>.
    Simmons et al. (2011) <doi:10.1177/0956797611417632>.
    Tingley et al. (2019) <https://CRAN.R-project.org/package=mediation>.
    Torchiano (2020) <https://CRAN.R-project.org/package=effsize>.
    Wickham et al. (2020) <https://CRAN.R-project.org/package=ggplot2>.
    Wilke (2021) <https://CRAN.R-project.org/package=ggridges>.",2021-04-24,Jin Kim,"https://github.com/jinkim3/kim, https://jinkim.science",TRUE,https://github.com/jinkim3/kim,4499,3,2021-08-21T07:11:35Z,1499.6666666666667
kindisperse,"Functions for simulating and estimating kinship-related dispersal. Based
    on the methods described in M. Jasper, T.L. Schmidt., N.W. Ahmad, S.P. Sinkins & A.A. 
    Hoffmann (2019) <doi:10.1111/1755-0998.13043> ""A genomic approach to inferring kinship 
    reveals limited intergenerational dispersal in the yellow fever mosquito"".
    Assumes an additive variance model of dispersal in two dimensions, compatible with 
    Wright's neighbourhood area. Simple and composite dispersal simulations are supplied, 
    as well as the functions needed to estimate parent-offspring dispersal for simulated or 
    empirical data, and to undertake sampling design for future field studies of dispersal. 
    For ease of use an integrated Shiny app is also included. ",2021-07-28,Moshe-Elijah Jasper,https://github.com/moshejasper/kindisperse,TRUE,https://github.com/moshejasper/kindisperse,2056,1,2021-07-28T08:35:48Z,2056
kit,"Basic functions, implemented in C, for large data manipulation. Fast vectorised ifelse()/nested if()/switch() functions, psum()/pprod() functions equivalent to pmin()/pmax() plus others which are missing from base R. Most of these functions are callable at C level.",2021-08-17,Morgan Jacob,NA,TRUE,https://github.com/2005m/kit,21359,20,2021-05-01T04:20:25Z,1067.95
kitagawa,"Provides tools to calculate the theoretical hydrodynamic response
    of an aquifer undergoing harmonic straining or pressurization, or analyze
    measured responses. There are
    two classes of models here, designed for use with confined
    aquifers: (1) for sealed wells, based on the model of 
    Kitagawa et al (2011, <doi:10.1029/2010JB007794>), 
    and (2) for open wells, based on the models of
    Cooper et al (1965, <doi:10.1029/JZ070i016p03915>), 
    Hsieh et al (1987, <doi:10.1029/WR023i010p01824>), 
    Rojstaczer (1988, <doi:10.1029/JB093iB11p13619>), 
    Liu et al (1989, <doi:10.1029/JB094iB07p09453>), and
    Wang et al (2018, <doi:10.1029/2018WR022793>). Wang's 
    solution is a special exception which
    allows for leakage out of the aquifer 
    (semi-confined); it is equivalent to Hsieh's model
    when there is no leakage (the confined case).
    These models treat strain (or aquifer head) as an input to the
    physical system, and fluid-pressure (or water height) as the output. The
    applicable frequency band of these models is characteristic of seismic
    waves, atmospheric pressure fluctuations, and solid earth tides.",2020-06-24,Andrew J. Barbour,https://github.com/abarbour/kitagawa,TRUE,https://github.com/abarbour/kitagawa,18986,3,2020-09-13T21:21:38Z,6328.666666666667
kknn,"Weighted k-Nearest Neighbors for Classification, Regression and Clustering.",2016-03-26,Klaus Schliep,https://github.com/KlausVigo/kknn,TRUE,https://github.com/klausvigo/kknn,313836,20,2021-05-31T12:42:01Z,15691.8
klassR,"Functions to search, retrieve and apply classifications 
  and codelists using Statistics Norway's API <https://www.ssb.no/klass> 
  from the system 'KLASS'. Retrieves classifications by date with options 
  to choose language, hierarchical level and formatting.",2021-03-30,Susie Jentoft,NA,TRUE,https://github.com/statisticsnorway/klassr,12614,2,2021-06-28T09:26:49Z,6307
klexdatr,"Six relational 'tibbles' from the Kootenay Lake Large Trout Exploitation study.
  The study which ran from 2008 to 2014 caught, tagged and released large Rainbow Trout and Bull Trout
  in Kootenay Lake by boat angling. 
  The fish were tagged with internal acoustic tags and/or high reward external tags
  and subsequently detected by an acoustic receiver array as well as reported by anglers.
  The data are analysed by Thorley and Andrusak (1994) <doi:10.7717/peerj.2874>
  to estimate the natural and fishing mortality of both species.",2021-05-29,Joe Thorley,https://github.com/poissonconsulting/klexdatr,TRUE,https://github.com/poissonconsulting/klexdatr,6025,0,2021-05-29T20:57:37Z,NA
KLexp,"Provides the function to calculate the kernel-lasso expansion, Z-score, and max-min-scale standardization.It can increase the dimension of existed dataset and remove abundant features by lasso. Z Dai, L Jiayi, T Gong, C Wang (2021) <doi:10.1088/1742-6596/1955/1/012047>.",2021-08-21,Zongrui Dai,https://github.com/Zongrui-Dai/Kernel-lasso-feature-expansion,TRUE,https://github.com/zongrui-dai/kernel-lasso-feature-expansion,158,1,2021-08-24T03:26:03Z,158
kmc,"Given constraints for right censored data, we use a recursive computational algorithm to calculate the the ""constrained"" Kaplan-Meier estimator. The constraint is assumed given in linear estimating equations or mean functions. We also illustrate how this leads to the empirical likelihood ratio test with right censored data and accelerated failure time model with given coefficients. EM algorithm from emplik package is used to get the initial value. The properties and performance of the EM algorithm is discussed in Mai Zhou and Yifan Yang (2015)<doi: 10.1007/s00180-015-0567-9> and Mai Zhou and Yifan Yang (2017) <10.1002/wics.1400>. More applications could be found in Mai Zhou (2015) <doi: 10.1201/b18598>.",2020-03-16,Yifan Yang,http://github.com/yfyang86/kmc,TRUE,https://github.com/yfyang86/kmc,13993,1,2021-06-17T10:09:50Z,13993
KMunicate,"Produce Kaplan–Meier plots in the style recommended
    following the KMunicate study by Morris et al. (2019)
    <doi:10.1136/bmjopen-2019-030215>. The KMunicate style consists of
    Kaplan-Meier curves with confidence intervals to quantify uncertainty
    and an extended risk table (per treatment arm) depicting the number of
    study subjects at risk, events, and censored observations over time.
    The resulting plots are built using 'ggplot2' and can be further
    customised to a certain extent, including themes, fonts, and colour
    scales.",2021-04-23,Alessandro Gasparini,https://ellessenne.github.io/KMunicate-package/,TRUE,https://github.com/ellessenne/kmunicate-package,5852,5,2021-04-26T08:13:14Z,1170.4
knitcitations,"Provides the ability to create dynamic citations
    in which the bibliographic information is pulled from the web rather
    than having to be entered into a local database such as 'bibtex' ahead of
    time. The package is primarily aimed at authoring in the R 'markdown'
    format, and can provide outputs for web-based authoring such as linked
    text for inline citations.  Cite using a 'DOI', URL, or
    'bibtex' file key.  See the package URL for details.",2021-01-10,Carl Boettiger,https://github.com/cboettig/knitcitations,TRUE,https://github.com/cboettig/knitcitations,96031,210,2021-01-08T17:07:01Z,457.29047619047617
knitr,"Provides a general-purpose tool for dynamic report generation in R
    using Literate Programming techniques.",2021-04-24,Yihui Xie,https://yihui.org/knitr/,TRUE,https://github.com/yihui/knitr,26847069,2092,2021-08-30T19:27:19Z,12833.206978967495
knitrdata,"Implements a data language engine for incorporating data directly in 
    'rmarkdown' documents so that they can be made completely standalone.",2020-12-08,David M. Kaplan,https://github.com/dmkaplan2000/knitrdata,TRUE,https://github.com/dmkaplan2000/knitrdata,7546,5,2020-12-08T09:56:42Z,1509.2
Knoema,"Using this package, users can access to the largest collection of public data and statistics on the Internet featuring about 2.5 billion time series from thousands of sources collected in 'Knoema' repository and use rich R calculations in order to analyze the data. Because data in 'Knoema' is time series data, 'Knoema' function offers data in a number of formats usable in R such as 'ts', 'xts' or 'zoo'. For more information about 'Knoema' API go to <https://knoema.com/dev/docs>.",2018-05-11,Pavel Pimenov,https://github.com/Knoema/knoema-r-driver,TRUE,https://github.com/knoema/knoema-r-driver,17216,2,2020-11-13T08:43:15Z,8608
kofdata,"Read Swiss time series data from the 'KOF Datenservice' API, <https://datenservice.kof.ethz.ch>. The API provides macro economic time series data mostly about Switzerland. The package itself is a set of wrappers around the 'KOF Datenservice' API. The 'kofdata' package is able to consume public information as well as data that requires an API token. ",2021-03-15,Matthias Bannert,https://github.com/KOF-ch/kofdata,TRUE,https://github.com/kof-ch/kofdata,13756,2,2021-09-02T09:42:11Z,6878
kokudosuuchi,"Provides utilities for 'Kokudo Suuchi', the GIS data service of the Japanese government.
    See <https://nlftp.mlit.go.jp/index.html> for more information.",2021-02-23,Hiroaki Yutani,"https://yutannihilation.github.io/kokudosuuchi/,
https://github.com/yutannihilation/kokudosuuchi",TRUE,https://github.com/yutannihilation/kokudosuuchi,18479,19,2021-02-23T15:00:13Z,972.578947368421
komaletter,"Write beautiful yet customizable letters in R Markdown and
  directly obtain the finished PDF. Smooth generation of PDFs is realized by
  'rmarkdown', the 'pandoc-letter' template and the 'KOMA-Script' letter class.
  'KOMA-Script' provides enhanced replacements for the standard 'LaTeX' classes
  with emphasis on typography and versatility. 'KOMA-Script' is particularly
  useful for international writers as it handles various paper formats well, 
  provides layouts for many common window envelope types (e.g. German, US, 
  French, Japanese) and lets you define your own layouts. The package comes 
  with a default letter layout based on 'DIN 5008B'.",2021-02-02,Robert Nuske,https://github.com/rnuske/komaletter,TRUE,https://github.com/rnuske/komaletter,17468,67,2021-08-30T16:02:12Z,260.7164179104478
konfound,"Statistical methods that quantify the conditions necessary to alter
    inferences, also known as sensitivity analysis, are becoming increasingly
    important to a variety of quantitative sciences. A series of recent works,
    including Frank (2000) <doi:10.1177/0049124100029002001> and Frank et al.
    (2013) <doi:10.3102/0162373713493129> extend previous sensitivity analyses
    by considering the characteristics of omitted variables or unobserved cases
    that would change an inference if such variables or cases were observed. These
    analyses generate statements such as ""an omitted variable would have to be
    correlated at xx with the predictor of interest (e.g., treatment) and outcome
    to invalidate an inference of a treatment effect"". Or ""one would have to replace
    pp percent of the observed data with null hypothesis cases to invalidate the
    inference"". We implement these recent developments of sensitivity analysis and
    provide modules to calculate these two robustness indices and generate such
    statements in R. In particular, the functions konfound(), pkonfound() and 
    mkonfound() allow users to calculate the robustness of inferences for a user's 
    own model, a single published study and multiple studies respectively.",2021-06-01,Joshua M Rosenberg,https://github.com/jrosen48/konfound,TRUE,https://github.com/jrosen48/konfound,17130,11,2021-07-13T16:13:26Z,1557.2727272727273
koRpus,"A set of tools to analyze texts. Includes, amongst others, functions for
          automatic language detection, hyphenation, several indices of lexical diversity
          (e.g., type token ratio, HD-D/vocd-D, MTLD) and readability (e.g., Flesch,
          SMOG, LIX, Dale-Chall). Basic import functions for language corpora are also
          provided, to enable frequency analyses (supports Celex and Leipzig Corpora
          Collection file formats) and measures like tf-idf. Note: For full functionality
          a local installation of TreeTagger is recommended. It is also recommended to
          not load this package directly, but by loading one of the available language
          support packages from the 'l10n' repository
          <https://undocumeantit.github.io/repos/l10n/>. 'koRpus' also includes a plugin
          for the R GUI and IDE RKWard, providing graphical dialogs for its basic
          features. The respective R package 'rkward' cannot be installed directly from a
          repository, as it is a part of RKWard. To make full use of this feature, please
          install RKWard from <https://rkward.kde.org> (plugins are detected
          automatically). Due to some restrictions on CRAN, the full package sources are
          only available from the project homepage. To ask for help, report bugs, request
          features, or discuss the development of the package, please subscribe to the
          koRpus-dev mailing list (<https://korpusml.reaktanz.de>).",2021-05-17,Meik Michalke,https://reaktanz.de/?c=hacking&s=koRpus,TRUE,https://github.com/undocumeantit/korpus,119084,36,2021-05-18T13:11:06Z,3307.8888888888887
koRpus.lang.en,"Adds support for the English language to the 'koRpus' package. To ask for help, report bugs, suggest feature improvements, or discuss the global development of the
                    package, please consider subscribing to the koRpus-dev mailing list (<https://korpusml.reaktanz.de>).",2020-10-24,Meik Michalke,https://reaktanz.de/?c=hacking&s=koRpus,TRUE,https://github.com/undocumeantit/korpus.lang.en,77488,0,2020-10-24T15:37:42Z,NA
KSgeneral,"Computes a p-value of the one-sample two-sided (or one-sided, as a special case) Kolmogorov-Smirnov (KS) statistic, for any fixed critical level, and an arbitrary, possibly large sample size for a pre-specified purely discrete, mixed or continuous cumulative distribution function (cdf) under the null hypothesis. If a data sample is supplied, 'KSgeneral' computes the p-value corresponding to the value of the KS test statistic computed based on the user provided data sample. The package 'KSgeneral' implements a novel, accurate and efficient method named Exact-KS-FFT, expressing the p-value as a double-boundary non-crossing probability for a homogeneous Poisson process, which is then efficiently computed using Fast Fourier Transform (FFT). The package can also be used to compute and plot the complementary cdf of the KS statistic which is known to depend on the hypothesized distribution when the latter is discontinuous (i.e. purely discrete or mixed). To cite this package in publication use: Dimitrina S. Dimitrova, Vladimir K. Kaishev, and Senren Tan. Computing the Kolmogorov-Smirnov Distribution When the Underlying CDF is Purely Discrete, Mixed, or Continuous. Journal of Statistical Software. 2020; 95(10): 1--42. <doi:10.18637/jss.v095.i10>. ",2020-10-02,Dimitrina S. Dimitrova,https://github.com/raymondtsr/KSgeneral,TRUE,https://github.com/raymondtsr/ksgeneral,19152,4,2020-10-08T16:17:20Z,4788
labelled,"Work with labelled data imported from 'SPSS'
    or 'Stata' with 'haven' or 'foreign'. This package
    provides useful functions to deal with ""haven_labelled"" and
    ""haven_labelled_spss"" classes introduced by 'haven' package.",2021-03-08,Joseph Larmarange,http://larmarange.github.io/labelled/,TRUE,https://github.com/larmarange/labelled,1954528,54,2021-06-07T08:16:41Z,36194.96296296296
labourR,"Allows the user to map multilingual free-text of occupations to a broad range
    of standardized classifications. The package facilitates automatic occupation coding
    (see, e.g., Gweon et al. (2017) <doi:10.1515/jos-2017-0006> and Turrell et al. (2019)
    <doi:10.3386/w25837>), where the ISCO to ESCO mapping is exploited to extend the
    occupations hierarchy, Le Vrang et al. (2014) <doi:10.1109/mc.2014.283>. Document
    vectorization is performed using the multilingual ESCO corpus. A method based on the 
    nearest neighbor search is used to suggest the closest ISCO occupation.",2020-07-18,Alexandros Kouretsis,https://github.com/AleKoure/labourR,TRUE,https://github.com/alekoure/labourr,5393,17,2021-08-25T15:39:18Z,317.2352941176471
lacrmr,"Connect to the 'Less Annoying CRM' API with ease to get your crm data in a clean and tidy format. 'Less Annoying CRM' is a simple CRM built for small businesses, more information is available on their website <https://www.lessannoyingcrm.com/>.",2020-09-02,Ronny Hernández Mora,https://ixpantia.github.io/lacrmr/,TRUE,https://github.com/ixpantia/lacrmr,4233,0,2021-03-20T19:55:12Z,NA
lacunaritycovariance,"Functions for estimating the gliding box lacunarity (GBL),
    covariance, and pair-correlation of a random closed set (RACS) in 2D
    from a binary coverage map (e.g. presence-absence land cover maps).
    Contains a number of newly-developed covariance-based estimators of
    GBL (Hingee et al., 2019) <doi:10.1007/s13253-019-00351-9> and
    balanced estimators, proposed by Picka (2000)
    <http://www.jstor.org/stable/1428408>, for covariance, centred
    covariance, and pair-correlation.  Also contains methods for
    estimating contagion-like properties of RACS and simulating 2D Boolean
    models.  Binary coverage maps are usually represented as raster images
    with pixel values of TRUE, FALSE or NA, with NA representing
    unobserved pixels.  A demo for extracting such a binary map from a
    geospatial data format is provided.  Binary maps may also be
    represented using polygonal sets as the foreground, however for most
    computations such maps are converted into raster images.  The package
    is based on research conducted during the author's PhD studies.",2021-03-24,Kassel Liam Hingee,https://github.com/kasselhingee/lacunaritycovariance,TRUE,https://github.com/kasselhingee/lacunaritycovariance,11956,1,2021-03-19T02:41:09Z,11956
LAGOSNE,"Client for programmatic access to the Lake
    Multi-scaled Geospatial and Temporal database <https://lagoslakes.org>, with functions
    for accessing lake water quality and ecological context data for the US.",2020-11-29,Joseph Stachelek,https://github.com/cont-limno/LAGOSNE,TRUE,https://github.com/cont-limno/lagosne,18604,7,2021-08-04T14:15:47Z,2657.714285714286
Lahman,"Provides the tables from the 'Sean Lahman Baseball Database' as
    a set of R data.frames. It uses the data on pitching, hitting and fielding
    performance and other tables from 1871 through 2019, as recorded in the 2020
    version of the database. Documentation examples show how many baseball
    questions can be investigated.",2021-04-09,Chris Dalzell,https://CRAN.R-project.org/package=Lahman,TRUE,https://github.com/cdalzell/lahman,891539,52,2021-04-09T06:35:50Z,17144.98076923077
lamW,"Implements both real-valued branches of the Lambert-W function
    (Corless et al, 1996) <doi:10.1007/BF02124750> without the need for
    installing the entire GSL.",2021-05-21,Avraham Adler,https://github.com/aadler/lamW,TRUE,https://github.com/aadler/lamw,459096,1,2021-05-21T00:02:53Z,459096
landmap,"Functions and tools for spatial interpolation and/or prediction of environmental variables (points to grids) 
   based on using Ensemble Machine Learning with geographical distances. Package also provides access to Global 
   Environmental Layers (<https://www.OpenLandMap.org>) produced by the OpenGeoHub.org foundation and collaborators. Some functions have been migrated and adopted from the
   Global Soil Information Facilities package.",2021-05-27,Tomislav Hengl,https://github.com/envirometrix/landmap/,TRUE,https://github.com/envirometrix/landmap,12916,29,2021-05-25T17:00:15Z,445.37931034482756
landscapemetrics,"Calculates landscape metrics for categorical landscape patterns in 
    a tidy workflow. 'landscapemetrics' reimplements the most common metrics from
    'FRAGSTATS' (<https://www.umass.edu/landeco/research/fragstats/fragstats.html>) 
    and new ones from the current literature on landscape metrics.
    This package supports 'raster' spatial objects and takes 
    RasterLayer, RasterStacks, RasterBricks or lists of RasterLayer from the
    'raster' package as input arguments. It further provides utility functions
    to visualize patches, select metrics and building blocks to develop new 
    metrics.",2021-09-03,Maximillian H.K. Hesselbarth,https://r-spatialecology.github.io/landscapemetrics/,TRUE,https://github.com/r-spatialecology/landscapemetrics,52712,165,2021-08-23T17:35:21Z,319.46666666666664
landscapeR,"Simulates categorical maps on actual geographical realms, starting from either empty landscapes or landscapes provided by the user (e.g. land use maps). Allows to tweak or create landscapes while retaining a high degree of control on its features, without the hassle of specifying each location attribute. In this it differs from other tools which generate null or neutral landscapes in a theoretical space. The basic algorithm currently implemented uses a simple agent style/cellular automata growth model, with no rules (apart from areas of exclusion) and von Neumann neighbourhood (four cells, aka Rook case). Outputs are raster dataset exportable to any common GIS format.",2017-07-05,Dario Masante,https://github.com/dariomasante/landscapeR,TRUE,https://github.com/dariomasante/landscaper,17534,0,2021-06-04T22:58:14Z,NA
languageserver,"An implementation of the Language Server Protocol
    for R. The Language Server protocol is used by an editor client to
    integrate features like auto completion. See
    <https://microsoft.github.io/language-server-protocol/> for details.",2021-07-22,Randy Lai,https://github.com/REditorSupport/languageserver/,TRUE,https://github.com/reditorsupport/languageserver,124488,374,2021-08-13T14:09:00Z,332.85561497326205
languageserversetup,"Allows to install the R 'languageserver' with all dependencies
    into a separate library and use that independent installation
    automatically when R is instantiated as a language server process.
    Useful for making language server seamless to use without
    running into package version conflicts.",2020-04-10,Jozef Hajnala,https://github.com/jozefhajnala/languageserversetup,TRUE,https://github.com/jozefhajnala/languageserversetup,12535,21,2021-02-19T21:29:19Z,596.9047619047619
LaplacesDemon,Provides a complete environment for Bayesian inference using a variety of different samplers (see ?LaplacesDemon for an overview).,2021-07-09,Henrik Singmann,https://github.com/LaplacesDemonR/LaplacesDemon,TRUE,https://github.com/laplacesdemonr/laplacesdemon,225822,66,2021-07-09T13:15:03Z,3421.5454545454545
lares,"Auxiliary package for better/faster analytics, visualization, data mining, and machine learning 
    tasks. With a wide variety of family functions, like Machine Learning, Data Wrangling, 
    Exploratory, and Scrapper, it helps the analyst or data scientist to get quick and robust 
    results, without the need of repetitive coding or extensive programming skills.",2021-06-09,Bernardo Lares,https://github.com/laresbernardo/lares,TRUE,https://github.com/laresbernardo/lares,6942,180,2021-09-02T14:41:38Z,38.56666666666667
LassoGEE,Fits generalized estimating equations with L1 regularization to longitudinal data with high dimensional covariates. Use a efficient iterative composite gradient descent algorithm.,2020-11-06,Yaguang Li,<https://github.com/liygCR/LassoGEE>,TRUE,https://github.com/liygcr/lassogee,3759,0,2020-10-29T11:50:21Z,NA
latentnet,"Fit and simulate latent position and cluster models for statistical networks. See Krivitsky and Handcock (2008) <10.18637/jss.v024.i05> and Krivitsky, Handcock, Raftery, and Hoff (2009) <10.1016/j.socnet.2009.04.001>.",2020-03-22,Pavel N. Krivitsky,http://www.statnet.org,TRUE,https://github.com/statnet/latentnet,98783,14,2021-06-16T14:17:20Z,7055.928571428572
later,"Executes arbitrary R or C functions some time after the current
    time, after the R execution stack has emptied. The functions are scheduled
    in an event loop.",2021-08-18,Winston Chang,https://github.com/r-lib/later,TRUE,https://github.com/r-lib/later,14712737,120,2021-08-18T16:56:23Z,122606.14166666666
latex2exp,"Parses and converts LaTeX math formulas to R's plotmath
    expressions, used to enter mathematical formulas and symbols to be rendered as
    text, axis labels, etc. throughout R's plotting system.",2021-03-18,Stefano Meschiari,https://github.com/stefano-meschiari/latex2exp,TRUE,https://github.com/stefano-meschiari/latex2exp,185896,152,2021-03-14T21:11:50Z,1223
latrend,"A framework for clustering longitudinal datasets in a standardized way. Provides an interface to existing R packages for clustering longitudinal univariate trajectories, facilitating reproducible and transparent analyses. Additionally, standard tools are provided to support cluster analyses, including repeated estimation, model validation, and model assessment. The interface enables users to compare results between methods, and to implement and evaluate new methods with ease. ",2021-04-14,Niek Den Teuling,https://github.com/philips-software/latrend,TRUE,https://github.com/philips-software/latrend,3032,6,2021-07-30T08:56:06Z,505.3333333333333
lattice,"A powerful and elegant high-level data visualization
  system inspired by Trellis graphics, with an emphasis on
  multivariate data. Lattice is sufficient for typical graphics needs,
  and is also flexible enough to handle most nonstandard requirements.
  See ?Lattice for an introduction.",2021-05-02,Deepayan Sarkar,http://lattice.r-forge.r-project.org/,TRUE,https://github.com/deepayan/lattice,3034954,43,2021-05-16T23:02:23Z,70580.32558139534
lava,"A general implementation of Structural Equation Models
	with latent variables (MLE, 2SLS, and composite likelihood
	estimators) with both continuous, censored, and ordinal
	outcomes (Holst and Budtz-Joergensen (2013) <doi:10.1007/s00180-012-0344-y>).
	Mixture latent variable models and non-linear latent variable models
	(Holst and Budtz-Joergensen (2019) <doi:10.1093/biostatistics/kxy082>).
	The package also provides methods for graph exploration (d-separation,
	back-door criterion), simulation of general non-linear latent variable
	models, and estimation of influence functions for a broad range of
	statistical models.",2021-09-02,Klaus K. Holst,https://kkholst.github.io/lava/,TRUE,https://github.com/kkholst/lava,3164405,27,2021-09-02T12:26:16Z,117200.18518518518
lavaanPlot,"Plots path diagrams from models in 'lavaan' using the plotting
    functionality from the 'DiagrammeR' package. 'DiagrammeR' provides nice path diagrams 
    via 'Graphviz', and these functions make it easy to generate these diagrams from a
    'lavaan' path model without having to write the DOT language graph specification.",2021-08-13,Alex Lishinski,https://github.com/alishinski/lavaanPlot,TRUE,https://github.com/alishinski/lavaanplot,35168,25,2021-08-12T19:06:57Z,1406.72
lavacreg,"Estimation of a multi-group count regression models (i.e., Poisson, 
    negative binomial) with latent covariates. This packages provides two extensions
    compared to ordinary count regression models based on a generalized linear model:
    First, measurement models for the predictors can be specified allowing to account 
    for measurement error. Second, the count regression can be simultaneously estimated 
    in multiple groups with stochastic group weights. The marginal maximum likelihood 
    estimation is described in Kiefer & Mayer (2020) <doi:10.1080/00273171.2020.1751027>.",2021-08-19,Christoph Kiefer,https://github.com/chkiefer/lavacreg,TRUE,https://github.com/chkiefer/lavacreg,2284,3,2021-08-20T12:53:28Z,761.3333333333334
LAWBL,"A variety of models to analyze latent variables based on Bayesian learning: the partially CFA (Chen, Guo, Zhang, & Pan, 2020) <DOI: 10.1037/met0000293>; generalized PCFA; partially confirmatory IRM (Chen, 2020) <DOI: 10.1007/s11336-020-09724-3>; Bayesian regularized EFA <DOI: 10.1080/10705511.2020.1854763>; Fully and partially EFA.",2021-04-01,Jinsong Chen,"https://github.com/Jinsong-Chen/LAWBL,
https://jinsong-chen.github.io/LAWBL/",TRUE,https://github.com/jinsong-chen/lawbl,6259,1,2021-04-02T13:00:32Z,6259
lax,"Performs adjusted inferences based on model objects fitted, using 
    maximum likelihood estimation, by the extreme value analysis packages
    'eva' <https://cran.r-project.org/package=eva>, 
    'evd' <https://cran.r-project.org/package=evd>, 
    'evir' <https://cran.r-project.org/package=evir>, 
    'extRemes' <https://cran.r-project.org/package=extRemes>, 
    'fExtremes' <https://cran.r-project.org/package=fExtremes>, 
    'ismev' <https://cran.r-project.org/package=ismev>, 
    'mev' <https://cran.r-project.org/package=mev>, 
    'POT' <https://cran.r-project.org/package=POT> and
    'texmex' <https://cran.r-project.org/package=texmex>. 
    Adjusted standard errors and an adjusted loglikelihood are provided, using    
    the 'chandwich' package <https://cran.r-project.org/package=chandwich>
    and the object-oriented features of the 'sandwich' package 
    <https://cran.r-project.org/package=sandwich>. The adjustment is based on a 
    robust sandwich estimator of the parameter covariance matrix, based on the 
    methodology in Chandler and Bate (2007) <doi:10.1093/biomet/asm015>. This 
    can be used for cluster correlated data when interest lies in the 
    parameters of the marginal distributions, or for performing inferences that 
    are robust to certain types of model misspecification.  Univariate extreme 
    value models, including regression models, are supported.  ",2021-07-20,Paul J. Northrop,"https://paulnorthrop.github.io/lax/,
https://github.com/paulnorthrop/lax",TRUE,https://github.com/paulnorthrop/lax,10281,2,2021-07-20T08:15:29Z,5140.5
lazyarray,"Multi-threaded serialization of compressed array that 
    fully utilizes modern solid state drives. It allows 
    to store and load extremely large data on demand within seconds
    without occupying too much memories. With data stored on hard drive, 
    a lazy-array data can be loaded, shared across multiple R sessions.
    For arrays with partition mode on, multiple R sessions can write to 
    a same array simultaneously along the last dimension (partition). 
    The internal storage format is provided by 'fstcore' package geared by 
    'LZ4' and 'ZSTD' compressors.",2020-07-18,Zhengjia Wang,https://github.com/dipterix/lazyarray,TRUE,https://github.com/dipterix/lazyarray,5953,16,2020-11-30T08:07:56Z,372.0625
lazyraster,"Read raster data at a specified resolution on-demand via 'GDAL' 
 (the Geospatial Data Abstraction Library <https://gdal.org/>). Augments the 
 'raster' package by never reading data from a raster source until necessary for 
 generating an in-memory 'raster' object. A 'lazyraster' object may be cropped 
 and converted to 'raster' object, and by default will only read a small amount 
 of data sufficient for an overall summary. The amount of data read can be 
 controlled by specifying the output dimensions. ",2019-10-09,Michael Sumner,https://github.com/hypertidy/lazyraster,TRUE,https://github.com/hypertidy/lazyraster,9509,23,2021-08-05T10:14:05Z,413.4347826086956
lazysf,"Lazy read for drawings. A 'dplyr' back end for data sources supported by 
    'GDAL' vector drivers, that allows working with local or remote sources as if they 
    are in-memory data frames. Basic features works with any drawing format ('GDAL vector 
    data source') supported by the 'sf' package. ",2020-11-14,Michael Sumner,https://github.com/mdsumner/lazysf,TRUE,https://github.com/mdsumner/lazysf,3306,14,2021-07-04T11:35:54Z,236.14285714285714
lazytrade,"Provide sets of functions and methods to learn and practice data science using idea of algorithmic trading.
    Main goal is to process information within ""Decision Support System"" to come up with analysis or predictions.
    There are several utilities such as dynamic and adaptive risk management using reinforcement learning
    and even functions to generate predictions of price changes using pattern recognition deep regression learning.
    Summary of Methods used: Awesome H2O tutorials: <https://github.com/h2oai/awesome-h2o>, 
    Market Type research of Van Tharp Institute: <https://www.vantharp.com/>,
    Reinforcement Learning R package: <https://CRAN.R-project.org/package=ReinforcementLearning>.",2021-06-20,Vladimir Zhbanko,"https://vladdsm.github.io/myblog_attempt/topics/lazy%20trading/,
https://github.com/vzhomeexperiments/lazytrade",TRUE,https://github.com/vzhomeexperiments/lazytrade,17661,16,2021-06-25T05:17:36Z,1103.8125
lcars,"Provides Shiny widgets and theme that support a 'Library Computer Access/Retrieval System' (LCARS) aesthetic for Shiny apps. 
    The package also includes functions for adding a minimal LCARS theme to static 'ggplot2' graphs. 
    More details about LCARS can be found at <https://en.wikipedia.org/wiki/LCARS>.",2021-06-01,Matthew Leonawicz,https://github.com/leonawicz/lcars,TRUE,https://github.com/leonawicz/lcars,9359,53,2021-05-29T18:48:06Z,176.58490566037736
lcmm,"Estimation of various extensions of the mixed models including latent class mixed models, joint latent latent class mixed models and mixed models for curvilinear univariate or multivariate longitudinal outcomes using a maximum likelihood estimation method (Proust-Lima, Philipps, Liquet (2017) <doi:10.18637/jss.v078.i02>).",2021-06-21,Cecile Proust-Lima,NA,TRUE,https://github.com/cecileproust-lima/lcmm,62125,11,2021-07-01T15:15:38Z,5647.727272727273
lconnect,"Provides functions to upload vectorial data and derive landscape
    connectivity metrics in habitat or matrix systems. Additionally, includes an 
    approach to assess individual patch contribution to the overall landscape 
    connectivity, enabling the prioritization of habitat patches. The computation
    of landscape connectivity and patch importance are very useful in Landscape 
    Ecology research. The metrics available are: number of components, number of 
    links, size of the largest component, mean size of components, class coincidence
    probability, landscape coincidence probability, characteristic path length, 
    expected cluster size, area-weighted flux and integral index of connectivity.
    Pascual-Hortal, L., and Saura, S. (2006) <doi:10.1007/s10980-006-0013-z>
    Urban, D., and Keitt, T. (2001) <doi:10.2307/2679983>
    Laita, A., Kotiaho, J., Monkkonen, M. (2011) <doi:10.1007/s10980-011-9620-4>.",2021-02-06,Frederico Mestre,NA,TRUE,https://github.com/fmestre1/lconnect,12134,3,2021-09-02T11:14:10Z,4044.6666666666665
lcopula,"Collections of functions allowing random number generations and
    estimation of 'Liouville' copulas, as described in Belzile and Neslehova (2017) <doi:10.1016/j.jmva.2017.05.008>.",2019-07-06,Leo Belzile,NA,TRUE,https://github.com/lbelzile/lcopula,70354,0,2020-11-29T17:56:55Z,NA
lcsm,"Helper functions to implement univariate and bivariate latent change score models in R using the 'lavaan' package.
  For details about Latent Change Score Modeling (LCSM) see McArdle (2009) <doi:10.1146/annurev.psych.60.110707.163612> and Grimm, An, McArdle, Zonderman and Resnick (2012) <doi:10.1080/10705511.2012.659627>.
  The package automatically generates 'lavaan' syntax for different model specifications and varying timepoints.
  The 'lavaan' syntax generated by this package can be returned and further specifications can be added manually.
  Longitudinal plots as well as simplified path diagrams can be created to visualise data and model specifications.
  Estimated model parameters and fit statistics can be extracted as data frames.
  Data for different univariate and bivariate LCSM can be simulated by specifying estimates for model parameters to explore their effects.
  This package combines the strengths of other R packages like 'lavaan', 'broom', and 'semPlot' by generating 'lavaan' syntax that helps these packages work together.",2020-07-24,Milan Wiedemann,https://milanwiedemann.github.io/lcsm/,TRUE,https://github.com/milanwiedemann/lcsm,7064,6,2021-07-27T11:30:05Z,1177.3333333333333
ldaPrototype,"Determine a Prototype from a number of runs of Latent Dirichlet Allocation (LDA) measuring its similarities with S-CLOP: A procedure to select the LDA run with highest mean pairwise similarity, which is measured by S-CLOP (Similarity of multiple sets by Clustering with Local Pruning), to all other runs. LDA runs are specified by its assignments leading to estimators for distribution parameters. Repeated runs lead to different results, which we encounter by choosing the most representative LDA run as prototype.",2021-09-02,Jonas Rieger,https://github.com/JonasRieger/ldaPrototype,TRUE,https://github.com/jonasrieger/ldaprototype,9911,4,2021-09-01T15:48:08Z,2477.75
LDAShiny,"Contains the development of a tool that provides a
    web-based graphical user interface (GUI) to perform a review of the
    scientific literature under the Bayesian approach of Latent Dirichlet
    Allocation (LDA)and machine learning algorithms. The application
    methodology is framed by the well known procedures in topic modelling
    on how to clean and process data. Contains methods described by 
    Blei, David M., Andrew Y. Ng, and Michael I. Jordan (2003) 
    <https://jmlr.org/papers/volume3/blei03a/blei03a.pdf> 
    Allocation""; Thomas L. Griffiths and Mark Steyvers (2004) 
    <doi:10.1073/pnas.0307752101> ; Xiong Hui, et al (2019) 
    <doi:10.1016/j.cie.2019.06.010>.",2021-03-29,Javier De La Hoz Maestre,NA,TRUE,https://github.com/javierdelahoz/ldashiny,5252,1,2021-03-30T21:32:53Z,5252
ldatuning,"For this first version only metrics to estimate the best fitting
    number of topics are implemented.",2020-04-21,Murzintcev Nikita,https://github.com/nikita-moor/ldatuning,TRUE,https://github.com/nikita-moor/ldatuning,52139,53,2021-08-22T15:16:54Z,983.7547169811321
LDlinkR,"Provides access to the 'LDlink' API (<https://ldlink.nci.nih.gov/?tab=apiaccess>)
    using the R console.  This programmatic access facilitates researchers who are 
    interested in performing batch queries in 1000 Genomes Project (2015) <doi:10.1038/nature15393> 
    data using 'LDlink'. 'LDlink' is an interactive and powerful suite of web-based tools for querying 
    germline variants in human population groups of interest. For more details, please see 
    Machiela et al. (2015) <doi:10.1093/bioinformatics/btv402>.",2021-02-19,Timothy A. Myers,https://ldlink.nci.nih.gov,TRUE,https://github.com/cbiit/ldlinkr,14592,14,2021-02-19T19:58:30Z,1042.2857142857142
ldsep,"Estimate haplotypic or composite pairwise linkage disequilibrium
    (LD) in polyploids, using either genotypes or genotype likelihoods. 
    Support is provided to estimate the popular measures of LD: the LD 
    coefficient D, the standardized LD coefficient D', and the Pearson 
    correlation coefficient r. All estimates are returned with corresponding 
    standard errors. These estimates and standard errors can then be used
    for shrinkage estimation. The main functions are ldfast(), ldest(), mldest(),
    sldest(), plot.lddf(), format_lddf(), and ldshrink(). Details of the methods
    are available in Gerard (2021a) <doi:10.1111/1755-0998.13349>
    and Gerard (2021b) <doi:10.1038/s41437-021-00462-5>.",2021-08-10,David Gerard,NA,TRUE,https://github.com/dcgerard/ldsep,6034,2,2021-08-10T18:12:20Z,3017
LeafArea,"An interface for the image processing program 'ImageJ', which
    allows a rapid digital image analysis for particle sizes. This package includes
    function to write an 'ImageJ' macro which is optimized for a leaf area analysis by
    default.",2019-07-03,Masatoshi Katabuchi,https://github.com/mattocci27/LeafArea,TRUE,https://github.com/mattocci27/leafarea,22903,19,2021-06-01T05:42:13Z,1205.421052631579
leafem,"Provides extensions for packages 'leaflet' & 'mapdeck', 
    many of which are used by package 'mapview'. 
    Focus is on functionality readily available in 
    Geographic Information Systems such as 'Quantum GIS'. Includes functions
    to display coordinates of mouse pointer position, query image values via 
    mouse pointer and zoom-to-layer buttons. Additionally, provides a feature 
    type agnostic function to add points, lines, polygons to a map.",2021-05-24,Tim Appelhans,https://github.com/r-spatial/leafem,TRUE,https://github.com/r-spatial/leafem,762570,80,2021-06-27T07:20:00Z,9532.125
leaflegend,"Provides extensions to the 'leaflet' package to 
    customize legends with images, text styling, orientation, sizing,
    and symbology.",2021-07-23,Thomas Roh,"https://leaflegend.roh.engineering,
https://github.com/tomroh/leaflegend",TRUE,https://github.com/tomroh/leaflegend,3785,12,2021-08-06T13:38:49Z,315.4166666666667
leaflet,"Create and customize interactive maps using the 'Leaflet'
    JavaScript library and the 'htmlwidgets' package. These maps can be used
    directly from the R console, from 'RStudio', in Shiny applications and R Markdown
    documents.",2021-01-07,Joe Cheng,https://rstudio.github.io/leaflet/,TRUE,https://github.com/rstudio/leaflet,2199537,691,2020-12-15T05:29:09Z,3183.121562952243
leaflet.extras2,"Several 'leaflet' plugins are integrated, which are available as extension to the 'leaflet' package.",2020-10-20,Gatscha Sebastian,"https://trafficonese.github.io/leaflet.extras2,
https://github.com/trafficonese/leaflet.extras2",TRUE,https://github.com/trafficonese/leaflet.extras2,42229,42,2021-07-19T08:40:13Z,1005.452380952381
leaflet.multiopacity,Extends Leaflet for R by adding widget to control opacity of multiple layers.,2020-12-14,Meantrix,https://github.com/meantrix/leaflet.multiopacity,TRUE,https://github.com/meantrix/leaflet.multiopacity,3495,0,2021-05-21T20:35:07Z,NA
leafpop,"Creates 'HTML' strings to embed tables, images or graphs in pop-ups
  of interactive maps created with packages like 'leaflet' or 'mapview'. Handles
  local images located on the file system or via remote URL. Handles graphs created 
  with 'lattice' or 'ggplot2' as well as interactive plots created with 'htmlwidgets'.",2021-05-22,Tim Appelhans,https://github.com/r-spatial/leafpop,TRUE,https://github.com/r-spatial/leafpop,582484,81,2021-05-22T09:12:09Z,7191.16049382716
leafR,"A set of functions for analyzing the structure 
  of forests based on the leaf area density (LAD) and leaf area index (LAI) measures 
  calculated from Airborne Laser Scanning (ALS), i.e., scanning lidar (Light Detection 
  and Ranging) data. The methodology is discussed and described in 
  Almeida et al. (2019) <doi:10.3390/rs11010092> and 
  Stark et al. (2012) <doi:10.1111/j.1461-0248.2012.01864.x>.",2021-07-04,Danilo Roberti Alves de Almeida,"https://github.com/DRAAlmeida/leafR,
https://leafr.r-forge.r-project.org",TRUE,https://github.com/draalmeida/leafr,10767,6,2021-07-02T01:36:28Z,1794.5
learnr,"Create interactive tutorials using R Markdown. Use a combination
  of narrative, figures, videos, exercises, and quizzes to create self-paced
  tutorials for learning about R and R packages.",2020-02-13,Barret Schloerke,"https://rstudio.github.io/learnr/,
https://github.com/rstudio/learnr",TRUE,https://github.com/rstudio/learnr,340568,443,2021-08-31T14:53:41Z,768.7765237020316
learnrbook,"Data, scripts and code from chunks used as examples in the book 
   ""Learn R: As a Language"" by Pedro J. Aphalo.
   ISBN 9780367182533 (pbk); ISBN 9780367182557 (hbk); ISBN 9780429060342 (ebk).",2021-07-04,Pedro J. Aphalo,https://docs.r4photobiology.info/learnrbook/,TRUE,https://github.com/aphalo/learnrbook-pkg,16649,1,2021-07-04T20:31:38Z,16649
legion,"Functions implementing multivariate state space models for purposes of time series analysis and forecasting.
             The focus of the package is on multivariate models, such as Vector Exponential Smoothing,
             Vector ETS (Error-Trend-Seasonal model) etc. It currently includes Vector Exponential
             Smoothing (VES, de Silva et al., 2010, <doi:10.1177/1471082X0901000401>), Vector ETS and
             simulation function for VES.",2021-05-17,"Ivan Svetunkov  (Lecturer at Centre for Marketing Analytics
    and Forecasting",https://github.com/config-i1/legion,TRUE,https://github.com/config-i1/legion,1342,3,2021-08-24T12:27:34Z,447.3333333333333
legislatoR,"Facilitates access to the Comparative Legislators Database (CLD). The CLD includes political, sociodemographic, career, online presence, public attention, and visual information for over 45,000 contemporary and historical politicians from ten countries.",2020-04-24,Sascha Goebel,https://github.com/saschagobel/legislatoR,TRUE,https://github.com/saschagobel/legislator,5538,69,2021-03-07T14:35:07Z,80.26086956521739
legocolors,"Provides a dataset containing several color naming conventions established by multiple sources, along with associated color metadata.
    The package also provides related helper functions for mapping among the different Lego color naming conventions and between Lego colors, hex colors, and 'R' color names, 
    making it easy to convert any color palette to one based on existing Lego colors while keeping as close to the original color palette as possible.
    The functions use nearest color matching based on Euclidean distance in RGB space. 
    Naming conventions for color mapping include those from 'BrickLink' (<https://www.bricklink.com>), 'The Lego Group' (<https://www.lego.com>), 'LDraw' (<https://www.ldraw.org/>), and 'Peeron' (<http://www.peeron.com/>).",2021-02-20,Matthew Leonawicz,https://github.com/leonawicz/legocolors,TRUE,https://github.com/leonawicz/legocolors,12288,4,2021-06-14T14:19:43Z,3072
leiden,"Implements the 'Python leidenalg' module to be called in R.
    Enables clustering using the leiden algorithm for partition a graph into communities.
    See the 'Python' repository for more details: <https://github.com/vtraag/leidenalg>
    Traag et al (2018) From Louvain to Leiden: guaranteeing well-connected communities. <arXiv:1810.08473>.",2021-07-27,S. Thomas Kelly,https://github.com/TomKellyGenetics/leiden,TRUE,https://github.com/tomkellygenetics/leiden,306441,19,2021-07-27T06:36:22Z,16128.473684210527
leidenAlg,"An R interface to the Leiden algorithm, an iterative community detection algorithm on networks. The algorithm is designed to converge to a partition in which all subsets of all communities are locally optimally assigned, yielding communities guaranteed to be connected. The implementation proves to be fast, scales well, and can be run on graphs of millions of nodes (as long as they can fit in memory). The original implementation was constructed as a python interface ""leidenalg"" found here: <https://github.com/vtraag/leidenalg>. The algorithm was originally described in Traag, V.A., Waltman, L. & van Eck, N.J. ""From Louvain to Leiden: guaranteeing well-connected communities"". Sci Rep 9, 5233 (2019) <doi:10.1038/s41598-019-41695-z>.",2021-03-03,Evan Biederstedt,https://github.com/kharchenkolab/leidenAlg,TRUE,https://github.com/kharchenkolab/leidenalg,4968,3,2021-05-06T01:14:04Z,1656
lemon,"Functions for working with legends and axis lines of 'ggplot2',
    facets that repeat axis lines on all panels, and some 'knitr' extensions.",2020-06-08,Stefan McKinnon Edwards,https://github.com/stefanedwards/lemon,TRUE,https://github.com/stefanedwards/lemon,75651,153,2021-02-26T09:01:09Z,494.45098039215685
leontief,"An implementation of the Input-Output model developed by
    Wassily Leontief that represents the interdependencies between different 
    sectors of a national economy or different regional economies.",2020-09-02,Mauricio Vargas,https://pachamaltese.github.io/leontief,TRUE,https://github.com/pachamaltese/leontief,4390,4,2020-09-05T16:34:22Z,1097.5
leri,"Finds and downloads Landscape Evaporative Response
    Index (LERI) data, then reads the data into 'R' using the 'raster'
    package. The LERI product measures anomalies in actual
    evapotranspiration, to support drought monitoring and early warning
    systems. More info on LERI is available at
    <https://www.esrl.noaa.gov/psd/leri/>.",2019-09-09,Max Joseph,https://github.com/earthlab/leri,TRUE,https://github.com/earthlab/leri,9752,2,2020-12-07T18:53:28Z,4876
letsR,"Handling, processing, and analyzing geographic
    data on species' distributions and environmental variables. 
    Read Vilela & Villalobos (2015) <doi: 10.1111/2041-210X.12401> for details.",2020-10-26,Bruno Vilela & Fabricio Villalobos,"https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12401,
https://github.com/macroecology/letsR",TRUE,https://github.com/macroecology/letsr,22683,17,2021-08-18T22:22:03Z,1334.2941176470588
levitate,"Provides string similarity calculations inspired by the
    Python 'fuzzywuzzy' package. Compare strings by edit distance,
    similarity ratio, best matching substring, ordered token matching and
    set-based token matching. A range of edit distance measures are
    available thanks to the 'stringdist' package.",2021-05-25,Lewin Appleton-Fox,"https://lewinfox.github.io/levitate/,
https://github.com/lewinfox/levitate/",TRUE,https://github.com/lewinfox/levitate,1157,27,2021-07-22T07:01:55Z,42.851851851851855
LexFindR,"Implements code to identify lexical competitors in a given list
  of words. We include many of the standard competitor types used in spoken word
  recognition research, such as functions to find cohorts, neighbors, and
  rhymes, amongst many others. The package includes documentation for using a
  variety of lexicon files, including those with form codes made up of multiple
  letters (i.e., phoneme codes) and also basic orthographies. Importantly, the
  code makes use of multiple CPU cores and vectorization when possible, making
  it extremely fast and able to handle large lexicons. Additionally, the package
  contains documentation for users to easily write new functions, allowing
  researchers to examine other relationships within a lexicon. <https://psyarxiv.com/8dyru/>.",2021-08-21,ZhaoBin Li,https://github.com/maglab-uconn/LexFindR,TRUE,https://github.com/maglab-uconn/lexfindr,367,1,2021-08-20T21:27:46Z,367
LexisNexisTools,"My PhD supervisor once told me that everyone doing newspaper
    analysis starts by writing code to read in files from the 'LexisNexis' newspaper
    archive (retrieved e.g., from <http://www.nexis.com/> or any of the partner
    sites). However, while this is a nice exercise I do recommend, not everyone has
    the time. This package takes files downloaded from the newspaper archive of
    'LexisNexis', reads them into R and offers functions for further processing.",2021-04-06,Johannes Gruber,https://github.com/JBGruber/LexisNexisTools,TRUE,https://github.com/jbgruber/lexisnexistools,22635,68,2021-05-14T07:30:24Z,332.86764705882354
LFApp,"Shiny apps for the quantitative analysis of images from lateral flow assays (LFAs). The images are segmented and background corrected and color intensities are extracted. The apps can be used to import and export intensity data and to calibrate LFAs by means of linear, loess, or gam models. The calibration models can further be saved and applied to intensity data from new images for determining concentrations.",2021-07-28,Filip Paskali,https://github.com/fpaskali/LFApp,TRUE,https://github.com/fpaskali/lfapp,459,1,2021-07-27T09:16:00Z,459
lfda,"Functions for performing and visualizing Local Fisher Discriminant
    Analysis(LFDA), Kernel Fisher Discriminant Analysis(KLFDA), and Semi-supervised
    Local Fisher Discriminant Analysis(SELF).",2019-07-31,Yuan Tang,https://github.com/terrytangyuan/lfda,TRUE,https://github.com/terrytangyuan/lfda,49868,73,2021-02-04T15:21:36Z,683.1232876712329
lgpr,"Interpretable nonparametric modeling of longitudinal data
    using additive Gaussian process regression. Contains functionality
    for inferring covariate effects and assessing covariate relevances.
    Models are specified using a convenient formula syntax, and can include
    shared, group-specific, non-stationary, heterogeneous and temporally
    uncertain effects. Bayesian inference for model parameters is performed
    using 'Stan'. The modeling approach and methods are described in detail in
    Timonen et al. (2021) <doi:10.1093/bioinformatics/btab021>.",2021-08-11,Juho Timonen,https://github.com/jtimonen/lgpr,TRUE,https://github.com/jtimonen/lgpr,818,17,2021-08-11T01:19:39Z,48.11764705882353
lgr,"A flexible, feature-rich yet light-weight logging
    framework based on 'R6' classes. It supports hierarchical loggers,
    custom log levels, arbitrary data fields in log events, logging to
    plaintext, 'JSON', (rotating) files, memory buffers. For extra
    appenders that support logging to databases, email and push
    notifications see the the package lgr.app.",2021-01-10,Stefan Fleck,https://s-fleck.github.io/lgr/,TRUE,https://github.com/s-fleck/lgr,389277,57,2021-07-22T09:19:25Z,6829.421052631579
lhs,Provides a number of methods for creating and augmenting Latin Hypercube Samples and Orthogonal Array Latin Hypercube Samples.,2020-10-05,Rob Carnell,https://github.com/bertcarnell/lhs,TRUE,https://github.com/bertcarnell/lhs,618415,16,2021-03-29T19:05:42Z,38650.9375
liayson,"Given an RNA-seq derived cell-by-gene matrix and an DNA-seq derived copy number segmentation, LIAYSON predicts the number of clones present in a tumor, their size, the copy number profile of each clone and the clone membership of each single cell (Andor, N. & Lau, B., et al. (2018) <doi:10.1101/445932>).",2021-06-21,Noemi Andor,"https://github.com/noemiandor/liayson,
https://groups.google.com/d/forum/liayson",TRUE,https://github.com/noemiandor/liayson,12473,4,2021-08-23T12:02:56Z,3118.25
libgeos,"Provides the Open Source Geometry Engine ('GEOS') as a
  C API that can be used to write high-performance C and C++
  geometry operations using R as an interface. Headers are provided
  to make linking to and using these functions from C++ code as
  easy and as safe as possible. This package contains an internal
  copy of the 'GEOS' library to guarantee the best possible
  consistency on multiple platforms.",2021-05-13,Dewey Dunnington,"https://paleolimbot.github.io/libgeos/,
https://github.com/paleolimbot/libgeos",TRUE,https://github.com/paleolimbot/libgeos,8384,14,2021-05-15T11:24:43Z,598.8571428571429
LibOPF,"The 'LibOPF' is a framework to develop pattern recognition techniques based on optimum-path forests (OPF), João P. Papa and Alexandre X. Falcão (2008) <doi:10.1007/978-3-540-89639-5_89>, with methods for supervised learning and data clustering.",2021-04-07,Rafael Junqueira Martarelli,"https://github.com/RafaelJM/LibOPF-in-R/,
https://github.com/jppbsi/LibOPF/wiki/,
https://www.ic.unicamp.br/~afalcao/libopf/",TRUE,https://github.com/rafaeljm/libopf-in-r,10722,1,2021-03-07T09:10:39Z,10722
libr,"Contains a set of functions to create data libraries,
    generate data dictionaries, and simulate a data step.
    The libname() function will load a directory of data into 
    a library in one line of code.  The dictionary() function
    will generate data dictionaries for individual
    data frames or an entire library.  And the datestep() function
    will perform row-by-row data processing.",2021-06-29,David J. Bosak,https://libr.r-sassy.org,TRUE,https://github.com/dbosak01/libr,3526,21,2021-08-08T19:50:43Z,167.9047619047619
librarian,"Automatically install, update, and load 'CRAN', 'GitHub', and 'Bioconductor' 
    packages in a single function call. By accepting bare unquoted names for packages, 
    it's easy to add or remove packages from the list.",2021-07-12,Desi Quintans,https://github.com/DesiQuintans/librarian,TRUE,https://github.com/desiquintans/librarian,20645,33,2021-07-14T04:29:42Z,625.6060606060606
libsoc,"Handle 'PharmML' (Pharmacometrics Markup Language) standard output (SO) XML files.
    SO files can be created, read, manipulated and written through a
    data binding from the XML structure to a tree structure of R objects.",2019-05-15,Rikard Nordgren,https://github.com/rikardn/libsoc,TRUE,https://github.com/rikardn/libsoc,14234,8,2021-08-06T14:07:25Z,1779.25
lidR,"Airborne LiDAR (Light Detection and Ranging) interface for data
    manipulation and visualization. Read/write 'las' and 'laz' files, computation
    of metrics in area based approach, point filtering, artificial point reduction,
    classification from geographic data, normalization, individual tree segmentation
    and other manipulations.",2021-06-21,Jean-Romain Roussel,https://github.com/Jean-Romain/lidR,TRUE,https://github.com/jean-romain/lidr,84134,335,2021-09-03T13:32:09Z,251.14626865671642
lifecontingencies,"Classes and methods that allow the user to manage life table,
    actuarial tables (also multiple decrements tables). Moreover, functions to easily
    perform demographic, financial and actuarial mathematics on life contingencies
    insurances calculations are contained therein. See Spedicato (2013)	<doi:10.18637/jss.v055.i10>.",2021-03-21,Giorgio Alfredo Spedicato,https://github.com/spedygiorgio/lifecontingencies,TRUE,https://github.com/spedygiorgio/lifecontingencies,55128,43,2021-03-20T22:36:04Z,1282.046511627907
lifecycle,"Manage the life cycle of your exported functions
    with shared conventions, documentation badges, and user-friendly
    deprecation warnings.",2021-02-15,Lionel Henry,"https://lifecycle.r-lib.org/, https://github.com/r-lib/lifecycle",TRUE,https://github.com/r-lib/lifecycle,27468386,70,2021-07-26T05:34:10Z,392405.5142857143
liftr,Persistent reproducible reporting by containerization of R Markdown documents.,2019-06-19,Nan Xiao,"https://nanx.me/liftr/, https://github.com/nanxstats/liftr",TRUE,https://github.com/nanxstats/liftr,21055,159,2021-07-17T01:24:53Z,132.42138364779873
liger,"Gene Set Enrichment Analysis (GSEA) is a computational method that determines whether an a priori defined set of genes shows statistically significant, concordant differences between two biological states. The original algorithm is detailed in Subramanian et al. with 'Java' implementations available through the Broad Institute (Subramanian et al. 2005 <doi:10.1073/pnas.0506580102>). The 'liger' package provides a lightweight R implementation of this enrichment test on a list of values (Fan et al., 2017 <doi:10.5281/zenodo.887386>). Given a list of values, such as p-values or log-fold changes derived from differential expression analysis or other analyses comparing biological states, this package enables you to test a priori defined set of genes for enrichment to enable interpretability of highly significant or high fold-change genes.",2021-01-25,Jean Fan,https://github.com/JEFworks/liger,TRUE,https://github.com/jefworks/liger,20431,47,2021-04-28T19:14:12Z,434.70212765957444
lightgbm,"Tree based algorithms can be improved by introducing boosting frameworks. 
    'LightGBM' is one such framework, based on Ke, Guolin et al. (2017) <https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision>.
    This package offers an R interface to work with it.
    It is designed to be distributed and efficient with the following advantages:
        1. Faster training speed and higher efficiency.
        2. Lower memory usage.
        3. Better accuracy.
        4. Parallel learning supported.
        5. Capable of handling large-scale data.
    In recognition of these advantages, 'LightGBM' has been widely-used in many winning solutions of machine learning competitions.
    Comparison experiments on public datasets suggest that 'LightGBM' can outperform existing boosting frameworks on both efficiency and accuracy, with significantly lower memory consumption. In addition, parallel experiments suggest that in certain circumstances, 'LightGBM' can achieve a linear speed-up in training time by using multiple machines.",2021-04-13,Guolin Ke,https://github.com/Microsoft/LightGBM,TRUE,https://github.com/microsoft/lightgbm,28677,12910,2021-09-03T14:34:13Z,2.2213013168086753
lightr,"Parse various reflectance/transmittance/absorbance spectra file
    formats to extract spectral data and metadata, as described in Gruson, White
    & Maia (2019) <doi:10.21105/joss.01857>. Among other formats, it can import
    files from 'Avantes' <https://www.avantes.com/>, 'CRAIC' 
    <https://www.microspectra.com/>, and 'OceanInsight' (formerly 'OceanOptics') 
    <https://www.oceaninsight.com/> brands.",2021-07-22,Hugo Gruson,"https://docs.ropensci.org/lightr/,
https://github.com/ropensci/lightr",TRUE,https://github.com/ropensci/lightr,16008,6,2021-08-23T08:59:45Z,2668
lightsout,"Lights Out is a puzzle game consisting of a grid of lights
    that are either on or off. Pressing any light will toggle it and its
    adjacent lights. The goal of the game is to switch all the lights off. This
    package provides an interface to play the game on different board sizes,
    both through the command line or with a visual application. Puzzles can
    also be solved using the automatic solver included. View a demo
    online at http://daattali.com/shiny/lightsout/.",2016-07-26,Dean Attali,https://github.com/daattali/lightsout,TRUE,https://github.com/daattali/lightsout,16247,34,2021-01-09T02:39:53Z,477.8529411764706
LIHKGr,"The goal of 'LIHKGr' is to scrape text data from LIHKG (<https://lihkg.com/>), the Hong Kong version of Reddit. LIHKG is currently protected by Google's 'reCAPTCHA', this package currently builds on 'RSelenium' and adopts a semi-manual approach to bypass it.",2020-09-08,Justin Chun-ting Ho,https://github.com/justinchuntingho/LIHKGr,TRUE,https://github.com/justinchuntingho/lihkgr,4302,11,2020-11-24T22:53:53Z,391.09090909090907
likert,"An approach to analyzing Likert response items, with an emphasis on visualizations. 
    The stacked bar plot is the preferred method for presenting Likert results. Tabular results
    are also implemented along with density plots to assist researchers in determining whether 
    Likert responses can be used quantitatively instead of qualitatively. See the likert(), 
    summary.likert(), and plot.likert() functions to get started.",2016-12-31,Jason Bryer,"http://jason.bryer.org/likert, http://github.com/jbryer/likert",TRUE,https://github.com/jbryer/likert,113449,229,2021-06-21T16:19:36Z,495.410480349345
lime,"When building complex models, it is often difficult to explain why
    the model should be trusted. While global measures such as accuracy are
    useful, they cannot be used for explaining why a model made a specific
    prediction. 'lime' (a port of the 'lime' 'Python' package) is a method for
    explaining the outcome of black box models by fitting a local model around
    the point in question an perturbations of this point. The approach is
    described in more detail in the article by Ribeiro et al. (2016) 
    <arXiv:1602.04938>.",2021-02-24,Thomas Lin Pedersen,"https://lime.data-imaginist.com, https://github.com/thomasp85/lime",TRUE,https://github.com/thomasp85/lime,109552,452,2021-02-24T15:26:31Z,242.3716814159292
liminal,"Compose interactive visualisations designed for exploratory 
        high-dimensional data analysis. With 'liminal' you can create linked
        interactive graphics to diagnose the quality of a dimension reduction
        technique and explore the global structure of a dataset with a tour. A
        complete description of the method is discussed in 
        ['Lee' & 'Laa' & 'Cook' (2020) <arXiv:2012.06077>].",2021-05-28,Stuart Lee,"https://github.com/sa-lee/liminal/,
https://sa-lee.github.io/liminal/",TRUE,https://github.com/sa-lee/liminal,1166,3,2021-05-28T08:49:28Z,388.6666666666667
LimnoPalettes,"Palettes generated from limnology based field and laboratory photos. Palettes can be used to generate color values to be used in any functions that calls for a color (i.e. ggplot(), plot(), flextable(), etc.). ",2020-10-19,Paul Julian,https://github.com/SwampThingPaul/LimnoPalettes,TRUE,https://github.com/swampthingpaul/limnopalettes,5080,3,2020-10-29T17:05:27Z,1693.3333333333333
linbin,"Short for 'linear binning', the linbin package provides functions
    for manipulating, binning, and plotting linearly referenced data. Although
    developed for data collected on river networks, it can be used with any interval
    or point data referenced to a 1-dimensional coordinate system. Flexible bin
    generation and batch processing makes it easy to compute and visualize variables
    at multiple scales, useful for identifying patterns within and between variables
    and investigating the influence of scale of observation on data interpretation.",2021-04-20,Ethan Z. Welty,https://github.com/ezwelty/linbin,TRUE,https://github.com/ezwelty/linbin,17801,1,2021-04-20T05:13:13Z,17801
lineartestr,"Tests whether the linear hypothesis of a model
    is correct specified using Dominguez-Lobato test. Also Ramsey's RESET (Regression Equation 
    Specification Error Test) test is implemented and Wald tests can be carried out. 
    Although RESET test is widely used to 
    test the linear hypothesis of a model, Dominguez and Lobato (2019) proposed a novel 
    approach that generalizes well known specification tests such as Ramsey's. This test 
    relies on wild-bootstrap; this package implements this approach to be 
    usable with any function that fits linear models and is compatible with 
    the update() function such as 'stats'::lm(), 'lfe'::felm() and 'forecast'::Arima(), 
    for ARMA (autoregressive–moving-average) models. 
    Also the package can handle custom statistics such as Cramer von Mises and Kolmogorov 
    Smirnov, described by the authors, and custom distributions such as Mammen (discrete 
    and continuous) and Rademacher.
    Manuel A. Dominguez & Ignacio N. Lobato (2019) <doi:10.1080/07474938.2019.1687116>.",2020-06-12,Federico Garza,https://github.com/FedericoGarza/lineartestr,TRUE,https://github.com/federicogarza/lineartestr,4669,2,2021-01-24T17:26:01Z,2334.5
linemap,"Create maps made of lines. The package contains two functions:
    linemap() and getgrid(). linemap() displays a map made of lines using a
    data frame of gridded data. getgrid() transforms a set of polygons
    (sf objects) into a suitable data frame for linemap().",2021-01-19,Timothée Giraud,https://github.com/riatelab/linemap,TRUE,https://github.com/riatelab/linemap,16013,100,2021-01-19T11:05:20Z,160.13
lineup,"Tools for detecting and correcting sample mix-ups between two sets
    of measurements, such as between gene expression data on two tissues.
    Broman et al. (2015) <doi:10.1534/g3.115.019778>.",2020-11-17,Karl W Broman,https://github.com/kbroman/lineup,TRUE,https://github.com/kbroman/lineup,20024,2,2021-08-12T14:16:40Z,10012
lineup2,"Tools for detecting and correcting sample mix-ups between two sets
    of measurements, such as between gene expression data on two
    tissues. This is a revised version of the 'lineup' package, to be
    more general and not tied to the 'qtl' package.",2021-06-15,Karl W Broman,https://github.com/kbroman/lineup2,TRUE,https://github.com/kbroman/lineup2,3911,0,2021-06-15T03:02:52Z,NA
lingmatch,"Measure similarity between texts. Offers a variety of processing
  tools and similarity metrics to facilitate flexible representation of texts and matching.
  Implements forms of Language Style Matching (Ireland & Pennebaker, 2010) <doi:10.1037/a0020386>
  and Latent Semantic Analysis (Landauer & Dumais, 1997) <doi:10.1037/0033-295X.104.2.211>.",2021-04-22,Micah Iserman,https://github.com/miserman/lingmatch,TRUE,https://github.com/miserman/lingmatch,2348,2,2021-07-19T04:23:04Z,1174
lingtypology,"Provides R with the Glottolog database <https://glottolog.org/> and some more abilities for purposes of linguistic mapping. The Glottolog database contains the catalogue of languages of the world. This package helps researchers to make a linguistic maps, using philosophy of the Cross-Linguistic Linked Data project <https://clld.org/>, which allows for while at the same time facilitating uniform access to the data across publications. A tutorial for this package is available on GitHub pages <https://docs.ropensci.org/lingtypology/> and package vignette. Maps created by this package can be used both for the investigation and linguistic teaching. In addition, package provides an ability to download data from typological databases such as WALS, AUTOTYP and some others and to create your own database website.",2021-05-15,George Moroz,"https://CRAN.R-project.org/package=lingtypology,
https://github.com/ropensci/lingtypology/,
https://ropensci.github.io/lingtypology/",TRUE,https://github.com/ropensci/lingtypology,24278,41,2021-06-05T16:44:17Z,592.1463414634146
link2GI,Functions to simplify the linking of open source GIS and remote sensing related command line interfaces.,2021-09-03,Chris Reudenbach,"https://github.com/r-spatial/link2GI/,
https://r-spatial.github.io/link2GI/",TRUE,https://github.com/r-spatial/link2gi,32271,20,2021-07-11T11:33:46Z,1613.55
linkcomm,"Link communities reveal the nested and overlapping
    structure in networks, and uncover the key nodes that form connections
    to multiple communities. linkcomm provides a set of tools for
    generating, visualizing, and analysing link communities in networks of
    arbitrary size and type. The linkcomm package also includes tools for
    generating, visualizing, and analysing Overlapping Cluster Generator
    (OCG) communities. Kalinka and Tomancak (2011) <doi:10.1093/bioinformatics/btr311>.",2021-02-04,Alex T. Kalinka,"https://alextkalinka.github.io/linkcomm/,
https://github.com/alextkalinka/linkcomm",TRUE,https://github.com/alextkalinka/linkcomm,29542,2,2021-02-04T16:00:17Z,14771
linkprediction,"Implementations of most of the existing proximity-based methods of 
  link prediction in graphs. Among the 20 implemented methods are e.g.:
  Adamic L. and Adar E. (2003) <doi:10.1016/S0378-8733(03)00009-1>,
  Leicht E., Holme P., Newman M. (2006) <doi:10.1103/PhysRevE.73.026120>,
  Zhou T. and Zhang Y (2009) <doi:10.1140/epjb/e2009-00335-8>, and
  Fouss F., Pirotte A., Renders J., and Saerens M. (2007) <doi:10.1109/TKDE.2007.46>.",2018-10-19,Michal Bojanowski,https://github.com/recon-icm/linkprediction,TRUE,https://github.com/recon-icm/linkprediction,12883,8,2020-12-28T16:39:26Z,1610.375
linl,"A 'LaTeX' Letter class for 'rmarkdown', using the
 'pandoc-letter' template adapted for use with 'markdown'.",2019-10-23,Dirk Eddelbuettel and Aaron Wolen,http://dirk.eddelbuettel.com/code/linl.html,TRUE,https://github.com/eddelbuettel/linl,19039,96,2021-04-04T21:45:57Z,198.32291666666666
linne,Conveniently generate 'CSS' using R code.,2020-10-26,John Coene,https://linne.john-coene.com/,TRUE,https://github.com/johncoene/linne,3482,70,2021-05-15T18:11:05Z,49.74285714285714
linpk,"Generate concentration-time profiles from linear pharmacokinetic
  (PK) systems, possibly with first-order absorption or zero-order infusion,
  possibly with one or more peripheral compartments, and possibly under
  steady-state conditions. Single or multiple doses may be specified. Secondary
  (derived) PK parameters (e.g. Cmax, Ctrough, AUC, Tmax, half-life, etc.) are
  computed.",2021-04-27,Benjamin Rich,https://github.com/benjaminrich/linpk,TRUE,https://github.com/benjaminrich/linpk,14549,8,2021-04-27T11:50:08Z,1818.625
lintools,"Variable elimination (Gaussian elimination, Fourier-Motzkin elimination), 
    Moore-Penrose pseudoinverse, reduction to reduced row echelon form, value substitution,  
    projecting a vector on the convex polytope described by a system of (in)equations, 
    simplify systems by removing spurious columns and rows and collapse implied equalities, 
    test if a matrix is totally unimodular, compute variable ranges implied by linear
    (in)equalities.",2021-04-29,Mark van der Loo,https://github.com/data-cleaning/lintools,TRUE,https://github.com/data-cleaning/lintools,23375,1,2021-04-29T10:20:02Z,23375
lintr,"Checks adherence to a given style, syntax errors and possible
    semantic issues.  Supports on the fly checking of R code edited with 'RStudio IDE', 'Emacs',
    'Vim', 'Sublime Text' and 'Atom'.",2020-02-19,Jim Hester,https://github.com/jimhester/lintr,TRUE,https://github.com/jimhester/lintr,1141404,879,2021-08-19T21:22:53Z,1298.5255972696245
lisa,"Contains 128 palettes from Color Lisa. All palettes are based on 
    masterpieces from the worlds greatest artists. For more information, see 
    <http://colorlisa.com/>.",2020-09-20,Tyler Littlefield,https://github.com/tyluRp/lisa,TRUE,https://github.com/tylurp/lisa,12941,40,2020-09-20T05:41:06Z,323.525
listcompr,"Syntactic shortcuts for creating synthetic lists, vectors, 
    data frames, and matrices using list comprehension.",2021-06-09,Patrick Roocks,https://github.com/patrickroocks/listcompr,TRUE,https://github.com/patrickroocks/listcompr,3096,2,2021-08-29T19:49:33Z,1548
listdown,Programmatically create R Markdown documents from lists.,2020-12-07,Michael J. Kane,https://github.com/kaneplusplus/listdown,TRUE,https://github.com/kaneplusplus/listdown,5879,13,2021-06-14T19:09:20Z,452.2307692307692
listenv,"List environments are environments that have list-like properties.  For instance, the elements of a list environment are ordered and can be accessed and iterated over using index subsetting, e.g. 'x <- listenv(a = 1, b = 2); for (i in seq_along(x)) x[[i]] <- x[[i]] ^ 2; y <- as.list(x)'.",2019-12-05,Henrik Bengtsson,https://github.com/HenrikBengtsson/listenv,TRUE,https://github.com/henrikbengtsson/listenv,2809953,18,2021-08-06T07:45:07Z,156108.5
littler,"A scripting and command-line front-end
 is provided by 'r' (aka 'littler') as a lightweight binary wrapper around
 the GNU R language and environment for statistical computing and graphics.
 While R can be used in batch mode, the r binary adds full support for
 both 'shebang'-style scripting (i.e. using a  hash-mark-exclamation-path
 expression as the first line in scripts) as well as command-line use in
 standard Unix pipelines. In other words, r provides the R language without
 the environment.",2021-07-24,Dirk Eddelbuettel and Jeff Horner,"https://github.com/eddelbuettel/littler,
https://dirk.eddelbuettel.com/code/littler.html",TRUE,https://github.com/eddelbuettel/littler,95103,247,2021-08-31T17:31:57Z,385.0323886639676
LLSR,"Originally design to characterise Aqueous Two Phase Systems, LLSR provide a simple way to analyse experimental data and obtain phase diagram parameters, among other properties, systematically. The package will include (every other update) new functions in order to comprise useful tools in liquid-liquid extraction research.",2021-02-17,Diego F Coelho,https://CRAN.R-project.org/package=LLSR,TRUE,https://github.com/diegofcoelho/llsr,22760,0,2021-02-01T16:36:41Z,NA
lme4,"Fit linear and generalized linear mixed-effects models.
    The models and their components are represented using S4 classes and
    methods.  The core computational algorithms are implemented using the
    'Eigen' C++ library for numerical linear algebra and 'RcppEigen' ""glue"".",2021-06-22,Douglas Bates,https://github.com/lme4/lme4/,TRUE,https://github.com/lme4/lme4,10624883,463,2021-08-21T19:32:52Z,22947.911447084232
lmeInfo,"Provides analytic derivatives and information matrices for
    fitted linear mixed effects (lme) models and generalized least squares (gls) models
    estimated using lme() (from package 'nlme') and gls() (from package 'nlme'), respectively.
    The package includes functions for estimating the sampling variance-covariance of variance
    component parameters using the inverse Fisher information. The variance components include
    the parameters of the random effects structure (for lme models), the variance structure,
    and the correlation structure. The expected and average forms of the Fisher information matrix
    are used in the calculations, and models estimated by full maximum likelihood or
    restricted maximum likelihood are supported. The package also includes a function for estimating
    standardized mean difference effect sizes (Pustejovsky, Hedges, and Shadish (2014) <DOI:10.3102/1076998614547577>)
    based on fitted lme or gls models.",2021-02-02,James Pustejovsky,https://jepusto.github.io/lmeInfo/,TRUE,https://github.com/jepusto/lmeinfo,12689,1,2021-08-16T21:30:12Z,12689
lmeresampler,"Bootstrap routines for nested linear mixed effects models fit using
    either 'lme4' or 'nlme'. The provided 'bootstrap()' function implements the
    parametric, residual, cases, random effect block (REB), and wild bootstrap 
    procedures. An overview of these procedures can be found 
    in Van der Leeden et al. (2008) <doi: 10.1007/978-0-387-73186-5_11>, 
    Carpenter, Goldstein & Rasbash (2003) <doi: 10.1111/1467-9876.00415>,
    and Chambers & Chandra (2013) <doi: 10.1080/10618600.2012.681216>.",2021-05-01,Adam Loy,https://github.com/aloy/lmeresampler,TRUE,https://github.com/aloy/lmeresampler,13336,28,2021-08-03T16:41:48Z,476.2857142857143
lmerTest,"Provides p-values in type I, II or III anova and summary tables
    for lmer model fits (cf. lme4) via Satterthwaite's degrees of freedom method. A
    Kenward-Roger method is also available via the pbkrtest package. Model selection
    methods include step, drop1 and anova-like tables for random effects (ranova).
    Methods for Least-Square means (LS-means) and tests of linear contrasts of fixed
    effects are also available.",2020-10-23,Rune Haubo Bojesen Christensen,https://github.com/runehaubo/lmerTestR,TRUE,https://github.com/runehaubo/lmertestr,1038427,29,2020-10-23T06:59:55Z,35807.8275862069
LMfilteR,"We present a method based on filtering algorithms to estimate the parameters of linear, i.e. the coefficients and the variance of the error term. The proposed algorithms make use of Particle Filters following Ristic, B., Arulampalam, S., Gordon, N. (2004, ISBN: 158053631X) resampling methods. Parameters of logistic regression models are also estimated using an evolutionary particle filter method.",2020-10-12,Christian Llano Robayo,NA,TRUE,https://github.com/chrisscod/lmfilter,13804,0,2020-10-12T07:24:40Z,NA
lmm,"It implements Expectation/Conditional Maximization Either (ECME)
             and rapidly converging algorithms as well as
             Bayesian inference for linear mixed models, 
             which is described in Schafer, J.L. (1998)
             ""Some improved procedures for linear mixed models"".
             Dept. of Statistics, The Pennsylvania State University.",2020-07-06,Original by Joseph L. Schafer,https://github.com/jinghuazhao/R,TRUE,https://github.com/jinghuazhao/r,30165,6,2021-08-27T21:09:36Z,5027.5
LMMELSM,"In addition to modeling the expectation (location) of an outcome, mixed effects location scale models (MELSMs) include submodels on the variance components (scales) directly. This allows models on the within-group variance with mixed effects, and between-group variances with fixed effects. The MELSM can be used to model volatility, intraindividual variance, uncertainty, measurement error variance, and more. Multivariate MELSMs (MMELSMs) extend the model to include multiple correlated outcomes, and therefore multiple locations and scales. The latent multivariate MELSM (LMMELSM) further includes multiple correlated latent variables as outcomes. This package implements two-level mixed effects location scale models on multiple observed or latent outcomes, and between-group variance modeling. Williams, Martin, Liu, and Rast (2020) <doi:10.1027/1015-5759/a000624>. Hedeker, Mermelstein, and Demirtas (2008) <doi:10.1111/j.1541-0420.2007.00924.x>.",2021-03-13,Stephen Martin,NA,TRUE,https://github.com/stephensrmmartin/lmmelsm,1566,0,2021-03-18T00:14:17Z,NA
lmQCM,"Implementation based on Zhang, Jie & Huang, Kun (2014) <doi:10.4137/CIN.S14021> Normalized ImQCM: An Algorithm for Detecting Weak Quasi-Cliques in Weighted Graph with Applications in Gene Co-Expression Module Discovery in Cancers. Cancer informatics, 13, CIN-S14021.",2021-03-03,Zhi Huang,https://github.com/huangzhii/lmQCM/,TRUE,https://github.com/huangzhii/lmqcm,13204,2,2021-03-03T00:39:59Z,6602
lmtp,"Non-parametric estimators for casual effects based on longitudinal modified treatment 
  policies as described in Diaz, Williams, and Hoffman (<arXiv:2006.01366>), traditional point treatment, 
  and traditional longitudinal effects. Continuous, binary, and categorical treatments are allowed as well are 
  censored outcomes. The treatment mechanism is estimated via a density ratio classification procedure 
  irrespective of treatment variable type. For both continuous and binary outcomes, additive treatment effects 
  can be calculated and relative risks and odds ratios may be calculated for binary outcomes.  ",2021-08-18,Nicholas Williams,https://github.com/nt-williams/lmtp,TRUE,https://github.com/nt-williams/lmtp,5338,21,2021-08-16T19:26:16Z,254.1904761904762
loadr,"Provides intuitive functions for loading objects into
    environments, encouraging less cluttered workspaces and sharing variables
    with large or reusable data across users and sessions. The user
    provides named variables which are loaded into the variable environment for
    later retrieval.",2021-04-17,Nathan Sheffield,https://github.com/databio/loadr,TRUE,https://github.com/databio/loadr,12784,3,2021-04-19T21:59:20Z,4261.333333333333
lobstr,"A set of tools for inspecting and understanding R data
    structures inspired by str(). Includes ast() for visualizing abstract 
    syntax trees, ref() for showing shared references, cst() for showing 
    call stack trees, and obj_size() for computing object sizes.",2019-07-02,Hadley Wickham,https://github.com/r-lib/lobstr,TRUE,https://github.com/r-lib/lobstr,350738,260,2021-05-17T17:25:51Z,1348.9923076923078
localFDA,"Implementation of a theoretically supported alternative to k-nearest neighbors for functional data 
  to solve problems of estimating unobserved segments of a partially observed functional data sample, 
  functional classification and outlier detection. The approximating neighbor curves are piecewise functions built from a functional sample. 
  Instead of a distance on a function space we use a locally defined distance function that satisfies stabilization criteria. 
  The package allows the implementation of the methodology and the replication of the results in Elías, A., Jiménez, R. and Yukich, J. (2020) <arXiv:2007.16059>.",2020-09-30,Antonio Elías,https://github.com/aefdz/localFDA,TRUE,https://github.com/aefdz/localfda,4998,0,2021-01-04T09:32:42Z,NA
localModel,"Local explanations of machine learning models describe, how features contributed to a single prediction. 
    This package implements an explanation method based on LIME 
    (Local Interpretable Model-agnostic Explanations, 
    see Tulio Ribeiro, Singh, Guestrin (2016) <doi:10.1145/2939672.2939778>) in which interpretable
    inputs are created based on local rather than global behaviour of each original feature.",2019-12-18,Mateusz Staniak,https://github.com/ModelOriented/localModel,TRUE,https://github.com/modeloriented/localmodel,12392,13,2021-03-07T23:21:53Z,953.2307692307693
locatexec,"A set of functions to locate some programs 
 available on the user machine. The package provides functions to locate 
 'Node.js', 'npm', 'LibreOffice', 'Microsoft Word', 'Microsoft PowerPoint', 
 'Microsoft Excel', 'Python', 'pip', 'Mozilla Firefox' and 'Google Chrome'.
 User can test the availability of a program with eventually a version 
 and call it with function system2() or system(). This allows the use of 
 a single function to retrieve the path to a program regardless of the 
 operating system and its configuration.",2021-05-19,David Gohel,NA,TRUE,https://github.com/ardata-fr/locatexec,5544,12,2021-05-18T23:04:57Z,462
log4r,"The log4r package is meant to provide a fast, lightweight,
  object-oriented approach to logging in R based on the widely-emulated
  'log4j' system and etymology.",2020-01-18,Aaron Jacobs,https://github.com/johnmyleswhite/log4r,TRUE,https://github.com/johnmyleswhite/log4r,235544,71,2021-08-20T15:08:12Z,3317.521126760563
logbin,"Methods for fitting log-link GLMs and GAMs to binomial data,
    including EM-type algorithms with more stable convergence properties than standard methods.",2021-08-09,Mark W. Donoghoe,https://github.com/mdonoghoe/logbin,TRUE,https://github.com/mdonoghoe/logbin,20956,9,2021-08-09T21:20:40Z,2328.4444444444443
logger,"Inspired by the the 'futile.logger' R package and 'logging' Python module, this utility provides a flexible and extensible way of formatting and delivering log messages with low overhead.",2021-07-06,Gergely Daróczi,https://daroczig.github.io/logger/,TRUE,https://github.com/daroczig/logger,315299,186,2021-04-14T15:46:30Z,1695.1559139784947
loggit,"
    An effortless 'ndjson' (newline-delimited 'JSON') logger, with two primary
    log-writing interfaces. It provides a set of wrappings for base R's
    message(), warning(), and stop() functions that maintain identical
    functionality, but also log the handler message to an 'ndjson' log file.
    'loggit' also exports its internal 'loggit()' function for powerful and
    configurable custom logging. No change in existing code is necessary to use
    this package, and should only require additions to fully leverage the power
    of the logging system. 'loggit' also provides a log reader for reading an
    'ndjson' log file into a data frame, log rotation, and live echo of the
    'ndjson' log messages to terminal 'stdout' for log capture by external
    systems (like containers). 'loggit' is ideal for Shiny apps, data pipelines,
    modeling work flows, and more. Please see the vignettes for detailed example
    use cases.",2021-02-28,Ryan Price,https://github.com/ryapric/loggit,TRUE,https://github.com/ryapric/loggit,88547,30,2021-03-07T21:52:05Z,2951.5666666666666
logistf,"Fit a logistic regression model using Firth's bias reduction method, equivalent to penalization of the log-likelihood by the Jeffreys 
	prior. Confidence intervals for regression coefficients can be computed by penalized profile likelihood. Firth's method was proposed as ideal
	solution to the problem of separation in logistic regression, see Heinze and Schemper (2002) <doi:10.1002/sim.1047>. If needed, the bias reduction can be turned off such that ordinary
	maximum likelihood logistic regression is obtained. Two new modifications of Firth's method, FLIC and FLAC, lead to unbiased predictions and are now available
	in the package as well, see Puhr, Heinze, Nold, Lusa and Geroldinger (2017) <doi:10.1002/sim.7273>.",2020-09-16,Georg Heinze,https://cemsiis.meduniwien.ac.at/en/kb/science-research/software/statistical-software/fllogistf/,TRUE,https://github.com/georgheinze/logistf,514470,3,2021-06-21T13:11:19Z,171490
logisticPCA,"Dimensionality reduction techniques for binary data including
    logistic PCA.",2016-03-14,Andrew J. Landgraf,https://github.com/andland/logisticPCA,TRUE,https://github.com/andland/logisticpca,20306,36,2020-09-07T18:47:03Z,564.0555555555555
logitr,"Estimation of multinomial (MNL) and mixed logit (MXL) models in R. Models can be estimated using ""Preference"" space or ""Willingness-to-pay"" (WTP) space utility parameterizations. Weighted models can also be estimated. An option is available to run a multistart optimization loop with random starting points in each iteration, which is useful for non-convex problems like MXL models or models with WTP space utility parameterizations. The main optimization loop uses the 'nloptr' package to minimize the negative log-likelihood function. Additional functions are available for computing and comparing WTP from both preference space and WTP space models and for predicting expected choices and choice probabilities for sets of alternatives based on an estimated model. MXL models assume uncorrelated heterogeneity covariances and are estimated using maximum simulated likelihood based on the algorithms in Train (2009) ""Discrete Choice Methods with Simulation, 2nd Edition"" <doi:10.1017/CBO9780511805271>.",2021-08-13,John Helveston,https://github.com/jhelvy/logitr,TRUE,https://github.com/jhelvy/logitr,3217,13,2021-08-30T17:08:33Z,247.46153846153845
logmult,"Functions to fit log-multiplicative models using 'gnm', with
  support for convenient printing, plots, and jackknife/bootstrap
  standard errors. For complex survey data, models can be fitted from
  design objects from the 'survey' package. Currently supported models
  include UNIDIFF (Erikson & Goldthorpe), a.k.a. log-multiplicative
  layer effect model (Xie), and several association models: Goodman's
  row-column association models of the RC(M) and RC(M)-L families
  with one or several dimensions; two skew-symmetric association
  models proposed by Yamaguchi and by van der Heijden & Mooijaart.
  Functions allow computing the intrinsic association coefficient
  (and therefore the Altham index), including via the Bayes shrinkage
  estimator proposed by Zhou; and the RAS/IPF/Deming-Stephan algorithm.",2020-12-10,Milan Bouchet-Valat,https://github.com/nalimilan/logmult,TRUE,https://github.com/nalimilan/logmult,24913,2,2020-12-10T09:50:47Z,12456.5
lognorm,"The lognormal distribution  
  (Limpert et al. (2001) <doi:10.1641/0006-3568(2001)051%5B0341:lndats%5D2.0.co;2>)
  can characterize uncertainty that is bounded by zero.
  This package provides estimation of distribution parameters, computation of
  moments and other basic statistics, and an approximation of the distribution
  of the sum of several correlated lognormally distributed variables 
  (Lo 2013 <doi:10.12988/ams.2013.39511>) and the approximation of the 
  difference of two correlated lognormally distributed variables
  (Lo 2012 <doi:10.1155/2012/838397>).",2021-03-10,Thomas Wutzler,https://github.com/bgctw/lognorm,TRUE,https://github.com/bgctw/lognorm,16947,5,2021-03-10T18:26:55Z,3389.4
logr,"Contains functions to help create log files.  The 
    package aims to overcome the difficulty of the base R sink() command.  The
    log_print() function will print to both the console and the file log, 
    without interfering in other write operations.",2021-07-30,David Bosak,https://logr.r-sassy.org,TRUE,https://github.com/dbosak01/logr,9934,8,2021-08-08T01:39:15Z,1241.75
lolog,"Estimation of Latent Order Logistic (LOLOG) Models for Networks.
    LOLOGs are a flexible and fully general class of statistical graph models. 
    This package provides functions for performing MOM, GMM and variational 
    inference. Visual diagnostics and goodness of fit metrics are provided. 
    See Fellows (2018) <arXiv:1804.04583> for a detailed description of the methods.",2021-07-01,Ian E. Fellows,https://github.com/statnet/lolog,TRUE,https://github.com/statnet/lolog,35372,3,2021-06-23T18:58:53Z,11790.666666666666
lolR,"Supervised learning techniques designed for the situation when the dimensionality exceeds the sample size have a tendency to overfit as the dimensionality of the data increases. To remedy this High dimensionality; low sample size (HDLSS) situation, we attempt to learn a lower-dimensional representation of the data before learning a classifier. That is, we project the data to a situation where the dimensionality is more manageable, and then are able to better apply standard classification or clustering techniques since we will have fewer dimensions to overfit. A number of previous works have focused on how to strategically reduce dimensionality in the unsupervised case, yet in the supervised HDLSS regime, few works have attempted to devise dimensionality reduction techniques that leverage the labels associated with the data. In this package and the associated manuscript Vogelstein et al. (2017) <arXiv:1709.01233>, we provide several methods for feature extraction, some utilizing labels and some not, along with easily extensible utilities to simplify cross-validative efforts to identify the best feature extraction method. Additionally, we include a series of adaptable benchmark simulations to serve as a standard for future investigative efforts into supervised HDLSS. Finally, we produce a comprehensive comparison of the included algorithms across a range of benchmark simulations and real data applications.",2020-06-26,Eric Bridgeford,https://github.com/neurodata/lol,TRUE,https://github.com/neurodata/lol,7134,13,2021-02-23T06:15:00Z,548.7692307692307
longpower,"Compute power and sample size for linear models of longitudinal
    data. Supported models include mixed-effects models and models fit by
    generalized least squares and generalized estimating equations. Relevant
    formulas are derived by Liu and Liang (1997) <DOI:10.2307/2533554>, 
    Diggle et al (2002) <ISBN:9780199676750>, and Lu, Luo, and Chen (2008)
    <DOI:10.2202/1557-4679.1098>.",2021-04-20,Michael C. Donohue,https://github.com/mcdonohue/longpower,TRUE,https://github.com/mcdonohue/longpower,28699,1,2021-04-19T17:33:58Z,28699
loo,"Efficient approximate leave-one-out cross-validation (LOO)
    for Bayesian models fit using Markov chain Monte Carlo, as 
    described in Vehtari, Gelman, and Gabry (2017) 
    <doi:10.1007/s11222-016-9696-4>. 
    The approximation uses Pareto smoothed importance sampling (PSIS), 
    a new procedure for regularizing importance weights. 
    As a byproduct of the calculations, we also obtain approximate 
    standard errors for estimated predictive errors and for the comparison 
    of predictive errors between models. The package also provides methods 
    for using stacking and other model weighting techniques to average 
    Bayesian predictive distributions.",2020-12-09,Jonah Gabry,"https://mc-stan.org/loo/, https://discourse.mc-stan.org",TRUE,https://github.com/stan-dev/loo,1676620,110,2021-06-30T22:26:51Z,15242
lookup,Simple functions to lookup items in key-value pairs. See  Mehta (2021) <doi:10.1007/978-1-4842-6613-7_6>.,2021-04-14,Kevin Wright,https://kwstat.github.io/lookup/,TRUE,https://github.com/kwstat/lookup,3162,2,2021-04-12T18:18:48Z,1581
loon,An extendable toolkit for interactive data visualization and exploration.,2021-06-14,R. Wayne Oldford,https://great-northern-diver.github.io/loon/,TRUE,https://github.com/great-northern-diver/loon,26131,35,2021-07-23T19:43:03Z,746.6
loon.ggplot,"It provides a bridge between the 'loon' and  'ggplot2' packages. Data analysts who value the grammar pipeline provided by 'ggplot2' can turn these static plots into interactive 'loon' plots. Conversely, data analysts who explore data interactively with 'loon' can turn any 'loon' plot into a 'ggplot2' plot structure. The function 'loon.ggplot()' is applied to one plot structure will return the other.",2021-06-10,Zehao Xu,NA,TRUE,https://github.com/great-northern-diver/loon.ggplot,7021,19,2021-07-23T19:39:51Z,369.5263157894737
loon.tourr,Implement tour algorithms in interactive graphical system 'loon'.,2021-07-25,Zehao Xu,NA,TRUE,https://github.com/z267xu/loon.tourr,1754,0,2021-07-23T13:56:01Z,NA
loose.rock,"Collection of functions to improve work-flow in survival analysis and data science.
             The package features include: the generation of balanced datasets, live retrieval of 
             protein coding genes from two public databases, generation of random 
             matrix based on covariance matrix, cache function to store function results.
             This work was supported by two grants from the Portuguese Foundation for Science
             and technology, and the EU Commission under SOUND project.",2021-04-29,André Veríssimo,https://github.com/averissimo/loose.rock,TRUE,https://github.com/averissimo/loose.rock,14257,1,2021-04-29T14:45:52Z,14257
lorentz,"The Lorentz transform in special relativity; also the gyrogroup structure of three-velocities.  Includes active and passive transforms and the ability to use units in which the speed of light is not one.  For general relativity, see the
  'schwarzschild' package.",2020-09-24,Robin K. S. Hankin,https://github.com/RobinHankin/lorentz,TRUE,https://github.com/robinhankin/lorentz,14428,2,2021-05-30T00:29:53Z,7214
lotri,"Provides a simple mechanism to specify a symmetric block diagonal matrices (often 
  used for covariance matrices).  This is based on the domain specific language implemented
  in 'nlmixr' but expanded to create matrices in R generally instead of specifying parts of matrices
  to estimate.",2021-01-05,Matthew L. Fidler,https://github.com/nlmixrdevelopment/lotri,TRUE,https://github.com/nlmixrdevelopment/lotri,26750,0,2021-04-18T13:27:32Z,NA
LPDynR,"It uses 'phenological' and productivity-related variables derived from time series of vegetation 
    indexes, such as the Normalized Difference Vegetation Index, to assess ecosystem dynamics and change, which 
    eventually might drive to land degradation. The final result of the Land Productivity Dynamics indicator 
    is a categorical map with 5 classes of land productivity dynamics, ranging from declining to increasing 
    productivity. See <https://eartharxiv.org/repository/view/2294/> for a description of the methods used in 
    the package to calculate the indicator.",2021-05-19,Xavier Rotllan-Puig,https://github.com/xavi-rp/LPDynR,TRUE,https://github.com/xavi-rp/lpdynr,2763,4,2021-05-20T07:17:36Z,690.75
lpirfs,"Provides functions to estimate and plot linear as well as nonlinear impulse 
             responses based on local projections by Jordà (2005) <doi:10.1257/0002828053828518>.",2021-03-23,Philipp Adämmer,NA,TRUE,https://github.com/adaemmerp/lpirfs,26289,16,2021-07-08T14:07:52Z,1643.0625
LPS,"An implementation of the Linear Predictor Score approach, as initiated by Radmacher et al. (J Comput Biol 2001) and enhanced by Wright et al. (PNAS 2003) for gene expression signatures. Several tools for unsupervised clustering of gene expression data are also provided.",2021-05-29,Sylvain Mareschal,https://bioinformatics.ovsa.fr/LPS,TRUE,https://github.com/maressyl/r.lps,20482,1,2021-05-29T11:13:26Z,20482
LREP,"The programs were developed for estimation of parameters and testing exponential versus Pareto distribution during our work on hydrologic extremes. See Kozubowski, T.J., A.K. Panorska, F. Qeadan, and A. Gershunov (2007) <doi:10.1080/03610910802439121>, and Panorska, A.K., A. Gershunov, and T.J. Kozubowski (2007) <doi:10.1007/978-0-387-34918-3_26>.",2021-08-17,Jiqiang Wu,NA,TRUE,https://github.com/jiqiaingwu/lrep,525,0,2021-08-17T03:34:49Z,NA
lsasim,"Provides functions to simulate data from large-scale educational
  assessments, including background questionnaire data and cognitive item
  responses that adhere to a multiple-matrix sampled design. The theoretical
  foundation can be found on
  Matta, T.H., Rutkowski, L., Rutkowski, D. et al. (2018)
  <doi:10.1186/s40536-018-0068-8>.",2021-06-24,Waldir Leoncio,NA,TRUE,https://github.com/tmatta/lsasim,17328,5,2021-06-24T09:44:21Z,3465.6
LSTS,A set of functions that allow stationary analysis and locally stationary time series analysis.,2021-07-29,Mauricio Vargas,https://pacha.dev/LSTS/,TRUE,https://github.com/pachadotdev/lsts,12305,1,2021-08-31T02:35:44Z,12305
LSX,"A word embeddings-based semisupervised model for document scaling Watanabe (2020) <doi:10.1080/19312458.2020.1832976>.
    LSS allows users to analyze large and complex corpora on arbitrary dimensions with seed words exploiting efficiency of word embeddings (SVD, Glove).
    It can generate word vectors on a users-provided corpus or incorporate a pre-trained word vectors.",2021-07-20,Kohei Watanabe,NA,TRUE,https://github.com/koheiw/lsx,6673,34,2021-08-02T22:36:04Z,196.26470588235293
ltm,"Analysis of multivariate dichotomous and polytomous data using latent trait models under the Item Response Theory approach. It includes the Rasch, the Two-Parameter Logistic, the Birnbaum's Three-Parameter, the Graded Response, and the Generalized Partial Credit Models.",2018-04-17,Dimitris Rizopoulos,https://github.com/drizopoulos/ltm,TRUE,https://github.com/drizopoulos/ltm,182976,23,2020-12-15T14:55:03Z,7955.478260869565
lubridate,"Functions to work with date-times and time-spans:
    fast and user friendly parsing of date-time data, extraction and
    updating of components of a date-time (years, months, days, hours,
    minutes, and seconds), algebraic manipulation on date-time and
    time-span objects. The 'lubridate' package has a consistent and
    memorable syntax that makes working with dates easy and fun.  Parts of
    the 'CCTZ' source code, released under the Apache 2.0 License, are
    included in this package. See <https://github.com/google/cctz> for
    more details.",2021-02-26,Vitalie Spinu,"https://lubridate.tidyverse.org,
https://github.com/tidyverse/lubridate",TRUE,https://github.com/tidyverse/lubridate,20485088,568,2021-08-19T09:30:24Z,36065.295774647886
lucid,"Print vectors (and data frames) of floating point numbers
    using a non-scientific format optimized for human readers.  Vectors
    of numbers are rounded using significant digits, aligned at the
    decimal point, and all zeros trailing the decimal point are dropped.
    See: Wright (2016). Lucid: An R Package for Pretty-Printing Floating Point
    Numbers. In JSM Proceedings, Statistical Computing Section. Alexandria,
    VA: American Statistical Association. 2270-2279.",2021-04-16,Kevin Wright,https://kwstat.github.io/lucid/,TRUE,https://github.com/kwstat/lucid,27151,25,2021-04-16T13:54:53Z,1086.04
LUCIDus,"An implementation for the 'LUCID' model (Peng (2019) <doi:10.1093/bioinformatics/btz667>) to jointly estimate latent unknown clusters/subgroups with integrated data. 
  An EM algorithm is used to obtain the latent cluster assignment and model parameter estimates. 
  Feature selection is achieved by applying the L1 regularization method.",2020-07-22,Yinqi Zhao,https://github.com/Yinqi93/LUCIDus,TRUE,https://github.com/yinqi93/lucidus,13892,2,2021-09-02T18:02:08Z,6946
ludic,"Probabilistic record linkage without direct identifiers using only 
    diagnosis codes. Method is detailed in: Hejblum, Weber, Liao, Palmer, 
    Churchill, Szolovits, Murphy, Kohane & Cai (2019) <doi: 10.1038/sdata.2018.298> ;
    Zhang, Hejblum, Weber, Palmer, Churchill, Szolovits, Murphy, Liao, Kohane 
    & Cai (2021) <doi: 10.1101/2021.05.02.21256490>.",2021-08-18,Boris P Hejblum,NA,TRUE,https://github.com/borishejblum/ludic,15590,0,2021-08-18T10:55:29Z,NA
lumberjack,"A framework that allows for easy logging of changes in data.
    Main features: start tracking changes by adding a single line of code to 
    an existing script. Track changes in multiple datasets, using multiple 
    loggers. Add custom-built loggers or use loggers offered by other 
    packages. <doi:10.18637/jss.v098.i01>.",2021-05-27,Mark van der Loo,https://github.com/markvanderloo/lumberjack,TRUE,https://github.com/markvanderloo/lumberjack,19491,56,2021-07-28T14:50:49Z,348.05357142857144
Luminescence,"A collection of various R functions for the purpose of Luminescence
    dating data analysis. This includes, amongst others, data import, export,
    application of age models, curve deconvolution, sequence analysis and
    plotting of equivalent dose distributions.",2021-09-02,Sebastian Kreutzer,https://CRAN.R-project.org/package=Luminescence,TRUE,https://github.com/r-lum/luminescence,54449,9,2021-08-24T15:06:02Z,6049.888888888889
lutz,"Input latitude and longitude values or an 'sf/sfc' POINT 
    object and get back the time zone in which they exist. Two methods are implemented. 
    One is very fast and uses 'Rcpp' in conjunction with data from the 'Javascript' library
    (<https://github.com/darkskyapp/tz-lookup/>). This method also works outside of countries' 
    borders and in international waters, however speed comes at the cost of accuracy - near time 
    zone borders away from populated centres there is a chance that it will return the incorrect
    time zone. The other method is slower but more accurate - it uses the 'sf' package to intersect 
    points with a detailed map of time zones from here: 
    <https://github.com/evansiroky/timezone-boundary-builder/>. The package also 
    contains several utility functions for helping to understand and visualize 
    time zones, such as listing of world time zones, including information about 
    daylight savings times and their offsets from UTC. You can also plot a 
    time zone to visualize the UTC offset over a year and when daylight savings 
    times are in effect.",2019-07-19,Andy Teucher,https://andyteucher.ca/lutz,TRUE,https://github.com/ateucher/lutz,120894,51,2021-07-17T00:30:57Z,2370.470588235294
luz,"A high level interface for 'torch' providing utilities to reduce the
    the amount of code needed for common tasks, abstract away torch details and 
    make the same code work on both the 'CPU' and 'GPU'. It's flexible enough to
    support expressing a large range of models. It's heavily inspired by 'fastai' by 
    Howard et al. (2020) <arXiv:2002.04688>, 'Keras' by Chollet et al. (2015) and 
    'Pytorch Lightning' by Falcon et al. (2019) <doi:10.5281/zenodo.3828935>.",2021-06-17,Daniel Falbel,"https://mlverse.github.io/luz/, https://github.com/mlverse/luz",TRUE,https://github.com/mlverse/luz,1087,28,2021-08-26T16:44:00Z,38.82142857142857
lvmcomp,"Provides stochastic EM algorithms for latent variable models
    with a high-dimensional latent space. So far, we provide functions for confirmatory item
    factor analysis based on the multidimensional two parameter logistic (M2PL) model and the 
    generalized multidimensional partial credit model. These functions scale well for problems
    with many latent traits (e.g., thirty or even more) and are virtually tuning-free.
    The computation is facilitated by multiprocessing 'OpenMP' API.
    For more information, please refer to:
    Zhang, S., Chen, Y., & Liu, Y. (2018). An Improved Stochastic EM Algorithm for Large-scale
    Full-information Item Factor Analysis. British Journal of Mathematical and Statistical
    Psychology. <doi:10.1111/bmsp.12153>.",2018-12-30,Siliang Zhang,https://github.com/slzhang-fd/lvmcomp,TRUE,https://github.com/slzhang-fd/lvmcomp,14362,2,2021-04-08T09:03:40Z,7181
lvmisc,"Contains a collection of useful
    functions for basic data computation and manipulation,
    wrapper functions for generating 'ggplot2' graphics,
    including statistical model diagnostic plots, methods
    for computing statistical models quality measures (such
    as AIC, BIC, r squared, root mean squared error) and
    general utilities.",2021-04-05,Lucas Veras,https://lveras.com/lvmisc/,TRUE,https://github.com/verasls/lvmisc,2190,1,2021-04-08T11:21:42Z,2190
LWFBrook90R,"Provides a flexible and easy-to use interface for the soil vegetation 
    atmosphere transport (SVAT) model LWF-BROOK90, written in Fortran.
    The model simulates daily transpiration, interception, soil and snow evaporation, 
    streamflow and soil water fluxes through a soil profile covered with vegetation, 
    as described in Hammel & Kennel (2001, ISBN:978-3-933506-16-0) and Federer et al. (2003) 
    <doi:10.1175/1525-7541(2003)004%3C1276:SOAETS%3E2.0.CO;2>. A set of high-level functions
    for model set up, execution and parallelization provides easy access to plot-level SVAT 
    simulations, as well as multi-run and large-scale applications. ",2021-02-24,Paul Schmidt-Walter,https://github.com/pschmidtwalter/LWFBrook90R,TRUE,https://github.com/pschmidtwalter/lwfbrook90r,2736,3,2021-04-09T13:02:33Z,912
lwgeom,"Access to selected functions found in 'liblwgeom' <https://github.com/postgis/postgis/tree/master/liblwgeom>, the light-weight geometry library used by 'PostGIS' <http://postgis.net/>.",2021-07-28,Edzer Pebesma,https://github.com/r-spatial/lwgeom/,TRUE,https://github.com/r-spatial/lwgeom,921822,47,2021-07-28T12:20:06Z,19613.23404255319
m61r,"Data manipulation in one package and in base R.
  Minimal. No dependencies.
  'dplyr' and 'tidyr'-like in one place.
  Nothing else than base R to build the package.",2021-04-15,Jean-Marie Lepioufle,https://github.com/pv71u98h1/m61r/,TRUE,https://github.com/pv71u98h1/m61r,1783,1,2021-06-07T19:29:17Z,1783
MAAPER,"A computational method developed for model-based analysis of alternative polyadenylation (APA) using 3' end-linked reads. It accurately assigns 3' RNA-seq reads to polyA sites through statistical modeling, and generates multiple statistics for APA analysis. Please also see Li WV, Zheng D, Wang R, Tian B (2021) <doi:10.1186/s13059-021-02429-5>.",2021-08-14,Wei Vivian Li,"https://github.com/Vivianstats/MAAPER,
https://genomebiology.biomedcentral.com/articles/10.1186/s13059-021-02429-5",TRUE,https://github.com/vivianstats/maaper,889,2,2021-08-14T05:18:35Z,444.5
maat,"Provides an extension of the shadow-test approach to computerized adaptive 
    testing (CAT) implemented in the 'TestDesign' package for the assessment framework 
    involving multiple tests administered periodically throughout the year. This framework 
    is referred to as the Multiple Administrations Adaptive Testing (MAAT) and supports 
    multiple item pools vertically scaled and multiple phases (stages) of CAT within each test. 
    Between phases and tests, transitioning from one item pool (and associated constraints) 
    to another is allowed as deemed necessary to enhance the quality of measurement.",2021-07-09,Seung W. Choi,https://choi-phd.github.io/maat/,TRUE,https://github.com/choi-phd/maat,645,0,2021-09-02T17:17:48Z,NA
MACER,"To assist biological researchers in assembling taxonomically and marker focused molecular sequence data sets. 'MACER' accepts a list of genera as a user input and uses NCBI-GenBank and BOLD as resources to download and assemble molecular sequence datasets. These datasets are then assembled by marker, aligned, trimmed, and cleaned. The use of this package allows the publication of specific parameters to ensure reproducibility. The 'MACER' package has four core functions and an example run through using all of these functions can be found in the associated repository <https://github.com/rgyoung6/MACER_example>.",2021-07-05,Robert G Young,<https://github.com/rgyoung6/MACER>,TRUE,https://github.com/rgyoung6/macer,852,0,2021-09-02T08:19:55Z,NA
MachineShop,"Meta-package for statistical and machine learning with a unified
    interface for model fitting, prediction, performance assessment, and
    presentation of results.  Approaches for model fitting and prediction of
    numerical, categorical, or censored time-to-event outcomes include
    traditional regression models, regularization methods, tree-based methods,
    support vector machines, neural networks, ensembles, data preprocessing,
    filtering, and model tuning and selection.  Performance metrics are provided
    for model assessment and can be estimated with independent test sets, split
    sampling, cross-validation, or bootstrap resampling.  Resample estimation
    can be executed in parallel for faster processing and nested in cases of
    model tuning and selection.  Modeling results can be summarized with
    descriptive statistics; calibration curves; variable importance; partial
    dependence plots; confusion matrices; and ROC, lift, and other performance
    curves.",2021-08-19,Brian J Smith,https://brian-j-smith.github.io/MachineShop/,TRUE,https://github.com/brian-j-smith/machineshop,27752,53,2021-08-20T00:00:20Z,523.622641509434
maclogp,"Following the common types of measures of uncertainty for parameter estimation, two measures of uncertainty were proposed for model selection, see Liu, Li and Jiang (2020) <doi:10.1007/s11749-020-00737-9>. The first measure is a kind of model confidence set that relates to the variation of model selection, called Mac. The second measure focuses on error of model selection, called LogP. They are all computed via bootstrapping. This package provides functions to compute these two measures. Furthermore, a similar model confidence set adapted from Bayesian Model Averaging can also be computed using this package. ",2021-04-22,Yuanyuan Li,https://github.com/YuanyuanLi96/maclogp,TRUE,https://github.com/yuanyuanli96/maclogp,1382,0,2021-04-25T20:34:26Z,NA
maditr,"Provides pipe-style interface for 'data.table'. Package preserves all 'data.table' features without
              significant impact on performance. 'let' and 'take' functions are simplified interfaces for most common data
              manipulation tasks. For example, you can write 'take(mtcars, mean(mpg), by = am)' for aggregation or 
              'let(mtcars, hp_wt = hp/wt, hp_wt_mpg = hp_wt/mpg)' for modification. Use 'take_if/let_if' for conditional
              aggregation/modification. Additionally there are some conveniences such as automatic 'data.frame' 
              conversion to 'data.table'.",2021-05-24,Gregory Demin,https://github.com/gdemin/maditr,TRUE,https://github.com/gdemin/maditr,27736,48,2021-05-24T19:03:14Z,577.8333333333334
madness,"An object that supports automatic differentiation
    of matrix- and multidimensional-valued functions with 
    respect to multidimensional independent variables. 
    Automatic differentiation is via 'forward accumulation'.",2020-02-08,Steven E. Pav,https://github.com/shabbychef/madness,TRUE,https://github.com/shabbychef/madness,20008,24,2021-04-03T22:36:05Z,833.6666666666666
madrat,"Provides a framework which should improve reproducibility and transparency in data processing. It provides functionality such as automatic meta data creation and management, rudimentary quality management, data caching, work-flow management and data aggregation.
    * The title is a wish not a promise. By no means we expect this package to deliver everything what is needed to achieve full reproducibility and transparency, but we believe that it supports efforts in this direction.",2019-12-17,Jan Philipp Dietrich,"https://github.com/pik-piam/madrat,
https://doi.org/10.5281/zenodo.1115490",TRUE,https://github.com/pik-piam/madrat,18122,9,2021-09-03T13:04:32Z,2013.5555555555557
MAFDash,"Mutation Annotation Format (MAF) is a tabular data format used for storing genetic mutation data. 
    For example, The Cancer Genome Atlas (TCGA) project has made MAF files from each project publicly available.
    This package contains a set of tools to easily create an HTML dashboard to summarize and visualize data from MAF file.
    The resulting HTML file serves as a self-contained report that can be used to explore the result.
    Mayank Tandon & Ashish Jain (2020) <doi:10.5281/zenodo.4472978>.",2021-02-08,Ashish Jain,https://github.com/ashishjain1988/MAFDash/,TRUE,https://github.com/ashishjain1988/mafdash,1453,0,2021-02-26T01:15:19Z,NA
magclass,"Data class for increased interoperability working with spatial-
    temporal data together with corresponding functions and methods (conversions,
    basic calculations and basic data manipulation). The class distinguishes
    between spatial, temporal and other dimensions to facilitate the development
    and interoperability of tools build for it. Additional features are name-based
    addressing of data and internal consistency checks (e.g. checking for the right
    data order in calculations).",2021-02-25,Jan Philipp Dietrich,"https://github.com/pik-piam/magclass,
https://doi.org/10.5281/zenodo.1158580",TRUE,https://github.com/pik-piam/magclass,26321,2,2021-09-01T19:19:19Z,13160.5
magic,"A collection of efficient, vectorized algorithms for the
 creation and investigation of magic squares and hypercubes, including
 a variety of functions for the manipulation and analysis of
 arbitrarily dimensioned arrays.  The package includes methods for
 creating normal magic squares of any order greater than 2.  The
 ultimate intention is for the package to be a computerized embodiment
 all magic square knowledge, including direct numerical verification
 of properties of magic squares (such as recent results on the
 determinant of odd-ordered semimagic squares).  Some antimagic
 functionality is included.  The package also
 serves as a rebuttal to the often-heard comment ""I thought R
 was just for statistics"".",2018-09-17,Robin K. S. Hankin,https://github.com/RobinHankin/magic.git,TRUE,https://github.com/robinhankin/magic,773044,1,2021-02-09T21:31:07Z,773044
magickGUI,Enables us to use the functions of the package 'magick' interactively.,2021-07-12,Shota Ochi,https://github.com/ShotaOchi/magickGUI,TRUE,https://github.com/shotaochi/magickgui,16252,7,2021-08-14T16:04:42Z,2321.714285714286
magrittr,"Provides a mechanism for chaining commands with a
    new forward-pipe operator, %>%. This operator will forward a value, or
    the result of an expression, into the next function call/expression.
    There is flexible support for the type of right-hand side expressions.
    For more information, see package vignette.  To quote Rene Magritte,
    ""Ceci n'est pas un pipe.""",2020-11-17,Lionel Henry,"https://magrittr.tidyverse.org,
https://github.com/tidyverse/magrittr",TRUE,https://github.com/tidyverse/magrittr,75833908,854,2020-12-14T09:32:58Z,88798.48711943794
maic,"A generalised workflow for generation of subject weights to be 
    used in Matching-Adjusted Indirect Comparison (MAIC) per Signorovitch et 
    al (2012) <doi:10.1016/j.jval.2012.05.004>, Signorovitch et al (2010) 
    <doi:10.2165/11538370-000000000-00000>. In MAIC, unbiased 
    comparison between outcomes of two trials is facilitated by weighting the
    subject-level outcomes of one trial with weights derived such that the 
    weighted aggregate measures of the prognostic or effect modifying variables 
    are equal to those of the sample in the comparator trial. The functions and
    classes included in this package wrap and abstract the process demonstrated
    in the UK National Institute for Health and Care Excellence Decision 
    Support Unit (NICE DSU)'s example (Phillippo et al, (2016) [see URL]),
    providing a repeatable and easily specifiable workflow for producing 
    multiple comparison variable sets against a variety of target studies, with
    preprocessing for a number of aggregate target forms (e.g. mean, median, 
    domain limits).",2021-05-11,Rob Young,"https://github.com/heorltd/maic,
http://nicedsu.org.uk/technical-support-documents/population-adjusted-indirect-comparisons-maic-and-stc/",TRUE,https://github.com/heorltd/maic,6058,2,2021-05-11T20:46:16Z,3029
mailmerge,"Mail merge using markdown documents and gmail. With
    this package you can parse markdown documents as the body of email,
    and the 'yaml' header to specify the subject line of the email.  Any
    '{}' braces in the email will be encoded with 'glue::glue()'. You can
    preview the email in the RStudio viewer pane, and send (draft) email
    using 'gmailr'.",2021-05-11,Andrie de Vries,"https://andrie.github.io/mailmerge/,
https://github.com/andrie/mailmerge",TRUE,https://github.com/andrie/mailmerge,1976,34,2021-05-11T08:57:29Z,58.11764705882353
mailtoR,"Allows the user to generate a friendly user interface for emails sending. 
    The user can choose from the most popular free email services ('Gmail', 'Outlook', 'Yahoo') and his default email application. 
    The package is a wrapper for the 'Mailtoui' 'JavaScript' library.  See <https://mailtoui.com/#menu> for more information. ",2020-06-25,Mohamed El Fodil Ihaddaden,https://github.com/feddelegrand7/mailtoR,TRUE,https://github.com/feddelegrand7/mailtor,4427,15,2020-12-09T21:55:47Z,295.1333333333333
maketools,"A collection of helper functions that interface with the appropriate
    system utilities to learn about the build environment. Lets you explore 'make' 
    rules to test the local configuration, or query 'pkg-config' to find compiler
    flags and libs needed for building packages with external dependencies. Also
    contains tools to analyze which libraries that a installed R package linked to
    by inspecting output from 'ldd' in combination with information from your
    distribution package manager, e.g. 'rpm' or 'dpkg'. Finally the package provides
    Windows-specific utilities to automatically find or install the suitable version
    of the 'Rtools' build environment, and diagnose some common problems.",2020-09-16,Jeroen Ooms,https://github.com/jeroen/maketools,TRUE,https://github.com/jeroen/maketools,4517,11,2021-04-19T12:46:07Z,410.6363636363636
malariaAtlas,"A suite of tools to allow you to download all 
  publicly available parasite rate survey points, mosquito occurrence points and raster surfaces from 
  the 'Malaria Atlas Project' <https://malariaatlas.org/> servers as well as utility functions for plotting
  the downloaded data.",2020-06-01,Daniel Pfeffer,https://github.com/malaria-atlas-project/malariaAtlas,TRUE,https://github.com/malaria-atlas-project/malariaatlas,18344,28,2021-08-03T01:36:09Z,655.1428571428571
malaytextr,"It is built to handle Bahasa Malaysia text. We provide 
    functions and data sets that will help handling Bahasa Malaysia to be much 
    easier. For word stemming in particular, we will find the Malay words in a dictionary and
    then proceed to remove ""extra suffix"" as explained in Khan, Rehman Ullah, 
    Fitri Suraya Mohamad, Muh Inam UlHaq, Shahren Ahmad Zadi Adruce, 
    Philip Nuli Anding, Sajjad Nawaz Khan, and Abdulrazak Yahya Saleh Al-Hababi
    (2017) <https://ijrest.net/vol-4-issue-12.html> . A dictionary of Malay 
    words provided in this package can be used as a dictionary to perform 
    word stemming.",2021-08-17,Zahier Nasrudin,https://github.com/zahiernasrudin/malaytextr,TRUE,https://github.com/zahiernasrudin/malaytextr,235,2,2021-08-19T12:42:07Z,117.5
ManagedCloudProvider,"
  Providing the kubernetes-like class 'ManagedCloudProvider' as a child class of 
  the 'CloudProvider' class in the 'DockerParallel' package. The class is able to
  manage the cloud instance made by the non-kubernetes cloud service.
  For creating a provider for the non-kubernetes cloud service, the developer
  needs to define a reference class inherited from 'ManagedCloudProvider' and
  define the method for the generics runDockerWorkerContainers(),
  getDockerWorkerStatus() and killDockerWorkerContainers(). For more information, please see
  the vignette in this package and <https://CRAN.R-project.org/package=DockerParallel>.",2021-06-14,Jiefei Wang,https://github.com/Jiefei-Wang/ManagedCloudProvider,TRUE,https://github.com/jiefei-wang/managedcloudprovider,992,0,2021-06-11T08:13:06Z,NA
mangoTraining,"Datasets to be used primarily in conjunction with Mango Solutions
    training materials but also for the book 'SAMS Teach Yourself R in 24 Hours' (ISBN: 978-0-672-33848-9).
    Version 1.0-7 is largely for use with the book; however, version 1.1 has a much greater focus on use with
    training materials, whilst retaining compatibility with the book.",2021-04-28,Andrew Little,https://www.mango-solutions.com/,TRUE,https://github.com/mangothecat/mangotraining,23005,7,2021-04-27T09:48:44Z,3286.4285714285716
manhattanly,"Create interactive manhattan, Q-Q and volcano plots that are usable from the R console, 
    in 'Dash' apps, in the 'RStudio' viewer pane, in 'R Markdown' documents, and in 'Shiny' apps.
    Hover the mouse pointer over a point to show details or drag a rectangle to
    zoom. A manhattan plot is a popular graphical method for visualizing results
    from high-dimensional data analysis such as a (epi)genome wide association study
    (GWAS or EWAS), in which p-values, Z-scores, test statistics are plotted on a scatter
    plot against their genomic position. Manhattan plots are used for visualizing
    potential regions of interest in the genome that are associated with a phenotype.
    Interactive manhattan plots allow the inspection of specific value (e.g. rs number or
    gene name) by hovering the mouse over a cell, as well as zooming into a region of the
    genome (e.g. a chromosome) by dragging a rectangle around the relevant area.
    This work is based on the 'qqman' package and the 'plotly.js'
    engine. It produces similar manhattan and Q-Q plots as the 'manhattan' and 'qq'
    functions in the 'qqman' package, with the advantage of including extra annotation 
    information and interactive web-based visualizations directly from R. 
    Once uploaded to a 'plotly' account, 'plotly' graphs (and the data behind them) 
    can be viewed and modified in a web browser.",2021-04-26,Sahir Bhatnagar,"https://github.com/sahirbhatnagar/manhattanly/,
https://sahirbhatnagar.com/manhattanly/",TRUE,https://github.com/sahirbhatnagar/manhattanly,27376,57,2021-04-26T13:14:18Z,480.280701754386
manipulateWidget,"Like package 'manipulate' does for static graphics, this package
    helps to easily add controls like sliders, pickers, checkboxes, etc. that 
    can be used to modify the input data or the parameters of an interactive 
    chart created with package 'htmlwidgets'.",2021-05-31,Veronique Bachelier,https://github.com/rte-antares-rpackage/manipulateWidget,TRUE,https://github.com/rte-antares-rpackage/manipulatewidget,1899276,121,2021-05-27T07:38:50Z,15696.495867768595
MANOVA.RM,"Implemented are various tests for semi-parametric repeated measures
    and general MANOVA designs that do neither assume multivariate normality nor
    covariance homogeneity, i.e., the procedures are applicable for a wide range
    of general multivariate factorial designs. In addition to asymptotic inference
    methods, novel bootstrap and permutation approaches are implemented as well. These provide more
    accurate results in case of small to moderate sample sizes. Furthermore, post-hoc 
    comparisons are provided for the multivariate analyses.
    Friedrich, S., Konietschke, F. and Pauly, M. (2019) <doi:10.32614/RJ-2019-051>.",2021-05-21,Sarah Friedrich,https://github.com/smn74/MANOVA.RM,TRUE,https://github.com/smn74/manova.rm,27526,4,2021-05-21T09:10:00Z,6881.5
manymodelr,"Frequently one needs a convenient way to build and tune
             several models in one go.The goal is to provide a number of machine learning convenience 
             functions. It provides the ability to build, tune and obtain predictions of 
             several models in one function. The models are built using functions from
             'caret' with easier to read syntax.
             Kuhn(2014) <arXiv:1405.6974>.",2021-08-17,Nelson Gonzabato,https://github.com/Nelson-Gon/manymodelr,TRUE,https://github.com/nelson-gon/manymodelr,14820,2,2021-08-19T11:25:58Z,7410
MaOEA,"A set of evolutionary algorithms to solve many-objective optimization. 
    Hybridization between the algorithms are also facilitated. Available algorithms are:
    'SMS-EMOA' <doi:10.1016/j.ejor.2006.08.008>
    'NSGA-III' <doi:10.1109/TEVC.2013.2281535>
    'MO-CMA-ES' <doi:10.1145/1830483.1830573>
    The following many-objective benchmark problems are also provided: 
    'DTLZ1'-'DTLZ4' from Deb, et al. (2001) <doi:10.1007/1-84628-137-7_6> and
    'WFG4'-'WFG9' from Huband, et al. (2005) <doi:10.1109/TEVC.2005.861417>.",2020-08-31,Dani Irawan,https://github.com/dots26/MaOEA,TRUE,https://github.com/dots26/maoea,9230,4,2021-03-19T09:55:06Z,2307.5
maotai,"Matrix is an universal and sometimes primary object/unit in applied mathematics and statistics. We provide a number of algorithms for selected problems in optimization and statistical inference. For general exposition to the topic with focus on statistical context, see the book by Banerjee and Roy (2014, ISBN:9781420095388).",2021-06-09,Kisung You,https://github.com/kisungyou/maotai,TRUE,https://github.com/kisungyou/maotai,33336,6,2021-06-09T05:08:45Z,5556
mapbayr,"Performs maximum a posteriori Bayesian estimation of individual pharmacokinetic parameters from a model defined in `mrgsolve`, typically for model-based therapeutic drug monitoring. Internally computes an objective function value from model and data, performs optimization and returns predictions in a convenient format.",2021-07-27,Felicien Le Louedec,https://github.com/FelicienLL/mapbayr,TRUE,https://github.com/felicienll/mapbayr,1659,7,2021-08-05T08:04:06Z,237
mapboxer,"Makes 'Mapbox GL JS' <https://docs.mapbox.com/mapbox-gl-js/api/>,
  an open source JavaScript library that uses WebGL to render interactive maps,
  available within R via the 'htmlwidgets' package. Visualizations can be used from the R console,
  in R Markdown documents and in Shiny apps.",2020-11-04,Stefan Kuethe,https://github.com/crazycapivara/mapboxer,TRUE,https://github.com/crazycapivara/mapboxer,3716,28,2021-07-18T10:55:13Z,132.71428571428572
mapchina,Geospatial shapefile data of China administrative divisions to the county/district-level.,2020-09-29,Mingchu Xu,https://github.com/xmc811/mapchina,TRUE,https://github.com/xmc811/mapchina,4313,46,2021-04-25T16:31:47Z,93.76086956521739
mapdeck,"Provides a mechanism to plot an interactive map using 'Mapbox GL' 
		(<https://docs.mapbox.com/mapbox-gl-js/api/>), a javascript library for interactive maps,
		and 'Deck.gl' (<https://deck.gl/>), a javascript library which uses 'WebGL' for 
		visualising large data sets.",2020-09-04,David Cooley,https://symbolixau.github.io/mapdeck/articles/mapdeck.html,TRUE,https://github.com/symbolixau/mapdeck,198013,309,2020-12-09T02:17:05Z,640.8187702265373
MAPITR,"A genetic analysis tool and variance component 
    model for identifying marginal epistasis between pathways 
    and the rest of the genome. 'MAPITR' uses as input a matrix 
    of genotypes, a vector of phenotypes, and a list of 
    pathways. 'MAPITR' then iteratively tests each pathway for 
    epistasis between any variants within the pathway versus 
    any variants remaining in the rest of the genome. 'MAPITR'
    returns results in the form of p-values for every pathway 
    indicating whether the null model of there being no 
    epistatic interactions between a pathway and the rest of 
    the genome can be rejected.",2020-09-28,Michael Turchin,https://github.com/mturchin20/MAPITR,TRUE,https://github.com/mturchin20/mapitr,4440,1,2020-09-25T04:17:09Z,4440
mapping,"Maps are an important tool to visualise variables distribution across different spatial object. The mapping process require to link the data with coordinates and then generate the correspondent map. This package provide coordinates, linking and mapping functions for an automatic, flexible and easy approach of mapping workflow of different geographical statistical unit.Geographical coordinates are provided in the package and automatically linked with the input data to generate maps with internal provided functions or external functions.provide an easy, flexible and automatic approach to potentially download updated coordinates, to link statistical units with coordinates and to aggregate variables based on the spatial hierarchy of units. The object returned from the package can be used for thematic maps with the build-in functions provided in mapping or with other packages already available.",2021-07-22,Alessio Serafini,https://github.com/serafinialessio/mapping,TRUE,https://github.com/serafinialessio/mapping,2979,3,2021-07-22T17:21:07Z,993
mappings,"Easily create functions to map between different sets of values,
  such as for re-labelling categorical variables.",2021-06-23,Benjamin Rich,https://github.com/benjaminrich/mappings,TRUE,https://github.com/benjaminrich/mappings,859,0,2021-06-19T14:42:07Z,NA
mappoly,"Construction of genetic maps in autopolyploid full-sib populations. 
             Uses pairwise recombination fraction estimation as the first 
             source of information to sequentially position allelic variants 
             in specific homologues. For situations where pairwise analysis has 
             limited power, the algorithm relies on the multilocus likelihood 
             obtained through a hidden Markov model (HMM). For more detail,
             please see  Mollinari and Garcia (2019) <doi:10.1534/g3.119.400378> 
             and Mollinari et al. (2020) <doi:10.1534/g3.119.400620>.",2021-04-20,Marcelo Mollinari,https://github.com/mmollina/MAPpoly,TRUE,https://github.com/mmollina/mappoly,4478,10,2021-08-26T19:28:05Z,447.8
mapr,"Utilities for visualizing species occurrence data. Includes
    functions to visualize occurrence data from 'spocc', 'rgbif',
    and other packages. Mapping options included for base R plots, 'ggplot2',
    'leaflet' and 'GitHub' 'gists'.",2020-10-12,Scott Chamberlain,"https://docs.ropensci.org/mapr/, https://github.com/ropensci/mapr",TRUE,https://github.com/ropensci/mapr,23743,35,2020-10-12T16:52:44Z,678.3714285714286
mapsapi,"Interface to the 'Google Maps' APIs: (1) routing directions based on the 'Directions' API, returned as 'sf' objects, either as single feature per alternative route, or a single feature per segment per alternative route; (2) travel distance or time matrices based on the 'Distance Matrix' API; (3) geocoded locations based on the 'Geocode' API, returned as 'sf' objects, either points or bounds; (4) map images using the 'Maps Static' API, returned as 'stars' objects.",2021-06-13,Michael Dorman,"https://michaeldorman.github.io/mapsapi/,
https://github.com/michaeldorman/mapsapi/",TRUE,https://github.com/michaeldorman/mapsapi,38126,42,2021-06-22T19:05:58Z,907.7619047619048
mapsf,"Create and integrate thematic maps in your workflow. This package
    helps to design various cartographic representations such as proportional
    symbols, choropleth or typology maps. It also offers several functions to
    display layout elements that improve the graphic presentation of maps
    (e.g. scale bar, north arrow, title, labels). 'mapsf' maps 'sf' objects on
    'base' graphics.",2021-04-29,Timothée Giraud,"https://github.com/riatelab/mapsf/,
https://riatelab.github.io/mapsf/",TRUE,https://github.com/riatelab/mapsf,4779,136,2021-07-29T15:01:06Z,35.13970588235294
mapSpain,"Administrative Boundaries of Spain at several levels (CCAA,
    Provinces, Municipalities) based on the GISCO Eurostat database
    <https://ec.europa.eu/eurostat/web/gisco> and 'CartoBase SIANE' from
    'Instituto Geografico Nacional' <https://www.ign.es/>.  It also
    provides a 'leaflet' plugin and the ability of downloading and
    processing static tiles.",2021-09-01,Diego Hernangómez,"https://ropenspain.github.io/mapSpain/,
https://github.com/rOpenSpain/mapSpain",TRUE,https://github.com/ropenspain/mapspain,4950,15,2021-09-01T00:06:41Z,330
maptiles,"To create maps from tiles, 'maptiles' downloads, composes and
    displays tiles from a large number of providers (e.g. 'OpenStreetMap',
    'Stamen', 'Esri', 'CARTO', or 'Thunderforest').",2021-06-09,Timothée Giraud,https://github.com/riatelab/maptiles/,TRUE,https://github.com/riatelab/maptiles,5807,47,2021-07-20T15:29:08Z,123.55319148936171
mapview,"Quickly and conveniently create interactive
    visualisations of spatial data with or without background maps.
    Attributes of displayed features are fully queryable via pop-up
    windows. Additional functionality includes methods to visualise true-
    and false-color raster images and bounding boxes.",2021-06-05,Tim Appelhans,https://github.com/r-spatial/mapview,TRUE,https://github.com/r-spatial/mapview,816487,375,2021-08-29T10:18:14Z,2177.2986666666666
marcher,"A set of tools for likelihood-based estimation, model selection and testing of two- and three-range shift and migration models for animal movement data as described in Gurarie et al. (2017) <doi: 10.1111/1365-2656.12674>.  Provided movement data (X, Y and Time), including irregularly sampled data, functions estimate the time, duration and location of one or two range shifts, as well as the ranging area and auto-correlation structure of the movment.  Tests assess, for example, whether the shift was ""significant"", and whether a two-shift migration was a true return migration.",2017-04-12,Eliezer Gurarie,NA,TRUE,https://github.com/eligurarie/marcher,15349,4,2020-10-15T14:51:47Z,3837.25
MareyMap,Local recombination rates are graphically estimated across a genome using Marey maps.,2020-12-04,Aurélie Siberchicot,"https://lbbe.univ-lyon1.fr/-MareyMap-.html ;
http://lbbe-shiny.univ-lyon1.fr/MareyMapOnline/",TRUE,https://github.com/aursiber/mareymap,22454,0,2021-02-18T08:04:35Z,NA
margins,"An R port of Stata's 'margins' command, which can be used to
    calculate marginal (or partial) effects from model objects.",2021-01-22,Thomas J. Leeper,https://github.com/leeper/margins,TRUE,https://github.com/leeper/margins,193035,238,2021-01-21T22:54:28Z,811.0714285714286
mark,"Miscellaneous functions and wrappers for development in other 
    packages created, maintained by Jordan Mark Barbone.",2021-08-23,Jordan Mark Barbone,https://github.com/jmbarbone/mark,TRUE,https://github.com/jmbarbone/mark,919,2,2021-08-31T23:32:35Z,459.5
markovchain,"Functions and S4 methods to create and manage discrete time Markov
    chains more easily. In addition functions to perform statistical (fitting
    and drawing random variates) and probabilistic (analysis of their structural
    proprieties) analysis are provided. See Spedicato (2017) <doi:10.32614/RJ-2017-036>.",2021-05-17,Giorgio Alfredo Spedicato,https://github.com/spedygiorgio/markovchain/,TRUE,https://github.com/spedygiorgio/markovchain,597525,87,2021-05-17T15:57:49Z,6868.103448275862
MarkowitzR,"A collection of tools for analyzing significance of 
    Markowitz portfolios.",2020-01-08,Steven E. Pav,https://github.com/shabbychef/MarkowitzR,TRUE,https://github.com/shabbychef/markowitzr,22791,5,2021-04-02T04:20:45Z,4558.2
marmap,"Import xyz data from the NOAA (National Oceanic and Atmospheric Administration, <https://www.noaa.gov>), GEBCO (General Bathymetric Chart of the Oceans, <https://www.gebco.net>) and other sources, plot xyz data to prepare publication-ready figures, analyze xyz data to extract transects, get depth / altitude based on geographical coordinates, or calculate z-constrained least-cost paths.",2020-11-19,Eric Pante,https://github.com/ericpante/marmap,TRUE,https://github.com/ericpante/marmap,61608,19,2020-11-19T12:37:03Z,3242.5263157894738
marqLevAlg,"This algorithm provides a numerical solution to the
        problem of unconstrained local minimization (or maximization). It is particularly suited for complex problems and more efficient than
        the Gauss-Newton-like algorithm when starting from points very
        far from the final minimum (or maximum). Each iteration is parallelized and convergence relies on a stringent stopping criterion based on the first and second derivatives. See Philipps et al, 2020 <arXiv:2009.03840>.",2021-04-01,Viviane Philipps,NA,TRUE,https://github.com/vivianephilipps/marqlevalgparallel,22301,1,2021-03-31T18:21:28Z,22301
MARSS,"The MARSS package provides maximum-likelihood parameter estimation for constrained and unconstrained linear multivariate autoregressive state-space (MARSS) models fit to multivariate time-series data.  Fitting is primarily via an Expectation-Maximization (EM) algorithm, although fitting via the BFGS algorithm (using the optim function) is also provided.  MARSS models are a class of dynamic linear model (DLM) and vector autoregressive model (VAR) model.  Functions are provided for parametric and innovations bootstrapping, Kalman filtering and smoothing, bootstrap model selection criteria (AICb), confidences intervals via the Hessian approximation and via bootstrapping and calculation of auxiliary residuals for detecting outliers and shocks.  The user guide shows examples of using MARSS for parameter estimation for a variety of applications, model selection, dynamic factor analysis, outlier and shock detection, and addition of covariates.  Type RShowDoc(""UserGuide"", package=""MARSS"") at the R command line to open the MARSS user guide.  Online workshops (lectures and computer labs) at <https://nwfsc-timeseries.github.io/>  See the NEWS file for update information.",2020-10-21,Eli Holmes,https://nwfsc-timeseries.github.io/MARSS/,TRUE,https://github.com/nwfsc-timeseries/marss,56937,35,2021-08-24T02:53:55Z,1626.7714285714285
mashr,"Implements the multivariate adaptive shrinkage (mash)
    method of Urbut et al (2019) <DOI:10.1038/s41588-018-0268-8> for
    estimating and testing large numbers of effects in many conditions
    (or many outcomes). Mash takes an empirical Bayes approach to
    testing and effect estimation; it estimates patterns of similarity
    among conditions, then exploits these patterns to improve accuracy
    of the effect estimates. The core linear algebra is implemented in
    C++ for fast model fitting and posterior computation.",2021-05-24,Peter Carbonetto,https://github.com/stephenslab/mashr,TRUE,https://github.com/stephenslab/mashr,5881,58,2021-07-22T18:53:02Z,101.39655172413794
matchingR,"Computes matching algorithms quickly using Rcpp.
    Implements the Gale-Shapley Algorithm to compute the stable
    matching for two-sided markets, such as the stable marriage
    problem and the college-admissions problem. Implements Irving's
    Algorithm for the stable roommate problem. Implements the top
    trading cycle algorithm for the indivisible goods trading problem.",2021-05-25,Jan Tilly,https://github.com/jtilly/matchingR/,TRUE,https://github.com/jtilly/matchingr,25737,42,2021-05-25T07:33:15Z,612.7857142857143
MatchIt,"Selects matched samples of the original treated and
    control groups with similar covariate distributions -- can be
    used to match exactly on covariates, to match on propensity
    scores, or perform a variety of other matching procedures.  The
    package also implements a series of recommendations offered in
    Ho, Imai, King, and Stuart (2007) <DOI:10.1093/pan/mpl013>.",2021-05-26,Daniel Ho,"https://kosukeimai.github.io/MatchIt/,
https://github.com/kosukeimai/MatchIt",TRUE,https://github.com/kosukeimai/matchit,588407,76,2021-08-25T18:10:02Z,7742.1973684210525
MatchThem,"Provides the necessary tools for the pre-processing techniques of matching and weighting multiply imputed datasets to control for effects of confounders and to reduce the degree of dependence on certain modeling assumptions in studying the causal associations between an exposure and an outcome. This package includes functions to perform matching within and across the multiply imputed datasets using several matching methods, to estimate weights of units in the imputed datasets using several weighting methods, to calculate the causal effect estimate in each matched or weighted dataset using parametric or non-parametric statistical models, and to pool the obtained estimates from these models according to Rubin's rules (please see <https://journal.r-project.org/archive/2021/RJ-2021-073/> for details).",2021-08-23,Farhad Pishgar,https://github.com/FarhadPishgar/MatchThem,TRUE,https://github.com/farhadpishgar/matchthem,18467,6,2021-08-23T03:33:43Z,3077.8333333333335
materialmodifier,"You can apply image processing effects that modifies the perceived material properties of objects
    in photos, such as gloss, smoothness, and blemishes. This is an implementation of the algorithm proposed by
    Boyadzhiev et al. (2015) ""Band-Sifting Decomposition for Image Based Material Editing"".
    Documentation and practical tips of the package is available at <https://github.com/tsuda16k/materialmodifier>.",2021-08-11,Hiroyuki Tsuda,https://github.com/tsuda16k/materialmodifier,TRUE,https://github.com/tsuda16k/materialmodifier,1555,2,2021-08-31T16:01:21Z,777.5
mathjaxr,Provides 'MathJax' and macros to enable its use within Rd files for rendering equations in the HTML help files.,2021-03-01,Wolfgang Viechtbauer,https://github.com/wviechtb/mathjaxr,TRUE,https://github.com/wviechtb/mathjaxr,262141,33,2021-06-25T19:04:24Z,7943.666666666667
matlib,"A collection of matrix functions for teaching and learning matrix
    linear algebra as used in multivariate statistical methods. These functions are
    mainly for tutorial purposes in learning matrix algebra ideas using R. In some
    cases, functions are provided for concepts available elsewhere in R, but where
    the function call or name is not obvious. In other cases, functions are provided
    to show or demonstrate an algorithm. In addition, a collection of functions are
    provided for drawing vector diagrams in 2D and 3D.",2021-08-21,Michael Friendly,https://github.com/friendly/matlib,TRUE,https://github.com/friendly/matlib,198001,50,2021-08-21T08:18:52Z,3960.02
MatrixExtra,"Extends sparse matrix and vector classes from the 'Matrix' package by providing: 
  (a) Methods and operators that work natively on CSR formats (compressed sparse row, 
  a.k.a. 'RsparseMatrix') such as slicing/sub-setting, assignment, rbind(), 
  mathematical operators for CSR and COO such as addition (""+"") or sqrt(), and methods such as diag(); 
  (b) Multi-threaded matrix multiplication and cross-product for many <sparse, dense> types, 
  including the 'float32' type from 'float'; 
  (c) Coercion methods between pairs of classes which are not present in 'Matrix', 
  such as 'dgCMatrix' -> 'ngRMatrix', as well as convenience conversion functions; 
  (d) Utility functions for sparse matrices such as sorting the indices or removing 
  zero-valued entries; 
  (e) Fast transposes that work by outputting in the opposite storage format;
  (f) Faster replacements for many 'Matrix' methods for all sparse types, such as
  slicing and elementwise multiplication.",2021-07-26,David Cortes,https://github.com/david-cortes/MatrixExtra,TRUE,https://github.com/david-cortes/matrixextra,1861,9,2021-09-02T01:07:20Z,206.77777777777777
matrixpls,"Partial Least Squares Path Modeling
    algorithm and related algorithms. The algorithm implementations aim for
    computational efficiency using matrix algebra and covariance data. The
    package is designed toward Monte Carlo simulations and includes functions
    to perform simple Monte Carlo simulations.",2021-04-28,Mikko Rönkkö,https://github.com/mronkko/matrixpls,TRUE,https://github.com/mronkko/matrixpls,20439,5,2021-05-31T10:03:47Z,4087.8
matrixprofiler,"This is the core functions needed by the 'tsmp' package.  The
    low level and carefully checked mathematical functions are here.
    These are implementations of the Matrix Profile concept that was
    created by CS-UCR <http://www.cs.ucr.edu/~eamonn/MatrixProfile.html>.",2021-05-26,Francisco Bischoff,https://github.com/matrix-profile-foundation/matrixprofiler,TRUE,https://github.com/matrix-profile-foundation/matrixprofiler,2584,4,2021-05-26T17:40:13Z,646
matrixStats,"High-performing functions operating on rows and columns of matrices, e.g. col / rowMedians(), col / rowRanks(), and col / rowSds().  Functions optimized per data type and for subsetted calculations such that both memory usage and processing time is minimized.  There are also optimized vector-based methods, e.g. binMeans(), madDiff() and weightedMedian().",2021-08-23,Henrik Bengtsson,https://github.com/HenrikBengtsson/matrixStats,TRUE,https://github.com/henrikbengtsson/matrixstats,8153197,162,2021-08-26T20:40:06Z,50328.37654320987
matrixTests,"Functions to perform fast statistical hypothesis tests on rows/columns of matrices.
  The main goals are: 1) speed via vectorization, 2) output that is detailed and easy to use,
  3) compatibility with tests implemented in R (like those available in the 'stats' package).",2020-05-01,Karolis Koncevičius,https://github.com/KKPMW/matrixTests,TRUE,https://github.com/kkpmw/matrixtests,27272,23,2021-01-20T14:58:30Z,1185.7391304347825
matsbyname,"An implementation of matrix mathematics wherein operations are performed ""by name.""",2021-09-02,Matthew Heun,https://github.com/MatthewHeun/matsbyname,TRUE,https://github.com/matthewheun/matsbyname,19796,0,2021-09-02T18:21:50Z,NA
matsindf,"Provides functions to collapse a tidy data frame into matrices in a data frame
    and expand a data frame of matrices into a tidy data frame.",2021-09-03,Matthew Heun,https://github.com/MatthewHeun/matsindf,TRUE,https://github.com/matthewheun/matsindf,15306,3,2021-04-11T19:11:23Z,5102
matuR,"Identifying maturation stages across young athletes is paramount for talent identification. Furthermore, the concept of biobanding, or grouping of athletes based on their biological development, instead of their chronological age, has been widely researched. The goal of this package is to help professionals working in the field of strength & conditioning and talent ID obtain common maturation metrics and as well as to quickly visualize this information via several plotting options. For the methods behind the computed maturation metrics implemented in this package refer to Khamis, H. J., & Roche, A. F. (1994) <https://pubmed.ncbi.nlm.nih.gov/7936860/>, Mirwald, R.L et al., (2002) <https://pubmed.ncbi.nlm.nih.gov/11932580/> and Cumming, Sean P. et al., (2017) <doi:10.1519/SSC.0000000000000281>. ",2020-11-19,Jose Fernandez,https://github.com/josedv82/matuR,TRUE,https://github.com/josedv82/matur,3160,3,2020-11-29T13:27:10Z,1053.3333333333333
mau,"Provides functions for the creation, evaluation and test of decision models based in
    Multi Attribute Utility Theory (MAUT). Can process and evaluate local risk aversion utilities
    for a set of indexes, compute utilities and weights for the whole decision tree defining the
    decision model and simulate weights employing Dirichlet distributions under addition constraints 
    in weights.",2018-01-17,Pedro Guarderas,https://github.com/pedroguarderas/mau,TRUE,https://github.com/pedroguarderas/mau,15004,1,2021-05-04T01:08:42Z,15004
maxnet,"Procedures to fit species distributions models from occurrence records and environmental variables, using 'glmnet' for model fitting. Model structure is the same as for the 'Maxent' Java package, version 3.4.0, with the same feature types and regularization options.  See the 'Maxent' website <http://biodiversityinformatics.amnh.org/open_source/maxent> for more details.",2021-07-09,Steven Phillips,https://github.com/mrmaxent/maxnet,TRUE,https://github.com/mrmaxent/maxnet,63708,51,2021-07-09T13:43:09Z,1249.1764705882354
MazamaCoreUtils,"A suite of utility functions providing functionality commonly
    needed for production level projects such as logging, error handling,
    cache management and date-time parsing. Functions for date-time parsing and 
    formatting require that time zones be specified explicitly, avoiding a common 
    source of error when working with environmental time series.",2021-08-18,Jonathan Callahan,https://github.com/MazamaScience/MazamaCoreUtils,TRUE,https://github.com/mazamascience/mazamacoreutils,23365,3,2021-08-17T20:06:52Z,7788.333333333333
MazamaLocationUtils,"A suite of utility functions for discovering and managing metadata 
    associated with sets of spatially unique ""known locations"".",2021-01-28,Jonathan Callahan,https://github.com/MazamaScience/MazamaLocationUtils,TRUE,https://github.com/mazamascience/mazamalocationutils,11445,0,2021-08-26T20:49:46Z,NA
MazamaSpatialUtils,"A suite of conversion functions to create internally standardized
    spatial polygons data frames. Utility functions use these data sets to
    return values such as country, state, timezone, watershed, etc. associated
    with a set of longitude/latitude pairs. (They also make cool maps.)",2020-12-01,Jonathan Callahan,https://github.com/MazamaScience/MazamaSpatialUtils,TRUE,https://github.com/mazamascience/mazamaspatialutils,30885,3,2021-08-26T19:20:22Z,10295
mazealls,"Supports the generation of parallelogram, equilateral
    triangle, regular hexagon, isosceles trapezoid, Koch snowflake,
    'hexaflake', Sierpinski triangle, Sierpinski carpet and Sierpinski
    trapezoid mazes via 'TurtleGraphics'. Mazes are generated by the recursive method:
    the domain is divided into sub-domains in which mazes are generated,
    then dividing lines with holes are drawn between them, see
    J. Buck, Recursive Division, <http://weblog.jamisbuck.org/2011/1/12/maze-generation-recursive-division-algorithm>.",2017-12-12,Steven E. Pav,https://github.com/shabbychef/mazealls,TRUE,https://github.com/shabbychef/mazealls,14480,37,2021-04-01T19:29:13Z,391.35135135135135
mazing,"Functionality for generating and plotting random mazes. The mazes are based on matrices, so can only consist of vertical and horizontal lines along a regular grid. But there is no need to use every possible space, so they can take on many different shapes.",2021-06-02,Kelly Street,NA,TRUE,https://github.com/kstreet13/mazing,871,1,2021-08-29T17:17:22Z,871
mbbefd,"Distributions that are typically used for exposure rating in
             general insurance, in particular to price reinsurance contracts.
             The vignette shows code snippets to fit the distribution to
             empirical data. See, e.g., Bernegger (1997) <doi:10.2143/AST.27.1.563208>
             freely available on-line.",2021-06-08,Christophe Dutang,https://github.com/spedygiorgio/mbbefd,TRUE,https://github.com/spedygiorgio/mbbefd,24057,10,2021-07-31T12:28:10Z,2405.7
mbend,"Bending non-positive-definite (symmetric) matrices to positive-definite, using weighted and unweighted methods.
   Jorjani, H., et al. (2003) <doi:10.3168/jds.S0022-0302(03)73646-7>.
   Schaeffer, L. R. (2014) <http://animalbiosciences.uoguelph.ca/~lrs/ELARES/PDforce.pdf>.",2020-10-11,Mohammad Ali Nilforooshan,https://github.com/nilforooshan/mbend,TRUE,https://github.com/nilforooshan/mbend,12273,3,2020-10-11T05:51:14Z,4091
MBmca,"Lightweight utilities for nucleic acid melting curve analysis are 
    important in life sciences and diagnostics. This software can be used for 
    the analysis and presentation of melting curve data from microbead-based 
    assays (surface melting curve analysis) and reactions in solution (e.g., 
    quantitative PCR (qPCR), real-time isothermal Amplification). Further 
    information are described in detail in two publications in The R Journal [
    <https://journal.r-project.org/archive/2013-2/roediger-bohm-schimke.pdf>; 
    <https://journal.r-project.org/archive/2015-1/RJ-2015-1.pdf>].",2021-09-03,Stefan Roediger,https://github.com/PCRuniversum/MBmca/,TRUE,https://github.com/pcruniversum/mbmca,19055,1,2021-03-08T08:20:12Z,19055
mbmixture,"Evaluate whether a microbiome sample is a mixture of two
    samples, by fitting a model for the number of read counts as a
    function of single nucleotide polymorphism (SNP) allele and the
    genotypes of two potential source samples.
    Lobo et al. (2019) <doi:10.1101/529040>.",2020-10-22,Karl W Broman,https://github.com/kbroman/mbmixture,TRUE,https://github.com/kbroman/mbmixture,3473,3,2020-11-24T15:39:05Z,1157.6666666666667
mboost,"Functional gradient descent algorithm
  (boosting) for optimizing general risk functions utilizing
  component-wise (penalised) least squares estimates or regression
  trees as base-learners for fitting generalized linear, additive
  and interaction models to potentially high-dimensional data.
  Models and algorithms are described in \doi{10.1214/07-STS242}, 
  a hands-on tutorial is available from \doi{10.1007/s00180-012-0382-5}.
  The package allows user-specified loss functions and base-learners.",2021-04-13,Torsten Hothorn,https://github.com/boost-R/mboost,TRUE,https://github.com/boost-r/mboost,225365,62,2021-08-13T10:47:24Z,3634.9193548387098
mboxr,Importing and converting an mbox file into a tibble object.,2019-10-28,JooYoung Seo,https://github.com/jooyoungseo/mboxr,TRUE,https://github.com/jooyoungseo/mboxr,17521,9,2020-12-27T18:35:13Z,1946.7777777777778
mbr,"Mass-balance-adjusted Regression algorithm for streamflow reconstruction at sub-annual resolution (e.g., seasonal or monthly). The algorithm implements a penalty term to minimize the differences between the total sub-annual flows and the annual flow. The method is described in Nguyen et al (2020) <DOI:10.1002/essoar.10504791.1>.",2021-02-16,Hung Nguyen,https://github.com/ntthung/mbr,TRUE,https://github.com/ntthung/mbr,2189,0,2021-03-10T07:08:39Z,NA
mcboost,"Implements 'Multi-Calibration Boosting' (2018) <https://proceedings.mlr.press/v80/hebert-johnson18a.html> and
    'Multi-Accuracy Boosting' (2019) <arXiv:1805.12317> for the multi-calibration of a machine learning model's prediction.
    'MCBoost' updates predictions for sub-groups in an iterative fashion in order to mitigate biases like poor calibration or large accuracy differences across subgroups.
    Multi-Calibration works best in scenarios where the underlying data & labels are unbiased, but resulting models are.
    This is often the case, e.g. when an algorithm fits a majority population while ignoring or under-fitting minority populations.",2021-08-03,Florian Pfisterer,https://github.com/mlr-org/mcboost,TRUE,https://github.com/mlr-org/mcboost,1183,9,2021-08-07T12:08:59Z,131.44444444444446
MCDA,"Support for the analyst in a Multicriteria Decision Aiding (MCDA) process with algorithms, 
    preference elicitation and data visualisation functions. Sébastien Bigaret, Richard Hodgett, Patrick Meyer, 
    Tatyana Mironova, Alexandru Olteanu (2017) Supporting the multi-criteria decision aiding process : 
    R and the MCDA package, Euro Journal On Decision Processes, Volume 5, Issue 1 - 4, 
    pages 169 - 194 <doi:10.1007/s40070-017-0064-1>.",2021-04-05,Patrick Meyer,https://github.com/paterijk/MCDA,TRUE,https://github.com/paterijk/mcda,24869,18,2021-05-03T12:32:58Z,1381.611111111111
mclogit,"Provides estimators for multinomial logit models in their
    conditional logit and baseline logit variants, with or without random effects,
    with or without overdispersion. 
    Random effects models are estimated using the PQL technique (based on a Laplace approximation)
    or the MQL technique (based on a Solomon-Cox approximation). Estimates should be treated
    with caution if the group sizes are small.",2021-03-29,Martin Elff,"http://mclogit.elff.eu,https://github.com/melff/mclogit/",TRUE,https://github.com/melff/mclogit,45190,16,2021-07-13T20:48:41Z,2824.375
mcmcderive,"Generates derived parameter(s) from Monte Carlo Markov Chain
    (MCMC) samples using R code. This allows Bayesian models to be fitted
    without the inclusion of derived parameters which add unnecessary
    clutter and slow model fitting. For more information on MCMC samples
    see Brooks et al. (2011) <isbn:978-1-4200-7941-8>.",2021-08-06,Joe Thorley,https://github.com/poissonconsulting/mcmcderive,TRUE,https://github.com/poissonconsulting/mcmcderive,12238,0,2021-08-06T15:28:54Z,NA
mcmcensemble,"Provides ensemble samplers for
    affine-invariant Monte Carlo Markov Chain, which allow a faster
    convergence for badly scaled estimation problems. Two samplers are
    proposed: the 'differential.evolution' sampler from ter Braak and
    Vrugt (2008) <doi:10.1007/s11222-008-9104-9> and the 'stretch' sampler
    from Goodman and Weare (2010) <doi:10.2140/camcos.2010.5.65>.",2021-04-28,Hugo Gruson,"https://github.com/Bisaloo/mcmcensemble,
https://bisaloo.github.io/mcmcensemble/",TRUE,https://github.com/bisaloo/mcmcensemble,4744,1,2021-04-30T11:29:10Z,4744
mcmcr,"Functions and classes to store, manipulate and summarise
    Monte Carlo Markov Chain (MCMC) samples. For more information see
    Brooks et al. (2011) <isbn:978-1-4200-7941-8>.",2021-08-05,Joe Thorley,https://github.com/poissonconsulting/mcmcr,TRUE,https://github.com/poissonconsulting/mcmcr,20343,14,2021-09-02T14:31:28Z,1453.0714285714287
MCMCvis,"Performs key functions for MCMC analysis using minimal code - visualizes, manipulates, and summarizes MCMC output. Functions support simple and straightforward subsetting of model parameters within the calls, and produce presentable and 'publication-ready' output. MCMC output may be derived from Bayesian model output fit with 'Stan', 'NIMBLE', 'JAGS', and other software.",2021-06-24,Casey Youngflesh,https://github.com/caseyyoungflesh/MCMCvis,TRUE,https://github.com/caseyyoungflesh/mcmcvis,37929,29,2021-06-24T18:07:09Z,1307.896551724138
mco,"A collection of function to solve multiple criteria optimization problems 
  using genetic algorithms (NSGA-II). Also included is a collection of test functions.",2020-10-09,Olaf Mersmann,https://github.com/olafmersmann/mco,TRUE,https://github.com/olafmersmann/mco,151416,5,2020-10-08T14:41:22Z,30283.2
MCPModPack,"An efficient implementation of the MCPMod (Multiple Comparisons and Modeling) method to support a simulation-based design and analysis of dose-finding trials with normally distributed, binary and count endpoints (Bretz et al. (2005) <doi:10.1111/j.1541-0420.2005.00344.x>).",2020-12-07,Alex Dmitrienko,https://github.com/medianainc/MCPModPack,TRUE,https://github.com/medianainc/mcpmodpack,9677,6,2020-12-15T03:30:45Z,1612.8333333333333
mdbr,"Use the open source 'MDB Tools' utilities
    <https://github.com/mdbtools/mdbtools/>. Primarily used for converting
    proprietary Microsoft Access files to simple text files and then
    reading those as data frames.",2020-11-09,Kiernan Nicholls,https://github.com/kiernann/mdbr,TRUE,https://github.com/kiernann/mdbr,3426,1,2021-03-03T15:44:16Z,3426
mde,"Correct identification and handling of missing data is one of the most important steps in any analysis. To aid this process, 'mde' provides a very easy to use yet robust framework to quickly get an idea of where the missing data
             lies and therefore find the most appropriate action to take.
             Graham WJ (2009) <doi:10.1146/annurev.psych.58.110405.085530>. ",2021-08-17,Nelson Gonzabato,https://github.com/Nelson-Gon/mde,TRUE,https://github.com/nelson-gon/mde,9425,4,2021-08-20T06:48:26Z,2356.25
mdftracks,"'MTrackJ' is an 'ImageJ' plugin for motion tracking and analysis (see 
    <https://imagescience.org/meijering/software/mtrackj/>). This package reads 
    and writes 'MTrackJ Data Files' ('.mdf', see 
    <https://imagescience.org/meijering/software/mtrackj/format/>). It supports
    2D data and read/writes cluster, point, and channel information. If desired, 
    generates track identifiers that are unique over the clusters.
    See the project page for more information and examples.",2021-01-13,Gerhard Burger,https://github.com/burgerga/mdftracks,TRUE,https://github.com/burgerga/mdftracks,16045,1,2021-04-28T15:21:17Z,16045
mdgc,"Provides functions to impute missing values using Gaussian 
    copulas for mixed data types as described by Christoffersen et al. 
    (2021) <arXiv:2102.02642>. The method is related to Hoff (2007) 
    <doi:10.1214/07-AOAS107> and Zhao and Udell (2019) <arXiv:1910.12845> 
    but differs by making a direct approximation of the log marginal likelihood 
    using an extended version of the Fortran code created by Genz and Bretz 
    (2002) <doi:10.1198/106186002394> in addition to also support multinomial 
    variables.",2021-06-14,Benjamin Christoffersen,https://github.com/boennecd/mdgc,TRUE,https://github.com/boennecd/mdgc,2828,2,2021-09-02T14:37:19Z,1414
mdir.logrank,"Implemented are the one-sided and two-sided 
  multiple-direction logrank test for two-sample right 
  censored data. In addition to the statistics p-values are calculated: 
  1. For the one-sided testing problem one p-value based on a
   wild bootstrap approach is determined. 2. In the two-sided case
   one p-value based on a chi-squared approximation and 
   a second p-values based on a permutation approach are calculated.
 Ditzhaus, M. and Friedrich, S. (2018) <arXiv:1807.05504>.
 Ditzhaus, M. and Pauly, M. (2018) <arXiv:1808.05627>.",2018-09-29,Marc Ditzhaus and Sarah Friedrich,NA,TRUE,https://github.com/marcdii/mdir.logrank,13245,0,2021-02-04T09:50:50Z,NA
MDMAPR,"Runs a Shiny web application that merges raw 'qPCR' fluorescence data with related 
    metadata to visualize species presence/absence detection patterns and assess data quality. 
    The application calculates threshold values from raw fluorescence data using a method based 
    on the second derivative method, Luu-The et al (2005) <doi:10.2144/05382RR05>,  and utilizes 
    the ‘chipPCR’ package by Rödiger, Burdukiewicz, & Schierack (2015) <doi:10.1093/bioinformatics/btv205> 
    to calculate Cq values. The application has the ability to connect to a custom developed MySQL 
    database to populate the applications interface. The application allows users to interact with 
    visualizations such as a dynamic map, amplification curves and standard curves, that allow for 
    zooming and/or filtering. It also enables the generation of customized exportable reports based
    on filtered mapping data. ",2021-06-23,Alka Benawra,https://github.com/HannerLab/MDMAPR,TRUE,https://github.com/hannerlab/mdmapr,2776,0,2021-03-26T19:42:41Z,NA
mdmb,"
    Contains model-based treatment of missing data for regression 
    models with missing values in covariates or the dependent 
    variable using maximum likelihood or Bayesian estimation 
    (Ibrahim et al., 2005; <doi:10.1198/016214504000001844>;
    Luedtke, Robitzsch, & West, 2020a, 2020b;
    <doi:10.1080/00273171.2019.1640104><doi:10.1037/met0000233>).
    The regression model can be nonlinear (e.g., interaction 
    effects, quadratic effects or B-spline functions). 
    Multilevel models with missing data in predictors are
    available for Bayesian estimation. Substantive-model compatible 
    multiple imputation can be also conducted.",2021-01-21,Alexander Robitzsch,"https://github.com/alexanderrobitzsch/mdmb,
https://sites.google.com/site/alexanderrobitzsch2/software",TRUE,https://github.com/alexanderrobitzsch/mdmb,68334,2,2021-08-25T11:16:31Z,34167
MDplot,"Provides automatization for plot generation succeeding common molecular dynamics analyses.
             This includes straightforward plots, such as RMSD (Root-Mean-Square-Deviation) and
             RMSF (Root-Mean-Square-Fluctuation) but also more sophisticated ones such as
             dihedral angle maps, hydrogen bonds, cluster bar plots and
             DSSP (Definition of Secondary Structure of Proteins) analysis. Currently able to load
             GROMOS, GROMACS and AMBER formats, respectively.",2017-07-04,Christian Margreitter,https://github.com/MDplot/MDplot,TRUE,https://github.com/mdplot/mdplot,17292,11,2021-05-18T11:04:31Z,1572
mdsr,"A complement to *Modern Data
    Science with R*, both the first (ISBN: 978-1498724487, publisher URL: 
    <https://www.routledge.com/Modern-Data-Science-with-R/Baumer-Kaplan-Horton/p/book/9781498724487>)
    and second editions (ISBN: 978-0367191498, publisher URL: 
    <https://www.routledge.com/Modern-Data-Science-with-R/Baumer-Kaplan-Horton/p/book/9780367191498>).
    This package contains data and code to complete exercises and 
    reproduce examples from the text. It also facilitates connections 
    to the SQL database server used in the book. Both editions of the book are 
    supported by this package.",2021-03-29,Benjamin S. Baumer,https://github.com/mdsr-book/mdsr,TRUE,https://github.com/mdsr-book/mdsr,43632,28,2021-03-29T20:05:33Z,1558.2857142857142
measurementProtocol,"Send server-side tracking data from R.
  The Measurement Protocol version 2 
  <https://developers.google.com/analytics/devguides/collection/protocol/ga4>
  allows sending HTTP tracking events from R code.",2021-04-15,Mark Edmondson,https://code.markedmondson.me/measurementProtocol/,TRUE,https://github.com/markedmondson1234/measurementprotocol,14819,2,2021-04-17T08:29:57Z,7409.5
mecor,"Covariate measurement error correction is implemented by means of regression calibration by Carroll RJ, Ruppert D, Stefanski LA & Crainiceanu CM (2006, ISBN:1584886331), efficient regression calibration by Spiegelman D, Carroll RJ & Kipnis V (2001) <doi:10.1002/1097-0258(20010115)20:1%3C139::AID-SIM644%3E3.0.CO;2-K> and maximum likelihood estimation by Bartlett JW, Stavola DBL & Frost C (2009) <doi:10.1002/sim.3713>. Outcome measurement error correction is implemented by means of the method of moments by Buonaccorsi JP (2010, ISBN:1420066560) and efficient method of moments by Keogh RH, Carroll RJ, Tooze JA, Kirkpatrick SI & Freedman LS (2014) <doi:10.1002/sim.7011>. Standard error estimation of the corrected estimators is implemented by means of the Delta method by Rosner B, Spiegelman D & Willett WC (1990) <doi:10.1093/oxfordjournals.aje.a115715> and Rosner B, Spiegelman D & Willett WC (1992) <doi:10.1093/oxfordjournals.aje.a116453>, the Fieller method described by Buonaccorsi JP (2010, ISBN:1420066560), and the Bootstrap by Carroll RJ, Ruppert D, Stefanski LA & Crainiceanu CM (2006, ISBN:1584886331).",2021-01-14,Linda Nab,https://github.com/LindaNab/mecor,TRUE,https://github.com/lindanab/mecor,2617,5,2021-05-31T08:56:04Z,523.4
medfate,Functions to simulate Mediterranean forest functioning and dynamics using cohort-based description of vegetation [De Caceres et al. (2015) <doi:10.1016/j.agrformet.2015.06.012>; De Caceres et al. (2021) <doi:10.1016/j.agrformet.2020.108233>].,2021-06-18,Miquel De Cáceres,https://github.com/emf-creaf/medfate,TRUE,https://github.com/emf-creaf/medfate,19073,2,2021-07-30T14:00:23Z,9536.5
medflex,"Run flexible mediation analyses using natural effect models as described in 
  Lange, Vansteelandt and Bekaert (2012) <DOI:10.1093/aje/kwr525>, 
  Vansteelandt, Bekaert and Lange (2012) <DOI:10.1515/2161-962X.1014> 
  and Loeys, Moerkerke, De Smet, Buysse, Steen and Vansteelandt (2013) <DOI:10.1080/00273171.2013.832132>.",2020-08-03,Johan Steen,https://github.com/jmpsteen/medflex,TRUE,https://github.com/jmpsteen/medflex,31079,7,2021-02-23T16:21:47Z,4439.857142857143
Mediana,"Provides a general framework for clinical trial simulations based
    on the Clinical Scenario Evaluation (CSE) approach. The package supports a
    broad class of data models (including clinical trials with continuous, binary,
    survival-type and count-type endpoints as well as multivariate outcomes that are
    based on combinations of different endpoints), analysis strategies and commonly
    used evaluation criteria.",2019-05-08,Gautier Paux,http://gpaux.github.io/Mediana/,TRUE,https://github.com/gpaux/mediana,20219,15,2021-05-29T12:54:24Z,1347.9333333333334
MedianaDesigner,"
    The following modules are included in the package: 
    Adaptive designs with data-driven sample size or event count re-estimation, 
    Adaptive designs with data-driven treatment selection, 
    Adaptive designs with data-driven population selection, 
    Optimal selection of a futility stopping rule, 
    Event prediction in event-driven trials.",2021-08-16,Alex Dmitrienko,https://github.com/medianasoft/MedianaDesigner,TRUE,https://github.com/medianasoft/medianadesigner,860,4,2021-08-18T23:59:23Z,215
medicaldata,"Provides access to well-documented medical datasets for teaching.
    Featuring several from the Teaching of Statistics in the Health Sciences 
    website <https://www.causeweb.org/tshs/category/dataset/>, a few reconstructed datasets of historical significance in medical
    research, some reformatted and extended from existing R packages, 
    and some data donations. ",2021-08-16,Peter Higgins,"https://higgi13425.github.io/medicaldata/,
https://github.com/higgi13425/medicaldata/",TRUE,https://github.com/higgi13425/medicaldata,243,10,2021-08-20T15:25:13Z,24.3
meditations,Prints a random quote from Marcus Aurelius' book Meditations.,2019-01-16,Jacob Kaplan,https://github.com/jacobkap/meditations,TRUE,https://github.com/jacobkap/meditations,12337,1,2020-12-30T22:26:08Z,12337
medrxivr,"An increasingly important source of health-related bibliographic 
    content are preprints - preliminary versions of research articles that have
    yet to undergo peer review. The two preprint repositories most relevant to 
    health-related sciences are medRxiv <https://www.medrxiv.org/> and
    bioRxiv <https://www.biorxiv.org/>, both of which are operated by the Cold 
    Spring Harbor Laboratory. 'medrxivr' provides programmatic access to the 
    'Cold Spring Harbour Laboratory (CSHL)' API <https://api.biorxiv.org/>,
    allowing users to easily download medRxiv and bioRxiv preprint metadata
    (e.g. title, abstract, publication date, author list, etc) into R. 
    'medrxivr' also provides functions to search the downloaded preprint records 
    using regular expressions and Boolean logic, as well as helper functions 
    that allow users to export their search results to a .BIB file for easy 
    import to a reference manager and to download the full-text PDFs of 
    preprints matching their search criteria.",2021-02-24,Luke McGuinness,https://github.com/ropensci/medrxivr,TRUE,https://github.com/ropensci/medrxivr,4737,22,2021-02-24T16:51:14Z,215.3181818181818
MEDseq,"Implements a model-based clustering method for categorical life-course sequences relying on mixtures of exponential-distance models introduced by Murphy et al. (2021) <doi:10.1111/rssa.12712>. A range of flexible precision parameter settings corresponding to weighted generalisations of the Hamming distance metric are considered, along with the potential inclusion of a noise component. Gating covariates can be supplied in order to relate sequences to baseline characteristics. Sampling weights are also accommodated. The models are fitted using the EM algorithm and tools for visualising the results are also provided.",2021-07-15,Keefe Murphy,https://cran.r-project.org/package=MEDseq,TRUE,https://github.com/keefe-murphy/medseq,10892,2,2021-07-15T16:14:21Z,5446
meltr,"The goal of 'meltr' is to provide a fast and friendly way to
    read non-rectangular data, such as ragged forms of csv (comma-separated
    values), tsv (tab-separated values), and fwf (fixed-width format) files.",2021-08-23,Duncan Garmonsway,https://github.com/r-lib/meltr,TRUE,https://github.com/r-lib/meltr,152,17,2021-08-24T12:23:09Z,8.941176470588236
mem,"The Moving Epidemic Method, created by T Vega and JE Lozano (2012, 2015) <doi:10.1111/j.1750-2659.2012.00422.x>, <doi:10.1111/irv.12330>, allows the weekly assessment of the epidemic and intensity status to help in routine respiratory infections surveillance in health systems. Allows the comparison of different epidemic indicators, timing and shape with past epidemics and across different regions or countries with different surveillance systems. Also, it gives a measure of the performance of the method in terms of sensitivity and specificity of the alert week.",2020-09-13,Jose E. Lozano,https://github.com/lozalojo/mem,TRUE,https://github.com/lozalojo/mem,27305,4,2021-03-31T11:52:47Z,6826.25
memapp,"The Moving Epidemic Method, created by T Vega and JE Lozano (2012, 2015) <doi:10.1111/j.1750-2659.2012.00422.x>, <doi:10.1111/irv.12330>, allows the weekly assessment of the epidemic and intensity status to help in routine respiratory infections surveillance in health systems. Allows the comparison of different epidemic indicators, timing and shape with past epidemics and across different regions or countries with different surveillance systems. Also, it gives a measure of the performance of the method in terms of sensitivity and specificity of the alert week. 'memapp' is a web application created in the Shiny framework for the 'mem' R package.",2020-09-10,Jose E. Lozano,https://github.com/lozalojo/memapp,TRUE,https://github.com/lozalojo/memapp,23892,2,2021-03-31T10:16:59Z,11946
meme,"The word 'Meme' was originated from the book, 'The Selfish Gene', authored by Richard Dawkins (1976).
             It is a unit of culture that is passed from one generation to another and correlates to the gene, the unit of physical heredity.
             The internet memes are captioned photos that are intended to be funny, ridiculous.
             Memes behave like infectious viruses and travel from person to person quickly through social media.
             The 'meme' package allows users to make custom memes.",2021-04-23,Guangchuang Yu,https://github.com/GuangchuangYu/meme/,TRUE,https://github.com/guangchuangyu/meme,22535,36,2021-04-19T07:37:43Z,625.9722222222222
memery,"Generates internet memes that optionally include a superimposed inset plot and other atypical features, 
    combining the visual impact of an attention-grabbing meme with graphic results of data analysis.
    The package differs from related packages that focus on imitating and reproducing standard memes.
    Some packages do this by interfacing with online meme generators whereas others achieve this natively.
    This package takes the latter approach. It does not interface with online meme generators or require any authentication with external websites.
    It reads images directly from local files or via URL and meme generation is done by the package.
    While this is similar to the 'meme' package available on CRAN, it differs in that the focus is on 
    allowing for non-standard meme layouts and hybrids of memes mixed with graphs.
    While this package can be used to make basic memes like an online meme generator would produce, 
    it caters primarily to hybrid graph-meme plots where the meme presentation can be seen as a backdrop highlighting 
    foreground graphs of data analysis results.
    The package also provides support for an arbitrary number of meme text labels with arbitrary size, position and other attributes 
    rather than restricting to the standard top and/or bottom text placement. 
    This is useful for proper aesthetic interleaving of plots of data between meme image backgrounds and overlain text labels.
    The package offers a selection of templates for graph placement and appearance with respect to the underlying meme.
    Graph templates also permit additional template-specific customization.
    Animated gif support is provided but this is optional and functional only if the 'magick' package is installed. 
    'magick' is not required unless gif functionality is desired.",2021-02-14,Matthew Leonawicz,https://github.com/leonawicz/memery,TRUE,https://github.com/leonawicz/memery,20418,17,2021-02-12T19:41:51Z,1201.0588235294117
memisc,"An infrastructure for the management of survey data including
        value labels, definable missing values, recoding of variables,
        production of code books, and import of (subsets of) 'SPSS' and
        'Stata' files is provided. Further, the package allows to produce
        tables and data frames of arbitrary descriptive statistics and
        (almost) publication-ready tables of regression model
        estimates, which can be exported to 'LaTeX' and HTML.",2020-11-18,Martin Elff (with contributions from Christopher N. Lawrence,"http://www.elff.eu/software/memisc/,https://github.com/melff/memisc/",TRUE,https://github.com/melff/memisc,387650,37,2021-08-01T21:15:59Z,10477.027027027027
memofunc,A simple way to memoize function results to improve performance by eliminating unnecessary computation or data retrieval activities.,2021-02-22,Roy Wetherall,"https://github.com/rwetherall/memofunc,
https://rwetherall.github.io/memofunc/",TRUE,https://github.com/rwetherall/memofunc,1994,1,2021-02-23T09:38:02Z,1994
memoiR,"Producing high-quality documents suitable for publication directly from R is made possible by the R Markdown ecosystem.
  'memoiR' makes it easy.
  It provides templates to knit memoirs, articles and slideshows with helpers to publish the documents on GitHub Pages and activate continuous integration.",2021-07-26,Eric Marcon,https://github.com/EricMarcon/memoiR,TRUE,https://github.com/ericmarcon/memoir,1812,1,2021-07-26T11:32:11Z,1812
memoise,"Cache the results of a function so that when you
    call it again with the same arguments it returns the pre-computed
    value.",2021-01-26,Jim Hester,https://github.com/r-lib/memoise,TRUE,https://github.com/r-lib/memoise,7787872,274,2021-06-29T14:00:54Z,28422.890510948906
memor,"A 'rmarkdown' template that supports company logo, contact info, 
    watermarks and more. Currently restricted to 'Latex'/'Markdown'; a similar 
    'HTML' theme will be added in the future. ",2021-01-19,Hao Zhu,https://github.com/hebrewseniorlife/memor,TRUE,https://github.com/hebrewseniorlife/memor,16062,76,2021-03-01T02:37:10Z,211.3421052631579
merDeriv,"Compute case-wise and cluster-wise derivative for mixed effects models with respect to fixed effects parameter, random effect (co)variances, and residual variance. This material is partially based on work supported by the National Science Foundation under Grant Number 1460719.",2021-01-26,Ting Wang,https://github.com/nctingwang/merDeriv,TRUE,https://github.com/nctingwang/merderiv,49208,1,2021-06-29T21:44:52Z,49208
MESS,"A mixed collection of useful and semi-useful diverse
    statistical functions, some of which may even be referenced in
    The R Primer book.",2020-07-21,Claus Thorn Ekstrøm,https://github.com/ekstroem/MESS,TRUE,https://github.com/ekstroem/mess,71182,3,2021-04-23T17:43:16Z,23727.333333333332
metaBMA,"Computes the posterior model probabilities for standard meta-analysis models 
    (null model vs. alternative model assuming either fixed- or random-effects, respectively).
    These posterior probabilities are used to estimate the overall mean effect size 
    as the weighted average of the mean effect size estimates of the random- and 
    fixed-effect model as proposed by Gronau, Van Erp, Heck, Cesario, Jonas, & 
    Wagenmakers (2017, <doi:10.1080/23743603.2017.1326760>). The user can define 
    a wide range of non-informative or informative priors for the mean effect size 
    and the heterogeneity coefficient. Moreover, using pre-compiled Stan models, 
    meta-analysis with continuous and discrete moderators with Jeffreys-Zellner-Siow (JZS) 
    priors can be fitted and tested. This allows to compute Bayes factors and 
    perform Bayesian model averaging across random- and fixed-effects meta-analysis 
    with and without moderators. For a primer on Bayesian model-averaged meta-analysis, 
    see Gronau, Heck, Berkhout, Haaf, & Wagenmakers (2020, <doi:10.31234/osf.io/97qup>).",2021-03-17,Daniel W. Heck,https://github.com/danheck/metaBMA,TRUE,https://github.com/danheck/metabma,120747,14,2021-03-17T12:11:27Z,8624.785714285714
metaboData,"Data sets from a variety of biological sample matrices, 
    analysed using a number of mass spectrometry based metabolomic analytical techniques.
    The example data sets are stored remotely using GitHub releases 
    <https://github.com/aberHRML/metaboData/releases> which can be accessed from R using the package.
    The package also includes the 'abr1' FIE-MS data set from the 'FIEmspro' package <https://users.aber.ac.uk/jhd/> <doi:10.1038/nprot.2007.511>.",2021-08-20,Jasen Finch,https://aberhrml.github.io/metaboData/,TRUE,https://github.com/aberhrml/metabodata,173,0,2021-08-19T11:23:42Z,NA
metabolic,"Dataset and functions from the meta-analysis published in Medicine & Science in Sports & Exercise. 
    It contains all the data and functions to reproduce the analysis.
    ""Effectiveness of HIIE versus MICT in Improving Cardiometabolic Risk Factors in Health and Disease: A Meta-analysis"".
    Felipe Mattioni Maturana, Peter Martus, Stephan Zipfel, Andreas M Nieß (2020) <doi:10.1249/MSS.0000000000002506>.",2020-09-25,Felipe Mattioni Maturana,https://github.com/fmmattioni/metabolic,TRUE,https://github.com/fmmattioni/metabolic,4187,1,2020-09-27T12:59:53Z,4187
metabolighteR,Access to the 'Metabolights' REST API <https://www.ebi.ac.uk/metabolights/index>. Retrieve elements of publicly available 'Metabolights' studies.  ,2021-04-05,Tom Wilson,https://github.com/aberHRML/metabolighteR,TRUE,https://github.com/aberhrml/metabolighter,8969,4,2021-03-29T15:22:03Z,2242.25
metacoder,"A set of tools for parsing, manipulating, and graphing data
    classified by a hierarchy (e.g. a taxonomy).",2021-06-23,Zachary Foster,https://grunwaldlab.github.io/metacoder_documentation/,TRUE,https://github.com/grunwaldlab/metacoder,37148,97,2021-07-14T17:04:50Z,382.96907216494844
metacom,"Functions to analyze coherence, boundary clumping, and turnover
    following the pattern-based metacommunity analysis of Leibold and Mikkelson
    2002  <doi:10.1034/j.1600-0706.2002.970210.x>. The package also includes 
		functions to visualize ecological networks, and to calculate modularity 
		as a replacement to boundary clumping.",2020-03-23,Tad Dallas,https://cran.r-project.org/package=metacom,TRUE,https://github.com/taddallas/metacom,27558,9,2021-05-11T15:40:32Z,3062
metadat,"A collection of meta-analysis datasets for teaching purposes, illustrating/testing meta-analytic methods, and validating published analyses.",2021-08-20,W. Kyle Hamilton,https://github.com/wviechtb/metadat,TRUE,https://github.com/wviechtb/metadat,254,11,2021-09-03T13:46:58Z,23.09090909090909
metaDigitise,"High-throughput, flexible and reproducible extraction of data from figures in primary research papers. metaDigitise() can extract data and / or automatically calculate summary statistics for users from box plots, bar plots (e.g., mean and errors), scatter plots and histograms.",2020-03-13,Daniel Noble,NA,TRUE,https://github.com/daniel1noble/metadigitise,15148,62,2021-03-31T00:44:11Z,244.32258064516128
metagam,"Meta-analysis of generalized additive
    models and generalized additive mixed models. A typical use case is
    when data cannot be shared across locations, and an overall meta-analytic
    fit is sought. 'metagam' provides functionality for removing individual
    participant data from models computed using the 'mgcv' and 'gamm4' packages such
    that the model objects can be shared without exposing individual data.
    Furthermore, methods for meta-analysing these fits are provided. The implemented
    methods are described in Sorensen et al. (2020), <arXiv:2002.02627>,
    extending previous works by Schwartz and Zanobetti (2000)
    and Crippa et al. (2018) <doi:10.6000/1929-6029.2018.07.02.1>.",2020-11-12,Oystein Sorensen,"https://lifebrain.github.io/metagam/,
https://github.com/Lifebrain/metagam",TRUE,https://github.com/lifebrain/metagam,10397,6,2021-06-24T12:08:00Z,1732.8333333333333
MetaIntegration,"An ensemble meta-prediction framework to integrate multiple regression 
    models into a current study. Gu, T., Taylor, J.M.G. and Mukherjee, B. (2020) 
    <arXiv:2010.09971>.
    A meta-analysis framework along with two weighted estimators as the ensemble 
    of empirical Bayes estimators, which combines the estimates from the different 
    external models. The proposed framework is flexible and robust in the ways 
    that (i) it is capable of incorporating external models that use a slightly 
    different set of covariates; (ii) it is able to identify the most relevant 
    external information and diminish the influence of information that is less 
    compatible with the internal data; and (iii) it nicely balances the bias-variance 
    trade-off while preserving the most efficiency gain. The proposed estimators 
    are more efficient than the naive analysis of the internal data and other 
    naive combinations of external estimators.",2021-03-17,Michael Kleinsasser,https://github.com/umich-biostatistics/MetaIntegration,TRUE,https://github.com/umich-biostatistics/metaintegration,3271,0,2021-06-17T22:53:28Z,NA
metajam,"A set of tools to foster the development of reproducible analytical workflow by simplifying the download of data and 
    metadata from 'DataONE' (<https://www.dataone.org>) and easily importing this information into R.",2020-11-03,Julien Brun,https://github.com/nceas/metajam,TRUE,https://github.com/nceas/metajam,3950,12,2021-05-03T18:52:33Z,329.1666666666667
metamer,"Creates data with identical statistics (metamers) using an iterative 
   algorithm proposed by Matejka & Fitzmaurice (2017) <DOI:10.1145/3025453.3025912>.",2019-09-18,Elio Campitelli,https://github.com/eliocamp/metamer,TRUE,https://github.com/eliocamp/metamer,11422,14,2021-07-20T15:41:44Z,815.8571428571429
metamicrobiomeR,"Generalized Additive Model for Location, Scale and Shape (GAMLSS) 
    with zero inflated beta (BEZI) family for analysis of microbiome relative abundance data 
    (with various options for data transformation/normalization to address compositional effects) and 
    random effects meta-analysis models for meta-analysis pooling estimates across microbiome studies 
    are implemented. 
    Random Forest model to predict microbiome age based on relative abundances of  
    shared bacterial genera with the Bangladesh data (Subramanian et al 2014), 
    comparison of multiple diversity indexes using linear/linear mixed effect models 
    and some data display/visualization are also implemented.
    The reference paper is published by 
    Ho NT, Li F, Wang S, Kuhn L (2019) <doi:10.1186/s12859-019-2744-2> . ",2020-11-09,Nhan Ho,https://github.com/nhanhocu/metamicrobiomeR,TRUE,https://github.com/nhanhocu/metamicrobiomer,6729,17,2020-11-12T02:58:42Z,395.8235294117647
metan,"Performs stability analysis of multi-environment
    trial data using parametric and non-parametric methods. Parametric
    methods includes Additive Main Effects and Multiplicative Interaction
    (AMMI) analysis by Gauch (2013) <doi:10.2135/cropsci2013.04.0241>,
    Ecovalence by Wricke (1965), Genotype plus Genotype-Environment (GGE)
    biplot analysis by Yan & Kang (2003) <doi:10.1201/9781420040371>,
    geometric adaptability index by Mohammadi & Amri (2008)
    <doi:10.1007/s10681-007-9600-6>, joint regression analysis by Eberhart
    & Russel (1966) <doi:10.2135/cropsci1966.0011183X000600010011x>,
    genotypic confidence index by Annicchiarico (1992), Murakami & Cruz's
    (2004) method, power law residuals
    (POLAR) statistics by Doring et al. (2015)
    <doi:10.1016/j.fcr.2015.08.005>, scale-adjusted coefficient of
    variation by Doring & Reckling (2018) <doi:10.1016/j.eja.2018.06.007>,
    stability variance by Shukla (1972) <doi:10.1038/hdy.1972.87>,
    weighted average of absolute scores by Olivoto et al. (2019a)
    <doi:10.2134/agronj2019.03.0220>, and multi-trait stability index by
    Olivoto et al. (2019b) <doi:10.2134/agronj2019.03.0221>.
    Non-parametric methods includes superiority index by Lin & Binns
    (1988) <doi:10.4141/cjps88-018>, nonparametric measures of phenotypic
    stability by Huehn (1990)
    <https://link.springer.com/article/10.1007/BF00024241>, TOP third
    statistic by Fox et al. (1990) <doi:10.1007/BF00040364>. Functions for
    computing biometrical analysis such as path analysis, canonical
    correlation, partial correlation, clustering analysis, and tools for
    inspecting, manipulating, summarizing and plotting typical
    multi-environment trial data are also provided.",2021-07-15,Tiago Olivoto,https://github.com/TiagoOlivoto/metan,TRUE,https://github.com/tiagoolivoto/metan,27695,13,2021-08-26T11:36:02Z,2130.3846153846152
metapack,"Contains functions performing Bayesian inference for meta-analytic and network meta-analytic models through Markov chain Monte Carlo algorithm. Currently, the package implements Hui Yao, Sungduk Kim, Ming-Hui Chen, Joseph G. Ibrahim, Arvind K. Shah, and Jianxin Lin (2015) <doi:10.1080/01621459.2015.1006065> and Hao Li, Daeyoung Lim, Ming-Hui Chen, Joseph G. Ibrahim, Sungduk Kim, Arvind K. Shah, Jianxin Lin (2021) <doi:10.1002/sim.8983>. For maximal computational efficiency, the Markov chain Monte Carlo samplers for each model, written in C++, are fine-tuned. This software has been developed under the auspices of the National Institutes of Health and Merck & Co., Inc., Kenilworth, NJ, USA.",2021-07-20,Daeyoung Lim,http://merlot.stat.uconn.edu/packages/metapack/,TRUE,https://github.com/daeyounglim/metapack,2841,0,2021-07-19T23:08:59Z,NA
metarep,"User-friendly package for reporting replicability-analysis methods, affixed to meta-analyses summary. The replicability-analysis output provides an assessment of the investigated intervention, where it offers quantification of effect replicability and assessment of the consistency of findings.
 - Replicability-analysis for fixed-effects and random-effect meta analysis: 
 - r(u)-value;
 - lower bounds on the number of studies with replicated positive and\or negative effect;
 - Allows detecting inconsistency of signals;
 - forest plots with the summary of replicability analysis results;
 - Allows Replicability-analysis with or without the common-effect assumption. ",2020-04-06,Iman Jaljuli,https://github.com/IJaljuli/metarep,TRUE,https://github.com/ijaljuli/metarep,6257,1,2021-07-06T14:36:54Z,6257
metaSEM,"A collection of functions for conducting meta-analysis using a
             structural equation modeling (SEM) approach via the 'OpenMx' and
             'lavaan' packages. It also implements various procedures to
			 perform meta-analytic structural equation modeling on the
             correlation and covariance matrices,
			 see Cheung (2015) <doi:10.3389/fpsyg.2014.01521>.",2021-05-17,Mike Cheung,https://github.com/mikewlcheung/metasem,TRUE,https://github.com/mikewlcheung/metasem,39732,21,2021-08-04T10:50:37Z,1892
MetaStan,"Performs Bayesian meta-analysis and model-based meta-analysis using 'Stan'. 
             Includes binomial-normal hierarchical models and option to use weakly informative priors for the
             heterogeneity parameter and the treatment effect parameter which are described in 
             Guenhan, Roever, and Friede (2020) <doi:10.1002/jrsm.1370>.",2021-07-25,Burak Kuersad Guenhan,https://github.com/gunhanb/MetaStan,TRUE,https://github.com/gunhanb/metastan,17958,6,2021-06-28T13:05:37Z,2993
metaSurvival,"To assess a summary survival curve from survival probabilities and number of at-risk patients collected at various points in time in various studies, and to test the between-strata heterogeneity.",2020-12-07,Shubhram Pandey,https://github.com/shubhrampandey/metaSurvival,TRUE,https://github.com/shubhrampandey/metasurvival,3113,4,2021-02-23T12:38:29Z,778.25
metathis,"Create meta tags for 'R Markdown' HTML documents and 'Shiny'
    apps for customized social media cards, for accessibility, and quality
    search engine indexing. 'metathis' currently supports HTML documents
    created with 'rmarkdown', 'shiny', 'xaringan', 'pagedown', 'bookdown',
    and 'flexdashboard'.",2021-06-29,Garrick Aden-Buie,"https://pkg.garrickadenbuie.com/metathis/,
https://github.com/gadenbuie/metathis",TRUE,https://github.com/gadenbuie/metathis,12372,56,2021-06-30T02:32:21Z,220.92857142857142
metavcov,"Compute variance-covariance matrix for multivariate meta-analysis. Effect sizes include correlation (r), mean difference (MD), standardized mean difference (SMD), log odds ratio (logOR), log risk ratio (logRR), and risk difference (RD).",2021-08-21,Min Lu,https://github.com/luminwin/metavcov,TRUE,https://github.com/luminwin/metavcov,21538,0,2021-08-29T01:00:38Z,NA
meteoForecast,"Access to several Numerical Weather Prediction services both in raster format and as a time series for a location. Currently it works with GFS <https://www.ncdc.noaa.gov/data-access/model-data/model-datasets/global-forcast-system-gfs>, MeteoGalicia <https://www.meteogalicia.gal/web/modelos/threddsIndex.action>, NAM <https://www.ncdc.noaa.gov/data-access/model-data/model-datasets/north-american-mesoscale-forecast-system-nam>, and RAP <https://www.ncdc.noaa.gov/data-access/model-data/model-datasets/rapid-refresh-rap>.",2021-03-02,Oscar Perpinan Lamigueiro,https://github.com/oscarperpinan/meteoForecast,TRUE,https://github.com/oscarperpinan/meteoforecast,22026,44,2021-03-02T14:15:16Z,500.59090909090907
meteor,"A set of functions for weather and climate data manipulation, and other helper functions, to support dynamic ecological modelling, particularly crop and crop disease modeling.",2019-08-08,Robert J. Hijmans,NA,TRUE,https://github.com/cropmodels/meteor,10509,3,2021-06-14T20:11:42Z,3503
MethComp,"Methods (standard and advanced) for analysis of agreement between measurement methods. These cover Bland-Altman plots, Deming regression, Lin's Total deviation index, and difference-on-average regression. See Carstensen B. (2010) ""Comparing Clinical Measurement Methods: A Practical Guide (Statistics in Practice)"" <doi:10.1002/9780470683019> for more information.",2020-01-19,Claus Thorn Ekstrøm,http://BendixCarstensen.com/MethComp/,TRUE,https://github.com/ekstroem/methcomp,15694,1,2021-05-27T20:23:38Z,15694
metR,"Many useful functions and extensions for dealing
    with meteorological data in the tidy data framework. Extends 'ggplot2'
    for better plotting of scalar and vector fields and provides commonly
    used analysis methods in the atmospheric sciences.",2021-08-07,Elio Campitelli,https://github.com/eliocamp/metR,TRUE,https://github.com/eliocamp/metr,45505,109,2021-08-06T22:16:31Z,417.4770642201835
MetricsWeighted,"Provides weighted versions of several metrics, scoring
    functions and performance measures used in machine learning, including
    average unit deviances of the Bernoulli, Tweedie, Poisson, and Gamma
    distributions, see Jorgensen B. (1997, ISBN: 978-0412997112). The
    package also contains a weighted version of generalized R-squared, see
    e.g. Cohen, J. et al. (2002, ISBN: 978-0805822236). Furthermore,
    'dplyr' chains are supported.",2021-06-06,Michael Mayer,https://github.com/mayer79/MetricsWeighted,TRUE,https://github.com/mayer79/metricsweighted,19244,6,2021-06-06T17:38:30Z,3207.3333333333335
metro,"The Washington Metropolitan Area Transit Authority
    is a government agency operating light rail and passenger buses in the
    Washington D.C. area. With a free developer account, access their
    'Metro Transparent Data Sets API' <https://developer.wmata.com/> to
    return data frames of transit data for easy analysis.",2021-03-01,Kiernan Nicholls,"https://github.com/kiernann/metro, https://kiernann.com/metro/",TRUE,https://github.com/kiernann/metro,2039,3,2021-03-09T00:28:03Z,679.6666666666666
mets,"Implementation of various statistical models for multivariate
    event history data <doi:10.1007/s10985-013-9244-x>. Including multivariate
    cumulative incidence models <doi:10.1002/sim.6016>, and  bivariate random
    effects probit models (Liability models) <doi:10.1016/j.csda.2015.01.014>.
    Also contains two-stage binomial modelling that can do pairwise odds-ratio
    dependence modelling based marginal logistic regression models. This is an
    alternative to the alternating logistic regression approach (ALR).",2020-09-28,Klaus K. Holst,https://kkholst.github.io/mets/,TRUE,https://github.com/kkholst/mets,130006,5,2021-09-02T14:14:35Z,26001.2
metScanR,"A tool for locating, mapping, and gathering environmental data and metadata, worldwide.  Users can search for and filter metadata from > 157,000 environmental monitoring stations among 219 countries/territories and >20 networks/organizations via elevation, location, active dates, elements measured (e.g., temperature, precipitation), country, network, and/or known identifier. Future updates to the package will allow the user to obtain datasets from stations within the database.",2019-10-16,Josh Roberti,https://github.com/jaroberti/metScanR,TRUE,https://github.com/jaroberti/metscanr,17824,11,2021-05-21T14:50:06Z,1620.3636363636363
mev,"Various tools for the analysis of univariate, multivariate and functional extremes. Exact simulation from max-stable processes [Dombry, Engelke and Oesting (2016) <doi:10.1093/biomet/asw008>, R-Pareto processes for various parametric models, including Brown-Resnick (Wadsworth and Tawn, 2014, <doi:10.1093/biomet/ast042>) and Extremal Student (Thibaud and Opitz, 2015, <doi:10.1093/biomet/asv045>). Threshold selection methods, including Wadsworth (2016) <doi:10.1080/00401706.2014.998345>, and Northrop and Coleman (2014) <doi:10.1007/s10687-014-0183-z>. Multivariate extreme diagnostics. Estimation and likelihoods for univariate extremes, e.g., Coles (2001) <doi:10.1007/978-1-4471-3675-0>.",2020-01-27,Leo Belzile,https://github.com/lbelzile/mev/,TRUE,https://github.com/lbelzile/mev,67806,3,2021-08-16T18:46:18Z,22602
mfbvar,"Functions and tools for estimation of mixed-frequency Bayesian vector autoregressive (VAR) models. The package implements a state space-based VAR model that handles mixed frequencies of the data as proposed by Schorfheide and Song (2015) <doi:10.1080/07350015.2014.954707>, and extensions thereof developed by Ankargren, Unosson and Yang (2020) <doi:10.1515/jtse-2018-0034>, Ankargren and Joneus (2019) <arXiv:1912.02231>, and Ankargren and Joneus (2020) <doi:10.1016/j.ecosta.2020.05.007>. The models are estimated using Markov Chain Monte Carlo to numerically approximate the posterior distribution. Prior distributions that can be used include normal-inverse Wishart and normal-diffuse priors as well as steady-state priors. Stochastic volatility can be handled by common or factor stochastic volatility models.",2021-02-10,Sebastian Ankargren,https://github.com/ankargren/mfbvar,TRUE,https://github.com/ankargren/mfbvar,14596,23,2021-02-10T21:50:23Z,634.6086956521739
mfe,"Extracts meta-features from datasets to support the design of 
  recommendation systems based on Meta-Learning. The meta-features, also called 
  characterization measures, are able to characterize the complexity of datasets
  and to provide estimates of algorithm performance. The package contains not 
  only the standard characterization measures, but also more recent 
  characterization measures. By making available a large set of meta-feature 
  extraction functions, tasks like comprehensive data characterization, deep 
  data exploration and large number of Meta-Learning based data analysis can be
  performed. These concepts are described in the paper: Rivolli A., Garcia L., 
  Soares c., Vanschoren J. and Carvalho A. (2018) <arXiv:1808.10406>.",2020-05-05,Adriano Rivolli,https://github.com/rivolli/mfe,TRUE,https://github.com/rivolli/mfe,16238,23,2021-01-15T14:00:51Z,706
mfGARCH,"Estimating GARCH-MIDAS (MIxed-DAta-Sampling) models (Engle, Ghysels, Sohn, 2013, <doi:10.1162/REST_a_00300>) and related statistical inference, accompanying the paper ""Two are better than one: Volatility forecasting using multiplicative component GARCH models"" by Conrad and Kleen (2020, <doi:10.1002/jae.2742>). The GARCH-MIDAS model decomposes the conditional variance of (daily) stock returns into a short- and long-term component, where the latter may depend on an exogenous covariate sampled at a lower frequency. ",2021-06-17,Onno Kleen,https://github.com/onnokleen/mfGARCH/,TRUE,https://github.com/onnokleen/mfgarch,17795,29,2021-06-22T07:28:20Z,613.6206896551724
mFLICA,"A leadership-inference framework for multivariate time series. The framework for multiple-faction-leadership inference from coordinated activities or 'mFLICA' uses a notion of a leader as an individual who initiates collective patterns that everyone in a group follows. Given a set of time series of individual activities, our goal is to identify periods of coordinated activity, find factions of coordination if more than one exist, as well as identify leaders of each faction. For each time step, the framework infers following relations between individual time series, then identifying a leader of each faction whom many individuals follow but it follows no one. A faction is defined as a group of individuals that everyone follows the same leader. 'mFLICA' reports following relations, leaders of factions, and members of each faction for each time step. Please see Chainarong Amornbunchornvej and Tanya Berger-Wolf (2018) <doi:10.1137/1.9781611975321.62> when referring to this package in publications.",2020-04-03,Chainarong Amornbunchornvej,https://github.com/DarkEyes/mFLICA,TRUE,https://github.com/darkeyes/mflica,8223,3,2021-04-30T15:57:21Z,2741
MFPCA,"Calculate a multivariate functional principal component analysis
    for data observed on different dimensional domains. The estimation algorithm
    relies on univariate basis expansions for each element of the multivariate
    functional data  (Happ & Greven, 2018) <doi:10.1080/01621459.2016.1273115>. 
    Multivariate and univariate functional data objects are
    represented by S4 classes for this type of data implemented in the package
    'funData'. For more details on the general concepts of both packages and a case 
    study, see Happ-Kurz (2020) <doi:10.18637/jss.v093.i05>.",2021-08-09,Clara Happ-Kurz,https://github.com/ClaraHapp/MFPCA,TRUE,https://github.com/clarahapp/mfpca,19734,18,2021-08-09T17:42:38Z,1096.3333333333333
mgc,"Multiscale Graph Correlation (MGC) is a framework developed by Vogelstein et al. (2019) <DOI:10.7554/eLife.41690> that extends global correlation procedures to be multiscale; consequently, MGC tests typically require far fewer samples than existing methods for a wide variety of dependence structures and dimensionalities, while maintaining computational efficiency. Moreover, MGC provides a simple and elegant multiscale characterization of the potentially complex latent geometry underlying the relationship.",2020-06-23,Eric Bridgeford,https://github.com/neurodata/r-mgc,TRUE,https://github.com/neurodata/r-mgc,9639,9,2021-02-23T04:35:24Z,1071
mgcViz,"Extension of the 'mgcv' package, providing visual tools for Generalized Additive Models that exploit the additive structure of such models, scale to large data sets and can be used in conjunction with a wide range of response distributions. The focus is providing visual methods for better understanding the model output and for aiding model checking and development beyond simple exponential family regression. The graphical framework is based on the layering system provided by 'ggplot2'.",2021-08-21,Matteo Fasiolo,https://github.com/mfasiolo/mgcViz,TRUE,https://github.com/mfasiolo/mgcviz,35067,69,2021-08-21T12:36:06Z,508.2173913043478
MGDrivE,"Provides a model designed to be a reliable testbed where various gene 
    drive interventions for mosquito-borne diseases control. It is being developed to 
    accommodate the use of various mosquito-specific gene drive systems within a 
    population dynamics framework that allows migration of individuals between patches 
    in landscape. Previous work developing the population dynamics can be found in Deredec et al. 
    (2001) <doi:10.1073/pnas.1110717108> and Hancock & Godfray (2007) <doi:10.1186/1475-2875-6-98>, 
    and extensions to accommodate CRISPR homing dynamics in Marshall et al. (2017) 
    <doi:10.1038/s41598-017-02744-7>.",2020-10-05,Héctor Manuel Sánchez Castellanos,"https://marshalllab.github.io/MGDrivE/,
https://www.marshalllab.com/",TRUE,https://github.com/marshalllab/mgdrive,11571,1,2021-02-24T18:55:10Z,11571
MGDrivE2,"A simulation modeling framework which significantly extends capabilities from the
    'MGDrivE' simulation package via a new mathematical and computational framework based on stochastic Petri nets.
    For more information about 'MGDrivE', see our publication: <https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13318>.
    Some of the notable capabilities of 'MGDrivE2' include: incorporation of human populations,
    epidemiological dynamics, time-varying parameters, and a continuous-time simulation
    framework with various sampling algorithms for both deterministic and stochastic interpretations.
    'MGDrivE2' relies on the genetic inheritance structures provided in package 'MGDrivE', so we
    suggest installing that package initially.",2021-01-13,Sean L. Wu,"https://marshalllab.github.io/MGDrivE/,
https://www.marshalllab.com/",TRUE,https://github.com/marshalllab/mgdrive,3706,1,2021-02-24T18:55:10Z,3706
mgm,Estimation of k-Order time-varying Mixed Graphical Models and mixed VAR(p) models via elastic-net regularized neighborhood regression. For details see Haslbeck & Waldorp (2020) <doi:10.18637/jss.v093.i08>.,2021-06-03,Jonas Haslbeck,https://arxiv.org/abs/1510.06871,TRUE,https://github.com/jmbh/mgm,54182,21,2021-06-03T08:59:47Z,2580.095238095238
mgss,Data smoothing with penalized splines is a popular method and is well established for one- or two-dimensional covariates. The extension to multiple covariates is straightforward but suffers from exponentially increasing memory requirements and computational complexity. This toolbox provides a matrix-free implementation of a conjugate gradient (CG) method for the regularized least squares problem resulting from tensor product B-spline smoothing with multivariate and scattered data. It further provides matrix-free preconditioned versions of the CG-algorithm where the user can choose between a simpler diagonal preconditioner and an advanced geometric multigrid preconditioner. The main advantage is that all algorithms are performed matrix-free and therefore require only a small amount of memory. For further detail see Siebenborn & Wagner (2021).,2021-05-10,Martin Siebenborn,NA,TRUE,https://github.com/splinesmoothing/mgss,3150,0,2021-05-07T10:57:48Z,NA
mgsub,"Designed to enable simultaneous substitution in strings in a safe fashion.
    Safe means it does not rely on placeholders (which can cause errors in same length matches).",2021-07-28,Mark Ewing,NA,TRUE,https://github.com/bmewing/mgsub,318338,8,2021-07-26T19:32:37Z,39792.25
mhcnuggetsr,"
    MHCnuggets (<https://github.com/KarchinLab/mhcnuggets>) is a Python
    tool to predict MHC class I and MHC class II epitopes.
    This package allows one to call MHCnuggets from R.",2020-11-04,Richèl J.C. Bilderbeek,https://github.com/richelbilderbeek/mhcnuggetsr/,TRUE,https://github.com/richelbilderbeek/mhcnuggetsr,3767,1,2021-07-31T14:52:24Z,3767
mi4p,"A framework for multiple imputation for proteomics is proposed by Marie Chion, Christine Carapito and Frederic Bertrand (2021) <arxiv:2108.07086>. It is dedicated to dealing with multiple imputation for proteomics.",2021-08-19,Marie Chion,"https://mariechion.github.io/mi4p/,
https://github.com/mariechion/mi4p/",TRUE,https://github.com/mariechion/mi4p,107,3,2021-08-20T17:30:46Z,35.666666666666664
MIAmaxent,"Tools for training, selecting, and evaluating maximum entropy
    (and standard logistic regression) distribution models. This package 
    provides tools for user-controlled transformation of explanatory variables, 
    selection of variables by nested model comparison, and flexible model 
    evaluation and projection. It follows principles based on the maximum-
    likelihood interpretation of maximum entropy modeling, and uses infinitely-
    weighted logistic regression for model fitting.",2020-12-01,Julien Vollering,https://github.com/julienvollering/MIAmaxent,TRUE,https://github.com/julienvollering/miamaxent,21006,10,2021-06-10T09:30:46Z,2100.6
micar,"'Mica' is a server application used to create data web portals for 
    large-scale epidemiological studies or multiple-study consortia. 'Mica' helps
    studies to provide scientifically robust data visibility and web presence 
    without significant information technology effort. 'Mica' provides a 
    structured description of consortia, studies, annotated and searchable data
    dictionaries, and data access request management. This 'Mica' client allows
    to perform data extraction for reporting purposes.",2021-04-16,Yannick Marcon,"https://www.obiba.org/ https://www.obiba.org/pages/products/mica/
https://doi.org/10.1093/ije/dyx180",TRUE,https://github.com/obiba/micar,14458,0,2021-04-16T12:17:57Z,NA
mice,"Multiple imputation using Fully Conditional Specification (FCS)
    implemented by the MICE algorithm as described in Van Buuren and
    Groothuis-Oudshoorn (2011) <doi:10.18637/jss.v045.i03>. Each variable has
    its own imputation model. Built-in imputation models are provided for
    continuous data (predictive mean matching, normal), binary data (logistic
    regression), unordered categorical data (polytomous logistic regression)
    and ordered categorical data (proportional odds). MICE can also impute
    continuous two-level data (normal model, pan, second-level variables).
    Passive imputation can be used to maintain consistency between variables.
    Various diagnostic plots are available to inspect the quality of the
    imputations.",2021-01-27,Stef van Buuren,"https://github.com/amices/mice, https://amices.org/mice/,
https://stefvanbuuren.name/fimd/",TRUE,https://github.com/amices/mice,2526280,254,2021-06-21T10:06:42Z,9945.984251968504
miceadds,"
    Contains functions for multiple imputation which
    complements existing functionality in R.
    In particular, several imputation methods for the
    mice package (van Buuren & Groothuis-Oudshoorn, 2011,
    <doi:10.18637/jss.v045.i03>) are included.
    Main features of the miceadds package include
    plausible value imputation (Mislevy, 1991,
    <doi:10.1007/BF02294457>), multilevel imputation for
    variables at any level or with any number of hierarchical
    and non-hierarchical levels (Grund, Luedtke & Robitzsch,
    2018, <doi:10.1177/1094428117703686>; van Buuren, 2018, 
    Ch.7, <doi:10.1201/9780429492259>), imputation using 
    partial least squares (PLS) for high dimensional 
    predictors (Robitzsch, Pham & Yanagida, 2016), 
    nested multiple imputation (Rubin, 2003, 
    <doi:10.1111/1467-9574.00217>), substantive model
    compatible imputation (Bartlett et al., 2015,
    <doi:10.1177/0962280214521348>), and features
    for the generation of synthetic datasets
    (Reiter, 2005, <doi:10.1111/j.1467-985X.2004.00343.x>;
    Nowok, Raab, & Dibben, 2016, <doi:10.18637/jss.v074.i11>).",2021-01-21,Alexander Robitzsch,"https://github.com/alexanderrobitzsch/miceadds,
https://sites.google.com/site/alexanderrobitzsch2/software",TRUE,https://github.com/alexanderrobitzsch/miceadds,414619,12,2021-08-27T08:03:54Z,34551.583333333336
miceFast,"
  Fast imputations under the object-oriented programming paradigm. 	
  Moreover there are offered a few functions built to work with popular R packages such as 'data.table' or 'dplyr'.
  The biggest improvement in time performance could be achieve for a calculation where a grouping variable have to be used.
  A single evaluation of a quantitative model for the multiple imputations is another major enhancement.
  A new major improvement is one of the fastest predictive mean matching in the R world because of presorting and binary search.",2021-07-10,Maciej Nasinski,https://github.com/Polkas/miceFast,TRUE,https://github.com/polkas/micefast,17568,10,2021-07-10T18:10:31Z,1756.8
miceRanger,"Multiple Imputation has been shown to 
  be a flexible method to impute missing values by 
  Van Buuren (2007) <doi:10.1177/0962280206074463>. 
  Expanding on this, random forests have been shown 
  to be an accurate model by Stekhoven and Buhlmann 
  <arXiv:1105.0828> to impute missing values in datasets. 
  They have the added benefits of returning out of bag 
  error and variable importance estimates, as well as 
  being simple to run in parallel.",2021-05-13,Sam Wilson,https://github.com/FarrellDay/miceRanger,TRUE,https://github.com/farrellday/miceranger,11678,38,2021-05-13T15:31:44Z,307.3157894736842
micompr,"A procedure for comparing multivariate samples associated with
    different groups. It uses principal component analysis to convert
    multivariate observations into a set of linearly uncorrelated statistical
    measures, which are then compared using a number of statistical methods. The
    procedure is independent of the distributional properties of samples and
    automatically selects features that best explain their differences, avoiding
    manual selection of specific points or summary statistics. It is appropriate
    for comparing samples of time series, images, spectrometric measures or
    similar multivariate observations.",2021-07-08,Nuno Fachada,https://github.com/fakenmc/micompr,TRUE,https://github.com/fakenmc/micompr,17886,2,2021-07-08T08:27:35Z,8943
microbenchmark,"Provides infrastructure to accurately measure and compare
        the execution time of R expressions.",2019-09-24,Joshua M. Ulrich,https://github.com/joshuaulrich/microbenchmark/,TRUE,https://github.com/joshuaulrich/microbenchmark,1061663,62,2021-07-30T14:43:21Z,17123.59677419355
microdemic,"The 'Microsoft Academic Knowledge' API provides programmatic access
	to scholarly articles in the 'Microsoft Academic Graph'
	(<https://academic.microsoft.com/>). Includes methods matching all 'Microsoft
	Academic' API routes, including search, graph search, text similarity, and
	interpret natural language query string.",2020-11-20,Scott Chamberlain,"https://github.com/ropensci/microdemic (devel),
https://docs.ropensci.org/microdemic/ (website)",TRUE,https://github.com/ropensci/microdemic,29053,16,2020-12-09T22:55:01Z,1815.8125
microhaplot,"A downstream bioinformatics tool to construct and assist 
    curation of microhaplotypes from short read sequences.",2019-10-03,Thomas Ng,https://github.com/ngthomas/microhaplot,TRUE,https://github.com/ngthomas/microhaplot,9541,11,2021-06-24T08:09:49Z,867.3636363636364
MicroSEC,"Clinical sequencing of tumor is usually performed on formalin-fixed
    and paraffin-embedded (FFPE) samples and have many sequencing errors. We
    found that the majority of these errors are detected in chimeric read
    caused by single-strand DNA with microhomology. Our filtering pipeline
    focuses on the uneven distribution of the artifacts in each read and removes
    such errors in FFPE samples without over-eliminating the true mutations
    detected in fresh frozen samples.",2020-12-02,Masachika Ikegami,https://github.com/MANO-B/MicroSEC/,TRUE,https://github.com/mano-b/microsec,2500,1,2021-08-14T07:18:27Z,2500
microservices,"'Microservice' architectural style is an approach to developing a 
    single application as a suite of small services, each running in its own 
    process and communicating with lightweight mechanisms, often an 'HTTP' 
    resource 'API'. These services are built around business capabilities and 
    independently deployable by fully automated deployment machinery. There 
    is a bare minimum of centralized management of these services, which may 
    be written in different programming languages and use different data storage 
    technologies.",2021-06-12,Harel Lustiger,https://github.com/tidylab/microservices,TRUE,https://github.com/tidylab/microservices,2478,10,2021-08-16T08:14:45Z,247.8
microsimulation,"Discrete event simulation using both R and C++ (Karlsson et al 2016; <doi:10.1109/eScience.2016.7870915>). The C++ code is adapted from the SSIM library <https://www.inf.usi.ch/carzaniga/ssim/>, allowing for both event-oriented and process-oriented simulation. The code includes a SummaryReport class for reporting events and costs by age and other covariates. The C++ code is available as a static library for linking to other packages. A priority queue implementation is given in C++ together with an S3 closure and a reference class implementation. Finally, some tools are provided for cost-effectiveness analysis.",2021-07-15,Mark Clements,https://github.com/mclements/microsimulation,TRUE,https://github.com/mclements/microsimulation,2047,22,2021-07-15T14:52:06Z,93.04545454545455
midasml,"The 'midasml' package implements estimation and prediction methods for high-dimensional mixed-frequency (MIDAS) time-series and panel data regression models. The regularized MIDAS models are estimated using orthogonal (e.g. Legendre) polynomials and sparse-group LASSO (sg-LASSO) estimator. For more information on the 'midasml' approach see Babii, Ghysels, and Striaukas (2021, JBES forthcoming) <doi:10.1080/07350015.2021.1899933>. The package is equipped with the fast implementation of the sg-LASSO estimator by means of proximal block coordinate descent. High-dimensional mixed frequency time-series data can also be easily manipulated with functions provided in the package.",2021-05-20,Jonas Striaukas,NA,TRUE,https://github.com/jstriaukas/midasml,8825,18,2021-05-28T09:54:26Z,490.27777777777777
midasr,"Methods and tools for mixed frequency time series data analysis.
    Allows estimation, model selection and forecasting for MIDAS regressions.",2021-02-23,Virmantas Kvedaras,http://mpiktas.github.io/midasr/,TRUE,https://github.com/mpiktas/midasr,31233,44,2021-07-11T12:59:21Z,709.8409090909091
midrangeMCP,"Apply tests of multiple comparisons based
    on studentized 'midrange' and 'range' distributions. 
    The tests are: Tukey Midrange ('TM' test),
    Student-Newman-Keuls Midrange ('SNKM' test), 
    Means Grouping Midrange ('MGM' test) and 
    Means Grouping Range ('MGR' test). The first two tests were published by 
    Batista and Ferreira (2020) <doi:10.1590/1413-7054202044008020>. 
    The last two are being published.",2020-12-15,Ben Deivide,"https://bendeivide.github.io/midrangeMCP/,
https://github.com/bendeivide/midrangeMCP",TRUE,https://github.com/bendeivide/midrangemcp,18901,0,2021-05-31T00:34:18Z,NA
mifa,"Impute the covariance matrix of incomplete data so that factor 
  analysis can be performed. Imputations are made using multiple imputation 
  by Multivariate Imputation with Chained Equations (MICE) and combined with 
  Rubin's rules. Parametric Fieller confidence intervals and nonparametric
  bootstrap confidence intervals can be obtained for the variance explained by 
  different numbers of principal components. The method is described in 
  Nassiri et al. (2018) <doi:10.3758/s13428-017-1013-4>.",2021-01-22,Tobias Busch,https://github.com/teebusch/mifa,TRUE,https://github.com/teebusch/mifa,2634,1,2021-01-22T18:55:06Z,2634
migest,"Indirect methods for estimating bilateral migration flows in the presence of partial or missing data, including the estimation of bilateral migration flows from changes in bilateral migrant stock tables (e.g. Abel (2013) <doi:10.4054/DemRes.2013.28.18>).",2021-07-25,Guy J. Abel,https://github.com/guyabel/migest/,TRUE,https://github.com/guyabel/migest,33566,22,2021-08-25T07:56:19Z,1525.7272727272727
migraph,"A set of tools that extend common social network analysis packages
   for analysing multimodal and multilevel networks.
   It includes functions for one- and two-mode (and sometimes three-mode) 
   centrality, centralization, clustering, and constraint,
   as well as for one- and two-mode network regression and block-modelling.
   All functions operate with matrices, edge lists, 
   and 'igraph', 'network'/'sna', and 'tidygraph' objects.
   The package is released as a complement to 
   'Multimodal Political Networks' (2021, ISBN:9781108985000),
   and includes various datasets used in the book.",2021-05-13,James Hollway [cph,https://github.com/snlab-ch/migraph,TRUE,https://github.com/snlab-ch/migraph,1756,7,2021-05-13T20:42:22Z,250.85714285714286
migrate,"Tools to help convert credit risk data at two time points 
    into traditional credit state migration (aka, ""transition"") matrices.
    At a higher level, 'migrate' is intended to help an analyst understand 
    how risk moved in their credit portfolio over a time interval. 
    References to this methodology include: 
    1. Schuermann, T. (2008) <doi:10.1002/9780470061596.risk0409>.
    2. Perederiy, V. (2017) <arXiv:1708.00062>.",2020-12-07,Michael Thomas,https://github.com/mthomas-ketchbrook/migrate,TRUE,https://github.com/mthomas-ketchbrook/migrate,3389,2,2020-12-07T16:36:42Z,1694.5
miic,"We report an information-theoretic method which learns a large
    class of causal or non-causal graphical models from purely observational
    data, while including the effects of unobserved latent variables, commonly
    found in many datasets. Starting from a complete graph, the method
    iteratively removes dispensable edges, by uncovering significant information
    contributions from indirect paths, and assesses edge-specific confidences
    from randomization of available data. The remaining edges are then oriented
    based on the signature of causality in observational data. This approach can
    be applied on a wide range of datasets and provide new biological insights
    on regulatory networks from single cell expression data, genomic alterations
    during tumor development and co-evolving residues in protein structures.
    For more information you can refer to:
    Cabeli et al. PLoS Comp. Bio. 2020 <doi:10.1371/journal.pcbi.1007866>,
    Verny et al. PLoS Comp. Bio. 2017 <doi:10.1371/journal.pcbi.1005662>.",2020-10-13,Vincent Cabeli,https://github.com/miicTeam/miic_R_package,TRUE,https://github.com/miicteam/miic_r_package,18106,8,2021-08-19T14:08:22Z,2263.25
MIIVsem,"Functions for estimating structural equation models using 
    instrumental variables.",2021-07-13,Zachary Fisher,https://github.com/zackfisher/MIIVsem,TRUE,https://github.com/zackfisher/miivsem,30376,5,2020-10-11T15:04:31Z,6075.2
mikropml,"An interface to build machine learning models for classification 
    and regression problems. 'mikropml' implements the ML pipeline described
    by Topçuoğlu et al. (2020) <doi:10.1128/mBio.00434-20> with reasonable 
    default options for data preprocessing, hyperparameter tuning, 
    cross-validation, testing, model evaluation, and interpretation steps. 
    See the website <https://www.schlosslab.org/mikropml/> for more information, 
    documentation, and examples.",2021-08-10,Begüm Topçuoğlu,"https://www.schlosslab.org/mikropml/,
https://github.com/SchlossLab/mikropml",TRUE,https://github.com/schlosslab/mikropml,3976,25,2021-08-26T14:56:01Z,159.04
milr,"The multiple instance data set consists of many independent
    subjects (called bags) and each subject is composed of several components
    (called instances). The outcomes of such data set are binary or categorical responses,
    and, we can only observe the subject-level outcomes. For example, in manufacturing
    processes, a subject is labeled as ""defective"" if at least one of its own
    components is defective, and otherwise, is labeled as ""non-defective"". The
    'milr' package focuses on the predictive model for the multiple instance
    data set with binary outcomes and performs the maximum likelihood estimation
    with the Expectation-Maximization algorithm under the framework of logistic
    regression. Moreover, the LASSO penalty is attached to the likelihood function
    for simultaneous parameter estimation and variable selection.",2020-10-31,Ping-Yang Chen,https://github.com/PingYangChen/milr,TRUE,https://github.com/pingyangchen/milr,16796,10,2020-10-28T09:51:26Z,1679.6
mime,"Guesses the MIME type from a filename extension using the data
    derived from /etc/mime.types in UNIX-type systems.",2021-06-23,Yihui Xie,https://github.com/yihui/mime,TRUE,https://github.com/yihui/mime,20775113,29,2021-06-23T04:49:21Z,716383.2068965518
MinBAR,"A versatile tool that aims at (1) defining the minimum background extent necessary to fit Species Distribution Models reliable enough to extract ecologically relevant conclusions from them and (2) optimizing the modelling process in terms of computation demands. See Rotllan-Puig, X. & Traveset, A. (2021) <https://www.sciencedirect.com/science/article/pii/S0304380020304191>.",2020-11-04,Xavier Rotllan-Puig,https://github.com/xavi-rp/MinBAR,TRUE,https://github.com/xavi-rp/minbar,7577,4,2020-11-03T22:15:51Z,1894.25
miniCRAN,"Makes it possible to create an internally consistent
    repository consisting of selected packages from CRAN-like repositories.
    The user specifies a set of desired packages, and 'miniCRAN' recursively
    reads the dependency tree for these packages, then downloads only this
    subset. The user can then install packages from this repository directly,
    rather than from CRAN.  This is useful in production settings, e.g. server
    behind a firewall, or remote locations with slow (or zero) Internet access.",2020-09-15,Andrie de Vries,https://github.com/andrie/miniCRAN,TRUE,https://github.com/andrie/minicran,110225,127,2020-11-26T19:21:07Z,867.9133858267717
minidown,"Create minimal, responsive, and style-agnostic HTML documents with
    the lightweight CSS frameworks such as 'sakura', 'Water.css', and 'spcss'.
    Powerful features include table of contents floating as a sidebar,
    folding codes and results, and more.",2021-04-09,Atsushi Yasumoto,"https://minidown.atusy.net, https://github.com/atusy/minidown",TRUE,https://github.com/atusy/minidown,22773,50,2021-05-07T14:10:34Z,455.46
mipfp,"An implementation of the iterative proportional fitting (IPFP), 
    maximum likelihood, minimum chi-square and weighted least squares procedures
    for updating a N-dimensional array with respect to given target marginal 
    distributions (which, in turn can be multidimensional). The package also
    provides an application of the IPFP to simulate multivariate Bernoulli
    distributions.",2018-08-29,Johan Barthelemy,https://github.com/jojo-/mipfp,TRUE,https://github.com/jojo-/mipfp,39679,15,2021-07-16T03:12:28Z,2645.266666666667
MIRES,"Estimates random effect latent measurement models, wherein the loadings, residual variances, intercepts, latent means, and latent variances all vary across groups. The random effect variances of the measurement parameters are then modeled using a hierarchical inclusion model, wherein the inclusion of the variances (i.e., whether it is effectively zero or non-zero) is informed by similar parameters (of the same type, or of the same item). This additional hierarchical structure allows the evidence in favor of partial invariance to accumulate more quickly, and yields more certain decisions about measurement invariance. Martin, Williams, and Rast (2020) <doi:10.31234/osf.io/qbdjt>.",2021-02-22,Stephen Martin,NA,TRUE,https://github.com/stephensrmmartin/mires,1622,0,2021-02-24T02:30:46Z,NA
mirt,"Analysis of dichotomous and polytomous response data using
    unidimensional and multidimensional latent trait models under the Item
    Response Theory paradigm (Chalmers (2012) <doi:10.18637/jss.v048.i06>). 
    Exploratory and confirmatory models can be estimated with quadrature (EM) 
    or stochastic (MHRM) methods. Confirmatory
    bi-factor and two-tier analyses are available for modeling item testlets.
    Multiple group analysis and mixed effects designs also are available for
    detecting differential item and test functioning as well as modeling
    item and person covariates. Finally, latent class models such as the DINA,
    DINO, multidimensional latent class, and several other discrete latent
    variable models, including mixture and zero-inflated response models, 
    are supported.",2021-06-28,Phil Chalmers,"https://github.com/philchalmers/mirt,
https://github.com/philchalmers/mirt/wiki,
https://groups.google.com/forum/#!forum/mirt-package",TRUE,https://github.com/philchalmers/mirt,231540,149,2021-08-31T16:39:56Z,1553.9597315436242
mirtCAT,"Provides tools to generate an HTML interface for creating adaptive
    and non-adaptive educational and psychological tests using the shiny
    package (Chalmers (2016) <doi:10.18637/jss.v071.i05>). 
    Suitable for applying unidimensional and multidimensional
    computerized adaptive tests (CAT) using item response theory methodology and for
    creating simple questionnaires forms to collect response data directly in R.
    Additionally, optimal test designs (e.g., ""shadow testing"") are supported
    for tests which contain a large number of item selection constraints.
    Finally, package contains tools useful for performing Monte Carlo simulations 
    for studying the behavior of computerized adaptive test banks.",2021-06-28,Phil Chalmers,"https://github.com/philchalmers/mirtCAT,
https://github.com/philchalmers/mirtCAT/wiki,
https://groups.google.com/forum/#!forum/mirt-package",TRUE,https://github.com/philchalmers/mirtcat,82332,68,2021-08-19T02:01:37Z,1210.764705882353
mirtjml,"Provides constrained joint maximum likelihood estimation
    algorithms for item factor analysis (IFA) based on multidimensional item response theory
    models. So far, we provide functions for exploratory and confirmatory IFA based on the 
    multidimensional two parameter logistic (M2PL) model for binary response data. Comparing 
    with traditional estimation methods for IFA, the methods implemented in this package scale
    better to data with large numbers of respondents, items, and latent factors. The computation
    is facilitated by multiprocessing 'OpenMP' API. For more information, please refer to:
    1. Chen, Y., Li, X., & Zhang, S. (2018). Joint Maximum Likelihood Estimation for 
    High-Dimensional Exploratory Item Factor Analysis. Psychometrika, 1-23. 
    <doi:10.1007/s11336-018-9646-5>;
    2. Chen, Y., Li, X., & Zhang, S. (2019). Structured Latent Factor Analysis for Large-scale Data: 
    Identifiability, Estimability, and Their Implications. Journal of the American Statistical 
    Association, <doi: 10.1080/01621459.2019.1635485>.",2020-06-08,Siliang Zhang,https://github.com/slzhang-fd/mirtjml,TRUE,https://github.com/slzhang-fd/mirtjml,11812,4,2021-04-27T15:59:35Z,2953
misaem,"Estimate parameters of linear regression and logistic regression with missing covariates with missing data, perform model selection and prediction, using EM-type algorithms. Jiang W., Josse J., Lavielle M., TraumaBase Group (2020) <doi:10.1016/j.csda.2019.106907>.",2021-04-12,Julie Josse,https://github.com/julierennes/misaem,TRUE,https://github.com/julierennes/misaem,13298,0,2021-04-12T09:13:37Z,NA
missCompare,"Offers a convenient pipeline to test and compare various missing data
  imputation algorithms on simulated and real data. These include simpler methods, such as mean and median
  imputation and random replacement, but also include more sophisticated algorithms already implemented in popular 
  R packages, such as 'mi', described by Su et al. (2011) <doi:10.18637/jss.v045.i02>; 'mice', described by van Buuren 
  and Groothuis-Oudshoorn (2011) <doi:10.18637/jss.v045.i03>; 'missForest', described by Stekhoven and Buhlmann (2012) 
  <doi:10.1093/bioinformatics/btr597>; 'missMDA', described by Josse and Husson (2016) <doi:10.18637/jss.v070.i01>; and 
  'pcaMethods', described by Stacklies et al. (2007) <doi:10.1093/bioinformatics/btm069>. The central assumption behind 
  'missCompare' is that structurally different datasets (e.g. larger datasets with a large number of correlated variables 
  vs. smaller datasets with non correlated variables) will benefit differently from different missing data imputation 
  algorithms. 'missCompare' takes measurements of your dataset and sets up a sandbox to try a curated list of standard and 
  sophisticated missing data imputation algorithms and compares them assuming custom missingness patterns.
  'missCompare' will also impute your real-life dataset for you after the selection of the best performing algorithm
  in the simulations. The package also provides various post-imputation diagnostics and visualizations to help you 
  assess imputation performance.   ",2020-12-01,Tibor V. Varga,NA,TRUE,https://github.com/tirgit/misscompare,12093,23,2020-11-30T16:32:15Z,525.7826086956521
missDiag,Implements the computation of discrepancy statistics summarizing differences between the density of imputed and observed values and the construction of weights to balance covariates that are part of the missing data mechanism as described in Marbach (2021) <arXiv:2107.05427>. ,2021-08-06,Moritz Marbach,https://github.com/sumtxt/missDiag/,TRUE,https://github.com/sumtxt/missdiag,333,1,2021-08-06T09:08:15Z,333
missMethods,"Supply functions for the creation and handling of
    missing data as well as tools to evaluate missing data methods. Nearly
    all possibilities of generating missing data discussed by Santos et.
    al (2019) <doi:10.1109/ACCESS.2019.2891360> and some additional are
    implemented.  Functions are supplied to compare parameter estimates
    and imputed values to true values to evaluate missing data methods.
    Evaluations of these types are done, for example, by Cetin-Berber et
    al. (2019) <doi:10.1177/0013164418805532> and Kim et al. (2005)
    <doi:10.1093/bioinformatics/bth499>.",2020-07-30,Tobias Rockel,https://github.com/torockel/missMethods,TRUE,https://github.com/torockel/missmethods,9003,0,2020-10-06T15:27:50Z,NA
missRanger,"Alternative implementation of the beautiful 'MissForest'
    algorithm used to impute mixed-type data sets by chaining random
    forests, introduced by Stekhoven, D.J. and Buehlmann, P. (2012)
    <doi:10.1093/bioinformatics/btr597>. Under the hood, it uses the
    lightning fast random jungle package 'ranger'. Between the iterative
    model fitting, we offer the option of using predictive mean matching.
    This firstly avoids imputation with values not already present in the
    original data (like a value 0.3334 in 0-1 coded variable).  Secondly,
    predictive mean matching tries to raise the variance in the resulting
    conditional distributions to a realistic level. This would allow e.g.
    to do multiple imputation when repeating the call to missRanger().  A
    formula interface allows to control which variables should be imputed
    by which.",2021-03-30,Michael Mayer,https://github.com/mayer79/missRanger,TRUE,https://github.com/mayer79/missranger,38324,37,2021-05-22T11:38:35Z,1035.7837837837837
missSBM,"When a network is partially observed (here, NAs in the adjacency matrix rather than 1 or 0 
  due to missing information between node pairs), it is possible to account for the underlying process
  that generates those NAs. 'missSBM', presented in 'Barbillon, Chiquet and Tabouy' (2021) <arXiv:1906.12201>,
  adjusts the popular stochastic block model from network data sampled under various missing data conditions, 
  as described in 'Tabouy, Barbillon and Chiquet' (2019) <doi:10.1080/01621459.2018.1562934>.",2021-06-04,Julien Chiquet,https://grosssbm.github.io/missSBM/,TRUE,https://github.com/grosssbm/misssbm,9895,8,2021-06-14T13:16:51Z,1236.875
mitml,"Provides tools for multiple imputation of missing data in multilevel
 modeling. Includes a user-friendly interface to the packages 'pan' and 'jomo',
 and several functions for visualization, data management and the analysis 
 of multiply imputed data sets.",2021-02-05,Simon Grund,NA,TRUE,https://github.com/simongrund1/mitml,882323,21,2021-02-05T14:07:39Z,42015.380952380954
mitre,"Extract, transform and load MITRE standards.
    This package gives you an approach to cybersecurity data sets.
    All data sets are build on runtime downloading raw data from MITRE public services.
    MITRE <https://www.mitre.org/> is a government-funded research organization 
    based in Bedford and McLean. Current version includes most used standards as
    data frames. It also provide a list of nodes and edges with all relationships.",2021-05-21,Humbert Costas,https://github.com/motherhack3r/mitre,TRUE,https://github.com/motherhack3r/mitre,4316,17,2021-05-19T20:12:37Z,253.88235294117646
MittagLeffleR,"Implements the Mittag-Leffler function, distribution,
  random variate generation, and estimation. Based on the Laplace-Inversion
  algorithm by Garrappa, R. (2015) <doi:10.1137/140971191>.",2018-04-25,Peter Straka,https://strakaps.github.io/MittagLeffleR/,TRUE,https://github.com/strakaps/mittagleffler,16760,3,2021-02-11T10:32:17Z,5586.666666666667
MIWilson,"Implements the Wilson confidence interval 
    for binomial proportions given multiple imputations of missing data (detailed 
    theory provided in ""Wilson Confidence Intervals for Binomial
    Proportions With Multiple Imputation for Missing
    Data"" (A. Lott & J. Reiter, 2018)). Our package also implements a Wald confidence
    interval and allows for both MIDs object and proportion vector arguments. ",2021-08-23,Frances Hung,https://github.com/hungf8342/MIWilson,TRUE,https://github.com/hungf8342/miwilson,121,0,2021-08-27T13:28:20Z,NA
mixAR,"Model time series using mixture autoregressive (MAR)
             models.  Implemented are frequentist (EM) and Bayesian
             methods for estimation, prediction and model
             evaluation. See Wong and Li (2002)
             <doi:10.1111/1467-9868.00222>, Boshnakov (2009)
             <doi:10.1016/j.spl.2009.04.009>), and the extensive
             references in the documentation.",2021-01-04,Georgi N. Boshnakov,"https://geobosh.github.io/mixAR/ (website),
https://github.com/GeoBosh/mixAR/ (devel)",TRUE,https://github.com/geobosh/mixar,6767,0,2021-08-24T15:44:15Z,NA
mixchar,"Deconvolution of thermal decay curves allows you to quantify proportions 
    of biomass components in plant litter. Thermal decay curves derived from 
    thermogravimetric analysis (TGA) are imported, modified, and then modelled in a 
    three- or four- part  mixture model using the Fraser-Suzuki function. The output 
    is estimates for weights of pseudo-components corresponding to hemicellulose, 
    cellulose, and lignin. For more information see: Müller-Hagedorn, M. and Bockhorn, 
    H. (2007) <doi:10.1016/j.jaap.2006.12.008>, Órfão, J. J. M. and Figueiredo, J. L. 
    (2001) <doi:10.1016/S0040-6031(01)00634-7>, and Yang, H. and Yan, R. and 
    Chen, H. and Zheng, C. and Lee, D. H. and Liang, D. T. (2006) <doi:10.1021/ef0580117>.",2018-08-16,Saras Windecker,http://github.com/smwindecker/mixchar,TRUE,https://github.com/smwindecker/mixchar,12481,6,2021-03-05T01:20:57Z,2080.1666666666665
MixedPsy,"Tools for the analysis of psychophysical data. This package allows to estimate the Point of Subjective Equivalence (PSE) 
    and the Just Noticeable Difference (JND), either from a psychometric function or from a Generalized Linear Mixed Model (GLMM). 
    Additionally, the package allows plotting the fitted models and the response data, simulating psychometric functions of different shapes, and simulating data sets.
    For a description of the use of GLMMs applied to psychophysical data, refer to Moscatelli et al. (2012), <doi:10.1167/12.11.26>.",2017-11-16,Alessandro Moscatelli,https://mixedpsychophysics.wordpress.com,TRUE,https://github.com/moskante/mixedpsy,14859,0,2021-02-10T12:05:57Z,NA
MixfMRI,"Utilizing model-based clustering (unsupervised)
        for functional magnetic resonance imaging (fMRI) data.
        The developed methods (Chen and Maitra (2021) <arXiv:2102.03639>)
        include 2D and 3D clustering analyses
        (for p-values with voxel locations) and
        segmentation analyses (for p-values alone) for fMRI data where p-values
        indicate significant level of activation responding to stimulate
        of interesting. The analyses are mainly identifying active
        voxel/signal associated with normal brain behaviors.
        Analysis pipelines (R scripts) utilizing this package
        (see examples in 'inst/workflow/') is also implemented with high
        performance techniques.",2021-02-13,Wei-Chen Chen,https://github.com/snoweye/MixfMRI,TRUE,https://github.com/snoweye/mixfmri,13110,1,2021-02-13T19:58:35Z,13110
mixIndependR,"Developed to deal with multi-locus genotype data, this package is especially designed for those panel which include different type of markers. Basic genetic parameters like allele frequency, genotype frequency, heterozygosity and Hardy-Weinberg test of mixed genetic data can be obtained.     In addition, a new test for mutual independence which is compatible for mixed genetic data is developed in this package.",2021-03-17,Bing Song,https://github.com/ice4prince/mixIndependR,TRUE,https://github.com/ice4prince/mixindependr,10243,2,2021-03-22T08:44:12Z,5121.5
mixmeta,"A collection of functions to perform various meta-analytical models
  through a unified mixed-effects framework, including standard univariate
  fixed and random-effects meta-analysis and meta-regression, and non-standard
  extensions such as multivariate, multilevel, longitudinal, and dose-response
  models.",2021-06-12,Antonio Gasparrini,"https://github.com/gasparrini/mixmeta,
http://www.ag-myresearch.com/package-mixmeta",TRUE,https://github.com/gasparrini/mixmeta,34760,7,2021-06-12T08:42:45Z,4965.714285714285
mixpoissonreg,"Fits mixed Poisson regression models (Poisson-Inverse Gaussian or Negative-Binomial) on data sets with response variables being count data. The models can have varying precision parameter, where a linear regression structure (through a link function) is assumed to hold on the precision parameter. The Expectation-Maximization algorithm for both these models (Poisson Inverse Gaussian and Negative Binomial) is an important contribution of this package. Another important feature of this package is the set of functions to perform global and local influence analysis. See Barreto-Souza and Simas (2016) <doi:10.1007/s11222-015-9601-6> for further details.  ",2021-03-10,Alexandre B. Simas,"https://github.com/vpnsctl/mixpoissonreg/,
https://vpnsctl.github.io/mixpoissonreg/",TRUE,https://github.com/vpnsctl/mixpoissonreg,1984,1,2021-03-12T21:56:52Z,1984
MixSIAR,"Creates and runs Bayesian mixing models to analyze
    biological tracer data (i.e. stable isotopes, fatty acids), which estimate the
    proportions of source (prey) contributions to a mixture (consumer). 'MixSIAR'
    is not one model, but a framework that allows a user to create a mixing model
    based on their data structure and research questions, via options for fixed/
    random effects, source data types, priors, and error terms. 'MixSIAR' incorporates
    several years of advances since 'MixSIR' and 'SIAR'.",2020-10-20,Brian Stock,https://github.com/brianstock/MixSIAR,TRUE,https://github.com/brianstock/mixsiar,26432,46,2020-10-20T02:48:40Z,574.6086956521739
mixtur,"A set of utility functions for analysing and modelling data from 
    continuous report short-term memory experiments using either the 2-component
    mixture model of Zhang and Luck (2008) <doi:10.1038/nature06860> or the 
    3-component mixture model of Bays et al. (2009) <doi:10.1167/9.10.7>. Users 
    are also able to simulate from these models.",2021-08-03,Jim Grange,https://github.com/JimGrange/mixtur,TRUE,https://github.com/jimgrange/mixtur,391,1,2021-08-03T14:31:58Z,391
mize,"Optimization algorithms implemented in R, including
    conjugate gradient (CG), Broyden-Fletcher-Goldfarb-Shanno (BFGS) and the
    limited memory BFGS (L-BFGS) methods. Most internal parameters can be set 
    through the call interface. The solvers hold up quite well for 
    higher-dimensional problems.",2020-08-30,James Melville,https://github.com/jlmelville/mize,TRUE,https://github.com/jlmelville/mize,24470,7,2021-03-13T19:10:42Z,3495.714285714286
mizer,"A set of classes and methods to set up and run multi-species, trait
    based and community size spectrum ecological models, focused on the marine
    environment.",2021-08-03,Gustav Delius,"https://sizespectrum.org/mizer/,
https://github.com/sizespectrum/mizer",TRUE,https://github.com/sizespectrum/mizer,22479,23,2021-09-03T13:17:17Z,977.3478260869565
mkin,"Calculation routines based on the FOCUS Kinetics Report (2006,
  2014).  Includes a function for conveniently defining differential equation
  models, model solution based on eigenvalues if possible or using numerical
  solvers.  If a C compiler (on windows: 'Rtools') is installed, differential
  equation models are solved using automatically generated C functions.  Please
  note that no warranty is implied for correctness of results or fitness for a
  particular purpose.",2021-04-20,Johannes Ranke,https://pkgdown.jrwb.de/mkin/,TRUE,https://github.com/jranke/mkin,40473,8,2021-07-29T09:26:06Z,5059.125
mlbstatsR,"Main use case is to gives users the ability to work with Major League Baseball data in a clean and detailed way.
      Which provides users with a variety of ways to improve visualizations.",2021-06-14,Ivan Villanueva,<https://github.com/IvoVillanueva/mlbstatsR>,TRUE,https://github.com/ivovillanueva/mlbstatsr,2716,0,2021-06-04T18:41:39Z,NA
mleap,"A 'sparklyr' <https://spark.rstudio.com> extension that provides
  an interface to 'MLeap' <https://github.com/combust/mleap>, an open source library
  that enables exporting and serving of 'Apache Spark' pipelines.",2021-01-27,Kevin Kuo,https://github.com/rstudio/mleap,TRUE,https://github.com/rstudio/mleap,19789,22,2021-01-27T17:40:13Z,899.5
mlfit,"The Iterative Proportional Fitting (IPF) algorithm operates on count data. 
    This package offers implementations for several algorithms that extend this to 
    nested structures: 'parent' and 'child' items for both of which constraints can be provided.
    The fitting algorithms include Iterative Proportional Updating <https://trid.trb.org/view/881554>,
    Hierarchical IPF <doi:10.3929/ethz-a-006620748>, Entropy Optimization <https://trid.trb.org/view/881144>,
    and Generalized Raking <doi:10.2307/2290793>. Additionally, a number of replication methods
    is also provided such as 'Truncate, replicate, sample' <doi:10.1016/j.compenvurbsys.2013.03.004>. ",2021-07-02,Kirill Müller,"https://mlfit.github.io/mlfit/, https://github.com/mlfit/mlfit",TRUE,https://github.com/mlfit/mlfit,793,5,2021-07-29T04:13:56Z,158.6
MLGL,"It implements a new procedure of variable selection in the context of redundancy between explanatory variables, which holds true with high dimensional data (Grimonprez et al. (2018) <https://hal.inria.fr/hal-01857242>).",2020-11-28,Quentin Grimonprez,NA,TRUE,https://github.com/modal-inria/mlgl,13644,1,2021-08-24T15:18:57Z,13644
MLPA,"Functions to import Applied Biosystems data files of multiplex ligation-dependent probe amplification (MLPA) analysis and process them. Gene-expression profiling methods are described in Mareschal, Ruminy et al (2015) <doi:10.1016/j.jmoldx.2015.01.007>. Gene-fusion detection methods are described in Mareschal, Palau et al (under review).",2020-05-01,Sylvain Mareschal,http://bioinformatics.ovsa.fr/MLPA,TRUE,https://github.com/maressyl/r.mlpa,7610,0,2021-07-13T12:49:47Z,NA
mlpack,"A fast, flexible machine learning library, written in C++, that
             aims to provide fast, extensible implementations of cutting-edge
             machine learning algorithms.  See also Curtin et al. (2018)
             <doi:10.21105/joss.00726>.",2020-12-18,Yashwant Singh Parihar,"https://www.mlpack.org/doc/mlpack-3.4.2/r_documentation.html,
https://github.com/mlpack/mlpack",TRUE,https://github.com/mlpack/mlpack,2830,3788,2021-09-01T00:31:50Z,0.7470960929250264
mlquantify,"Quantification is a prominent machine learning task that has received an 
    increasing amount of attention in the last years. The objective is to predict the 
    class distribution of a data sample. This package is a collection of machine learning 
    algorithms for class distribution estimation. This package include algorithms from
    different paradigms of quantification. These methods are described in the paper: 
    A. Maletzke, W. Hassan, D. dos Reis, and G. Batista. The importance of the test set 
    size in quantification assessment. In Proceedings of the Twenty-Ninth International 
    Joint Conference on Artificial Intelligence, IJCAI20, pages 2640–2646, 2020.
    <doi:10.24963/ijcai.2020/366>.",2021-04-13,Andre Maletzke,https://github.com/andregustavom/mlquantify,TRUE,https://github.com/andregustavom/mlquantify,3983,4,2021-04-11T13:47:45Z,995.75
mlr3,"Efficient, object-oriented programming on the
    building blocks of machine learning. Provides 'R6' objects for tasks,
    learners, resamplings, and measures. The package is geared towards
    scalability and larger datasets by supporting parallelization and
    out-of-memory data-backends like databases. While 'mlr3' focuses on
    the core computational operations, add-on packages provide additional
    functionality.",2021-08-05,Michel Lang,"https://mlr3.mlr-org.com, https://github.com/mlr-org/mlr3",TRUE,https://github.com/mlr-org/mlr3,212357,538,2021-09-02T21:19:55Z,394.7156133828996
mlr3benchmark,"Implements methods for post-hoc analysis and
    visualisation of benchmark experiments, for 'mlr3' and beyond.",2021-04-19,Sonabend Raphael,"https://mlr3benchmark.mlr-org.com,
https://github.com/mlr-org/mlr3benchmark",TRUE,https://github.com/mlr-org/mlr3benchmark,4102,5,2021-04-21T14:28:37Z,820.4
mlr3cluster,Extends the 'mlr3' package with cluster analysis.,2021-09-03,Damir Pulatov,"https://mlr3cluster.mlr-org.com,
https://github.com/mlr-org/mlr3cluster",TRUE,https://github.com/mlr-org/mlr3cluster,11672,10,2021-09-01T07:01:39Z,1167.2
mlr3data,"A small collection of interesting and educational machine
    learning data sets which are used as examples in the 'mlr3' book
    (<https://mlr3book.mlr-org.com>), the use case gallery
    (<https://mlr3gallery.mlr-org.com>), or in other examples. All data
    sets are properly preprocessed and ready to be analyzed by most
    machine learning algorithms.  Data sets are automatically added to the
    dictionary of tasks if 'mlr3' is loaded.",2021-06-29,Michel Lang,https://github.com/mlr-org/mlr3data,TRUE,https://github.com/mlr-org/mlr3data,45734,1,2021-06-29T13:17:45Z,45734
mlr3db,"Extends the 'mlr3' package with a backend to
    transparently work with databases. Includes two extra backends:
    One relies on relies on the abstraction of package 'dbplyr' to interact with
    one of the many supported database management systems (DBMS). The other one
    is specialized for package 'duckdb'.",2021-04-13,Michel Lang,"https:///mlr3db.mlr-org.com, https://github.com/mlr-org/mlr3db",TRUE,https://github.com/mlr-org/mlr3db,15347,16,2021-08-04T13:37:51Z,959.1875
mlr3filters,"Extends 'mlr3' with filter methods for feature selection.
    Besides standalone filter methods built-in methods of any
    machine-learning algorithm are supported.  Partial scoring of
    multivariate filter methods is supported.",2021-07-12,Patrick Schratz,"https://mlr3filters.mlr-org.com,
https://github.com/mlr-org/mlr3filters",TRUE,https://github.com/mlr-org/mlr3filters,31759,8,2021-08-23T07:02:39Z,3969.875
mlr3fselect,"Implements methods for feature selection with
    'mlr3', e.g.  random search and sequential selection. Various
    termination criteria can be set and combined. The class
    'AutoFSelector' provides a convenient way to perform nested resampling
    in combination with 'mlr3'.",2021-03-09,Marc Becker,"https://mlr3fselect.mlr-org.com,
https://github.com/mlr-org/mlr3fselect",TRUE,https://github.com/mlr-org/mlr3fselect,12756,7,2021-09-02T11:59:25Z,1822.2857142857142
mlr3hyperband,"Implements hyperband method for hyperparameter
    tuning.  Various termination criteria can be set and combined. The
    class 'AutoTuner' provides a convenient way to perform nested
    resampling in combination with 'mlr3'. The hyperband algorithm was
    proposed by Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin
    Rostamizadeh and Ameet Talwalkar (2018) <arXiv:1603.06560>.",2021-01-29,Marc Becker,"https://mlr3hyperband.mlr-org.com,
https://github.com/mlr-org/mlr3hyperband",TRUE,https://github.com/mlr-org/mlr3hyperband,4831,15,2021-09-02T11:50:23Z,322.06666666666666
mlr3learners,"Recommended Learners for 'mlr3'. Extends 'mlr3' and 'mlr3proba'
    with interfaces to essential machine learning packages on CRAN.  This
    includes, but is not limited to: (penalized) linear and logistic
    regression, linear and quadratic discriminant analysis, k-nearest
    neighbors, naive Bayes, support vector machines, and gradient
    boosting.",2021-08-17,Michel Lang,"https://mlr3learners.mlr-org.com,
https://github.com/mlr-org/mlr3learners",TRUE,https://github.com/mlr-org/mlr3learners,54232,59,2021-09-01T12:46:37Z,919.1864406779661
mlr3measures,"Implements multiple performance measures for
    supervised learning.  Includes over 40 measures for regression and
    classification. Additionally, meta information about the performance
    measures can be queried, e.g. what the best and worst possible
    performances scores are.",2021-01-06,Michel Lang,"https:///mlr3measures.mlr-org.com,
https://github.com/mlr-org/mlr3measures",TRUE,https://github.com/mlr-org/mlr3measures,208821,6,2021-06-27T10:04:26Z,34803.5
mlr3misc,"Frequently used helper functions and assertions
    used in 'mlr3' and its companion packages. Comes with helper functions
    for functional programming, for printing, to work with 'data.table',
    as well as some generally useful 'R6' classes. This package also
    supersedes the package 'BBmisc'.",2021-07-14,Michel Lang,"https://mlr3misc.mlr-org.com, https://github.com/mlr-org/mlr3misc",TRUE,https://github.com/mlr-org/mlr3misc,241551,6,2021-09-01T12:48:43Z,40258.5
mlr3oml,"Provides an interface to 'OpenML.org' to list and
    download machine learning data and tasks. Data and tasks can be
    automatically converted to 'mlr3' tasks. For a more sophisticated
    interface which also allows uploading experiments, see the 'OpenML'
    package.",2021-04-16,Michel Lang,"https://mlr3oml.mlr-org.com, https://github.com/mlr-org/mlr3oml",TRUE,https://github.com/mlr-org/mlr3oml,7273,1,2021-07-20T09:01:16Z,7273
mlr3pipelines,"Dataflow programming toolkit that enriches 'mlr3' with a diverse
  set of pipelining operators ('PipeOps') that can be composed into graphs.
  Operations exist for data preprocessing, model fitting, and ensemble
  learning. Graphs can themselves be treated as 'mlr3' 'Learners' and can
  therefore be resampled, benchmarked, and tuned.",2021-08-05,Martin Binder,"https://mlr3pipelines.mlr-org.com,
https://github.com/mlr-org/mlr3pipelines",TRUE,https://github.com/mlr-org/mlr3pipelines,63588,105,2021-08-22T21:12:35Z,605.6
mlr3proba,"Provides extensions for probabilistic supervised
    learning for 'mlr3'.  This includes extending the regression task to
    probabilistic and interval regression, adding a survival task, and
    other specialized models, predictions, and measures. mlr3extralearners is available
    from <https://github.com/mlr-org/mlr3extralearners>.",2021-04-18,Raphael Sonabend,"https://mlr3proba.mlr-org.com,
https://github.com/mlr-org/mlr3proba",TRUE,https://github.com/mlr-org/mlr3proba,59475,55,2021-09-02T08:18:23Z,1081.3636363636363
mlr3spatiotempcv,"Extends the mlr3 ML framework with spatio-temporal resampling
    methods to account for the presence of spatiotemporal autocorrelation
    (STAC) in predictor variables. STAC may cause highly biased
    performance estimates in cross-validation if ignored.",2021-08-19,Patrick Schratz,"https://mlr3spatiotempcv.mlr-org.com/,
https://github.com/mlr-org/mlr3spatiotempcv,
https://mlr3book.mlr-org.com",TRUE,https://github.com/mlr-org/mlr3spatiotempcv,5411,35,2021-08-29T14:20:52Z,154.6
mlr3tuning,"Implements methods for hyperparameter tuning with
    'mlr3', e.g. Grid Search, Random Search, or Simulated Annealing.
    Various termination criteria can be set and combined.  The class
    'AutoTuner' provides a convenient way to perform nested resampling in
    combination with 'mlr3'.",2021-03-12,Marc Becker,"https://mlr3tuning.mlr-org.com,
https://github.com/mlr-org/mlr3tuning",TRUE,https://github.com/mlr-org/mlr3tuning,41089,31,2021-09-02T11:59:41Z,1325.4516129032259
mlr3verse,"The 'mlr3' package family is a set of packages for
    machine-learning purposes built in a modular fashion. This wrapper
    package is aimed to simplify the installation and loading of the core
    'mlr3' packages. Get more information about the 'mlr3' project at
    <https://mlr3book.mlr-org.com/>.",2021-08-11,Michel Lang,"https://mlr3verse.mlr-org.com,
https://github.com/mlr-org/mlr3verse",TRUE,https://github.com/mlr-org/mlr3verse,19536,14,2021-08-11T12:45:22Z,1395.4285714285713
mlr3viz,"Provides visualizations for 'mlr3' objects such as
    tasks, predictions, resample results or benchmark results via the
    autoplot() generic of 'ggplot2'. The returned 'ggplot' objects are
    intended to provide sensible defaults, yet can easily be customized to
    create camera-ready figures. Visualizations include barplots,
    boxplots, histograms, ROC curves, and Precision-Recall curves.",2021-08-12,Michel Lang,"https://mlr3viz.mlr-org.com, https://github.com/mlr-org/mlr3viz",TRUE,https://github.com/mlr-org/mlr3viz,29038,31,2021-08-30T08:04:26Z,936.7096774193549
mlrCPO,"Toolset that enriches 'mlr' with a diverse set of preprocessing
    operators. Composable Preprocessing Operators (""CPO""s) are first-class
    R objects that can be applied to data.frames and 'mlr' ""Task""s to modify
    data, can be attached to 'mlr' ""Learner""s to add preprocessing to machine
    learning algorithms, and can be composed to form preprocessing pipelines.",2021-02-24,Martin Binder,https://github.com/mlr-org/mlrCPO,TRUE,https://github.com/mlr-org/mlrcpo,21243,36,2020-11-16T18:31:17Z,590.0833333333334
mlrintermbo,"The 'mlrMBO' package can ordinarily not be used for optimization within 'mlr3', because of
  incompatibilities of their respective class systems. 'mlrintermbo' offers a compatibility
  interface that provides 'mlrMBO' as an 'mlr3tuning' 'Tuner' object, for tuning of machine
  learning algorithms within 'mlr3', as well as a 'bbotk' 'Optimizer' object for optimization
  of general objective functions using the 'bbotk' black box optimization framework. The
  control parameters of 'mlrMBO' are faithfully reproduced as a 'paradox' 'ParamSet'.",2021-03-01,Martin Binder,https://github.com/mb706/mlrintermbo,TRUE,https://github.com/mb706/mlrintermbo,2037,3,2021-05-16T18:24:36Z,679
mlrMBO,"Flexible and comprehensive R toolbox for model-based optimization
    ('MBO'), also known as Bayesian optimization. It implements the Efficient
    Global Optimization Algorithm and is designed for both single- and multi-
    objective optimization with mixed continuous, categorical and conditional
    parameters. The machine learning toolbox 'mlr' provide dozens of regression
    learners to model the performance of the target algorithm with respect to
    the parameter settings. It provides many different infill criteria to guide
    the search process. Additional features include multi-point batch proposal,
    parallel execution as well as visualization and sophisticated logging
    mechanisms, which is especially useful for teaching and understanding of
    algorithm behavior. 'mlrMBO' is implemented in a modular fashion, such that
    single components can be easily replaced or adapted by the user for specific
    use cases.",2020-10-23,Bernd Bischl,https://github.com/mlr-org/mlrMBO,TRUE,https://github.com/mlr-org/mlrmbo,135949,172,2021-08-28T07:58:13Z,790.4011627906976
mlsjunkgen,"Generate a stream of pseudo-random numbers generated using the MLS 
    Junk Generator algorithm. Functions exist to generate single pseudo-random 
    numbers as well as a vector, data frame, or matrix of pseudo-random numbers.",2021-05-02,Steve Myles,"https://stevemyles.site/mlsjunkgen/,
https://github.com/scumdogsteev/mlsjunkgen",TRUE,https://github.com/scumdogsteev/mlsjunkgen,15915,0,2021-05-09T17:29:06Z,NA
mltools,"A collection of machine learning helper functions, particularly assisting in the Exploratory Data Analysis phase. Makes heavy use of the 'data.table' package for optimal speed and memory efficiency. Highlights include a versatile bin_data() function, sparsify() for converting a data.table to sparse matrix format with one-hot encoding, fast evaluation metrics, and empirical_cdf() for calculating empirical Multivariate Cumulative Distribution Functions.",2018-05-12,Ben Gorman,https://github.com/ben519/mltools,TRUE,https://github.com/ben519/mltools,115947,65,2021-08-11T14:11:38Z,1783.8
MLVSBM,"Simulation, inference and clustering of multilevel networks using a
    Stochastic Block Model framework as described in Chabert-Liddell, Barbillon,
    Donnet and Lazega (2021) <doi:10.1016/j.csda.2021.107179>. 
    A multilevel network is defined as the junction of two interaction networks, 
    the upper level or inter-organizational level and the lower level or 
    inter-individual level. The inter-level represents an affiliation 
    relationship.",2021-06-10,Saint-Clair Chabert-Liddell,https://github.com/Chabert-Liddell/MLVSBM,TRUE,https://github.com/chabert-liddell/mlvsbm,2664,0,2021-06-10T15:42:05Z,NA
mlxR,"Simulation and visualization of complex
    models for longitudinal data. The models are encoded using the model coding
    language 'Mlxtran' and automatically converted into C++ codes. 
    That allows one to implement very easily complex ODE-based models and complex 
    statistical models, including mixed effects models, for continuous, count, 
    categorical, and time-to-event data.",2021-01-19,Marc Lavielle,http://simulx.webpopix.org,TRUE,https://github.com/marclavielle/mlxr,27741,16,2021-05-07T12:19:24Z,1733.8125
mmaqshiny,"Mobile-monitoring or ""sensors on a mobile platform"", is an increasingly 
    popular approach to measure high-resolution pollution data at the street level. 
    Coupled with location data, spatial visualisation of air-quality parameters 
    helps detect localized areas of high air-pollution, also called hotspots. 
    In this approach, portable sensors are mounted on a vehicle and driven on 
    predetermined routes to collect high frequency data (1 Hz). 
    'mmaqshiny' is for analysing, visualising and spatial mapping of 
    high-resolution air-quality data collected by specific devices installed on 
    a moving platform. 1 Hz data of PM2.5 (mass concentrations of particulate  
    matter with size less than 2.5 microns), Black carbon mass concentrations 
    (BC), ultra-fine particle number concentrations, carbon dioxide along with 
    GPS coordinates and relative humidity (RH) data collected by popular 
    portable instruments (TSI DustTrak-8530, Aethlabs microAeth-AE51, TSI CPC3007, 
    LICOR Li-830, Garmin GPSMAP 64s, Omega USB RH probe respectively). It 
    incorporates device specific cleaning and correction algorithms. RH correction 
    is applied to DustTrak PM2.5 following the Chakrabarti et al., (2004) 
    <doi:10.1016/j.atmosenv.2004.03.007>. Provision is given to add linear 
    regression coefficients for correcting the PM2.5 data (if required). BC data
    will be cleaned for the vibration generated noise, by adopting the statistical 
    procedure as explained in Apte et al., (2011) <doi:10.1016/j.atmosenv.2011.05.028>, 
    followed by a loading correction as suggested by Ban-Weiss et al., (2009)  
    <doi:10.1021/es8021039>. For the number concentration data, provision is 
    given for dilution correction factor (if a diluter is used with CPC3007; 
    default value is 1). The package joins the raw, cleaned and corrected data 
    from the above said instruments and outputs as a downloadable csv file. ",2020-06-26,Adithi R. Upadhya,https://github.com/meenakshi-kushwaha/mmaqshiny,TRUE,https://github.com/meenakshi-kushwaha/mmaqshiny,4875,3,2021-07-22T06:51:38Z,1625
mmb,"Supports Bayesian models with full and partial (hence
    arbitrary) dependencies between random variables. Discrete and continuous
    variables are supported, and conditional joint probabilities and probability
    densities are estimated using Kernel Density Estimation (KDE). The full
    general form, which implements an extension to Bayes' theorem, as well as
    the simple form, which is just a Bayesian network, both support regression
    through segmentation and KDE and estimation of probability or relative
    likelihood of discrete or continuous target random variables. This package
    also provides true statistical distance measures based on Bayesian models.
    Furthermore, these measures can be facilitated on neighborhood searches,
    and to estimate the similarity and distance between data points.
    Related work is by Bayes (1763) <doi:10.1098/rstl.1763.0053>
    and by Scutari (2010) <doi:10.18637/jss.v035.i03>.",2020-09-23,Sebastian Hönel,https://github.com/MrShoenel/R-mmb,TRUE,https://github.com/mrshoenel/r-mmb,3903,0,2020-11-13T10:16:35Z,NA
MMDCopula,"Provides functions for the robust estimation of 
	parametric families of copulas using minimization of 
	the Maximum Mean Discrepancy, following the article
	Alquier, Chérief-Abdellatif, Derumigny and Fermanian (2020) <arXiv:2010.00408>.",2021-08-10,Alexis Derumigny,NA,TRUE,https://github.com/alexisderumigny/mmdcopula,3701,2,2021-08-09T10:55:06Z,1850.5
mnis,"An API package for the Members' Name Information Service operated 
    by the UK parliament. Documentation for the API itself can be found 
    here: <http://data.parliament.uk/membersdataplatform/default.aspx>.",2021-04-03,Evan Odell,https://docs.evanodell.com/mnis,TRUE,https://github.com/evanodell/mnis,16931,2,2021-04-04T15:38:08Z,8465.5
MNP,"Fits the Bayesian multinomial probit model via Markov chain
 Monte Carlo.  The multinomial probit model is often used to analyze 
 the discrete choices made by individuals recorded in survey data. 
 Examples where the multinomial probit model may be useful include the 
 analysis of product choice by consumers in market research and the 
 analysis of candidate or party choice by voters in electoral studies.  
 The MNP package can also fit the model with different choice sets for 
 each individual, and complete or partial individual choice orderings 
 of the available alternatives from the choice set. The estimation is
 based on the efficient marginal data augmentation algorithm that is 
 developed by Imai and van Dyk (2005). ""A Bayesian Analysis of the 
 Multinomial Probit Model Using the Data Augmentation."" Journal of 
 Econometrics, Vol. 124, No. 2 (February), pp. 311-334. 
 <doi:10.1016/j.jeconom.2004.02.002>  Detailed examples are given in 
 Imai and van Dyk (2005). ""MNP: R Package for Fitting the Multinomial 
 Probit Model.""  Journal of Statistical Software, Vol. 14, No. 3 (May), 
 pp. 1-32. <doi:10.18637/jss.v014.i03>.",2021-05-13,Kosuke Imai,https://github.com/kosukeimai/MNP,TRUE,https://github.com/kosukeimai/mnp,47426,8,2021-05-12T20:50:10Z,5928.25
mob,"Generate the monotonic binning and
    perform the woe (weight of evidence) transformation for the logistic regression
    used in the consumer credit scorecard development. The woe transformation is a piecewise
    transformation that is linear to the log odds. For a numeric variable, all of its monotonic 
    functional transformations will converge to the same woe transformation. ",2021-07-31,WenSui Liu,https://github.com/statcompute/mob,TRUE,https://github.com/statcompute/mob,11881,48,2020-11-07T16:57:22Z,247.52083333333334
mobsim,"Tools for the simulation, analysis and sampling of spatial
    biodiversity data (May et al. 2017) <doi:10.1101/209502>.
    In the simulation tools user define the numbers of
    species and individuals, the species abundance distribution and species
    aggregation. Functions for analysis include species rarefaction 
    and accumulation curves, species-area relationships and the distance
    decay of similarity. ",2021-03-23,Felix May,https://github.com/MoBiodiv/mobsim,TRUE,https://github.com/mobiodiv/mobsim,16355,13,2021-07-22T10:00:13Z,1258.076923076923
mockery,"
    The two main functionalities of this package are creating mock
    objects (functions) and selectively intercepting calls to a given
    function that originate in some other function. It can be used
    with any testing framework available for R. Mock objects can
    be injected with either this package's own stub() function or a
    similar with_mock() facility present in the 'testthat' package.",2019-09-03,Jim Hester,https://github.com/jfiksel/mockery,TRUE,https://github.com/jfiksel/mockery,784906,72,2020-09-21T14:41:08Z,10901.472222222223
mockr,"Provides a means to mock a package function, i.e., temporarily substitute it for testing. Designed as a drop-in replacement for 'testthat::with_mock()', which may break in R 3.4.0 and later.",2017-04-29,Kirill Müller,"https://github.com/krlmlr/mockr, http://krlmlr.github.io/mockr",TRUE,https://github.com/krlmlr/mockr,99249,13,2021-07-29T04:13:59Z,7634.538461538462
mockthat,"With the deprecation of mocking capabilities shipped with
    'testthat' as of 'edition 3' it is left to third-party packages to replace
    this functionality, which in some test-scenarios is essential in order to
    run unit tests in limited environments (such as no Internet connection).
    Mocking in this setting means temporarily substituting a function with a
    stub that acts in some sense like the original function (for example by
    serving a HTTP response that has been cached as a file). The only exported
    function 'with_mock()' is modeled after the eponymous 'testthat' function
    with the intention of providing a drop-in replacement.",2021-04-23,Nicolas Bennett,https://nbenn.github.io/mockthat/,TRUE,https://github.com/nbenn/mockthat,5444,10,2021-04-23T08:42:59Z,544.4
modelbased,"Implements a general interface for model-based
    estimations for a wide variety of models (see support list of insight;
    Lüdecke, Waggoner & Makowski (2019) <doi:10.21105/joss.01412>), used
    in the computation of marginal means, contrast analysis and
    predictions.",2021-06-06,Dominique Makowski  (<https://orcid.org/0000-0001-5375-9967>,https://easystats.github.io/modelbased/,TRUE,https://github.com/easystats/modelbased,49792,139,2021-08-19T01:08:38Z,358.2158273381295
modeldata,"Data sets used for demonstrating or testing model-related
    packages are contained in this package.",2021-07-14,Max Kuhn,"https://modeldata.tidymodels.org,
https://github.com/tidymodels/modeldata",TRUE,https://github.com/tidymodels/modeldata,548759,17,2021-08-31T15:15:07Z,32279.941176470587
modeldb,Uses 'dplyr' and 'tidyeval' to fit statistical models inside the database. It currently supports KMeans and linear regression models.,2020-02-10,Max Kuhn,https://github.com/tidymodels/modeldb,TRUE,https://github.com/tidymodels/modeldb,26819,74,2021-03-31T17:46:09Z,362.4189189189189
modelDown,"Website generator with HTML summaries for predictive models.
    This package uses 'DALEX' explainers to describe global model behavior. 
    We can see how well models behave (tabs: Model Performance, Auditor),
    how much each variable contributes to predictions (tabs: Variable Response) 
    and which variables are the most important for a given model (tabs: Variable Importance).
    We can also compare Concept Drift for pairs of models (tabs: Drifter).
    Additionally, data available on the website can be easily recreated in current R session.
    Work on this package was financially supported by the NCN Opus grant 2017/27/B/ST6/01307 
    at Warsaw University of Technology, Faculty of Mathematics and Information Science.",2020-04-15,Kamil Romaszko,https://github.com/ModelOriented/modelDown,TRUE,https://github.com/modeloriented/modeldown,11788,105,2020-10-23T16:06:21Z,112.26666666666667
modelimpact,"Calculate the financial impact of using a churn model in terms of cost, revenue, profit and return on investment.",2021-05-06,Peer Christensen,https://github.com/PeerChristensen/modelimpact,TRUE,https://github.com/peerchristensen/modelimpact,1395,1,2021-05-06T13:52:46Z,1395
modeLLtest,"An implementation of the cross-validated difference in means (CVDM) test by Desmarais and Harden (2014) <doi:10.1007/s11135-013-9884-7> (see also Harden and Desmarais, 2011 <doi:10.1177/1532440011408929>) and the cross-validated median fit (CVMF) test by Desmarais and Harden (2012) <doi:10.1093/pan/mpr042>. These tests use leave-one-out cross-validated log-likelihoods to assist in selecting among model estimations. You can also utilize data from Golder (2010) <doi:10.1177/0010414009341714> and Joshi & Mason (2008) <doi:10.1177/0022343308096155> that are included to facilitate examples from real-world analysis.",2020-08-07,Shana Scogin,https://github.com/ShanaScogin/modeLLtest,TRUE,https://github.com/shanascogin/modelltest,11587,11,2020-12-06T02:38:36Z,1053.3636363636363
modelplotr,"Plots to assess the quality of predictive models from a business perspective. 
    Using these plots, it can be shown how implementation of the model will impact business 
    targets like response on a campaign or return on investment. Different scopes can be selected: 
    compare models, compare datasets or compare target class values and various plot customization 
    and highlighting options are available.
    targets like response on a campaign. Different scopes can be selected: compare models, compare 
    datasets or compare target class values and various plot customization and highlighting options 
    are available.",2020-10-13,Jurriaan Nagelkerke,https://github.com/jurrr/modelplotr,TRUE,https://github.com/jurrr/modelplotr,13615,14,2020-10-13T00:08:08Z,972.5
modelStudio,"Automate the explanatory analysis of machine learning predictive models.
    Generate advanced interactive model explanations in the form of a serverless HTML
    site with only one line of code. This tool is model-agnostic, therefore compatible
    with most of the black-box predictive models and frameworks. The main function
    computes various (instance and model-level) explanations and produces
    a customisable dashboard, which consists of multiple panels for plots with their
    short descriptions. Easily save the dashboard and share it with others. Tools
    for Explanatory Model Analysis unite with tools for Exploratory Data Analysis
    to give a broad overview of the model behavior.",2021-07-13,Hubert Baniecki,"https://modelstudio.drwhy.ai,
https://github.com/ModelOriented/modelStudio",TRUE,https://github.com/modeloriented/modelstudio,20117,204,2021-08-15T10:16:29Z,98.61274509803921
modelsummary,"Create beautiful and customizable tables to summarize several
    statistical models side-by-side. Draw coefficient plots, multi-level
    cross-tabs, dataset summaries, balance tables (a.k.a. ""Table 1s""), and
    correlation matrices. This package supports dozens of statistical models,
    and it can produce tables in HTML, LaTeX, Word, Markdown, PDF, PowerPoint,
    Excel, RTF, JPG, or PNG. Tables can easily be embedded in 'Rmarkdown' or 
    'knitr' dynamic documents.",2021-08-12,Vincent Arel-Bundock,https://vincentarelbundock.github.io/modelsummary/,TRUE,https://github.com/vincentarelbundock/modelsummary,51196,489,2021-08-25T20:42:47Z,104.69529652351738
modeltests,"Provides a number of testthat tests that can be
    used to verify that tidy(), glance() and augment() methods meet
    consistent specifications. This allows methods for the same generic to
    be spread across multiple packages, since all of those packages can
    make the same guarantees to users about returned objects.",2021-01-13,Alex Hayes,https://github.com/alexpghayes/modeltests,TRUE,https://github.com/alexpghayes/modeltests,38854,6,2021-01-13T16:50:52Z,6475.666666666667
modeltime,"
    The time series forecasting framework for use with the 'tidymodels' ecosystem. 
    Models include ARIMA, Exponential Smoothing, and additional time series models
    from the 'forecast' and 'prophet' packages. Refer to ""Forecasting Principles & Practice, Second edition"" 
    (<https://otexts.com/fpp2/>).
    Refer to ""Prophet: forecasting at scale"" 
    (<https://research.fb.com/blog/2017/02/prophet-forecasting-at-scale/>.).",2021-07-16,Matt Dancho,https://github.com/business-science/modeltime,TRUE,https://github.com/business-science/modeltime,48384,292,2021-08-26T20:23:44Z,165.6986301369863
modeltime.ensemble,"
    A 'modeltime' extension that implements time series ensemble forecasting methods including model averaging, 
    weighted averaging, and stacking. These techniques are popular methods 
    to improve forecast accuracy and stability. Refer to papers such as 
    ""Machine-Learning Models for Sales Time Series Forecasting"" Pavlyshenko, B.M. (2019) <doi:10.3390>.",2021-07-16,Matt Dancho,https://github.com/business-science/modeltime.ensemble,TRUE,https://github.com/business-science/modeltime.ensemble,9671,53,2021-08-01T13:24:48Z,182.47169811320754
modeltime.gluonts,"
    Use the 'GluonTS' deep learning library inside of 'modeltime'.
    Available models include 'DeepAR', 'N-BEATS', and 'N-BEATS' Ensemble. 
    Refer to ""GluonTS - Probabilistic Time Series Modeling"" 
    (<https://ts.gluon.ai/index.html>).",2020-11-30,Matt Dancho,https://github.com/business-science/modeltime.gluonts,TRUE,https://github.com/business-science/modeltime.gluonts,4095,24,2021-08-04T11:09:56Z,170.625
modeltime.h2o,"
    Use the 'H2O' machine learning library inside of 'modeltime'.
    Available models include 'AutoML' for Automatic Machine Learning. 
    Please see H2O.ai for more information <https://github.com/h2oai/h2o-3>.",2021-04-05,Matt Dancho,https://github.com/business-science/modeltime.h2o,TRUE,https://github.com/business-science/modeltime.h2o,2767,26,2021-07-02T14:29:22Z,106.42307692307692
modeltime.resample,"
    A 'modeltime' extension that implements forecast resampling tools
    that assess time-based model performance and stability for a single time series, 
    panel data, and cross-sectional time series analysis. ",2021-03-14,Matt Dancho,https://github.com/business-science/modeltime.resample,TRUE,https://github.com/business-science/modeltime.resample,7804,11,2021-03-22T15:25:52Z,709.4545454545455
moderndive,"Datasets and wrapper functions for tidyverse-friendly introductory linear regression, used in ""Statistical Inference via Data Science: A ModernDive into R and the Tidyverse"" available at <https://moderndive.com/>.",2021-07-21,Albert Y. Kim,https://github.com/moderndive/moderndive/issues,TRUE,https://github.com/moderndive/moderndive,69151,72,2021-07-29T12:40:49Z,960.4305555555555
MODISTools,"Programmatic interface to the Oak Ridge National Laboratories
    'MODIS Land Products Subsets' web services 
    (<https://modis.ornl.gov/data/modis_webservice.html>). Allows for easy
    downloads of 'MODIS' time series directly to your R workspace or
    your computer.",2020-03-05,Hufkens Koen,"https://docs.ropensci.org/MODISTools,
https://github.com/ropensci/MODISTools",TRUE,https://github.com/ropensci/modistools,30600,36,2021-06-15T14:17:43Z,850
MODIStsp,"Allows automating the creation of time series of rasters derived
    from MODIS satellite land products data. It performs several typical
    preprocessing steps such as download, mosaicking, reprojecting and resizing
    data acquired on a specified time period. All processing parameters
    can be set using a user-friendly GUI. Users can select which layers of
    the original MODIS HDF files they want to process, which additional
    quality indicators should be extracted from aggregated MODIS quality
    assurance layers and, in the case of surface reflectance products,
    which spectral indexes should be computed from the original reflectance
    bands. For each output layer, outputs are saved as single-band raster
    files corresponding to each available acquisition date. Virtual files
    allowing access to the entire time series as a single file are also created.
    Command-line execution exploiting a previously saved processing options
    file is also possible, allowing users to automatically update time series
    related to a MODIS product whenever a new image is available.
    For additional documentation refer to the following article: 
    Busetto and Ranghetti (2016) <doi:10.1016/j.cageo.2016.08.020>.",2021-08-03,Lorenzo Busetto,"https://github.com/ropensci/MODIStsp/,
https://docs.ropensci.org/MODIStsp/",TRUE,https://github.com/ropensci/modistsp,33376,126,2021-08-03T11:48:14Z,264.8888888888889
modmarg,"Calculate predicted levels and marginal effects,
    using the delta method to calculate standard errors. This is an R-based
    version of the 'margins' command from Stata.",2020-11-22,Annie Wang,https://github.com/anniejw6/modmarg,TRUE,https://github.com/anniejw6/modmarg,13884,15,2020-11-21T20:19:39Z,925.6
ModStatR,"Datasets and functions for the book ""Modélisation statistique par la pratique avec R"", F. Bertrand, E. Claeys and M. Maumy-Bertrand (2019, ISBN:9782100793525, Dunod, Paris). The first chapter of the book is dedicated to an introduction to the R statistical software. The second chapter deals with correlation analysis: Pearson, Spearman and Kendall simple, multiple and partial correlation coefficients. New wrapper functions for permutation tests or bootstrap of matrices of correlation are provided with the package. The third chapter is dedicated to data exploration with factorial analyses (PCA, CA, MCA, MDA) and clustering. The fourth chapter is dedicated to regression analysis: fitting and model diagnostics are detailed. The exercises focus on covariance analysis, logistic regression, Poisson regression, two-way analysis of variance for fixed or random factors. Various example datasets are shipped with the package: for instance on pokemon, world of warcraft, house tasks or food nutrition analyses.",2021-03-14,Frederic Bertrand,"https://fbertran.github.io/homepage/,
https://github.com/fbertran/ModStatR/",TRUE,https://github.com/fbertran/modstatr,8763,5,2021-04-12T17:10:34Z,1752.6
modules,"Provides modules as an organizational unit for source code. Modules
    enforce to be more rigorous when defining dependencies and have
    a local search path. They can be used as a sub unit within packages
    or in scripts.",2021-02-06,Sebastian Warnholz,https://github.com/wahani/modules,TRUE,https://github.com/wahani/modules,42938,61,2021-02-06T21:08:51Z,703.9016393442623
MoEClust,"Clustering via parsimonious Gaussian Mixtures of Experts using the MoEClust models introduced by Murphy and Murphy (2020) <doi:10.1007/s11634-019-00373-8>. This package fits finite Gaussian mixture models with a formula interface for supplying gating and/or expert network covariates using a range of parsimonious covariance parameterisations from the GPCM family via the EM/CEM algorithm. Visualisation of the results of such models using generalised pairs plots and the inclusion of an additional noise component is also facilitated. A greedy forward stepwise search algorithm is provided for identifying the optimal model in terms of the number of components, the GPCM covariance parameterisation, and the subsets of gating/expert network covariates.",2021-06-22,Keefe Murphy,https://cran.r-project.org/package=MoEClust,TRUE,https://github.com/keefe-murphy/moeclust,20940,5,2021-06-22T14:53:26Z,4188
moexer,"This is a thin wrapper around the 'MOEX' 'ISS' REST interface, see
    <https://iss.moex.com>.  It allows to quickly fetch price candles for a 
    particular security, obtain its profile information and so on.",2021-02-01,Dmitry Zotikov,https://github.com/x1o/moexer,TRUE,https://github.com/x1o/moexer,2443,0,2021-01-23T23:00:53Z,NA
mojson,"Support JSON flattening in a long data frame way, where the nesting keys will be stored in the absolute path. It also
   provides an easy way to summarize the basic description of a JSON list. The idea of 'mojson' is to transform a JSON object in an 
   absolute serialization way, which means the early key-value pairs will appear in the heading rows of the resultant data frame. 
   'mojson' also provides an alternative way of comparing two different JSON lists, returning the left/inner/right-join style results.",2021-02-11,Bo Wei,https://github.com/chriswweibo/mojson,TRUE,https://github.com/chriswweibo/mojson,2298,1,2021-03-01T09:22:24Z,2298
MolgenisArmadillo,"A set of functions to be able to manage data shared on a
  'MOLGENIS Armadillo' storage server ('MinIO').",2021-02-09,Sido Haakma,"https://github.com/molgenis/molgenis-r-armadillo/,
https://molgenis.github.io/molgenis-r-armadillo/",TRUE,https://github.com/molgenis/molgenis-r-armadillo,2598,1,2021-06-07T14:24:20Z,2598
MolgenisAuth,"Discover 'OpenID Connect' endpoints and authenticate
    using device flow. Used by 'MOLGENIS' packages.",2020-11-06,Fleur Kelpin,"https://github.com/molgenis/molgenis-r-auth/,
https://molgenis.github.io/molgenis-r-auth/",TRUE,https://github.com/molgenis/molgenis-r-auth,3652,1,2021-06-07T14:26:23Z,3652
molic,"Outlier detection in, possibly high-dimensional, categorical data following
	     Mads Lindskou et al. (2019) <doi:10.1111/sjos.12407>.",2021-06-02,Mads Lindskou,https://github.com/mlindsk/molic,TRUE,https://github.com/mlindsk/molic,5268,3,2021-06-01T07:48:53Z,1756
mombf,Bayesian model selection and averaging for regression and mixtures for non-local and selected local priors.,2021-07-13,David Rossell,https://github.com/davidrusi/mombf,TRUE,https://github.com/davidrusi/mombf,34692,2,2021-07-08T09:56:56Z,17346
momentuHMM,"Extended tools for analyzing telemetry data using generalized hidden Markov models. Features of momentuHMM (pronounced ``momentum'') include data pre-processing and visualization, fitting HMMs to location and auxiliary biotelemetry or environmental data, biased and correlated random walk movement models, hierarchical HMMs, multiple imputation for incorporating location measurement error and missing data, user-specified design matrices and constraints for covariate modelling of parameters, random effects, decoding of the state process, visualization of fitted models, model checking and selection, and simulation. See McClintock and Michelot (2018) <doi:10.1111/2041-210X.12995>.",2021-09-03,Brett McClintock,"https://github.com/bmcclintock/momentuHMM,
https://github.com/bmcclintock/momentuHMM/discussions",TRUE,https://github.com/bmcclintock/momentuhmm,26154,22,2021-09-02T21:48:35Z,1188.8181818181818
Momocs,"The goal of 'Momocs' is to provide a complete, convenient, 
       reproducible and open-source toolkit for 2D morphometrics.
       It includes most common 2D morphometrics approaches on outlines, open outlines, 
       configurations of landmarks, traditional morphometrics, and facilities for data preparation, 
       manipulation and visualization with a consistent grammar throughout.
       It allows reproducible, complex morphometrics analyses and other morphometrics approaches 
       should be easy to plug in, or develop from, on top of this canvas. ",2020-10-06,Vincent Bonhomme,https://github.com/MomX/Momocs/,TRUE,https://github.com/momx/momocs,29968,46,2021-01-08T11:53:18Z,651.4782608695652
monaco,"A HTML widget rendering the 'Monaco' editor. The 'Monaco' editor is the code editor which powers 'VS Code'. It is particularly well developed for 'JavaScript'. In addition to the built-in features of the 'Monaco' editor, the widget allows to prettify multiple languages, to view the 'HTML' rendering of 'Markdown' code, and to view and resize 'SVG' images.",2021-03-06,Stéphane Laurent,https://github.com/stla/monaco,TRUE,https://github.com/stla/monaco,4002,2,2021-03-06T09:57:37Z,2001
mondate,"Keep track of dates in terms of fractional calendar months 
  per Damien Laker ""Time Calculations for Annualizing Returns: the Need for Standardization"", 
  The Journal of Performance Measurement, 2008.
  Model dates as of close of business.
  Perform date arithmetic in units of ""months"" and ""years"".
  Allow ""infinite"" dates to model ""ultimate"" time.",2021-01-29,Dan Murphy,"https://www.R-project.org, https://github.com/chiefmurph/mondate",TRUE,https://github.com/chiefmurph/mondate,201220,0,2021-02-01T16:06:58Z,NA
Mondrian,"The unique function of this package allows representing in a single graph the relative occurrence and co-occurrence of events measured in a sample. 
  As examples, the package was applied to describe the occurrence and co-occurrence of different species of bacterial or viral symbionts infecting arthropods at the individual level. The graphics allows determining the prevalence of each symbiont and the patterns of multiple infections (i.e. how different symbionts share or not the same individual hosts). 
  We named the package after the famous painter as the graphical output recalls Mondrian’s paintings.",2020-05-22,Aurélie Siberchicot,"https://github.com/aursiber/Mondrian ;
http://lbbe-shiny.univ-lyon1.fr/MondrianShiny/",TRUE,https://github.com/aursiber/mondrian,16836,1,2021-02-18T08:05:56Z,16836
mongolite,"High-performance MongoDB client based on 'mongo-c-driver' and 'jsonlite'.
    Includes support for aggregation, indexing, map-reduce, streaming, encryption,
    enterprise authentication, and GridFS. The online user manual provides an overview 
    of the available methods in the package: <https://jeroen.github.io/mongolite/>.",2021-04-30,Jeroen Ooms,"https://github.com/jeroen/mongolite/ (devel)
https://jeroen.github.io/mongolite/ (user manual)
http://mongoc.org/ (upstream)",TRUE,https://github.com/jeroen/mongolite,275147,265,2021-04-30T13:38:45Z,1038.2905660377357
monobin,"Performs monotonic binning of numeric risk factor in credit rating models (PD, LGD, EAD) 
	development. All functions handle both binary and continuous target variable. 
	Functions that use isotonic regression in the first stage of binning process have an additional 
	feature for correction of minimum percentage of observations and minimum target rate per bin. 	
	Additionally, monotonic trend can be identified based on raw data or, if known in advance,
	forced by functions' argument. Missing values and other possible special values are treated 
	separately from so-called complete cases.",2021-08-02,Andrija Djurovic,https://github.com/andrija-djurovic/monobin,TRUE,https://github.com/andrija-djurovic/monobin,1378,2,2021-08-13T09:56:20Z,689
monobinShiny,"This is an add-on package to the 'monobin' package that simplifies its use. It provides shiny-based user interface (UI) 
	     that is especially handy for less experienced 'R' users as well as for those who intend to perform quick scanning 
	     of numeric risk factors when building credit rating models. The additional functions implemented in 
	     'monobinShiny' that do no exist in 'monobin' package are: descriptive statistics, special case and outliers imputation. 
	     The function descriptive statistics is exported and can be used in 'R' sessions independently from the user interface, 
	     while special case and outlier imputation functions are written to be used with shiny UI.",2021-08-23,Andrija Djurovic,https://github.com/andrija-djurovic/monobinShiny,TRUE,https://github.com/andrija-djurovic/monobinshiny,121,0,2021-08-21T13:33:11Z,NA
monoClust,"Implementation of the Monothetic Clustering
    algorithm (Chavent, 1998 <doi:10.1016/S0167-8655(98)00087-7>) on
    continuous data sets. A lot of extensions are included in the package,
    including applying Monothetic clustering on data sets with circular
    variables, visualizations with the results, and permutation and
    cross-validation based tests to support the decision on the number of
    clusters.",2021-02-15,Tan Tran,"https://vinhtantran.github.io/monoClust/,
https://github.com/vinhtantran/monoClust",TRUE,https://github.com/vinhtantran/monoclust,4124,1,2021-02-22T00:28:21Z,4124
monotonicity,"Test for monotonicity in financial variables sorted by portfolios. It is conventional practice in empirical research to form portfolios of assets ranked by a certain sort variable. A t-test is then used to consider the mean return spread between the portfolios with the highest and lowest values of the sort variable. Yet comparing only the average returns on the top and bottom portfolios does not provide a sufficient way to test for a monotonic relation between expected returns and the sort variable. This package provides nonparametric tests for the full set of monotonic patterns by Patton, A. and Timmermann, A. (2010) <doi:10.1016/j.jfineco.2010.06.006> and compares the proposed results with extant alternatives such as t-tests, Bonferroni bounds, and multivariate inequality tests through empirical applications and simulations.",2019-12-05,Siegfried Köstlmeier,https://github.com/skoestlmeier/monotonicity,TRUE,https://github.com/skoestlmeier/monotonicity,15079,2,2020-10-12T07:35:48Z,7539.5
moodleR,"A collection of functions to connect to a 'Moodle' database, cache relevant tables locally and generate learning analytics. 
    'Moodle' is an open source Learning Management System (LMS) developed by MoodleHQ. For more information about Moodle, visit <https://moodle.org>.",2021-04-20,Aleksander Dietrichson,"https://github.com/chi2labs/moodleR,
https://chi2labs.github.io/moodleR/",TRUE,https://github.com/chi2labs/moodler,1501,0,2021-04-21T16:57:52Z,NA
moonBook,"Several analysis-related functions for the book entitled ""R
    statistics and graph for medical articles"" (written in Korean), version 1,
    by Keon-Woong Moon with Korean demographic data with several plot
    functions.",2021-01-08,Keon-Woong Moon,https://github.com/cardiomoon/moonBook,TRUE,https://github.com/cardiomoon/moonbook,164499,23,2021-07-07T00:45:47Z,7152.130434782609
mopac,"Provides real & simulated datasets containing time-series traffic observations
    and additional information pertaining to Loop 1 ""Mopac"" located in Austin, Texas.",2021-04-28,Scott McKenzie,https://github.com/sccmckenzie/mopac,TRUE,https://github.com/sccmckenzie/mopac,1524,0,2021-04-27T15:29:35Z,NA
morpheus,"Mixture of logistic regressions parameters (H)estimation with
    (U)spectral methods. The main methods take d-dimensional inputs and a vector
    of binary outputs, and return parameters according to the GLMs mixture model
    (General Linear Model). For more details see chapter 3 in the PhD thesis of
		Mor-Absa Loum: <http://www.theses.fr/s156435>, available here
		<https://tel.archives-ouvertes.fr/tel-01877796/document>.",2020-01-12,Benjamin Auder,https://github.com/yagu0/morpheus,TRUE,https://github.com/yagu0/morpheus,13744,0,2021-06-07T22:09:00Z,NA
Morpho,"A toolset for Geometric Morphometrics and mesh processing. This
    includes (among other stuff) mesh deformations based on reference points,
    permutation tests, detection of outliers, processing of sliding
    semi-landmarks and semi-automated surface landmark placement.",2020-03-09,Stefan Schlager,https://github.com/zarquon42b/Morpho,TRUE,https://github.com/zarquon42b/morpho,47767,33,2021-07-16T05:51:23Z,1447.4848484848485
morse,"Tools for ecotoxicologists and regulators dedicated to the
    mathematical and statistical modelling of toxicity test data. They use advanced and
    innovative methods for a valuable quantitative environmental risk assessment. See also
    Delignette-Muller et al. (2017) <doi:10.1021/acs.est.6b05326>. and Baudrot et al. (2018) <doi:10.1021/acs.est.7b05464>.",2021-02-04,Virgile Baudrot,https://cran.r-project.org/package=morse,TRUE,https://github.com/pveber/morse,24383,6,2021-04-20T12:59:06Z,4063.8333333333335
MortalityGaps,"Life expectancy is highly correlated over time among countries and 
  between males and females. These associations can be used to improve forecasts. 
  Here we have implemented a method for forecasting female life expectancy based on 
  analysis of the gap between female life expectancy in a country compared with
  the record level of female life expectancy in the world. Second, to forecast 
  male life expectancy, the gap between male life expectancy and female life 
  expectancy in a country is analysed. We named this method the Double-Gap model.
  For a detailed description of the method see Pascariu et al. (2017). 
  <doi:10.1016/j.insmatheco.2017.09.011>.",2018-07-20,Marius D. Pascariu,https://github.com/mpascariu/MortalityGaps,TRUE,https://github.com/mpascariu/mortalitygaps,12756,0,2021-05-02T17:45:51Z,NA
MortalityLaws,"Fit the most popular human mortality 'laws', and construct 
  full and abridge life tables given various input indices. A mortality
  law is a parametric function that describes the dying-out process of 
  individuals in a population during a significant portion of their 
  life spans. For a comprehensive review of the most important mortality 
  laws see Tabeau (2001) <doi:10.1007/0-306-47562-6_1>. 
  Practical functions for downloading data from various human mortality 
  databases are provided as well.  ",2020-09-16,Marius D. Pascariu,https://github.com/mpascariu/MortalityLaws,TRUE,https://github.com/mpascariu/mortalitylaws,37742,12,2021-05-02T16:40:40Z,3145.1666666666665
mortyr,"Returns information about characters, locations, and episodes from
    'The Rick and Morty' API: <https://rickandmortyapi.com>.",2021-01-10,Michael Page,https://github.com/mikejohnpage/mortyr,TRUE,https://github.com/mikejohnpage/mortyr,8181,21,2021-01-10T19:59:11Z,389.57142857142856
mosaic,"Data sets and utilities from Project MOSAIC (<http://www.mosaic-web.org>) used
    to teach mathematics, statistics, computation and modeling.  Funded by the
    NSF, Project MOSAIC is a community of educators working to tie together
    aspects of quantitative work that students in science, technology,
    engineering and mathematics will need in their professional lives, but
    which are usually taught in isolation, if at all.",2021-01-18,Randall Pruim,"https://github.com/ProjectMOSAIC/mosaic,
https://www.mosaic-web.org/mosaic/",TRUE,https://github.com/projectmosaic/mosaic,703987,88,2021-06-17T14:27:29Z,7999.852272727273
mosaic.find,"Provides a function (mosaic_find()) designed to find rhythmic 
    and non-rhythmic trends in multi-omics time course data using model 
    selection and joint modeling, a method called MOSAIC (Multi-Omics 
    Selection with Amplitude Independent Criteria). For more information,
    see H. De los Santos et al. (2020) <doi:10.1093/bioinformatics/btaa877>.",2020-11-20,Jennifer Hurley,https://github.com/delosh653/MOSAIC,TRUE,https://github.com/delosh653/mosaic,6102,1,2021-05-04T00:43:04Z,6102
mosaicCore,"Common utilities used in other MOSAIC-family packages are 
    collected here.",2021-01-16,Randall Pruim,https://github.com/ProjectMOSAIC/mosaicCore,TRUE,https://github.com/projectmosaic/mosaiccore,486907,1,2021-01-16T07:16:20Z,486907
mosaicData,"Data sets from Project MOSAIC (<http://www.mosaic-web.org>) used
    to teach mathematics, statistics, computation and modeling.  Funded by the
    NSF, Project MOSAIC is a community of educators working to tie together
    aspects of quantitative work that students in science, technology,
    engineering and mathematics will need in their professional lives, but
    which are usually taught in isolation, if at all.",2021-01-16,Randall Pruim,https://github.com/ProjectMOSAIC/mosaicData,TRUE,https://github.com/projectmosaic/mosaicdata,457815,1,2021-07-01T19:57:17Z,457815
MOSS,"High dimensionality, noise and heterogeneity among
    samples and features challenge the omic integration task. Here we
    present an omic integration method based on sparse singular value
    decomposition (SVD) to deal with these limitations, by: a. obtaining
    the main axes of variation of the combined omics, b. imposing sparsity
    constraints at both subjects (rows) and features (columns) levels
    using Elastic Net type of shrinkage, and c. allowing both linear and
    non-linear projections (via t-Stochastic Neighbor Embedding) of the
    omic data to detect clusters in very convoluted data
    (Gonzalez-Reymundez & Vazquez, 2020) <doi:10.1038/s41598-020-65119-5>.",2021-07-16,Agustin Gonzalez-Reymundez,https://github.com/agugonrey/MOSS,TRUE,https://github.com/agugonrey/moss,3615,1,2021-07-19T13:34:51Z,3615
motif,"Describes spatial patterns of categorical raster data for 
    any defined regular and irregular areas. 
    Patterns are described quantitatively using built-in signatures 
    based on co-occurrence matrices but also allows for 
    any user-defined functions. 
    It enables spatial analysis such as search, change detection,
    and clustering to be performed on spatial patterns (Nowosad (2021) <doi:10.1007/s10980-020-01135-0>).",2021-08-23,Jakub Nowosad,https://nowosad.github.io/motif/,TRUE,https://github.com/nowosad/motif,3378,32,2021-08-23T11:33:37Z,105.5625
motifcluster,"
    Tools for spectral clustering of weighted directed networks using motif
    adjacency matrices. Methods perform well on large and sparse networks, and
    random sampling methods for generating weighted directed networks are also
    provided. Based on methodology detailed in Underwood, Elliott and Cucuringu
    (2020) <arXiv:2004.01293>.",2020-08-11,William George Underwood,https://github.com/wgunderwood/motifcluster,TRUE,https://github.com/wgunderwood/motifcluster,5774,9,2020-09-12T23:22:50Z,641.5555555555555
motifr,"Tools for motif analysis in multi-level networks.
    Multi-level networks combine multiple networks in one, e.g.
    social-ecological networks. Motifs are small configurations of nodes
    and edges (subgraphs) occurring in networks. 'motifr' can visualize
    multi-level networks, count multi-level network motifs and compare
    motif occurrences to baseline models. It also identifies contributions
    of existing or potential edges to motifs to find critical or missing
    edges. The package is in many parts an R wrapper for the excellent
    'SESMotifAnalyser' 'Python' package written by Tim Seppelt.",2020-12-10,Mario Angst,https://marioangst.github.io/motifr/,TRUE,https://github.com/marioangst/motifr,3265,5,2021-05-24T14:32:42Z,653
mountainplot,"Lattice functions for drawing folded empirical cumulative
    distribution plots, or mountain plots. A mountain plot is similar
    to an empirical CDF plot, except that the curve increases from
    0 to 0.5, then decreases from 0.5 to 1 using an inverted scale at
    the right side. See Monti (1995) <doi:10.1080/00031305.1995.10476179>.",2021-04-16,Kevin Wright,https://kwstat.github.io/mountainplot/,TRUE,https://github.com/kwstat/mountainplot,16228,1,2021-04-16T16:09:52Z,16228
mousetrap,"Mouse-tracking, the analysis of mouse movements in computerized
    experiments, is a method that is becoming increasingly popular in the
    cognitive sciences. The mousetrap package offers functions for importing,
    preprocessing, analyzing, aggregating, and visualizing mouse-tracking data.
    An introduction into mouse-tracking analyses using mousetrap can be found
    in Kieslich, Henninger, Wulff, Haslbeck, & Schulte-Mecklenbeck (2019)
    <doi:10.4324/9781315160559-9> (preprint: <https://psyarxiv.com/zuvqa/>).",2021-06-05,Pascal J. Kieslich,https://github.com/pascalkieslich/mousetrap,TRUE,https://github.com/pascalkieslich/mousetrap,26905,28,2021-06-05T10:40:47Z,960.8928571428571
mpath,"Algorithms compute concave convex (CC) estimators including robust (penalized) generalized linear models and robust support vector machines via the COCO - composite optimization by conjugation operator. The package also contains penalized Poisson, negative binomial, zero-inflated Poisson, zero-inflated negative binomial regression models and robust models with non-convex loss functions. Wang et al. (2014) <doi:10.1002/sim.6314>,
      Wang et al. (2015) <doi:10.1002/bimj.201400143>,
      Wang et al. (2016) <doi:10.1177/0962280214530608>,
      Wang (2019) <arXiv:1912.11119>,
      Wang (2020) <arXiv:2010.02848>.",2021-04-05,Zhu Wang,https://github.com/zhuwang46/mpath,TRUE,https://github.com/zhuwang46/mpath,45713,0,2021-04-05T15:57:27Z,NA
mpcmp,"A collection of functions for estimation, testing and diagnostic checking for the mean-parametrized Conway-Maxwell-Poisson (COM-Poisson) regression model of Huang (2017) <doi:10.1177/1471082X17697749>.",2020-10-26,Thomas Fung,https://github.com/thomas-fung/mpcmp,TRUE,https://github.com/thomas-fung/mpcmp,12219,3,2021-07-01T08:59:20Z,4073
MPGE,"Interaction between a genetic variant (e.g., a single nucleotide polymorphism) and an environmental variable (e.g., physical activity) can have a shared effect on multiple phenotypes (e.g., blood lipids). We implement a two-step method to test for an overall interaction effect on multiple phenotypes. In first step, the method tests for an overall marginal genetic association between the genetic variant and the multivariate phenotype. The genetic variants which show an evidence of marginal overall genetic effect in the first step are prioritized while testing for an overall gene-environment interaction effect in the second step. Methodology is available from: A Majumdar, KS Burch, S Sankararaman, B Pasaniuc, WJ Gauderman, JS Witte (2020) <doi:10.1101/2020.07.06.190256>.",2020-10-23,Arunabha Majumdar,https://github.com/ArunabhaCodes/MPGE,TRUE,https://github.com/arunabhacodes/mpge,3713,0,2020-10-14T20:42:18Z,NA
mplot,"Model stability and variable inclusion plots [Mueller and Welsh
    (2010, <doi:10.1111/j.1751-5823.2010.00108.x>); Murray, Heritier and Mueller
    (2013, <doi:10.1002/sim.5855>)] as well as the adaptive fence [Jiang et al.
    (2008, <doi:10.1214/07-AOS517>); Jiang et al. 
    (2009, <doi:10.1016/j.spl.2008.10.014>)] for linear and generalised linear models.",2021-07-10,Garth Tarr,"https://garthtarr.github.io/mplot/,
https://github.com/garthtarr/mplot",TRUE,https://github.com/garthtarr/mplot,266071,9,2021-07-10T10:36:39Z,29563.444444444445
MplusAutomation,"Leverages the R language to automate latent variable model estimation
	and interpretation using 'Mplus', a powerful latent variable modeling program
	developed by Muthen and Muthen (<https://www.statmodel.com>). Specifically, this package
    provides routines for creating related groups of models, running batches of
    models, and extracting and tabulating model parameters and fit statistics.",2021-07-01,Michael Hallquist,https://github.com/michaelhallquist/MplusAutomation,TRUE,https://github.com/michaelhallquist/mplusautomation,94568,61,2021-09-03T06:28:29Z,1550.295081967213
mpoly,Symbolic computing with multivariate polynomials in R.,2020-02-20,David Kahle,https://github.com/dkahle/mpoly,TRUE,https://github.com/dkahle/mpoly,36590,7,2020-12-19T22:40:02Z,5227.142857142857
mppR,"Analysis of experimental multi-parent populations to detect
             regions of the genome (called quantitative trait loci, QTLs)
             influencing phenotypic traits. The population must be composed of crosses
             between a set of at least three parents (e.g. factorial design,
             'diallel', or nested association mapping). The functions cover data
             processing, QTL detection, and results visualization. The implemented
             methodology is described by Garin, Wimmer, Mezmouk, Malosetti and
             van Eeuwijk (2017) <doi:10.1007/s00122-017-2923-3>.",2021-05-15,Vincent Garin,https://github.com/vincentgarin/mppR,TRUE,https://github.com/vincentgarin/mppr,14013,2,2021-05-15T11:27:20Z,7006.5
MQMF,"Complements the book ""Using R for Modelling and  
    Quantitative Methods in Fisheries"" ISBN: 9780367469894, being 
    published in September 2020 by Chapman & Hall in their ""Using R series"". 
    There are numerous functions and data-sets that are used in the 
    book's many practical examples. ",2020-08-31,Malcolm Haddon,https://github.com/haddonm/MQMF,TRUE,https://github.com/haddonm/mqmf,8665,4,2021-05-12T23:54:08Z,2166.25
mrbayes,"Bayesian estimation of inverse variance weighted (IVW), Burgess 
    et al. (2013) <doi:10.1002/gepi.21758>, and MR-Egger, Bowden et 
    al. (2015) <doi:10.1093/ije/dyv080>, summary data models for Mendelian 
    randomization analyses.",2021-07-13,Okezie Uche-Ikonne,https://github.com/okezie94/mrbayes,TRUE,https://github.com/okezie94/mrbayes,11076,2,2021-08-19T12:58:56Z,5538
mrbin,"Nuclear Magnetic Resonance is widely used in Life Science
    research. The package (<doi:10.1021/acs.jproteome.0c00684>) converts 1D 
    or 2D data into a matrix of values suitable for further data analysis and
    performs basic processing steps in a reproducible way. Negative values, a
    common issue in such data, are replaced by positive values. All used
    parameters are stored in a readable text file and can be restored from that
    file to enable exact reproduction of the data at a later time.",2021-08-23,Matthias Klein,"http://www.kleinomicslab.com/software/,
https://github.com/kleinomicslab/mrbin",TRUE,https://github.com/kleinomicslab/mrbin,13583,0,2021-04-29T17:20:25Z,NA
mrds,"Animal abundance estimation via conventional, multiple covariate
    and mark-recapture distance sampling (CDS/MCDS/MRDS). Detection function
    fitting is performed via maximum likelihood. Also included are diagnostics
    and plotting for fitted detection functions. Abundance estimation is via a
    Horvitz-Thompson-like estimator.",2021-07-01,Jeff Laake,https://github.com/DistanceDevelopment/mrds/,TRUE,https://github.com/distancedevelopment/mrds,45837,2,2021-08-16T18:12:57Z,22918.5
mrf2d,"Model fitting, sampling and visualization
    for the (Hidden) Markov Random Field model with pairwise interactions and 
    general interaction structure from 
    Freguglia, Garcia & Bicas (2020) <doi:10.1002/env.2613>,
    which has many popular models used in 2-dimensional lattices
    as particular cases, like the Ising Model and Potts Model.",2020-10-29,Victor Freguglia,"https://github.com/Freguglia/mrf2d,
https://arxiv.org/abs/2006.00383",TRUE,https://github.com/freguglia/mrf2d,10208,6,2021-08-09T19:15:09Z,1701.3333333333333
MRFcov,"Approximate node interaction parameters of Markov Random Fields 
    graphical networks. Models can incorporate additional covariates, allowing users to estimate
    how interactions between nodes in the graph are predicted to change across
    covariate gradients. The general methods implemented in this package are described 
    in Clark et al. (2018) <doi:10.1002/ecy.2221>.",2021-03-18,Nicholas J Clark,https://github.com/nicholasjclark/MRFcov,TRUE,https://github.com/nicholasjclark/mrfcov,14510,17,2021-03-22T21:13:44Z,853.5294117647059
mrgsim.sa,"Perform sensitivity analysis on ordinary differential equation 
    based models, including ad-hoc graphical analyses based on structured 
    sequences of parameters as well as local sensitivity analysis. Functions 
    are provided for creating inputs, simulating scenarios and plotting outputs.",2020-11-30,Kyle Baron,https://github.com/kylebaron/mrgsim.sa,TRUE,https://github.com/kylebaron/mrgsim.sa,3118,3,2020-11-27T22:13:37Z,1039.3333333333333
mrgsolve,"Fast simulation from ordinary differential equation
    (ODE) based models typically employed in quantitative pharmacology and
    systems biology.",2021-05-10,Kyle T Baron,https://github.com/metrumresearchgroup/mrgsolve,TRUE,https://github.com/metrumresearchgroup/mrgsolve,32800,75,2021-08-30T15:56:45Z,437.3333333333333
mRpostman,"An easy-to-use IMAP client that provides tools for message searching,
    selective fetching of message attributes, mailbox management, attachment extraction, 
    and several other IMAP features, paving the way for e-mail data analysis.",2020-11-30,Allan Quadros,https://allanvc.github.io/,TRUE,https://github.com/allanvc/mrpostman,13029,17,2021-05-13T22:30:06Z,766.4117647058823
MRReg,"We provide the framework to analyze multiresolution partitions (e.g. country, provinces, subdistrict) where each individual data point belongs to only one partition in each layer (e.g. i belongs to subdistrict A, province P, and country Q).   We assume that a partition in a higher layer subsumes lower-layer partitions (e.g. a nation is at the 1st layer subsumes all provinces at the 2nd layer). Given N individuals that have a pair of real values (x,y) that generated from  independent variable X and dependent variable Y. Each individual i belongs to one partition per layer. Our goal is to find which partitions at which highest level that all individuals  in the these partitions share the same linear model Y=f(X) where f is a linear function. The framework deploys the Minimum Description Length principle (MDL) to infer solutions. The publication of this package is at Chainarong Amornbunchornvej, Navaporn Surasvadi, Anon Plangprasopchok, and Suttipong Thajchayapong (2021) <doi:10.1145/3424670>.",2021-03-28,Chainarong Amornbunchornvej,https://github.com/DarkEyes/MRReg,TRUE,https://github.com/darkeyes/mrreg,8657,2,2021-03-28T10:06:59Z,4328.5
MrSGUIDE,"An R implementation of 'GUIDE' style algorithm focusing on subgroup identification problem 
            under multiple responses of Loh et al. (2019) <doi:10.1002/widm.1326>. This package is intended for use 
            for randomized trials and observational studies.",2020-10-15,Peigen Zhou,"http://www.stat.wisc.edu/~loh/guide.html,
https://baconzhou.github.io/MrSGUIDE/,
http://pages.stat.wisc.edu/~loh/treeprogs/guide/LZ20.pdf,
http://pages.stat.wisc.edu/~loh/treeprogs/guide/sm19.pdf",TRUE,https://github.com/baconzhou/mrsguide,5586,3,2021-03-27T02:58:06Z,1862
msaeDB,"Implements Benchmarking Method for Multivariate Small Area Estimation under Fay Herriot Model. Multivariate Small Area Estimation (MSAE) is a development of Univariate Small Area Estimation that considering the correlation among response variables and borrowing the strength from related areas and auxiliary variables to increase the effectiveness of sample size, the multivariate model in this package is based on multivariate model 1 proposed by Roberto Benavent and Domingo Morales (2016) <doi:10.1016/j.csda.2015.07.013>. Benchmarking in Small Area Estimation is a modification of Small Area Estimation model to guarantee that the aggregate weighted mean of the county predictors equals the corresponding weighted mean of survey estimates. Difference Benchmarking is the simplest benchmarking method but widely used by multiplying empirical best linear unbiased prediction (EBLUP) estimator by the common adjustment factors (J.N.K Rao and Isabel Molina, 2015).",2021-04-08,Zaza Yuda Perwira,https://github.com/zazaperwira/msaeDB,TRUE,https://github.com/zazaperwira/msaedb,3489,0,2021-04-01T03:08:05Z,NA
msaenet,"Multi-step adaptive elastic-net (MSAENet) algorithm for
    feature selection in high-dimensional regressions proposed in
    Xiao and Xu (2015) <DOI:10.1080/00949655.2015.1016944>,
    with support for multi-step adaptive MCP-net (MSAMNet) and
    multi-step adaptive SCAD-net (MSASNet) methods.",2019-05-17,Nan Xiao,"https://nanx.me/msaenet/, https://github.com/nanxstats/msaenet",TRUE,https://github.com/nanxstats/msaenet,29402,12,2021-07-17T05:47:31Z,2450.1666666666665
msaeRB,"Implements multivariate ratio benchmarking small area estimation. This package provides ratio benchmarking estimation for univariate and multivariate small area estimation and its MSE. In fact, MSE estimators for ratio benchmark are not readily available, so resampling method that called parametric bootstrap is applied. The ratio benchmark model and parametric bootstrap in this package are based on the model proposed in small area estimation. J.N.K Rao and Isabel Molina (2015, ISBN: 978-1-118-73578-7). ",2021-06-12,Zenda Oka Briantiko,https://github.com/zendaokab/msaeRB,TRUE,https://github.com/zendaokab/msaerb,2368,0,2021-06-08T20:15:43Z,NA
MSbox,"Common mass spectrometry tools described in John Roboz (2013) <doi:10.1201/b15436>. It allows checking element
 isotopes, calculating (isotope labelled) exact monoisitopic mass, m/z values and mass accuracy, and inspecting possible contaminant mass peaks,
 examining possible adducts in electrospray ionization (ESI) and matrix-assisted laser desorption ionization (MALDI)
 ion sources. ",2021-05-25,Yonghui Dong,https://github.com/YonghuiDong/MSbox,TRUE,https://github.com/yonghuidong/msbox,18329,0,2021-05-26T10:50:13Z,NA
mschart,"Create native charts for 'Microsoft PowerPoint' and 'Microsoft Word' documents. 
 These can then be edited and annotated. Functions are provided to let users create charts, modify 
 and format their content. The chart's underlying data is automatically saved within the 
 'Word' document or 'PowerPoint' presentation. It extends package 'officer' that does 
 not contain any feature for 'Microsoft' native charts production. ",2021-09-02,David Gohel,"https://ardata-fr.github.io/officeverse/,
https://ardata-fr.github.io/mschart/",TRUE,https://github.com/ardata-fr/mschart,26908,109,2021-09-02T12:05:18Z,246.86238532110093
mscstts,"R Client for the Microsoft Cognitive Services 
  'Text-to-Speech' REST API, including voice synthesis. A valid account 
  must be registered at the Microsoft Cognitive Services website 
  <https://azure.microsoft.com/services/cognitive-services/> in order to 
  obtain a (free) API key. Without an API key, this package will not 
  work properly.",2020-10-28,John Muschelli,https://github.com/muschellij2/mscstts,TRUE,https://github.com/muschellij2/mscstts,24368,8,2021-04-05T16:01:07Z,3046
MSEtool,"Development, simulation testing, and implementation of management procedures for fisheries 
    (see Carruthers & Hordyk (2018) <doi:10.1111/2041-210X.13081>).",2021-08-13,Adrian Hordyk,NA,TRUE,https://github.com/blue-matter/msetool,22746,2,2021-09-03T02:11:45Z,11373
MSG,A companion to the Chinese book ``Modern Statistical Graphics''.,2021-07-21,Yihui Xie,https://github.com/yihui/MSG,TRUE,https://github.com/yihui/msg,22161,33,2021-08-15T17:14:36Z,671.5454545454545
MSGARCH,"Fit (by Maximum Likelihood or MCMC/Bayesian), simulate, and forecast various Markov-Switching GARCH models as described in Ardia et al. (2019) <doi:10.18637/jss.v091.i04>.",2020-04-20,David Ardia,https://github.com/keblu/MSGARCH,TRUE,https://github.com/keblu/msgarch,29849,53,2021-08-17T19:31:11Z,563.188679245283
msgr,"Provides new functions info(), warn() and error(), similar to message(),
    warning() and stop() respectively. However, the new functions can have a 'level'
    associated with them, so that when executed the global level option determines whether
    they are shown or not. This allows debug modes, outputting more information. The can also
    output all messages to a log file.",2019-12-16,Chad Goymer,https://github.com/ChadGoymer/msgr,TRUE,https://github.com/chadgoymer/msgr,8043,1,2021-04-06T09:05:01Z,8043
msigdbr,"Provides the 'Molecular Signatures Database' (MSigDB) gene sets
    typically used with the 'Gene Set Enrichment Analysis' (GSEA) software
    (Subramanian et al. 2005 <doi:10.1073/pnas.0506580102>, Liberzon et al. 2015
    <doi:10.1016/j.cels.2015.12.004>) in a standard R data frame with key-value
    pairs. The package includes the human genes as listed in MSigDB as well as
    the corresponding symbols and IDs for frequently studied model organisms
    such as mouse, rat, pig, fly, and yeast.",2021-05-05,Igor Dolgalev,https://igordot.github.io/msigdbr/,TRUE,https://github.com/igordot/msigdbr,70307,43,2021-05-05T17:04:21Z,1635.046511627907
msm,"Functions for fitting continuous-time Markov and hidden
    Markov multi-state models to longitudinal data.  Designed for
    processes observed at arbitrary times in continuous time (panel data)
    but some other observation schemes are supported. Both Markov
    transition rates and the hidden Markov output process can be modelled
    in terms of covariates, which may be constant or piecewise-constant
    in time.",2019-12-16,Christopher Jackson,https://github.com/chjackson/msm,TRUE,https://github.com/chjackson/msm,437149,27,2021-08-11T15:27:48Z,16190.703703703704
msmtools,"A fast and general method for restructuring classical longitudinal data into
    augmented ones. The reason for this is to facilitate the modeling of longitudinal data under
    a multi-state framework using the 'msm' package.",2021-04-12,Francesco Grossetti,https://github.com/contefranz/msmtools,TRUE,https://github.com/contefranz/msmtools,20033,3,2021-04-10T16:38:40Z,6677.666666666667
msos,"Multivariate Analysis methods and data sets used
    in John Marden's book Multivariate Statistics: Old School (2015) <ISBN:978-1456538835>.
    This also serves as a companion package for the 
    STAT 571: Multivariate Analysis course offered by the Department of Statistics
    at the University of Illinois at Urbana-Champaign ('UIUC'). ",2020-10-31,James Balamuta,"https://github.com/coatless/msos, https://coatless.github.io/msos/",TRUE,https://github.com/coatless/msos,19071,3,2020-10-30T01:10:51Z,6357
msSPChelpR,"A collection of helper functions for analyzing Second Primary Cancer data, 
    including functions to reshape data, to calculate patient states and analyze cancer incidence.",2021-07-01,Marian Eberl,https://marianschmidt.github.io/msSPChelpR/,TRUE,https://github.com/marianschmidt/msspchelpr,3645,1,2021-07-01T17:57:40Z,3645
mstate,"Contains functions for data preparation, descriptives, hazard estimation and prediction with Aalen-Johansen or simulation in competing risks and multi-state models, see Putter, Fiocco, Geskus (2007) <doi:10.1002/sim.2712>.",2020-12-17,Hein Putter,https://www.lumc.nl/org/bds/research/medische-statistiek/survival-analysis/,TRUE,https://github.com/hputter/mstate,172467,0,2021-07-09T12:44:20Z,NA
mStats,"This is a tool for epidemiologist, medical data analyst, 
    medical or public health professionals. It contains three domains of functions:
    1) data management, 2) statistical analysis and 3) calculating 
    epidemiological measures.",2020-11-23,Myo Minn Oo,https://myominnoo.github.io/,TRUE,https://github.com/myominnoo/mstats,10392,1,2021-01-26T08:15:11Z,10392
MTA,Build multiscalar territorial analysis based on various contexts.,2021-01-20,Timothée Giraud,https://github.com/riatelab/MTA/,TRUE,https://github.com/riatelab/mta,16812,5,2021-01-20T13:45:09Z,3362.4
MTE,"Several robust estimators for linear regression and variable selection are provided. 
              Included are Maximum tangent likelihood estimator (Qin, et al., 2017), 
              least absolute deviance estimator and Huber regression. The penalized version of each of these 
              estimator incorporates L1 penalty function, i.e., LASSO and Adaptive Lasso. They are able to 
              produce consistent estimates for both fixed and high-dimensional settings. ",2021-05-11,Shaobo Li,GitHub: https://github.com/shaobo-li/MTE,TRUE,https://github.com/shaobo-li/mte,11638,0,2021-05-05T21:00:39Z,NA
MTLR,"An implementation of Multi-Task Logistic Regression (MTLR) for R. 
  This package is based on the method proposed by Yu et al. (2011) which utilized MTLR for generating individual survival curves
  by learning feature weights which vary across time. This model was further extended to account for left and interval censored data.",2019-06-03,Humza Haider,https://github.com/haiderstats/MTLR,TRUE,https://github.com/haiderstats/mtlr,13086,9,2020-12-16T05:04:15Z,1454
MtreeRing,"Use morphological image processing and edge detection algorithms to automatically measure tree ring widths on digital images. Users can also manually mark tree rings on species with complex anatomical structures. The arcs of inner-rings and angles of successive inclined ring boundaries are used to correct ring-width series. The package provides a Shiny-based application, allowing R beginners to easily analyze tree ring images and export ring-width series in standard file formats.",2021-04-19,Jingning Shi,"https://docs.ropensci.org/MtreeRing,
https://github.com/ropensci/MtreeRing",TRUE,https://github.com/ropensci/mtreering,16575,14,2021-04-30T06:53:08Z,1183.9285714285713
MTSYS,"Mahalanobis-Taguchi (MT) system is a collection of multivariate
    analysis methods developed for the field of quality engineering. MT system
    consists of two families depending on their purpose. One is a family of
    Mahalanobis-Taguchi (MT) methods (in the broad sense) for diagnosis (see
    Woodall, W. H., Koudelik, R., Tsui, K. L., Kim, S. B., Stoumbos, Z. G., and
    Carvounis, C. P. (2003) <doi:10.1198/004017002188618626>) and the other is a
    family of Taguchi (T) methods for forecasting (see Kawada, H., and Nagata, Y.
    (2015) <doi:10.17929/tqs.1.12>). The MT package contains three basic methods
    for the family of MT methods and one basic method for the family of T
    methods. The MT method (in the narrow sense), the Mahalanobis-Taguchi
    Adjoint (MTA) methods, and the Recognition-Taguchi (RT) method are for the
    MT method and the two-sided Taguchi (T1) method is for the family of T
    methods. In addition, the Ta and Tb methods, which are the improved versions
    of the T1 method, are included.",2017-09-10,Akifumi Okayama,https://github.com/okayaa/MTSYS,TRUE,https://github.com/okayaa/mtsys,14880,3,2021-03-11T13:38:14Z,4960
muHVT,Constructing hierarchical voronoi tessellations for a given data set and overlay heatmap for variables at various levels of the tessellations for in-depth data analysis. See <https://en.wikipedia.org/wiki/Voronoi_diagram> for more information. Credits to Mu Sigma for their continuous support throughout the development of the package.  ,2020-08-03,Mu Sigma,https://github.com/Mu-Sigma/muHVT,TRUE,https://github.com/mu-sigma/muhvt,14100,11,2020-11-23T10:41:12Z,1281.8181818181818
mully,"Allows the user to create graph with multiple layers. The user can also modify the layers, the nodes, and the edges. The graph can also be visualized.
    Zaynab Hammoud and Frank Kramer (2018) <doi:10.3390/genes9110519>.
    More about multilayered graphs and their usage can be found in our review paper:
    Zaynab Hammoud and Frank Kramer (2020) <doi:10.1186/s41044-020-00046-0>.",2021-06-02,Zaynab Hammoud,https://github.com/frankkramer-lab/mully,TRUE,https://github.com/frankkramer-lab/mully,4494,30,2021-06-02T11:24:05Z,149.8
multdyn,Multiregression Dynamic Models for directed dynamic functional brain network analysis.,2017-08-24,Simon Schwab,https://github.com/schw4b/multdyn,TRUE,https://github.com/schw4b/multdyn,14098,19,2021-05-18T19:21:33Z,742
multgee,"GEE solver for correlated nominal or ordinal multinomial responses
    using a local odds ratios parameterization.",2021-05-13,Anestis Touloumis,https://github.com/AnestisTouloumis/multgee,TRUE,https://github.com/anestistouloumis/multgee,42313,6,2021-05-13T12:02:55Z,7052.166666666667
multibridge,"Evaluate hypotheses concerning the distribution of multinomial
  proportions using bridge sampling. The bridge sampling routine is able to
  compute Bayes factors for hypotheses that entail inequality constraints,
  equality constraints, free parameters, and mixtures of all three. These
  hypotheses are tested against the encompassing hypothesis, that all parameters
  vary freely or against the null hypothesis that all category proportions are equal.
  For more information see Sarafoglou et al. (2020) <doi:10.31234/osf.io/bux7p>.",2021-02-23,Alexandra Sarafoglou,https://github.com/asarafoglou/multibridge/,TRUE,https://github.com/asarafoglou/multibridge,2692,0,2021-08-05T13:58:01Z,NA
multiclassPairs,"A toolbox to train a single sample classifier that uses in-sample feature relationships. The relationships are represented as feature1 < feature2 (e.g. gene1 < gene2). We provide two options to go with. First is based on 'switchBox' package which uses Top-score pairs algorithm. Second is a novel implementation based on random forest algorithm. For simple problems we recommend to use one-vs-rest using TSP option due to its simplicity and for being easy to interpret.  For complex problems RF performs better.  Both lines filter the features first then combine the filtered features to make the list of all the possible rules (i.e. rule1: feature1 < feature2, rule2: feature1 < feature3, etc...).  Then the list of rules will be filtered and the most important and informative rules will be kept. The informative rules will be assembled in an one-vs-rest model or in an RF model.  We provide a detailed description with each function in this package to explain the filtration and training methodology in each line. Reference: Marzouka & Eriksson (2021) <doi:10.1093/bioinformatics/btab088>.",2021-05-16,Nour-al-dain Marzouka,https://github.com/NourMarzouka/multiclassPairs,TRUE,https://github.com/nourmarzouka/multiclasspairs,4086,5,2021-05-16T19:58:49Z,817.2
multidplyr,"Partition a data frame across multiple worker
    processes to provide simple multicore parallelism.",2021-02-08,Hadley Wickham,https://github.com/tidyverse/multidplyr,TRUE,https://github.com/tidyverse/multidplyr,6670,537,2021-07-01T20:03:22Z,12.420856610800746
multifear,"A suite of functions for performing analyses, based on a multiverse approach, for conditioning data. Specifically, given the appropriate data, the functions are able to perform t-tests, analyses of variance, and mixed models for the provided data and return summary statistics and plots. The function is also able to return for all those tests p-values, confidence intervals, and Bayes factors. The methods are described in Lonsdorf, Gerlicher, Klingelhofer-Jens, & Krypotos <doi:10.31234/osf.io/2z6pd>.",2021-06-01,Angelos-Miltiadis Krypotos,https://github.com/AngelosPsy/multifear,TRUE,https://github.com/angelospsy/multifear,1912,0,2021-06-14T15:03:43Z,NA
multigraph,"Functions to plot and manipulate multigraphs, signed and valued graphs, bipartite graphs, multilevel graphs, and Cayley graphs with various layout options. Please note that this package still under a devel version. ",2021-08-09,Antonio Rivero Ostoic,https://github.com/mplex/multigraph/,TRUE,https://github.com/mplex/multigraph,18706,18,2021-08-09T10:06:04Z,1039.2222222222222
multilinguer,"Provides install functions of other languages 
             such as 'java', 'python' for windows and macos.",2020-01-31,Chanyub Park,https://github.com/mrchypark/multilinguer,TRUE,https://github.com/mrchypark/multilinguer,71846,7,2021-05-04T17:23:32Z,10263.714285714286
multimorbidity,"Identifying comorbidities, frailty, and multimorbidity in claims 
    and administrative data is often a duplicative process.
    The functions contained in this package are meant to first prepare the data to a format
    acceptable by all other packages, then provide a uniform and simple approach to
    generate comorbidity and multimorbidity metrics based on these claims data. The package
    is ever evolving to include new metrics, and is always looking for new measures to include.
    The citations used in this package include the following publications: 
    Anne Elixhauser, Claudia Steiner, D. Robert Harris, Rosanna M. Coffey (1998) <doi:10.1097/00005650-199801000-00004>,
    Brian J Moore, Susan White, Raynard Washington, et al. (2017) <doi:10.1097/MLR.0000000000000735>,
    Mary E. Charlson, Peter Pompei, Kathy L. Ales, C. Ronald MacKenzie (1987) <doi:10.1016/0021-9681(87)90171-8>,
    Richard A. Deyo, Daniel C. Cherkin, Marcia A. Ciol (1992) <doi:10.1016/0895-4356(92)90133-8>,
    Hude Quan, Vijaya Sundararajan, Patricia Halfon, et al. (2005) <doi:10.1097/01.mlr.0000182534.19832.83>,
    Dae Hyun Kim, Sebastian Schneeweiss, Robert J Glynn, et al. (2018) <doi:10.1093/gerona/glx229>,
    Melissa Y Wei, David Ratz, Kenneth J Mukamal (2020) <doi:10.1111/jgs.16310>,
    Kathryn Nicholson, Amanda L. Terry, Martin Fortin, et al. (2015) <doi:10.15256/joc.2015.5.61>,
    Martin Fortin, José Almirall, and Kathryn Nicholson (2017)<doi:10.15256/joc.2017.7.122>.",2021-08-20,Wyatt Bensken,https://github.com/WYATTBENSKEN/multimorbidity,TRUE,https://github.com/wyattbensken/multimorbidity,183,1,2021-08-23T15:01:59Z,183
multinma,"Network meta-analysis and network meta-regression models for 
    aggregate data, individual patient data, and mixtures of both individual 
    and aggregate data using multilevel network meta-regression as described by
    Phillippo et al. (2020) <doi:10.1111/rssa.12579>. Models are estimated in a
    Bayesian framework using 'Stan'.",2021-03-18,David M. Phillippo,"https://dmphillippo.github.io/multinma/,
https://github.com/dmphillippo/multinma",TRUE,https://github.com/dmphillippo/multinma,6432,9,2021-03-18T14:02:13Z,714.6666666666666
multiplex,"Algebraic procedures for the analysis of multiple social networks are delivered with this 
	    package as described in Ostoic (2020) <DOI:10.18637/jss.v092.i11>. Among other things, it 
	    makes it possible to create and manipulate multiplex, multimode, and multilevel network data 
	    with different formats. There are effective ways available to treat multiple networks with 
	    routines that combine algebraic systems like the partially ordered semigroup or the semiring 
	    structure with the relational bundles occurring in different types of multivariate network 
	    data sets. It also provides an algebraic approach for affiliation networks through Galois 
	    derivations between families of the pairs of subsets in the two domains.",2020-02-28,Antonio Rivero Ostoic,http://github.com/mplex/multiplex/,TRUE,https://github.com/mplex/multiplex,35361,15,2021-01-06T09:25:42Z,2357.4
multitaper,"Implements multitaper spectral analysis using discrete prolate spheroidal sequences (Slepians) and sine tapers. It includes an adaptive weighted multitaper spectral estimate, a coherence estimate, Thomson's Harmonic F-test, and complex demodulation. The Slepians sequences are generated efficiently using a tridiagonal matrix solution, and jackknifed confidence intervals are available for most estimates. This package is an implementation of the method described in D.J. Thomson (1982) ""Spectrum estimation and harmonic analysis"" <doi:10.1109/PROC.1982.12433>.",2020-10-03,Karim Rahim,https://github.com/krahim/multitaper/,TRUE,https://github.com/krahim/multitaper,36867,6,2021-02-27T22:07:33Z,6144.5
multiverse,"Implement 'multiverse' style analyses (Steegen S., Tuerlinckx F, Gelman A., Vanpaemal, W., 2016)
    <doi:10.1177/1745691616658637>, (Dragicevic P., Jansen Y., Sarma A., Kay M., Chevalier F., 2019) <doi:10.1145/3290605.3300295> 
    to show the robustness of statistical inference. 'Multiverse analysis' is a philosophy of 
    statistical reporting where paper authors report the outcomes of many different statistical 
    analyses in order to show how fragile or robust their findings are. 
    The 'multiverse' package (Sarma A., Kale A., Moon M., Taback N., Chevalier F., Hullman J., Kay M., 2021) <doi:10.31219/osf.io/yfbwm>
    allows users to concisely and flexibly implement 'multiverse-style' 
    analysis, which involve declaring alternate ways of performing an analysis step, in R and R Notebooks.",2021-06-01,Abhraneel Sarma,"https://mucollective.github.io/multiverse/,
https://github.com/mucollective/multiverse/",TRUE,https://github.com/mucollective/multiverse,1121,11,2021-06-14T20:56:11Z,101.9090909090909
MultSurvTests,"Multivariate version of the two-sample Gehan and logrank tests, as described in L.J Wei & J.M Lachin (1984) and Persson et al. (2019).",2021-06-18,Lukas Arnroth,https://github.com/lukketotte/MultSurvTests,TRUE,https://github.com/lukketotte/multsurvtests,898,0,2021-06-14T13:36:17Z,NA
munfold,"Multidimensional unfolding using Schoenemann's algorithm for metric
   and Procrustes rotation of unfolding results.",2016-02-08,Martin Elff,"http://www.elff.eu/software/munfold/,http://github.com/melff/munfold/",TRUE,https://github.com/melff/munfold,19836,1,2021-05-27T21:45:22Z,19836
music,"An aid for learning and using music theory. You can build chords, scales, and chord progressions using 12-note equal temperament tuning (12-ET) or user-defined tuning. Includes functions to visualize notes on a piano using ASCII plots in the console and to plot waveforms using base graphics. It allows simple playback of notes and chords using the 'audio' package.",2019-04-20,Efstathios D. Gennatas,https://github.com/egenn/music,TRUE,https://github.com/egenn/music,12822,31,2021-08-09T02:05:02Z,413.61290322580646
mutualinf,"The Mutual Information Index (M) introduced to social science literature by
    Theil and Finizza (1971) <doi:10.1080/0022250X.1971.9989795> is a multigroup
    segregation measure that is highly decomposable and that according to Frankel
    and Volij (2011) <doi:10.1016/j.jet.2010.10.008> and Mora and Ruiz-Castillo
    (2011) <doi:10.1111/j.1467-9531.2011.01237.x> satisfies the Strong Unit
    Decomposability and Strong Group Decomposability properties. This package allows
    computing and decomposing the total index value into its ""between"" and
    ""within"" terms. These last terms can also be decomposed into their
    contributions, either by group or unit characteristics. The factors that produce
    each ""within"" term can also be displayed at the user's request. The results can
    be computed considering a variable or sets of variables that define separate
    clusters.",2021-08-23,Rafael Fuentealba-Chaura,https://github.com/RafaelFuentealbaC/mutualinf,TRUE,https://github.com/rafaelfuentealbac/mutualinf,870,1,2021-08-25T02:04:10Z,870
mvGPS,"Methods for estimating and utilizing the multivariate generalized propensity score (mvGPS) for multiple continuous exposures described in Williams, J.R, and Crespi, C.M. (2020) <arxiv:2008.13767>. The methods allow estimation of a dose-response surface relating the joint distribution of multiple continuous exposure variables to an outcome. Weights are constructed assuming a multivariate normal density for the marginal and conditional distribution of exposures given a set of confounders. Confounders can be different for different exposure variables. The weights are designed to achieve balance across all exposure dimensions and can be used to estimate dose-response surfaces.",2021-04-28,Justin Williams,https://github.com/williazo/mvGPS,TRUE,https://github.com/williazo/mvgps,4424,5,2021-09-02T15:46:52Z,884.8
mvMORPH,"Fits multivariate (Brownian Motion, Early Burst, ACDC, Ornstein-Uhlenbeck and Shifts) models of continuous traits evolution on trees and time series. 'mvMORPH' also proposes high-dimensional multivariate comparative tools (linear models using Generalized Least Squares and multivariate tests) based on penalized likelihood.  See
    Clavel et al. (2015) <DOI:10.1111/2041-210X.12420>, Clavel et al. (2019) <DOI:10.1093/sysbio/syy045>, and Clavel & Morlon (2020) <DOI:10.1093/sysbio/syaa010>.",2021-03-17,Julien Clavel,https://github.com/JClavel/mvMORPH,TRUE,https://github.com/jclavel/mvmorph,35559,13,2021-06-25T14:38:07Z,2735.3076923076924
mvnfast,"Provides computationally efficient tools related to the
    multivariate normal and Student's t distributions. The main functionalities
    are: simulating multivariate random vectors, evaluating multivariate normal or
    Student's t densities and Mahalanobis distances. These tools are very efficient
    thanks to the use of C++ code and of the OpenMP API.",2021-05-20,Matteo Fasiolo,https://github.com/mfasiolo/mvnfast/,TRUE,https://github.com/mfasiolo/mvnfast,168675,21,2021-05-25T09:51:38Z,8032.142857142857
mvp,"Fast manipulation of symbolic multivariate polynomials
  using the 'Map' class of the Standard Template Library.  The package
  uses print and coercion methods from the 'mpoly' package (Kahle 2013,
  ""Multivariate polynomials in R"".  The R Journal, 5(1):162), but offers
  speed improvements.  It is comparable in speed to the 'spray' package
  for sparse arrays, but retains the symbolic benefits of 'mpoly'.",2019-09-05,Robin K. S. Hankin,https://github.com/RobinHankin/mvp.git,TRUE,https://github.com/robinhankin/mvp,16117,4,2021-08-13T21:52:16Z,4029.25
MVR,"This is a non-parametric method for joint adaptive mean-variance regularization and variance stabilization of high-dimensional data. It is suited for handling difficult problems posed by high-dimensional multivariate datasets (p >> n paradigm). Among those are that the variance is often a function of the mean, variable-specific estimators of variances are not reliable, and tests statistics have low powers due to a lack of degrees of freedom. Key features include:
            (i) Normalization and/or variance stabilization of the data,
            (ii) Computation of mean-variance-regularized t-statistics (F-statistics to follow),
            (iii) Generation of diverse diagnostic plots,
            (iv) Computationally efficient implementation using C/C++ interfacing and an option for parallel computing to enjoy a faster and easier experience in the R environment.",2018-09-10,Jean-Eudes Dazard,https://github.com/jedazard/MVR,TRUE,https://github.com/jedazard/mvr,20584,1,2020-10-24T03:10:29Z,20584
mvrsquared,"Compute the coefficient of determination for outcomes in n-dimensions. 
  May be useful for multidimensional predictions (such as a multinomial model) or
  calculating goodness of fit from latent variable models such as probabilistic
  topic models like latent Dirichlet allocation or deterministic topic models 
  like latent semantic analysis. Based on Jones (2019) <arXiv:1911.11061>.",2020-10-21,Tommy Jones,https://github.com/TommyJones/mvrsquared,TRUE,https://github.com/tommyjones/mvrsquared,9525,0,2020-10-28T02:41:57Z,NA
mwaved,"Computes the Wavelet deconvolution estimate of a common signal
    present in multiple channels that have possible different levels of blur
    and long memory additive error, see Kulik, Sapatinas and Wishart (2015), <doi:10.1016/j.acha.2014.04.004>.",2019-11-10,Justin Rory Wishart,https://github.com/jrwishart/mwaved,TRUE,https://github.com/jrwishart/mwaved,19240,2,2021-05-07T09:24:39Z,9620
myTAI,Investigate the evolution of biological processes by capturing evolutionary signatures in transcriptomes (Drost et al. (2017) <doi:10.1093/bioinformatics/btx835>). The aim of this tool is to provide a transcriptome analysis environment to quantify the average evolutionary age of genes contributing to a transcriptome of interest (Drost et al. (2016) <doi:10.1101/051565>).,2021-02-24,Hajk-Georg Drost,https://github.com/drostlab/myTAI,TRUE,https://github.com/drostlab/mytai,23732,22,2021-06-06T13:50:04Z,1078.7272727272727
n1qn1,"Provides 'Scilab' 'n1qn1'. This takes more memory than traditional L-BFGS.  The n1qn1 routine is useful since it allows prespecification of a Hessian.
       If the Hessian is near enough the truth in optimization it can speed up the optimization problem. The algorithm is described in the
       'Scilab' optimization documentation located at 
       <https://www.scilab.org/sites/default/files/optimization_in_scilab.pdf>. This version uses manually modified code from 'f2c' to make this a C only binary.",2020-11-17,Matthew Fidler,https://github.com/nlmixrdevelopment/n1qn1c,TRUE,https://github.com/nlmixrdevelopment/n1qn1c,27897,1,2020-11-17T06:14:31Z,27897
N2H4,"Provides some functions to get Korean text sample from news articles in
             Naver which is popular news portal service <https://news.naver.com/> in Korea.",2021-07-10,Chanyub Park,https://github.com/forkonlp/N2H4,TRUE,https://github.com/forkonlp/n2h4,12329,181,2021-07-10T11:54:27Z,68.11602209944752
N2R,"Implements methods to perform fast approximate K-nearest neighbor search on input matrix. Algorithm based on the 'N2' implementation of an approximate nearest neighbor search using hierarchical  Navigable Small World (NSW) graphs. The original algorithm is described in ""Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs"", Y. Malkov and D. Yashunin, <doi:10.1109/TPAMI.2018.2889473>, <arXiv:1603.09320>.",2020-12-14,Evan Biederstedt,https://github.com/kharchenkolab/N2R,TRUE,https://github.com/kharchenkolab/n2r,6155,6,2021-02-28T12:58:00Z,1025.8333333333333
naaccr,"Functions for reading cancer record files which follow a format
    defined by the North American Association of Central Cancer Registries
    (NAACCR).",2019-12-17,Nathan Werth,https://github.com/WerthPADOH/naaccr,TRUE,https://github.com/werthpadoh/naaccr,8651,7,2021-01-27T19:08:31Z,1235.857142857143
NACHO,"NanoString nCounter data are gene expression assays
    where there is no need for the use of enzymes or amplification
    protocols and work with fluorescent barcodes (Geiss et al. (2018)
    <doi:10.1038/nbt1385>). Each barcode is assigned a
    messenger-RNA/micro-RNA (mRNA/miRNA) which after bonding with its
    target can be counted. As a result each count of a specific barcode
    represents the presence of its target mRNA/miRNA. 'NACHO' (NAnoString
    quality Control dasHbOard) is able to analyse the exported NanoString
    nCounter data and facilitates the user in performing a quality
    control. 'NACHO' does this by visualising quality control metrics,
    expression of control genes, principal components and sample specific
    size factors in an interactive web application.",2021-01-14,Mickaël Canouil,"https://github.com/mcanouil/NACHO/, https://m.canouil.fr/NACHO/",TRUE,https://github.com/mcanouil/nacho,12709,5,2021-01-14T15:29:22Z,2541.8
NADA2,"Contains methods described by Dennis Helsel in 
             his book ""Statistics for Censored Environmental Data
             using Minitab and R"" (2011) and courses and videos at 
             <https://practicalstats.com>. This package adds new functions to 
             the `NADA` Package.",2021-04-30,Paul Julian,https://github.com/SwampThingPaul/NADA2,TRUE,https://github.com/swampthingpaul/nada2,2752,2,2021-04-29T22:36:31Z,1376
NADIA,Creates a uniform interface for several advanced imputations missing data methods. Every available method can be used as a part of 'mlr3' pipelines which allows easy tuning and performance evaluation. Most of the used functions work separately on the training and test sets (imputation is trained on the training set and impute training data. After that imputation is again trained on the test set and impute test data).,2021-01-06,Jan Borowski,NA,TRUE,https://github.com/modeloriented/emma,3647,19,2020-12-22T10:08:27Z,191.94736842105263
nadiv,"Constructs (non)additive genetic relationship matrices, and their
    inverses, from a pedigree to be used in linear mixed effect models (A.K.A.
    the 'animal model'). Also includes other functions to facilitate the use of
    animal models. Some functions have been created to be used in conjunction
    with the R package 'asreml' for the 'ASReml' software, which can be
    obtained upon purchase from 'VSN' international 
    (<https://www.vsni.co.uk/software/asreml>).",2021-04-06,Matthew Wolak,https://github.com/matthewwolak/nadiv,TRUE,https://github.com/matthewwolak/nadiv,43519,10,2021-04-07T01:43:16Z,4351.9
naijR,"A set of convenience functions as well as geographical/political
  data about Nigeria, aimed at simplifying work with data and information that
  are specific to the country.",2021-04-30,Victor Ordu,https://brovic.github.io/naijR/,TRUE,https://github.com/brovic/naijr,9786,2,2021-04-30T19:07:09Z,4893
nametagger,"Wraps the 'nametag' library <https://github.com/ufal/nametag>, allowing users to find and extract entities (names, persons, locations, addresses, ...) in raw text and build your own entity recognition models.
    Based on a maximum entropy Markov model which is described in Strakova J., Straka M. and Hajic J. (2013) <http://ufal.mff.cuni.cz/~straka/papers/2013-tsd_ner.pdf>.",2020-10-11,Jan Wijffels,https://github.com/bnosac/nametagger,TRUE,https://github.com/bnosac/nametagger,5405,2,2020-10-11T17:52:58Z,2702.5
nandb,"Calculation of molecular number and brightness from
    fluorescence microscopy image series. The software was published in a
    2016 paper <doi:10.1093/bioinformatics/btx434>. The seminal paper for
    the technique is Digman et al. 2008 <doi:10.1529/biophysj.107.114645>.
    A review of the technique was published in 2017
    <doi:10.1016/j.ymeth.2017.12.001>.",2021-05-16,Rory Nolan,"https://rorynolan.github.io/nandb/,
https://github.com/rorynolan/nandb",TRUE,https://github.com/rorynolan/nandb,19762,2,2021-07-13T01:54:42Z,9881
naniar,"Missing values are ubiquitous in data and need to be explored and
    handled in the initial stages of analysis. 'naniar' provides data structures 
    and functions that facilitate the plotting of missing values and examination 
    of imputations. This allows missing data dependencies to be explored with 
    minimal deviation from the common work patterns of 'ggplot2' and tidy data. 
    The work is fully discussed at Tierney & Cook (2018) <arXiv:1809.02264>.",2021-05-14,Nicholas Tierney,https://github.com/njtierney/naniar,TRUE,https://github.com/njtierney/naniar,454631,584,2021-08-11T14:00:09Z,778.4777397260274
nanostringr,"Provides quality control (QC), normalization, and
    batch effect correction operations for 'NanoString nCounter' data,
    Talhouk et al. (2016) <doi:10.1371/journal.pone.0153844>.  Various
    metrics are used to determine which samples passed or failed QC.  Gene
    expression should first be normalized to housekeeping genes, before a
    reference-based approach is used to adjust for batch effects.  Raw
    NanoString data can be imported in the form of Reporter Code Count
    (RCC) files.",2021-02-06,Derek Chiu,"https://github.com/TalhoukLab/nanostringr/,
https://talhouklab.github.io/nanostringr/",TRUE,https://github.com/talhouklab/nanostringr,12571,4,2021-04-14T21:22:08Z,3142.75
nanotime,"Full 64-bit resolution date and time functionality with
 nanosecond granularity is provided, with easy transition to and from
 the standard 'POSIXct' type. Three additional classes offer interval,
 period and duration functionality for nanosecond-resolution timestamps.",2021-08-09,Dirk Eddelbuettel and Leonardo Silvestri,"https://github.com/eddelbuettel/nanotime,
https://dirk.eddelbuettel.com/code/nanotime.html",TRUE,https://github.com/eddelbuettel/nanotime,345450,43,2021-08-09T20:35:40Z,8033.720930232558
nardl,"Computes the nonlinear cointegrating autoregressive distributed lag model with automatic bases aic and bic lags selection of independent variables proposed by (Shin, Yu & Greenwood-Nimmo, 2014 <doi:10.1007/978-1-4899-8008-3_9>).",2021-01-06,Taha Zaghdoudi,https://github.com/zedtaha/nardl,TRUE,https://github.com/zedtaha/nardl,33332,2,2021-01-14T18:01:01Z,16666
narray,"Stacking arrays according to dimension names, subset-aware
    splitting and mapping of functions, intersecting along arbitrary
    dimensions, converting to and from data.frames, and many other helper
    functions.",2021-05-10,Michael Schubert,https://github.com/mschubert/narray,TRUE,https://github.com/mschubert/narray,36635,22,2021-05-10T12:37:47Z,1665.2272727272727
nasapower,"Client for 'NASA' 'POWER' global meteorology, surface solar
    energy and climatology data 'API'.  'POWER' (Prediction Of Worldwide Energy 
    Resource) data are freely available for download with a spatial resolution 
    of 0.5 x 0.625 degree latitude and longitude for meteorology and  1 x 1
    degree latitude and longitude for solar parameters with various temporal
    resolutions depending on the POWER parameter and community.  This work is
    funded through the 'NASA' Earth Science Directorate Applied Science Program.
    For more on the data themselves, the methodologies used in creating, a web-
    based data viewer and web access, please see <https://power.larc.nasa.gov/>.",2021-08-23,Adam H. Sparks,https://docs.ropensci.org/nasapower/,TRUE,https://github.com/ropensci/nasapower,25642,64,2021-09-01T12:51:22Z,400.65625
nat,"NeuroAnatomy Toolbox (nat) enables analysis and visualisation of 3D
    biological image data, especially traced neurons. Reads and writes 3D images
    in NRRD and 'Amira' AmiraMesh formats and reads surfaces in 'Amira' hxsurf
    format. Traced neurons can be imported from and written to SWC and 'Amira'
    LineSet and SkeletonGraph formats. These data can then be visualised in 3D
    via 'rgl', manipulated including applying calculated registrations, e.g.
    using the 'CMTK' registration suite, and analysed. There is also a simple
    representation for neurons that have been subjected to 3D skeletonisation
    but not formally traced; this allows morphological comparison between
    neurons including searches and clustering (via the 'nat.nblast' extension
    package).",2020-09-22,Gregory Jefferis,"https://github.com/natverse/nat, http://natverse.org/",TRUE,https://github.com/natverse/nat,31392,51,2021-07-17T21:35:48Z,615.5294117647059
nat.nblast,"Extends package 'nat' (NeuroAnatomy Toolbox) by providing a
    collection of NBLAST-related functions for neuronal morphology comparison (Costa et al. (2016) <doi: 10.1016/j.neuron.2016.06.012>).",2020-01-23,Gregory Jefferis,"https://github.com/natverse/nat.nblast, https://natverse.github.io",TRUE,https://github.com/natverse/nat.nblast,16028,13,2021-06-16T14:03:10Z,1232.923076923077
nat.templatebrains,"Extends package 'nat' (NeuroAnatomy Toolbox) by
    providing objects and functions for handling template brains.",2020-10-19,Gregory Jefferis,"http://natverse.org/nat.templatebrains/,
https://github.com/natverse/nat.templatebrains",TRUE,https://github.com/natverse/nat.templatebrains,18219,3,2021-04-22T13:42:32Z,6073
natcpp,"Fast functions implemented in C++ via 'Rcpp' to support the
    'NeuroAnatomy Toolbox' ('nat') ecosystem. These functions provide large
    speed-ups for basic manipulation of neuronal skeletons over pure R
    functions found in the 'nat' package. The expectation is that end
    users will not use this package directly, but instead the 'nat'
    package will automatically use routines from this package when it is
    available to enable large performance gains.",2021-07-13,Gregory Jefferis,https://github.com/natverse/natcpp,TRUE,https://github.com/natverse/natcpp,668,0,2021-07-13T21:26:00Z,NA
natmanager,"Provides streamlined installation for packages from the 'natverse',
    a suite of R packages for computational neuroanatomy built on top of the
    'nat' 'NeuroAnatomy Toolbox' package. Installation of the complete
    'natverse' suite requires a 'GitHub' user account and personal access token
    'GITHUB_PAT'. 'natmanager' will help the end user set this up if necessary.",2021-05-30,Sridhar Jagannathan,https://github.com/natverse/natmanager,TRUE,https://github.com/natverse/natmanager,11544,1,2021-05-30T10:47:46Z,11544
natserv,"Interface to 'NatureServe' (<https://www.natureserve.org/>).
    Includes methods to get data, image metadata, search taxonomic names,
    and make maps.",2020-05-16,Scott Chamberlain,"https://docs.ropensci.org/natserv,
https://github.com/ropensci/natserv",TRUE,https://github.com/ropensci/natserv,92448,10,2021-05-04T23:00:45Z,9244.8
natstrat,"Natural strata fix a constant ratio of controls to treated units within 
    each stratum. This ratio need not be an integer. The control units are 
    chosen using randomized rounding of a linear program that balances many 
    covariates.
    To solve the linear program, the 'Gurobi' commercial optimization software 
    is recommended, but not required. The 'gurobi' R package can be installed following the instructions 
    at <https://www.gurobi.com/documentation/9.1/refman/ins_the_r_package.html>.",2021-05-26,Katherine Brumberg,"https://github.com/kkbrum/natstrat,
https://kkbrum.github.io/natstrat/,
https://www.gurobi.com/documentation/9.1/refman/ins_the_r_package.html",TRUE,https://github.com/kkbrum/natstrat,982,0,2021-05-24T20:12:23Z,NA
naturaList,"Classify occurrence records based on confidence
    levels of species identification. In addition, implement tools to filter
    occurrence inside grid cells and to manually check for possibles errors with 
	an interactive shiny application.",2021-08-21,Arthur Vinicius Rodrigues,https://github.com/avrodrigues/naturaList,TRUE,https://github.com/avrodrigues/naturalist,167,0,2021-08-23T12:38:05Z,NA
NatureSounds,Collection of example animal sounds for bioacoustic analysis.,2021-04-23,Marcelo Araya-Salas,https://github.com/maRce10/NatureSounds,TRUE,https://github.com/marce10/naturesounds,23789,0,2021-04-23T15:15:20Z,NA
nbapalettes,Palettes generated from NBA jersey colorways.,2021-01-07,Murray Josh,https://github.com/murrayjw/nbapalettes,TRUE,https://github.com/murrayjw/nbapalettes,2691,2,2021-01-07T17:46:37Z,1345.5
nberwp,Catalogue of NBER working papers published between June 1973 and June 2021.,2021-07-21,Benjamin Davies,https://github.com/bldavies/nberwp,TRUE,https://github.com/bldavies/nberwp,605,13,2021-08-23T04:47:03Z,46.53846153846154
nbTransmission,Estimates the relative transmission probabilities between cases in an infectious disease outbreak or cluster using naive Bayes. Included are various functions to use these probabilities to estimate transmission parameters such as the generation/serial interval and reproductive number as well as finding the contribution of covariates to the probabilities and visualizing results. The ideal use is for an infectious disease dataset with metadata on the majority of cases but more informative data such as contact tracing or pathogen whole genome sequencing on only a subset of cases. For a detailed description of the methods see Leavitt et al. (2020) <doi:10.1093/ije/dyaa031>.,2021-01-06,Sarah V Leavitt,https://sarahleavitt.github.io/nbTransmission/,TRUE,https://github.com/sarahleavitt/nbtransmission,5207,4,2021-01-06T19:12:52Z,1301.75
nc,"User-friendly functions for extracting a data
 table (row for each match, column for each group)
 from non-tabular text data using regular expressions,
 and for melting columns that match a regular expression.
 Patterns are defined using a readable syntax
 that makes it easy to build complex patterns
 in terms of simpler, re-usable sub-patterns.
 Named R arguments are translated to column names
 in the output; capture groups without names are used
 internally in order to provide a standard interface
 to three regular expression C libraries (PCRE, RE2, ICU).
 Output can also include numeric columns via
 user-specified type conversion functions.
 RE2 engine (re2r package) was removed from CRAN in Mar 2020
 so must be installed from github.",2020-08-10,Toby Dylan Hocking,https://github.com/tdhock/nc,TRUE,https://github.com/tdhock/nc,13578,12,2021-06-28T14:27:59Z,1131.5
ncdfgeom,Tools to create time series and geometry 'NetCDF' files.,2021-04-23,David Blodgett,https://code.usgs.gov/water/ncdfgeom,TRUE,https://github.com/usgs-r/ncdfgeom,68947,11,2021-04-23T12:03:34Z,6267.909090909091
ncmeta,"Extract metadata from 'NetCDF' data sources, these can be files, file handles or
 servers. This package leverages and extends the lower level functions of the 'RNetCDF' package 
 providing a consistent set of functions that all return data frames. We introduce named concepts 
 of 'grid', 'axis' and 'source' which are all meaningful entities without formal definition in the 
 'NetCDF' library <https://www.unidata.ucar.edu/software/netcdf/>. 'RNetCDF' matches the library 
 itself with only the named concepts of 'variables', 'dimensions' and 'attributes'. ",2020-08-27,Michael Sumner,https://github.com/hypertidy/ncmeta,TRUE,https://github.com/hypertidy/ncmeta,117461,6,2021-04-01T03:17:33Z,19576.833333333332
nCov2019,"Provides easy-to-use programming API to access real time and historical data of 'COVID'-19 cases, vaccine and therapeutics data, and a Shiny app to help users exploring the data.  Fetching data using API provided by <https://disease.sh> . ",2021-06-10,Guangchuang Yu,https://github.com/YuLab-SMU/nCov2019,TRUE,https://github.com/yulab-smu/ncov2019,3552,5,2021-06-10T10:12:48Z,710.4
ncvreg,"Fits regularization paths for linear regression, GLM, and Cox
  regression models using lasso or nonconvex penalties, in particular the
  minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD)
  penalty, with options for additional L2 penalties (the ""elastic net"" idea).
  Utilities for carrying out cross-validation as well as post-fitting
  visualization, summarization, inference, and prediction are also provided.
  For more information, see Breheny and Huang (2011) <doi:10.1214/10-AOAS388>
  or visit the ncvreg homepage <https://pbreheny.github.io/ncvreg/>.",2021-03-30,Patrick Breheny,"https://pbreheny.github.io/ncvreg/,
https://github.com/pbreheny/ncvreg",TRUE,https://github.com/pbreheny/ncvreg,112332,25,2021-08-26T19:50:15Z,4493.28
ndtv,"Renders dynamic network data from 'networkDynamic' objects as movies, interactive animations, or other representations of changing relational structures and attributes.",2021-07-30,Skye Bender-deMoll,https://github.com/statnet/ndtv,TRUE,https://github.com/statnet/ndtv,78046,40,2021-07-24T22:24:10Z,1951.15
neatStats,"User-friendly, clear and simple statistics, primarily for
  publication in psychological science. The main functions are wrappers for
  other packages, but there are various additions as well. Every relevant step
  from data aggregation to reportable printed statistics is covered for basic
  experimental designs.",2021-08-18,Gáspár Lukács,https://github.com/gasparl/neatstats,TRUE,https://github.com/gasparl/neatstats,8927,1,2021-08-16T11:51:14Z,8927
negenes,"Estimating the number of essential genes in a genome on
    the basis of data from a random transposon mutagenesis experiment,
    through the use of a Gibbs sampler.
    Lamichhane et al. (2003) <doi:10.1073/pnas.1231432100>.",2019-08-05,Karl W Broman,https://github.com/kbroman/negenes,TRUE,https://github.com/kbroman/negenes,19081,1,2020-11-29T14:24:15Z,19081
neo2R,"The aim of the neo2R is to provide simple and low level connectors
   for querying neo4j graph databases (<https://neo4j.com/>).
   The objects returned by the query functions are either lists or data.frames
   with very few post-processing.
   It allows fast processing of queries returning many records.
   And it let the user handle post-processing according to the data model
   and his needs.",2020-03-28,Patrice Godard,https://github.com/patzaw/neo2r,TRUE,https://github.com/patzaw/neo2r,5991,3,2020-12-14T10:59:06Z,1997
NEONiso,"Functions for downloading,
    calibrating, and analyzing atmospheric isotope data bundled
    into the eddy covariance data products of the National Ecological
    Observatory Network (NEON) <https://www.neonscience.org>. 
    In this version, calibration tools are provided for only the 
    carbon isotope products. Tools for calibrating water isotope 
    products are under development. More details are found in Fiorella et al. (2021)
    <doi:10.1029/2020JG005862>, and the readme 
    file at <https://github.com/SPATIAL-Lab/NEONiso>.",2021-08-12,Rich Fiorella,https://github.com/SPATIAL-Lab/NEONiso,TRUE,https://github.com/spatial-lab/neoniso,2102,0,2021-08-12T15:02:44Z,NA
neonUtilities,"NEON data packages can be accessed through the NEON Data Portal <https://www.neonscience.org>
    or through the NEON Data API (see <https://data.neonscience.org/data-api> for documentation). Data delivered from
    the Data Portal are provided as monthly zip files packaged within a parent zip file, while individual files
    can be accessed from the API. This package provides tools that aid in discovering, downloading, and reformatting 
    data prior to use in analyses. This includes downloading data via the API, merging data tables by type, and 
    converting formats. For more information, see the readme file at <https://github.com/NEONScience/NEON-utilities>.",2021-09-02,Claire Lunch,https://github.com/NEONScience/NEON-utilities,TRUE,https://github.com/neonscience/neon-utilities,33391,45,2021-09-01T14:45:57Z,742.0222222222222
neotoma,"Access paleoecological datasets from the Neotoma Paleoecological
    Database using the published API (<http://api.neotomadb.org/>).  The functions
    in this package access various pre-built API functions and attempt to return
    the results from Neotoma in a usable format for researchers and the public.",2019-01-05,Simon J. Goring,https://github.com/ropensci/neotoma,TRUE,https://github.com/ropensci/neotoma,28597,25,2020-09-20T18:21:11Z,1143.88
neptune,"Interface to 'Neptune', experiment tracking tool that helps you organize
    your machine learning experiments. You can log your hyperparameter, metrics, model binaries and
    performance charts, organize them with tags and names and share everything easily in the app.
    For more information see <https://neptune.ai/>.",2021-01-11,Jakub Czakon,https://github.com/neptune-ai/neptune-r,TRUE,https://github.com/neptune-ai/neptune-r,6176,7,2021-01-08T13:17:05Z,882.2857142857143
nestfs,"Implementation of forward selection based on cross-validated
             linear and logistic regression.",2019-09-21,Marco Colombo,https://github.com/mcol/nestfs,TRUE,https://github.com/mcol/nestfs,16095,0,2021-07-08T16:20:48Z,NA
netCoin,"Create interactive analytic networks. It joins the data analysis power of R to obtain coincidences, co-occurrences and correlations, and the visualization libraries of 'JavaScript' in one package.",2021-02-03,Modesto Escobar,https://modesto-escobar.github.io/netCoin-1.0/,TRUE,https://github.com/modesto-escobar/netcoin-1.0,21679,1,2021-01-25T10:06:22Z,21679
netcom,Infer system functioning with empirical NETwork COMparisons.,2021-08-02,Ryan Langendorf,https://github.com/langendorfr/netcom,TRUE,https://github.com/langendorfr/netcom,14241,0,2021-08-17T22:28:58Z,NA
netdiffuseR,"Empirical statistical analysis, visualization and simulation of
    diffusion and contagion processes on networks. The package implements algorithms
    for calculating network diffusion statistics such as transmission rate, hazard
    rates, exposure models, network threshold levels, infectiousness (contagion),
    and susceptibility. The package is inspired by work published in Valente,
    et al., (2015) <DOI:10.1016/j.socscimed.2015.10.001>; Valente (1995) <ISBN:
    9781881303213>, Myers (2000) <DOI:10.1086/303110>, Iyengar and others (2011)
    <DOI:10.1287/mksc.1100.0566>, Burt (1987) <DOI:10.1086/228667>; among others.",2021-05-28,George Vega Yon  (<https://orcid.org/0000-0002-3171-0844>,"https://github.com/USCCANA/netdiffuseR,
https://USCCANA.github.io/netdiffuseR/",TRUE,https://github.com/usccana/netdiffuser,18345,63,2021-05-22T05:09:42Z,291.1904761904762
netgen,"Methods for the generation of a wide range of network geographies,
    e.g., grid networks or clustered networks. Useful for the generation of
    benchmarking instances for the investigation of, e.g., Vehicle-Routing-Problems
    or Travelling Salesperson Problems.",2020-01-08,Jakob Bossek,https://github.com/jakobbossek/netgen,TRUE,https://github.com/jakobbossek/netgen,17845,9,2021-06-30T06:59:48Z,1982.7777777777778
netgsa,"Carry out network-based gene set analysis by incorporating external information about interactions among genes, as well as novel interactions learned from data. Implements methods described in Shojaie A, Michailidis G (2010) <doi:10.1093/biomet/asq038>, Shojaie A, Michailidis G (2009) <doi:10.1089/cmb.2008.0081>, and Ma J, Shojaie A, Michailidis G (2016) <doi:10.1093/bioinformatics/btw410>.",2021-05-15,Michael Hellstern,https://github.com/mikehellstern/netgsa,TRUE,https://github.com/mikehellstern/netgsa,17879,1,2021-08-25T02:55:10Z,17879
NetLogoR,"Build and run spatially explicit
    agent-based models using only the R platform. 'NetLogoR' follows the same
    framework as the 'NetLogo' software
    (Wilensky, 1999 <http://ccl.northwestern.edu/netlogo/>) and is a translation
    in R of the structure and functions of 'NetLogo'.
    'NetLogoR' provides new R classes to define model agents and functions to
    implement spatially explicit agent-based models in the R environment.
    This package allows benefiting of the fast and easy coding phase from the
    highly developed 'NetLogo' framework, coupled with the versatility, power
    and massive resources of the R software.
    Examples of three models (Ants <http://ccl.northwestern.edu/netlogo/models/Ants>,
    Butterfly (Railsback and Grimm, 2012) and Wolf-Sheep-Predation
    <http://ccl.northwestern.edu/netlogo/models/WolfSheepPredation>) written using
    'NetLogoR' are available. The 'NetLogo' code of the original version of these
    models is provided alongside.
    A programming guide inspired from the 'NetLogo' Programming Guide
    (<https://ccl.northwestern.edu/netlogo/docs/programming.html>) and a dictionary
    of 'NetLogo' primitives (<https://ccl.northwestern.edu/netlogo/docs/dictionary.html>)
    equivalences are also available.
    NOTE: To increment 'time', these functions can use a for loop or can be
    integrated with a discrete event simulator, such as 'SpaDES'
    (<https://cran.r-project.org/package=SpaDES>).
    The suggested package 'fastshp' can be installed with
    'install.packages(""fastshp"", repos = ""https://rforge.net"", type = ""source"")'.",2021-07-10,Sarah Bauduin,"https://netlogor.predictiveecology.org,
https://github.com/PredictiveEcology/NetLogoR/,
https://groups.google.com/g/netlogor",TRUE,https://github.com/predictiveecology/netlogor,16864,29,2021-07-09T20:29:59Z,581.5172413793103
netmhc2pan,"The field of immunology benefits from software that can
    predict which peptide sequences trigger an immune response.
    'NetMHCIIpan' is a such a tool: it predicts the
    binding strength of a short peptide to a Major Histocompatibility
    Complex class II (MHC-II) molecule.
    'NetMHCIIpan' can be used from a web server at 
    <https://services.healthtech.dtu.dk/service.php?NetMHCIIpan-3.2/>
    or from the command-line, using a local installation. This package
    allows to call 'NetMHCIIpan' from R.",2020-11-18,Richèl J.C. Bilderbeek,https://github.com/richelbilderbeek/netmhc2pan/,TRUE,https://github.com/richelbilderbeek/netmhc2pan,3414,0,2020-12-10T14:08:21Z,NA
NetMix,"Stochastic collapsed variational inference on mixed-membership stochastic blockmodel for networks,
             incorporating node-level predictors of mixed-membership vectors, as well as 
             dyad-level predictors. For networks observed over time, the model defines a hidden
             Markov process that allows the effects of node-level predictors to evolve in discrete,
             historical periods. In addition, the package offers a variety of utilities for 
             exploring results of estimation, including tools for conducting posterior 
             predictive checks of goodness-of-fit and several plotting functions. The package 
             implements methods described in Olivella, Pratt and Imai (2019) 'Dynamic Stochastic
             Blockmodel Regression for Social Networks: Application to International Conflicts',
             available at <https://www.santiagoolivella.info/pdfs/socnet.pdf>.",2021-03-01,Santiago Olivella,NA,TRUE,https://github.com/solivella/netmix,10057,6,2021-03-01T14:57:52Z,1676.1666666666667
NetOrigin,"Performs network-based source estimation. Different approaches are available: effective distance median, recursive backtracking, and centrality-based source estimation. Additionally, we provide public transportation network data as well as methods for data preparation, source estimation performance analysis and visualization.",2021-04-01,Juliane Manitz,https://netorigin.manitz.org/,TRUE,https://github.com/jmanitz/netorigin,15042,0,2021-03-17T13:44:33Z,NA
netplot,"A graph visualization engine that puts an emphasis on 
  aesthetics at the same time of providing default parameters that yield
  out-of-the-box-nice visualizations. The package is built on top of
  'The Grid Graphics Package' and seamlessly work with 'igraph' and 
  'network' objects.",2021-06-16,George Vega Yon,https://github.com/USCCANA/netplot,TRUE,https://github.com/usccana/netplot,1024,42,2021-06-16T19:19:21Z,24.38095238095238
netrankr,"Implements methods for centrality related analyses of networks. 
    While the package includes the possibility to build more than 20 indices, 
    its main focus lies on index-free assessment of centrality via partial 
    rankings obtained by neighborhood-inclusion or positional dominance. These 
    partial rankings can be analyzed with different methods, including 
    probabilistic methods like computing expected node ranks and relative 
    rank probabilities (how likely is it that a node is more central than another?).
    The methodology is described in depth in the vignettes and in
    Schoch (2018) <doi:10.1016/j.socnet.2017.12.003>.",2021-07-16,David Schoch,https://github.com/schochastics/netrankr/,TRUE,https://github.com/schochastics/netrankr,55577,34,2021-08-25T19:32:09Z,1634.6176470588234
NetRep,"Functions for assessing the replication/preservation of a network 
  module's topology across datasets through permutation testing; Ritchie et al. 
  (2015) <doi: 10.1016/j.cels.2016.06.012>.",2020-10-07,Scott Ritchie,NA,TRUE,https://github.com/sritchie73/netrep,16102,11,2020-10-07T13:41:48Z,1463.8181818181818
nets,Sparse VAR estimation based on LASSO.,2020-10-27,Christian Brownlees,https://github.com/ctbrownlees/R-Package-nets,TRUE,https://github.com/ctbrownlees/r-package-nets,21590,4,2020-10-27T17:33:18Z,5397.5
netseg,"Segregation is a network-level property such that edges between 
  predefined groups of vertices are relatively less likely. Network homophily 
  is a individual-level tendency to form relations with people who are similar 
  on some attribute (e.g. gender, music taste, social status, etc.). In general
  homophily leads to segregation, but segregation might arise without 
  homophily. This package implements descriptive indices measuring 
  homophily/segregation. It is a computational companion 
  to Bojanowski & Corten (2014) <doi:10.1016/j.socnet.2014.04.001>.",2021-02-17,Michal Bojanowski,https://mbojan.github.io/netseg/,TRUE,https://github.com/mbojan/netseg,2321,11,2021-02-17T21:52:35Z,211
nettskjemar,"Enables users to retrieve data, meta-data, and codebooks 
    from <https://nettskjema.no/>. The data from the API is richer than from the
    online data portal. Mowinckel (2021) <doi:10.5281/zenodo.4745481>.",2021-05-20,Athanasia Mo Mowinckel,"https://github.com/LCBC-UiO/nettskjemar,
https://zenodo.org/badge/latestdoi/206264675",TRUE,https://github.com/lcbc-uio/nettskjemar,1232,2,2021-06-03T15:16:01Z,616
NetWeaver,Implements various simple function utilities and flexible pipelines to generate circular images for visualizing complex genomic and network data analysis features.,2019-02-26,Minghui Wang,https://github.com/mw201608/NetWeaver/,TRUE,https://github.com/mw201608/netweaver,14442,1,2021-04-12T03:45:34Z,14442
networkABC,"We developed an inference tool based on approximate Bayesian computation to decipher network data and assess the strength of the inferred links between network's actors. It is a new multi-level approximate Bayesian computation (ABC) approach. At the first level, the method captures the global properties of the network, such as scale-freeness and clustering coefficients, whereas the second level is targeted to capture local properties, including the probability of each couple of genes being linked. Up to now, Approximate Bayesian Computation (ABC) algorithms have been scarcely used in that setting and, due to the computational overhead, their application was limited to a small number of genes. On the contrary, our algorithm was made to cope with that issue and has low computational cost. It can be used, for instance, for elucidating gene regulatory network, which is an important step towards understanding the normal cell physiology and complex pathological phenotype. Reverse-engineering consists in using gene expressions over time or over different experimental conditions to discover the structure of the gene network in a targeted cellular process. The fact that gene expression data are usually noisy, highly correlated, and have high dimensionality explains the need for specific statistical methods to reverse engineer the underlying network. ",2021-03-20,Frederic Bertrand,"https://fbertran.github.io/networkABC/,
https://github.com/fbertran/networkABC/",TRUE,https://github.com/fbertran/networkabc,12488,3,2021-07-14T22:05:26Z,4162.666666666667
networktree,"Network trees recursively partition the data with respect to covariates. Two network tree algorithms are available: model-based trees based on a multivariate normal model and nonparametric trees based on covariance structures. After partitioning, correlation-based networks (psychometric networks) can be fit on the partitioned data. For details see Jones, Mair, Simon, & Zeileis (2020) <doi:10.1007/s11336-020-09731-4>. ",2021-02-04,Payton Jones,https://paytonjjones.github.io/networktree/,TRUE,https://github.com/paytonjjones/networktree,15944,9,2021-06-23T18:07:46Z,1771.5555555555557
neuralnet,"Training of neural networks using backpropagation,
    resilient backpropagation with (Riedmiller, 1994) or without
    weight backtracking (Riedmiller and Braun, 1993) or the
    modified globally convergent version by Anastasiadis et al.
    (2005). The package allows flexible settings through
    custom-choice of error and activation function. Furthermore,
    the calculation of generalized weights (Intrator O & Intrator
    N, 1993) is implemented.",2019-02-07,Marvin N. Wright,https://github.com/bips-hb/neuralnet,TRUE,https://github.com/bips-hb/neuralnet,632333,19,2020-09-23T04:50:48Z,33280.68421052631
NeuralNetTools,"Visualization and analysis tools to aid in the interpretation of
    neural network models.  Functions are available for plotting,
    quantifying variable importance, conducting a sensitivity analysis, and
    obtaining a simple list of model weights.",2018-07-26,Marcus W. Beck,NA,TRUE,https://github.com/fawda123/neuralnettools,151874,66,2020-12-21T12:03:23Z,2301.121212121212
NeuralSens,"Analysis functions to quantify inputs importance in neural network models.
  Functions are available for calculating and plotting the inputs importance and obtaining
  the activation function of each neuron layer and its derivatives. The importance of a given
  input is defined as the distribution of the derivatives of the output with respect to that
  input in each training data point.",2020-11-16,José Portela González,https://github.com/JaiPizGon/NeuralSens,TRUE,https://github.com/jaipizgon/neuralsens,15929,6,2020-12-22T10:54:50Z,2654.8333333333335
neurobase,"Base package for 'Neuroconductor', which includes many helper 
    functions that interact with objects of class 'nifti', implemented by
    package 'oro.nifti', for reading/writing and also other manipulation 
    functions.",2021-04-07,John Muschelli,NA,TRUE,https://github.com/muschellij2/neurobase,39761,4,2021-04-07T19:43:44Z,9940.25
neurohcp,"Downloads and reads data from Human 'Connectome' Project 
    <https://db.humanconnectome.org> using Amazon Web Services ('AWS') 
    'S3' buckets.",2020-10-14,John Muschelli,https://db.humanconnectome.org,TRUE,https://github.com/muschellij2/neurohcp,16841,4,2021-02-18T05:33:24Z,4210.25
neutralitytestr,Package takes frequencies of mutations as reported by high throughput sequencing data from cancer and fits a theoretical neutral model of tumour evolution. Package outputs summary statistics and contains code for plotting the data and model fits. See Williams et al 2016 <doi:10.1038/ng.3489> and Williams et al 2017 <doi:10.1101/096305> for further details of the method.,2021-02-16,Marc Williams,https://github.com/marcjwilliams1/neutralitytestr,TRUE,https://github.com/marcjwilliams1/neutralitytestr,12519,8,2021-02-16T17:30:41Z,1564.875
neverhpfilter,"In the working paper titled ""Why You Should Never Use the Hodrick-Prescott 
   Filter"", James D. Hamilton proposes a new alternative to economic time series 
   filtering. The neverhpfilter package provides functions and data for reproducing his work. Hamilton (2017) <doi:10.3386/w23429>.",2021-06-18,Justin M. Shea,https://justinmshea.github.io/neverhpfilter/,TRUE,https://github.com/justinmshea/neverhpfilter,14987,9,2021-07-20T03:31:55Z,1665.2222222222222
newscatcheR,"Programmatically collect normalized news from
    (almost) any website. An 'R' clone of the
    <https://github.com/kotartemiy/newscatcher> 'Python' module.",2020-07-13,Novica Nakov,https://github.com/discindo/newscatcheR/,TRUE,https://github.com/discindo/newscatcher,5879,12,2020-10-28T17:24:58Z,489.9166666666667
newsmap,"Semissupervised model for geographical document classification (Watanabe 2018) <doi:10.1080/21670811.2017.1293487>. 
    This package currently contains seed dictionaries in English, German, French, Spanish, Italian, Russian, Hebrew, Arabic Japanese and Chinese (Simplified and Traditional).",2021-05-18,Kohei Watanabe,https://github.com/koheiw/newsmap,TRUE,https://github.com/koheiw/newsmap,23800,45,2021-05-17T22:44:19Z,528.8888888888889
newsmd,"Adding updates (version or bullet points) to the
    NEWS.md file.",2021-02-24,Jakob Gepp,https://github.com/Dschaykib/newsmd,TRUE,https://github.com/dschaykib/newsmd,2012,4,2021-08-15T11:46:07Z,503
nfl4th,"A set of functions to estimate outcomes of fourth down
    plays in the National Football League and obtain fourth down plays
    from <https://www.nfl.com/> and <https://www.espn.com/>.",2021-03-17,Ben Baldwin,"https://www.nfl4th.com/, https://github.com/guga31bb/nfl4th/",TRUE,https://github.com/guga31bb/nfl4th,1975,3,2021-09-01T12:54:02Z,658.3333333333334
nflfastR,"A set of functions to access National Football
    League play-by-play data from <https://www.nfl.com/>.",2021-08-03,Ben Baldwin,"https://www.nflfastr.com/, https://github.com/nflverse/nflfastR",TRUE,https://github.com/nflverse/nflfastr,16593,198,2021-09-03T12:51:15Z,83.8030303030303
nflreadr,"A low-level package for downloading data from 'GitHub'
    repositories of the 'nflverse' project.",2021-09-02,Tan Ho,"https://nflreadr.nflverse.com,
https://github.com/nflverse/nflreadr",TRUE,https://github.com/nflverse/nflreadr,626,9,2021-09-03T02:01:42Z,69.55555555555556
nflseedR,"A set of functions to simulate National Football
    League seasons including the sophisticated tie-breaking procedures.",2021-04-10,Sebastian Carl,"https://nflseedr.com, https://github.com/leesharpe/nflseedR",TRUE,https://github.com/leesharpe/nflseedr,4309,9,2021-07-08T17:33:24Z,478.77777777777777
NFLSimulatoR,"The intent here is to enable the simulation of plays/drives and
    evaluate game-play strategies in the National Football League (NFL).
    Built-in strategies include going for it on fourth down and varying the 
    proportion of passing/rushing plays during a drive. The user should be
    familiar with nflscrapR data before trying to write his/her own 
    strategies. This work is inspired by a blog post by Mike Lopez, 
    currently the  Director of Data and Analytics at the NFL, Lopez (2019) <https://statsbylopez.netlify.app/post/resampling-nfl-drives/>.",2021-01-06,Ryan Elmore,https://github.com/rtelmore/NFLSimulatoR/,TRUE,https://github.com/rtelmore/nflsimulator,3707,11,2021-01-31T20:52:47Z,337
NFP,"An implementation of the network fingerprint framework that introduced 
  in paper ""Network fingerprint: a knowledge-based characterization of biomedical 
  networks"" (Cui, 2015) <doi:10.1038/srep13286>. This method worked by making 
  systematic comparisons to a set of well-studied ""basic networks"", measuring 
  both the functional and topological similarity.  A biological could be
  characterized as a spectrum-like vector consisting of similarities to basic 
  networks. It shows great potential in biological network study.",2021-04-12,Yang Cao,https://github.com/yiluheihei/NFP,TRUE,https://github.com/yiluheihei/nfp,15870,3,2021-04-12T13:14:40Z,5290
NGLVieweR,"Provides an 'htmlwidgets' <https://www.htmlwidgets.org/> interface to 'NGL.js' <http://nglviewer.org/ngl/api/>.
    'NGLvieweR' can be used to visualize and interact with protein databank ('PDB') and structural files in R and Shiny applications.
    It includes a set of API functions to manipulate the viewer after creation in Shiny.",2021-06-01,Niels van der Velden,https://github.com/nvelden/NGLVieweR,TRUE,https://github.com/nvelden/nglviewer,1090,15,2021-07-22T15:43:26Z,72.66666666666667
ngramr,"Retrieve and plot word frequencies through time from the ""Google
    Ngram Viewer"" <https://books.google.com/ngrams>.",2021-05-13,Sean Carmody,https://github.com/seancarmody/ngramr,TRUE,https://github.com/seancarmody/ngramr,7987,37,2021-05-13T23:49:22Z,215.86486486486487
NGSSEML,"Due to a large quantity of non-Gaussian time series and reliability data, the R-package non-Gaussian state-space with exact marginal likelihood is useful for modeling and forecasting non-Gaussian time series and reliability data via non-Gaussian state-space models with the exact marginal likelihood easily, see Gamerman, Santos and Franco (2013) <doi:10.1111/jtsa.12039> and Santos, Gamerman and Franco (2017) <doi:10.1109/TR.2017.2670142>. The package gives codes for formulating and specifying the non-Gaussian state-space models in the R language. Inferences for the parameters of the model can be made under the classical and Bayesian. Furthermore, prediction, filtering, and smoothing procedures can be used to perform inferences for the latent parameters. Applications include, e.g., count, volatility, piecewise exponential, and software reliability data.",2021-09-02,Thiago Rezende dos Santos,https://github.com/hadht/NGSSEML-R-Package,TRUE,https://github.com/hadht/ngsseml-r-package,10631,0,2021-09-02T18:15:06Z,NA
nhanesA,"Utility to retrieve data from the National Health and Nutrition 
	Examination Survey (NHANES) website <https://www.cdc.gov/nchs/nhanes/index.htm>.",2021-01-31,Christopher J. Endres,https://cran.r-project.org/package=nhanesA,TRUE,https://github.com/cjendres1/nhanes,28455,10,2021-01-30T21:42:51Z,2845.5
nhdR,"Tools for working with the National Hydrography Dataset, with 
    functions for querying, downloading, and networking both the NHD 
    <https://www.usgs.gov/core-science-systems/ngp/national-hydrography> 
    and NHDPlus <https://www.epa.gov/waterdata/nhdplus-national-hydrography-dataset-plus> datasets. ",2021-07-17,Jemma Stachelek,https://github.com/jsta/nhdR,TRUE,https://github.com/jsta/nhdr,14565,28,2021-07-13T15:38:05Z,520.1785714285714
nhlapi,"Retrieves and processes the data exposed by the open 'NHL' API. This includes information on players, teams, games, tournaments, drafts, standings, schedules and other endpoints. A lower-level interface to access the data via URLs directly is also provided.",2021-02-20,Jozef Hajnala,https://github.com/jozefhajnala/nhlapi,TRUE,https://github.com/jozefhajnala/nhlapi,5690,21,2021-03-02T15:47:39Z,270.95238095238096
nhsnumber,"Provides functions for working with NHS number checksums.
    The UK's National Health Service issues NHS numbers to all users of its
    services and this package implements functions for verifying that the
    numbers are valid according to the checksum scheme the NHS use.
    Numbers can be validated and checksums created.",2020-12-07,Mark Sellors,https://github.com/sellorm/nhsnumber,TRUE,https://github.com/sellorm/nhsnumber,2820,3,2021-01-05T17:18:37Z,940
NHSRdatasets,"Free United Kingdom National Health Service (NHS) and other healthcare, or population health-related data for education and training purposes. This package contains synthetic data based on real healthcare datasets, or cuts of open-licenced official data.  This package exists to support skills development in the NHS-R community: <https://nhsrcommunity.com/>.",2021-03-14,Tom Jemmett,"https://github.com/nhs-r-community/NHSRdatasets,
https://nhs-r-community.github.io/NHSRdatasets/",TRUE,https://github.com/nhs-r-community/nhsrdatasets,11830,40,2021-08-06T09:27:43Z,295.75
nimble,"A system for writing hierarchical statistical models largely
    compatible with 'BUGS' and 'JAGS', writing nimbleFunctions to operate models
    and do basic R-style math, and compiling both models and nimbleFunctions via
    custom-generated C++. 'NIMBLE' includes default methods for MCMC, Monte Carlo
    Expectation Maximization, and some other tools. The nimbleFunction system makes
    it easy to do things like implement new MCMC samplers from R, customize the
    assignment of samplers to different parts of a model from R, and compile the
    new samplers automatically via C++ alongside the samplers 'NIMBLE' provides.
    'NIMBLE' extends the 'BUGS'/'JAGS' language by making it extensible: New
    distributions and functions can be added, including as calls to external
    compiled code. Although most people think of MCMC as the main goal of the
    'BUGS'/'JAGS' language for writing models, one can use 'NIMBLE' for writing
    arbitrary other kinds of model-generic algorithms as well. A full User Manual is
    available at <https://r-nimble.org>.",2021-05-23,Christopher Paciorek,"https://r-nimble.org, https://github.com/nimble-dev/nimble",TRUE,https://github.com/nimble-dev/nimble,73335,106,2021-08-28T17:00:16Z,691.8396226415094
nimbleEcology,"Common ecological distributions for 'nimble' models in the form of nimbleFunction objects. 
  Includes Cormack-Jolly-Seber, occupancy, dynamic occupancy, hidden Markov, dynamic hidden Markov, and N-mixture models.
  (Jolly (1965) <DOI: 10.2307/2333826>, Seber (1965) <DOI: 10.2307/2333827>, Turek et al. (2016) <doi:10.1007/s10651-016-0353-z>).",2021-05-04,Benjamin R. Goldstein,https://github.com/nimble-dev/nimbleEcology,TRUE,https://github.com/nimble-dev/nimbleecology,13348,9,2021-06-23T17:32:02Z,1483.111111111111
nimbleSMC,"Includes five particle filtering algorithms for use with state space
    models in the 'nimble' system: 'Auxiliary', 'Bootstrap', 'Ensemble Kalman filter',
    'Iterated Filtering 2', and 'Liu-West'. A full User Manual is available at
    <https://r-nimble.org>.",2020-11-09,Christopher Paciorek,"https://r-nimble.org, https://github.com/nimble-dev/nimbleSMC",TRUE,https://github.com/nimble-dev/nimblesmc,3328,0,2021-08-17T20:51:37Z,NA
nipals,Principal Components Analysis of a matrix using Non-linear Iterative Partial Least Squares or weighted Expectation Maximization PCA with Gram-Schmidt orthogonalization of the scores and loadings. Optimized for speed. See Andrecut (2009) <doi:10.1089/cmb.2008.0221>.,2020-01-24,Kevin Wright,http://kwstat.github.io/nipals/,TRUE,https://github.com/kwstat/nipals,23731,6,2021-03-10T14:30:11Z,3955.1666666666665
nipnTK,"An implementation of the National Information Platforms for 
    Nutrition or NiPN's analytic methods for assessing quality of anthropometric 
    datasets that include measurements of weight, height or length, middle upper 
    arm circumference, sex and age. The focus is on anthropometric status but 
    many of the presented methods could be applied to other variables.",2020-11-30,Mark Myatt,"https://nutriverse.io/nipnTK/,
https://github.com/nutriverse/nipnTK",TRUE,https://github.com/nutriverse/nipntk,3047,4,2020-11-30T11:42:33Z,761.75
nitrcbot,Parses and downloads images from various 'NeuroImaging Tools and Resources Collaboratory' <https://www.nitrc.org> sets.,2018-06-01,Adi Gherman,https://www.nitrc.org,TRUE,https://github.com/adigherman/nitrcbot,12016,0,2021-05-12T18:06:49Z,NA
njtr1,"Download and analyze motor vehicle crash data released by the New Jersey Department of Transportation (NJDOT).
  The data in this package is collected through the filing of NJTR-1 form by police officers, which provide a standardized way of documenting a motor vehicle crash that occurred in New Jersey.
  3 different data tables containing data on crashes, vehicles & pedestrians released from 2001 to the present can be downloaded & cleaned using this package.",2021-06-13,Gavin Rozzi,"https://gavinrozzi.github.io/njtr1/,
https://github.com/gavinrozzi/njtr1/,
https://www.gavinrozzi.com/project/njtr1/",TRUE,https://github.com/gavinrozzi/njtr1,3228,1,2021-08-09T04:12:47Z,3228
nlaR,"Client for programmatic access to the 2007 and 2012 National 
  Lakes Assessment database <https://www.epa.gov/national-aquatic-resource-surveys/nla> 
  containing data for hundreds of lakes in the lower 48 states of the contiguous US.",2019-01-22,Joseph Stachelek,https://github.com/jsta/nlaR,TRUE,https://github.com/jsta/nlar,12887,3,2021-08-03T13:48:25Z,4295.666666666667
nlist,"Create and manipulate numeric list ('nlist') objects.  An
    'nlist' is an S3 list of uniquely named numeric objects.  An numeric
    object is an integer or double vector, matrix or array.  An 'nlists'
    object is a S3 class list of 'nlist' objects with the same names,
    dimensionalities and typeofs.  Numeric list objects are of interest
    because they are the raw data inputs for analytic engines such as
    'JAGS', 'STAN' and 'TMB'.  Numeric lists objects, which are useful for
    storing multiple realizations of of simulated data sets, can be
    converted to coda::mcmc and coda::mcmc.list objects.",2021-09-02,Joe Thorley,https://github.com/poissonconsulting/nlist,TRUE,https://github.com/poissonconsulting/nlist,18229,2,2021-09-02T14:54:58Z,9114.5
nlmixr,"Fit and compare nonlinear mixed-effects models in differential
    equations with flexible dosing information commonly seen in pharmacokinetics
    and pharmacodynamics (Almquist, Leander, and Jirstrand 2015 
    <doi:10.1007/s10928-015-9409-1>). Differential equation solving is 
    by compiled C code provided in the 'RxODE' package
    (Wang, Hallow, and James 2015 <doi:10.1002/psp4.12052>).",2021-04-17,Rik Schoemaker,https://github.com/nlmixrdevelopment/nlmixr,TRUE,https://github.com/nlmixrdevelopment/nlmixr,22414,88,2021-08-31T18:02:48Z,254.70454545454547
nlraa,"Additional nonlinear regression functions using self-start (SS) algorithms. One of the functions is the Beta growth function proposed by Yin et al. (2003) <doi:10.1093/aob/mcg029>. There are several other functions with breakpoints (e.g. linear-plateau, plateau-linear, exponential-plateau, plateau-exponential, quadratic-plateau, plateau-quadratic and bilinear), a non-rectangular hyperbola and a bell-shaped curve. Twenty one (21) new self-start (SS) functions in total. This package also supports the publication 'Nonlinear regression Models and applications in agricultural research' by Archontoulis and Miguez (2015) <doi:10.2134/agronj2012.0506>, a book chapter with similar material <doi:10.2134/appliedstatistics.2016.0003.c15> and a publication by Oddi et. al. (2019) in Ecology and Evolution <doi:10.1002/ece3.5543>. The function 'nlsLMList' uses 'nlsLM' for fitting, but it is otherwise almost identical to 'nlme::nlsList'.In addition, this release of the package provides functions for conducting simulations for 'nlme' and 'gnls' objects as well as bootstrapping. These functions are intended to work with the modeling framework of the 'nlme' package. It also provides four vignettes with extended examples.",2021-08-18,Fernando Miguez,NA,TRUE,https://github.com/femiguez/nlraa,12067,8,2021-08-20T13:23:13Z,1508.375
nLTT,"Provides functions to calculate the normalised Lineage-Through-
    Time (nLTT) statistic, given two phylogenetic trees. The nLTT statistic measures
    the difference between two Lineage-Through-Time curves, where each curve is
    normalised both in time and in number of lineages.",2020-01-13,Thijs Janzen,https://github.com/thijsjanzen/nLTT,TRUE,https://github.com/thijsjanzen/nltt,25010,5,2020-12-16T12:47:17Z,5002
nmaINLA,"Performs network meta-analysis using integrated nested Laplace approximations ('INLA') 
             which is described in Guenhan, Held, and Friede (2018) <doi:10.1002/jrsm.1285>. 
             Includes methods to assess the heterogeneity and inconsistency in the network. 
             Contains more than ten different network meta-analysis dataset. 
             'INLA' package can be obtained from <https://www.r-inla.org>. ",2021-07-21,Burak Kuersad Guenhan,https://github.com/gunhanb/nmaINLA,TRUE,https://github.com/gunhanb/nmainla,15664,3,2021-02-23T10:54:03Z,5221.333333333333
NMAoutlier,"A set of functions providing several outlier (i.e., studies with extreme findings) and influential detection measures and methodologies in network meta-analysis:
               - Simple outlier and influential deletion measures provided: (a) Raw, (b) Standardized, (c) Studentized residuals; (d) Mahalanobis distance and (c) leverage.
               - Outlier and influential detection measures by considering study deletion (Shift the mean): (a) Raw (b) Standardized, (c) Studentized deleted residuals; 
                 (d) Cook distance; (e) Ratio of variance-covariance matrix; (f) weight leave one out; (g) leverage leave one out; (h) heterogeneity leave one out; 
                 (i) R heterogeneity; (k) R Q total; (l) R Q heterogeneity (m);  R Q inconsistency and (n) statistic that indicate the effect that deleting each study has on the treatment estimates.
               - Plots for outlier and influential detection simply and deletion measures (all the above measures) and Q-Q plot for network meta-analysis.
               - Forward Search algorithm in network meta-analysis. 
               - Forward plots for the monitoring statistics in each step of Forward search algorithm:
                 (a) P-scores; (b) z-values for difference of direct and indirect evidence with back-calculation method; 
                 (c) Standardized residuals; (d) heterogeneity variance estimator; (e) cook distance; (f) ratio of variances; 
                 (g) Q statistics.
               - Forward plots for summary estimates and their confidence intervals in each step of forward search algorithm.   ",2021-02-01,Maria Petropoulou,https://github.com/petropouloumaria/NMAoutlier,TRUE,https://github.com/petropouloumaria/nmaoutlier,12226,2,2021-05-12T11:35:32Z,6113
NMdata,"Efficient tools for preparation, checking and post-processing of data in PK/PD (pharmacokinetics/pharmacodynamics) modeling, with focus on use of Nonmem. Helps with trivial but tedious tasks and tries to identify errors to save time on debugging. Implemented in 'data.table', but easily integrated with 'base' and 'tidyverse'.",2021-08-09,Philip Delff,https://philipdelff.github.io/NMdata/,TRUE,https://github.com/philipdelff/nmdata,738,0,2021-08-19T15:55:37Z,NA
Nmisc,"Contains functions useful for debugging, set operations on vectors,
    and 'UTC' date and time functionality. It adds a few vector manipulation 
    verbs to 'purrr' and 'dplyr' packages. It can also generate an R file to 
    install and update packages to simplify deployment into production. The 
    functions were developed at the data science firm 'Numeract LLC' and are 
    used in several packages and projects.",2021-04-28,Mike Badescu,https://github.com/numeract/Nmisc,TRUE,https://github.com/numeract/nmisc,13017,0,2021-04-28T13:28:39Z,NA
NMOF,"Functions, examples and data from the first and
  the second edition of ""Numerical Methods and Optimization
  in Finance"" by M. Gilli, D. Maringer and E. Schumann
  (2019, ISBN:978-0128150658).  The package provides
  implementations of optimisation heuristics (Differential
  Evolution, Genetic Algorithms, Particle Swarm
  Optimisation, Simulated Annealing and Threshold
  Accepting), and other optimisation tools, such as grid
  search and greedy search.  There are also functions for
  the valuation of financial instruments such as bonds and
  options, for portfolio selection and functions that help
  with stochastic simulations.",2021-04-10,Enrico Schumann,"http://enricoschumann.net/NMOF.htm,
https://github.com/enricoschumann/NMOF, https://gitlab.com/NMOF",TRUE,https://github.com/enricoschumann/nmof,53910,19,2021-09-03T12:25:12Z,2837.3684210526317
NMproject,"Industrialisation of 'NONMEM'
  <https://www.iconplc.com/innovation/nonmem/> via fully and rapidly reusable 
  model development 'workflows' entirely within 'RStudio'. Quickly get started
  with new models by importing 'NONMEM' templates from the built-in code
  library. Manipulate 'NONMEM' code from within R either via the tracked 
  'manual edit' interface or 'programmatically' via convenience functions. 
  Script 'workflows' by piping sequences of model building steps from control 
  file creation, to execution, to post-processing and evaluation. Run caching 
  makes 'workflows' R markdown friendly for easy documentation of thoughts and
  modelling decisions alongside executable code. Share, reuse and recycle
  'workflows' for new problems.",2021-09-02,Tarj Sahota,"https://tsahota.github.io/NMproject/,
https://github.com/tsahota/NMproject",TRUE,https://github.com/tsahota/nmproject,282,22,2021-09-02T19:01:19Z,12.818181818181818
nmslibR,"A Non-Metric Space Library ('NMSLIB' <https://github.com/nmslib/nmslib>) wrapper, which according to the authors ""is an efficient cross-platform similarity search library and a toolkit for evaluation of similarity search methods. The goal of the 'NMSLIB' <https://github.com/nmslib/nmslib> Library is to create an effective and comprehensive toolkit for searching in generic non-metric spaces. Being comprehensive is important, because no single method is likely to be sufficient in all cases. Also note that exact solutions are hardly efficient in high dimensions and/or non-metric spaces. Hence, the main focus is on approximate methods"". The wrapper also includes Approximate Kernel k-Nearest-Neighbor functions based on the 'NMSLIB' <https://github.com/nmslib/nmslib> 'Python' Library.",2021-05-07,Lampros Mouselimis,https://github.com/mlampros/nmslibR,TRUE,https://github.com/mlampros/nmslibr,14912,9,2021-05-07T16:58:44Z,1656.888888888889
NNbenchmark,"Datasets and functions to benchmark (convergence, speed, ease of use) R packages dedicated to regression with neural networks (no classification in this version). The templates for the tested packages are available in the R, R Markdown and HTML formats at <https://github.com/pkR-pkR/NNbenchmarkTemplates> and <https://theairbend3r.github.io/NNbenchmarkWeb/index.html>. The submitted article to the R-Journal can be read at <https://www.inmodelia.com/gsoc2020.html>.",2021-06-05,Patrice Kiener,https://github.com/pkR-pkR/NNbenchmark,TRUE,https://github.com/pkr-pkr/nnbenchmark,4492,3,2021-08-04T21:29:21Z,1497.3333333333333
nngeo,"K-nearest neighbor search for projected and non-projected 'sf' spatial layers. Nearest neighbor search uses (1) C code from 'GeographicLib' for lon-lat point layers, (2) function knn() from package 'nabor' for projected point layers, or (3) function st_distance() from package 'sf' for line or polygon layers. The package also includes several other utility functions for spatial analysis.",2021-06-13,Michael Dorman,"https://michaeldorman.github.io/nngeo/,
https://github.com/michaeldorman/nngeo/",TRUE,https://github.com/michaeldorman/nngeo,40987,55,2021-08-04T06:46:20Z,745.2181818181818
nnlib2Rcpp,"Contains versions of Autoencoder, BP, LVQ, MAM NN and a module to define custom neural networks.",2021-05-05,Vasilis Nikolaidis,https://github.com/VNNikolaidis/nnlib2Rcpp,TRUE,https://github.com/vnnikolaidis/nnlib2rcpp,5996,8,2021-06-25T13:28:34Z,749.5
NNS,"Nonlinear nonparametric statistics using partial moments.  Partial moments are the elements of variance and asymptotically approximate the area of f(x).  These robust statistics provide the basis for nonlinear analysis while retaining linear equivalences.  NNS offers: Numerical integration, Numerical differentiation, Clustering, Correlation, Dependence, Causal analysis, ANOVA, Regression, Classification, Seasonality, Autoregressive modeling, Normalization and Stochastic dominance.  All routines based on: Viole, F. and Nawrocki, D. (2013), Nonlinear Nonparametric Statistics: Using Partial Moments (ISBN: 1490523995).",2021-08-06,Fred Viole,NA,TRUE,https://github.com/ovvo-financial/nns,45000,24,2021-09-03T12:25:16Z,1875
nnTensor,"Some functions for performing non-negative matrix factorization, non-negative CANDECOMP/PARAFAC (CP) decomposition, non-negative Tucker decomposition, and generating toy model data. See Andrzej Cichock et al (2009) <doi:10.1002/9780470747278> and the reference section of GitHub README.md <https://github.com/rikenbit/nnTensor>, for details of the methods.",2021-09-03,Koki Tsuyuzaki,https://github.com/rikenbit/nnTensor,TRUE,https://github.com/rikenbit/nntensor,18849,8,2021-09-02T08:07:01Z,2356.125
noaastormevents,"Allows users to explore and plot data from the
    National Oceanic and Atmospheric Administration (NOAA) 
    Storm Events database through R for United States counties. 
    Functionality includes matching storm event listings by time and 
    location to hurricane best tracks data. This work was 
    supported by grants from the Colorado Water Center, the National Institute 
    of Environmental Health Sciences (R00ES022631) and the National Science 
    Foundation (1331399). ",2021-01-21,Brooke Anderson,https://github.com/geanders/noaastormevents,TRUE,https://github.com/geanders/noaastormevents,13571,9,2021-01-20T20:50:04Z,1507.888888888889
noah,"Generate pseudonymous animal names that are delightful and easy to 
    remember like the Likable Leech and the Proud Chickadee. A unique pseudonym 
    can be created for every unique element in a vector or row in a data frame. 
    Pseudonyms can be customized and tracked over time, so that the same input 
    is always assigned the same pseudonym.",2021-01-18,Tobias Busch,https://github.com/Teebusch/noah,TRUE,https://github.com/teebusch/noah,2523,3,2021-01-19T09:47:29Z,841
noctua,"Designed to be compatible with the 'R' package 'DBI' (Database Interface)
    when connecting to Amazon Web Service ('AWS') Athena <https://aws.amazon.com/athena/>.
    To do this the 'R' 'AWS' Software Development Kit ('SDK') 'paws' 
    <https://github.com/paws-r/paws> is used as a driver.",2021-07-27,Dyfan Jones,https://github.com/DyfanJones/noctua,TRUE,https://github.com/dyfanjones/noctua,16739,19,2021-08-04T09:21:05Z,881
nodbi,"Simplified document database manipulation and analysis,
    including support for many 'NoSQL' databases, including document 
    databases ('Elasticsearch', 'CouchDB', 'MongoDB'), 
    'key-value' databases ('Redis'), and (with limitations)
    SQLite/json1.",2021-07-23,Ralf Herold,"https://docs.ropensci.org/nodbi/,
https://github.com/ropensci/nodbi",TRUE,https://github.com/ropensci/nodbi,17365,58,2021-07-23T12:27:15Z,299.3965517241379
nodiv,"An implementation of the nodiv algorithm, see Borregaard, M.K., Rahbek, C., Fjeldsaa, J., Parra, J.L., Whittaker, R.J. & Graham, C.H. 2014. Node-based analysis of species distributions. Methods in Ecology and Evolution 5(11): 1225-1235. <DOI:10.1111/2041-210X.12283>. Package for phylogenetic analysis of species distributions. The main function goes through each node in the phylogeny, compares the distributions of the two descendant nodes, and compares the result to a null model. This highlights nodes where major distributional divergence have occurred. The distributional divergence for these nodes is mapped using the SOS statistic.",2020-05-26,Michael Krabbe Borregaard,https://github.com/mkborregaard/nodiv,TRUE,https://github.com/mkborregaard/nodiv,18709,2,2021-01-18T08:31:00Z,9354.5
nofrills,"Provides a compact variation of the usual syntax of function
  declaration, in order to support tidyverse-style quasiquotation of a
  function's arguments and body.",2021-01-08,Eugene Ha,https://github.com/egnha/nofrills,TRUE,https://github.com/egnha/nofrills,15118,37,2021-01-08T18:01:22Z,408.5945945945946
noisyCE2,"Cross-Entropy optimisation of unconstrained deterministic and noisy
    functions illustrated in Rubinstein and Kroese (2004, ISBN:
    978-1-4419-1940-3) through a highly flexible and customisable function which 
    allows user to define custom variable domains, sampling distributions,
    updating and smoothing rules, and stopping criteria. Several built-in
    methods and settings make the package very easy-to-use under standard
    optimisation problems.",2020-11-09,Flavio Santi,https://www.flaviosanti.it/software/noisyCE2,TRUE,https://github.com/f-santi/noisyce2,8977,0,2020-11-03T21:50:04Z,NA
noisyr,"Quantifies and removes technical noise from high-throughput 
        sequencing data. Two approaches are used, one based on the count
        matrix, and one using the alignment BAM files directly.
        Contains several options for every step of the process, as well
        as tools to quality check and assess the stability of output.",2021-04-16,Ilias Moutsopoulos,https://github.com/Core-Bioinformatics/noisyR,TRUE,https://github.com/core-bioinformatics/noisyr,2382,5,2021-06-03T19:29:51Z,476.4
nombre,"Converts numeric vectors to character vectors of
    English number names. Provides conversion to cardinals, ordinals,
    numerators, and denominators. Supports negative and non-integer
    numbers.",2020-09-12,Alexander Rossell Hayes,"https://nombre.rossellhayes.com,
https://github.com/rossellhayes/nombre",TRUE,https://github.com/rossellhayes/nombre,6488,7,2020-09-13T00:33:39Z,926.8571428571429
nomisr,"Access UK official statistics from the 'Nomis' database. 
    'Nomis' includes data from the Census, the Labour Force Survey, DWP benefit 
    statistics and other economic and demographic data from the Office for 
    National Statistics, based around statistical geographies. See 
    <https://www.nomisweb.co.uk/api/v01/help> for full API documentation.",2021-01-23,Evan Odell,"https://github.com/ropensci/nomisr,
https://docs.evanodell.com/nomisr",TRUE,https://github.com/ropensci/nomisr,17538,28,2021-04-04T15:58:38Z,626.3571428571429
nomnoml,"A tool for drawing sassy 'UML' diagrams based on a simple syntax,
  see <https://www.nomnoml.com>. Supports styling, R Markdown and exporting diagrams 
  in the PNG format.",2020-12-15,Andrie de Vries,https://github.com/rstudio/nomnoml,TRUE,https://github.com/rstudio/nomnoml,15093,182,2020-12-17T08:30:27Z,82.92857142857143
nonlinearTseries,"Functions for nonlinear time series analysis. This package permits
    the computation of the  most-used nonlinear statistics/algorithms
    including generalized correlation dimension, information dimension,
    largest Lyapunov exponent, sample entropy and Recurrence
    Quantification Analysis (RQA), among others. Basic routines
    for surrogate data testing are also included. Part of this work
    was based on the  book ""Nonlinear time series analysis"" by
    Holger Kantz and Thomas Schreiber (ISBN: 9780521529020).",2021-05-12,Constantino A. Garcia,https://github.com/constantino-garcia/nonlinearTseries,TRUE,https://github.com/constantino-garcia/nonlineartseries,52123,22,2021-05-12T11:33:43Z,2369.2272727272725
normalr,"The robustness of many of the statistical techniques, such as factor analysis, applied in 
          the social sciences rests upon the assumption of item-level normality. However, when dealing 
          with real data, these assumptions are often not met. The Box-Cox transformation (Box & Cox, 1964)
          <http://www.jstor.org/stable/2984418> provides an optimal transformation for non-normal variables. Yet, for 
          large datasets of continuous variables, its application in current software programs is cumbersome
          with analysts having to take several steps to normalise each variable. We present an R package 
          'normalr' that enables researchers to make convenient optimal transformations of multiple variables
          in datasets. This R package enables users to quickly and accurately: (1) anchor all of their 
          variables at 1.00, (2) select the desired precision with which the optimal lambda is estimated, 
          (3) apply each unique exponent to its variable, (4) rescale resultant values to within their 
          original X1 and X(n) ranges, and (5) provide original and transformed estimates of skewness, 
          kurtosis, and other inferential assessments of normality.",2018-03-30,Kevin Chang,https://github.com/kcha193/normalr,TRUE,https://github.com/kcha193/normalr,21147,2,2021-02-19T23:31:47Z,10573.5
nortsTest,"Despite that several tests for normality in stationary processes have been proposed
   in the literature, consistent implementations of these tests in programming languages are limited. 
   Four normality test are implemented. The Lobato and Velasco's, Epps, Psaradakis and  Vavra, and the 
   random projections tests for stationary process. Some other diagnostics such as, unit root test for 
   stationarity, seasonal tests for seasonality, and arch effect test for volatility; are also performed. 
   The package also offers residual diagnostic for linear time series models developed in several packages.",2021-08-16,Asael Alonzo Matamoros,https://github.com/asael697/nortsTest,TRUE,https://github.com/asael697/nortstest,6136,1,2021-08-16T10:59:58Z,6136
nosoi,"The aim of 'nosoi' (pronounced no.si) is to provide a flexible agent-based stochastic transmission chain/epidemic simulator (Lequime et al. Methods in Ecology and Evolution 11:1002-1007). It is named after the daimones of plague, sickness and disease that escaped Pandora's jar in the Greek mythology. 'nosoi' is able to take into account the influence of multiple variable on the transmission process (e.g. dual-host systems (such as arboviruses), within-host viral dynamics, transportation, population structure), alone or taken together, to create complex but relatively intuitive epidemiological simulations.",2021-08-17,Sebastian Lequime,https://github.com/slequime/nosoi,TRUE,https://github.com/slequime/nosoi,9046,4,2021-08-17T16:01:03Z,2261.5
novelforestSG,"
    The dataset and model used in Lai et al. (2021) 
    Decoupled responses of native and exotic tree diversities to 
    distance from old-growth forest and soil phosphorous in 
    novel secondary forests. Applied Vegetation Science, 24, e12548.",2021-02-20,Hao Ran Lai,"https://hrlai.github.io/novelforestSG/,
https://github.com/hrlai/novelforestSG,
https://doi.org/10.1111/avsc.12548",TRUE,https://github.com/hrlai/novelforestsg,2200,0,2021-02-19T20:43:17Z,NA
np,"Nonparametric (and semiparametric) kernel methods that seamlessly handle a mix of continuous, unordered, and ordered factor data types. We would like to gratefully acknowledge support from the Natural Sciences and Engineering Research Council of Canada (NSERC, <https://www.nserc-crsng.gc.ca/>), the Social Sciences and Humanities Research Council of Canada (SSHRC, <https://www.sshrc-crsh.gc.ca/>), and the Shared Hierarchical Academic Research Computing Network (SHARCNET, <https://sharcnet.ca/>).",2021-06-14,Jeffrey S. Racine,https://github.com/JeffreyRacine/R-Package-np,TRUE,https://github.com/jeffreyracine/r-package-np,331075,36,2021-06-07T21:17:45Z,9196.527777777777
nprcgenekeepr,"Provides genetic tools for colony management and is a derivation 
    of the work in Amanda Vinson and Michael J Raboin (2015) 
    <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4671785/> ""A Practical 
    Approach for Designing Breeding Groups to Maximize Genetic Diversity in a 
    Large Colony of Captive Rhesus Macaques ('Macaca' 'mulatto')"".
    It provides a 'Shiny' application with an exposed API. 
    The application supports five groups of functions: 
    (1) Quality control of studbooks contained in text files or 'Excel' 
    workbooks and of pedigrees within 'LabKey' Electronic Health Records 
    (EHR); 
    (2) Creation of pedigrees from a list of animals using the 'LabKey' EHR 
    integration; 
    (3) Creation and display of an age by sex pyramid plot of the living 
    animals within the designated pedigree; 
    (4) Generation of genetic value analysis reports; and 
    (5) Creation of potential breeding groups with and without proscribed sex 
    ratios and defined maximum kinships.",2021-03-31,R. Mark Sharp,"https://rmsharp.github.io/nprcgenekeepr/,
https://github.com/rmsharp/nprcgenekeepr",TRUE,https://github.com/rmsharp/nprcgenekeepr,4932,0,2021-04-21T16:04:30Z,NA
nse,Collection of functions designed to calculate numerical standard error (NSE) of univariate time series as described in Ardia et al. (2018) <doi:10.1515/jtse-2017-0011> and Ardia and Bluteau (2017) <doi:10.21105/joss.00172>.,2021-04-25,David Ardia,https://github.com/keblu/nse,TRUE,https://github.com/keblu/nse,21611,0,2021-04-25T17:02:41Z,NA
nse2r,"Fetch data related to stocks, index, futures & options from the 
    'NSE (National Stock Exchange, India)'. This package is community maintained 
    and is not officially supported by 'NSE'. The accuracy of data is only as 
    correct as provided on <https://www.nseindia.com>.",2021-05-20,Aravind Hebbali,"https://github.com/rsquaredacademy/nse2r,
https://nse2r.rsquaredacademy.com/",TRUE,https://github.com/rsquaredacademy/nse2r,10777,19,2021-05-21T02:59:47Z,567.2105263157895
nser,Download Historical Bhavcopy and get Live Market data from NSE India of Equities and Derivatives(F&O) segment. Data source  <https://www.nseindia.com/>. ,2021-08-22,Nandan Patil,https://github.com/nandp1/nser/,TRUE,https://github.com/nandp1/nser,932,0,2021-08-29T05:29:26Z,NA
NSO1212,National Statistical Office of Mongolia (NSO) is the national statistical service and an organization of Mongolian government. NSO provides open access to official data via its API <http://opendata.1212.mn/en/doc>. The package NSO1212 has functions for accessing the API service. The functions are compatible with the API v2.0 and get data sets and its detailed informations from the API.,2021-06-22,Makhgal Ganbold,https://github.com/galaamn/NSO1212,TRUE,https://github.com/galaamn/nso1212,11233,4,2021-08-16T01:32:16Z,2808.25
nsrr,"Allows users to access data from the National Sleep
    Research Resource ('NSRR') <https://sleepdata.org/>.",2020-06-24,John Muschelli,https://github.com/muschellij2/nsrr,TRUE,https://github.com/muschellij2/nsrr,11616,4,2021-08-02T15:44:26Z,2904
NST,"To estimate ecological stochasticity in community assembly. Understanding the community assembly mechanisms controlling biodiversity patterns is a central issue in ecology. Although it is generally accepted that both deterministic and stochastic processes play important roles in community assembly, quantifying their relative importance is challenging. The new index, normalized stochasticity ratio (NST), is to estimate ecological stochasticity, i.e. relative importance of stochastic processes, in community assembly. With functions in this package, NST can be calculated based on different similarity metrics and/or different null model algorithms, as well as some previous indexes, e.g. previous Stochasticity Ratio (ST), Standard Effect Size (SES), modified Raup-Crick metrics (RC). Functions for permutational test and bootstrapping analysis are also included. Previous ST is published by Zhou et al (2014) <doi:10.1073/pnas.1324044111>. NST is modified from ST by considering two alternative situations and normalizing the index to range from 0 to 1 (Ning et al 2019) <doi:10.1073/pnas.1904623116>. A modified version, MST, is a special case of NST, used in some recent or upcoming publications, e.g. Liang et al (2020) <doi:10.1016/j.soilbio.2020.108023>. SES is calculated as described in Kraft et al (2011) <doi:10.1126/science.1208584>. RC is calculated as reported by Chase et al (2011) <doi:10.1890/es10-00117.1> and Stegen et al (2013) <doi:10.1038/ismej.2013.93>. Version 3 added NST based on phylogenetic beta diversity, used by Ning et al (2020) <doi:10.1038/s41467-020-18560-z>.",2021-01-08,Daliang Ning,https://github.com/DaliangNing/NST,TRUE,https://github.com/daliangning/nst,11965,4,2021-08-26T00:37:17Z,2991.25
nsyllable,Counts syllables in character vectors for English words.  Imputes syllables as the number of vowel sequences for words not found.  ,2020-11-30,Kenneth Benoit,https://github.com/quanteda/nsyllable,TRUE,https://github.com/quanteda/nsyllable,15536,5,2020-11-28T17:44:03Z,3107.2
nucim,"
    Tools for 4D nucleome imaging. 
    Quantitative analysis of the 3D nuclear landscape recorded with super-resolved fluorescence microscopy. 
    See Volker J. Schmid, Marion Cremer, Thomas Cremer (2017) <doi:10.1016/j.ymeth.2017.03.013>.",2021-06-10,Volker Schmid,https://bioimaginggroup.github.io/nucim/,TRUE,https://github.com/bioimaginggroup/nucim,16388,2,2021-06-09T20:45:05Z,8194
nullabor,"Tools for visual inference. Generate null data sets
    and null plots using permutation and simulation. Calculate distance metrics
    for a lineup, and examine the distributions of metrics.",2020-02-25,Di Cook,http://github.com/dicook/nullabor,TRUE,https://github.com/dicook/nullabor,31417,50,2020-10-25T19:27:19Z,628.34
nvctr,"The n-vector framework uses the normal vector to
    the Earth ellipsoid (called n-vector) as a non-singular position
    representation that turns out to be very convenient for practical
    position calculations.  The n-vector is simple to use and gives exact
    answers for all global positions, and all distances, for both
    ellipsoidal and spherical Earth models.  This package is a translation
    of the 'Matlab' library from FFI, the Norwegian Defence Research
    Establishment, as described in Gade (2010)
    <doi:10.1017/S0373463309990415>.",2020-10-28,Enrico Spinielli,https://github.com/euctrl-pru/nvctr,TRUE,https://github.com/euctrl-pru/nvctr,11141,9,2020-10-28T16:35:18Z,1237.888888888889
nycflights13,"Airline on-time data for all flights departing NYC in 2013.
    Also includes useful 'metadata' on airlines, airports, weather, and
    planes.",2021-04-12,Hadley Wickham,https://github.com/hadley/nycflights13,TRUE,https://github.com/hadley/nycflights13,1410950,92,2021-04-20T13:15:12Z,15336.41304347826
o2geosocial,"Bayesian reconstruction of who infected whom during past outbreaks using routinely-collected surveillance data. Inference of transmission trees using genotype, age specific social contacts, distance between cases and onset dates of the reported cases. (Robert A, Kucharski AJ, Gastanaduy PA, Paul P, Funk S. 2020 <doi:10.1098/rsif.2020.0084>).",2020-09-25,Alexis Robert,https://github.com/alxsrobert/o2geosocial,TRUE,https://github.com/alxsrobert/o2geosocial,4300,5,2021-08-09T16:51:06Z,860
oai,"A general purpose client to work with any 'OAI-PMH'
    (Open Archives Initiative Protocol for 'Metadata' Harvesting) service.
    The 'OAI-PMH' protocol is described at
    <http://www.openarchives.org/OAI/openarchivesprotocol.html>.
    Functions are provided to work with the 'OAI-PMH' verbs: 'GetRecord',
    'Identify', 'ListIdentifiers', 'ListMetadataFormats', 'ListRecords', and
    'ListSets'.",2021-05-13,Scott Chamberlain,"https://docs.ropensci.org/oai/, https://github.com/ropensci/oai",TRUE,https://github.com/ropensci/oai,99057,12,2021-06-10T21:41:24Z,8254.75
obfuscatoR,"When people make decisions, they may do so using a wide variety of decision rules. The package allows users to easily create obfuscation games to test the obfuscation hypothesis. It provides an easy to use interface and multiple options designed to vary the difficulty of the game and tailor it to the user's needs. For more detail: Chorus et al., 2021, Obfuscation maximization-based decision-making: Theory, methodology and first empirical evidence, Mathematical Social Sciences, 109, 28-44. ",2020-11-19,Erlend Dancke Sandorf,"https://obfuscator.edsandorf.me,
https://github.com/edsandorf/obfuscatoR",TRUE,https://github.com/edsandorf/obfuscator,10439,0,2020-12-14T07:46:35Z,NA
objectremover,"An 'RStudio' addin to assist with removing objects from the global environment. Features include removing objects according to name patterns and object type. During the course of an analysis, temporary objects are often created and this tool assists with removing them quickly. This can be useful when memory management within 'R' is important.",2021-08-16,Alan Yeung,https://github.com/alan-y/objectremover,TRUE,https://github.com/alan-y/objectremover,10961,1,2021-08-15T19:59:31Z,10961
occCite,"Facilitates the gathering of biodiversity occurrence data 
  from disparate sources. Metadata is managed throughout the process to facilitate 
  reporting and enhanced ability to repeat analyses.",2021-07-23,Hannah L. Owens,https://hannahlowens.github.io/occCite/,TRUE,https://github.com/hannahlowens/occcite,4725,12,2021-07-26T13:00:02Z,393.75
oce,"Supports the analysis of Oceanographic data, including 'ADCP'
    measurements, measurements made with 'argo' floats, 'CTD' measurements,
    sectional data, sea-level time series, coastline and topographic data, etc.
    Provides specialized functions for calculating seawater properties such as
    potential temperature in either the 'UNESCO' or 'TEOS-10' equation of state.
    Produces graphical displays that conform to the conventions of the
    Oceanographic literature. This package is discussed extensively in
    Dan Kelley's book Oceanographic Analysis with R, published
    in 2018 by 'Springer-Verlag' with ISBN 978-1-4939-8842-6.",2021-03-28,Dan Kelley,https://dankelley.github.io/oce/,TRUE,https://github.com/dankelley/oce,106260,107,2021-08-26T17:45:39Z,993.0841121495328
oceanis,"Creating maps for statistical analysis such as proportional circles, chroropleth, typology and flows. Some functions use 'shiny' or 'leaflet' technologies for dynamism and interactivity.
	The great features are :
	- Create maps in a web environment where the parameters are modifiable on the fly ('shiny' and 'leaflet' technology).
	- Create interactive maps through zoom and pop-up ('leaflet' technology).
	- Create frozen maps with the possibility to add labels.",2021-04-26,Sébastien CALVET - PSAR-AT - DR Provence-Alpes-Cote dAzur - INSEE,https://github.com/insee-psar-at/oceanis-package/,TRUE,https://github.com/insee-psar-at/oceanis-package,16915,10,2021-07-13T09:10:00Z,1691.5
oceanwaves,"Calculate ocean wave height summary statistics and process data 
    from bottom-mounted pressure sensor data loggers. Derived primarily from 
    MATLAB functions provided by U. Neumeier at 
    <http://neumeier.perso.ch/matlab/waves.html>. Wave number 
    calculation based on the algorithm in Hunt, J. N. (1979, ISSN:0148-9895) 
    ""Direct Solution of Wave Dispersion Equation"", American Society of Civil 
    Engineers Journal of the Waterway, Port, Coastal, and Ocean Division, 
    Vol 105, pp 457-459.",2021-06-02,Luke Miller,"https://github.com/millerlp/oceanwaves,
https://millerlp.github.io/oceanwaves/",TRUE,https://github.com/millerlp/oceanwaves,8861,3,2021-06-11T03:55:17Z,2953.6666666666665
ocedata,"Several Oceanographic data sets are provided for use
    by the 'oce' package and for other purposes.",2020-06-06,Dan Kelley,https://dankelley.github.io/ocedata,TRUE,https://github.com/dankelley/ocedata,26414,5,2021-01-06T13:34:36Z,5282.8
ocs4R,Provides an Interface to Open Collaboration Services 'OCS' (<https://www.open-collaboration-services.org/>) REST API.,2021-07-05,Emmanuel Blondel,https://github.com/eblondel/ocs4R,TRUE,https://github.com/eblondel/ocs4r,7004,4,2021-07-05T07:19:17Z,1751
OCSdata,"
    Provides functions to access and download data from the 'Open Case Studies' <https://www.opencasestudies.org/> 
    repositories on 'GitHub' <https://github.com/opencasestudies>. Different functions enable 
    users to grab the data they need at different sections in the case study, as well as 
    download the whole case study repository. All the user needs to do is input the name of 
    the case study being worked on. The package relies on the httr::GET() function to access
    files through the 'GitHub' API. The functions usethis::use_zip() and usethis::create_from_github() 
    are used to clone and/or download the case study repositories. To cite an individual case study,
    please see the respective 'README' file at <https://github.com/opencasestudies/>.
    <https://github.com/opencasestudies/ocs-bp-rural-and-urban-obesity> 
    <https://github.com/opencasestudies/ocs-bp-air-pollution>
    <https://github.com/opencasestudies/ocs-bp-vaping-case-study>
    <https://github.com/opencasestudies/ocs-bp-opioid-rural-urban>
    <https://github.com/opencasestudies/ocs-bp-RTC-wrangling>
    <https://github.com/opencasestudies/ocs-bp-RTC-analysis>
    <https://github.com/opencasestudies/ocs-bp-youth-disconnection>
    <https://github.com/opencasestudies/ocs-bp-youth-mental-health>
    <https://github.com/opencasestudies/ocs-bp-school-shootings-dashboard>
    <https://github.com/opencasestudies/ocs-bp-co2-emissions>
    <https://github.com/opencasestudies/ocs-bp-diet>.",2021-08-20,Carrie Wright,"https://github.com/opencasestudies/OCSdata,
https://doi.org/10.5281/zenodo.5214347,
https://www.opencasestudies.org/",TRUE,https://github.com/opencasestudies/ocsdata,404,1,2021-09-01T22:23:43Z,404
od,"The aim of 'od' is to provide tools and example datasets for working with
  origin-destination ('OD') datasets of the type used to describe aggregate
  urban mobility patterns (Carey et al. 1981) <doi:10.1287/trsc.15.1.32>.
  The package builds on functions for working with 'OD' data in the package 'stplanr',
  (Lovelace and Ellison 2018) <doi:10.32614/RJ-2018-053> with a focus on computational
  efficiency and support for  the 'sf' class system (Pebesma 2018) <doi:10.32614/RJ-2018-009>.
  With few dependencies and a simple class system based on data frames,
  the package is intended to facilitate efficient analysis of 'OD' datasets
  and to provide a place for developing new functions.
  The package enables the creation and analysis of geographic entities
  representing large scale mobility patterns,
  from daily travel between zones in cities to migration between countries.",2021-07-11,Robin Lovelace,"https://github.com/itsleeds/od, https://itsleeds.github.io/od/",TRUE,https://github.com/itsleeds/od,5595,19,2021-07-10T22:44:10Z,294.4736842105263
odbc,A DBI-compatible interface to ODBC databases.,2021-04-03,Jim Hester,"https://github.com/r-dbi/odbc, https://db.rstudio.com",TRUE,https://github.com/r-dbi/odbc,2254035,300,2021-04-05T13:21:55Z,7513.45
OddsPlotty,"Uses the outputs of a logistic regression model, from caret <https://CRAN.R-project.org/package=caret>, to build an odds plot.
    This allows for the rapid visualisation of odds plot ratios and works best with the outputs of CARET's GLM model class, by returning the final trained model. ",2021-06-22,Gary Hutson,https://github.com/StatsGary/OddsPlotty,TRUE,https://github.com/statsgary/oddsplotty,987,12,2021-07-26T09:14:40Z,82.25
oddsratio,"Simplified odds ratio calculation of GAM(M)s &
    GLM(M)s. Provides structured output (data frame) of all predictors and
    their corresponding odds ratios and confident intervals for further
    analyses.  It helps to avoid false references of predictors and
    increments by specifying these parameters in a list instead of using
    'exp(coef(model))' (standard approach of odds ratio calculation for
    GLMs) which just returns a plain numeric output.  For GAM(M)s, odds
    ratio calculation is highly simplified with this package since it
    takes care of the multiple 'predict()' calls of the chosen predictor
    while holding other predictors constant. Also, this package allows
    odds ratio calculation of percentage steps across the whole predictor
    distribution range for GAM(M)s.  In both cases, confident intervals
    are returned additionally. Calculated odds ratio of GAM(M)s can be
    inserted into the smooth function plot.",2020-05-24,Patrick Schratz,https://github.com/pat-s/oddsratio,TRUE,https://github.com/pat-s/oddsratio,43947,28,2021-07-16T07:14:08Z,1569.5357142857142
odeGUTS,"Allows performing forwards prediction for the General Unified 
          Threshold model of Survival using compiled ode code. This package 
          was created to avoid dependency with the 'morse' package that requires 
          the installation of 'JAGS'. This package is based on functions from 
          the 'morse' package v3.3.1: Virgile Baudrot, Sandrine Charles, 
          Marie Laure Delignette-Muller, Wandrille Duchemin, Benoit Goussen, 
          Nils Kehrein, Guillaume Kon-Kam-King, Christelle Lopes, Philippe Ruiz, 
          Alexander Singer and Philippe Veber (2021) <https://CRAN.R-project.org/package=morse>.",2021-08-13,Benoit Goussen,https://github.com/bgoussen/odeGUTS,TRUE,https://github.com/bgoussen/odeguts,266,2,2021-08-30T09:01:55Z,133
odin,"Generate systems of ordinary differential equations
    (ODE) and integrate them, using a domain specific language
    (DSL).  The DSL uses R's syntax, but compiles to C in order to
    efficiently solve the system.  A solver is not provided, but
    instead interfaces to the packages 'deSolve' and 'dde' are
    generated.  With these, while solving the differential equations,
    no allocations are done and the calculations remain entirely in
    compiled code.  Alternatively, a model can be transpiled to R for
    use in contexts where a C compiler is not present.  After
    compilation, models can be inspected to return information about
    parameters and outputs, or intermediate values after calculations.
    'odin' is not targeted at any particular domain and is suitable
    for any system that can be expressed primarily as mathematical
    expressions.  Additional support is provided for working with
    delays (delay differential equations, DDE), using interpolated
    functions during interpolation, and for integrating quantities
    that represent arrays.",2021-07-14,Rich FitzJohn,https://github.com/mrc-ide/odin,TRUE,https://github.com/mrc-ide/odin,24342,76,2021-08-03T11:56:27Z,320.2894736842105
oenb,"Tools to access data from the data web service of the Oesterreichische Nationalbank (OeNB), <https://www.oenb.at/en/Statistics/User-Defined-Tables/webservice.html>.",2021-03-22,Franz X. Mohr,https://github.com/franzmohr/oenb,TRUE,https://github.com/franzmohr/oenb,7981,0,2021-03-22T11:10:12Z,NA
officedown,"Allows production of 'Microsoft' corporate documents from 'R Markdown' by 
 reusing formatting defined in 'Microsoft Word' documents. You can reuse table styles, 
 list styles but also add column sections, landscape oriented pages. Table and image 
 captions as well as cross-references are transformed into 'Microsoft Word' fields, 
 allowing documents edition and merging without issue with references; the syntax 
 conforms to the 'bookdown' cross-reference definition. Objects generated by 
 the 'officer' package are also supported in the 'knitr' chunks. 
 'Microsoft PowerPoint' presentations also benefit from this as well as the 
 ability to produce editable vector graphics in 'PowerPoint' and also to 
 define placeholder where content is to be added.",2021-04-05,David Gohel,"https://ardata-fr.github.io/officeverse/,
https://davidgohel.github.io/officedown/",TRUE,https://github.com/davidgohel/officedown,17400,299,2021-08-03T13:26:29Z,58.19397993311037
officer,"Access and manipulate 'Microsoft Word' and 'Microsoft PowerPoint' documents from R. 
  The package focuses on tabular and graphical reporting from R; it also provides two functions
  that let users get document content into data objects. A set of functions 
  lets add and remove images, tables and paragraphs of text in new or existing documents. 
  The package does not require any installation of Microsoft products to be able to write Microsoft 
  files.",2021-07-21,David Gohel,"https://ardata-fr.github.io/officeverse/,
https://davidgohel.github.io/officer/",TRUE,https://github.com/davidgohel/officer,1390017,441,2021-09-02T10:35:35Z,3151.9659863945576
ohenery,"Supports the modeling of ordinal random variables, 
    like the outcomes of races, via Softmax regression,
    under the Harville <doi:10.1080/01621459.1973.10482425> and
    Henery <doi:10.1111/j.2517-6161.1981.tb01153.x> models.",2019-10-15,Steven E. Pav,https://github.com/shabbychef/ohenery,TRUE,https://github.com/shabbychef/ohenery,8467,3,2021-04-03T23:11:49Z,2822.3333333333335
OHPL,"Ordered homogeneity pursuit lasso (OHPL)
    algorithm for group variable selection proposed in Lin et al. (2017)
    <DOI:10.1016/j.chemolab.2017.07.004>. The OHPL method exploits the
    homogeneity structure in high-dimensional data and enjoys the
    grouping effect to select groups of important variables
    automatically. This feature makes it particularly useful for
    high-dimensional datasets with strongly correlated variables,
    such as spectroscopic data.",2019-05-18,Nan Xiao,"https://ohpl.io, https://ohpl.io/doc/,
https://github.com/nanxstats/OHPL",TRUE,https://github.com/nanxstats/ohpl,15566,4,2021-03-04T02:25:55Z,3891.5
olsrr,"Tools designed to make it easier for users, particularly beginner/intermediate R users 
    to build ordinary least squares regression models. Includes comprehensive regression output, 
    heteroskedasticity tests, collinearity diagnostics, residual diagnostics, measures of influence, 
    model fit assessment and variable selection procedures.",2020-02-10,Aravind Hebbali,"https://olsrr.rsquaredacademy.com/,
https://github.com/rsquaredacademy/olsrr",TRUE,https://github.com/rsquaredacademy/olsrr,276562,91,2021-08-05T08:37:20Z,3039.1428571428573
OmicNavigator,"
  A tool for interactive exploration of the results from 'omics'
  experiments to facilitate novel discoveries from high-throughput biology. The
  software includes R functions for the 'bioinformatician' to deposit study
  metadata and the outputs from statistical analyses (e.g. differential
  expression, enrichment). These results are then exported to an interactive
  JavaScript dashboard that can be interrogated on the user’s local machine or
  deployed online to be explored by collaborators. The dashboard includes
  'sortable' tables, interactive plots including network visualization, and
  fine-grained filtering based on statistical significance.",2021-08-05,Terrence Ernst,https://github.com/abbvie-external/OmicNavigator,TRUE,https://github.com/abbvie-external/omicnavigator,593,12,2021-09-03T14:04:51Z,49.416666666666664
omicwas,"In bulk epigenome/transcriptome experiments, molecular expression
    is measured in a tissue, which is a mixture of multiple types of cells.
    This package tests association of a disease/phenotype with a molecular marker
    for each cell type.
    The proportion of cell types in each sample needs to be given as input.
    The package is applicable to epigenome-wide association study (EWAS) and
    differential gene expression analysis.
    Takeuchi and Kato (submitted)
    ""omicwas: cell-type-specific epigenome-wide and transcriptome association study"".",2020-10-08,Fumihiko Takeuchi,https://github.com/fumi-github/omicwas,TRUE,https://github.com/fumi-github/omicwas,6885,1,2021-03-23T04:31:08Z,6885
ompr,"Model mixed integer linear programs in an algebraic way directly in R.
             The model is solver-independent and thus offers the possibility
             to solve a model with different solvers. It currently only supports
             linear constraints and objective functions. See the 'ompr'
             website <https://dirkschumacher.github.io/ompr/> for more information,
             documentation and examples.",2020-12-04,Dirk Schumacher,https://github.com/dirkschumacher/ompr,TRUE,https://github.com/dirkschumacher/ompr,35335,209,2021-04-28T12:21:14Z,169.06698564593302
omsvg,"Build 'SVG' components using element-based functions. With
    an 'svg' object, we can modify its graphical elements with a suite of
    transform functions.",2021-02-10,Richard Iannone,https://github.com/rich-iannone/omsvg,TRUE,https://github.com/rich-iannone/omsvg,2389,45,2021-02-24T16:09:05Z,53.08888888888889
onbabynames,"A database containing the names 
  of the babies born in Ontario between 1917 and 2018. 
  Counts of fewer than 5 names were suppressed for privacy.",2021-05-03,Marc-Andre Desautels,<https://github.com/desautm/onbabynames>,TRUE,https://github.com/desautm/onbabynames,3168,0,2021-05-04T11:49:39Z,NA
onbrand,Automated reporting in Word and PowerPoint can require customization for each organizational template. This package works around this by adding standard reporting functions and an abstraction layer to facilitate automated reporting workflows that can be replicated across different organizational templates.,2021-09-02,John Harrold,https://onbrand.ubiquity.tools/,TRUE,https://github.com/john-harrold/onbrand,723,7,2021-08-28T14:35:30Z,103.28571428571429
oneclust,"Maximum homogeneity clustering algorithm for one-dimensional data
    described in W. D. Fisher (1958) <doi:10.1080/01621459.1958.10501479>
    via dynamic programming.",2020-09-01,Nan Xiao,"https://nanx.me/oneclust/, https://github.com/nanxstats/oneclust",TRUE,https://github.com/nanxstats/oneclust,4120,4,2021-07-17T06:20:40Z,1030
onemap,"Analysis of molecular marker data from model (backcrosses,
    F2 and recombinant inbred lines) and non-model systems (i. e.
    outcrossing species). For the later, it allows statistical
    analysis by simultaneously estimating linkage and linkage
    phases (genetic map construction) according to Wu et al. (2002)
    <doi:10.1006/tpbi.2002.1577>. All analysis are based on multipoint 
    approaches using hidden Markov models.",2020-02-17,Gabriel Margarido,https://github.com/augusto-garcia/onemap,TRUE,https://github.com/augusto-garcia/onemap,23377,23,2021-03-17T18:16:16Z,1016.3913043478261
onepass,Interaction with 1Password via the command-line tool <https://1password.com/downloads/command-line/> to read vault contents and download credentials.,2021-05-28,Jonathan Lin,"https://jonlinca.github.io/onepass/,
https://github.com/jonlinca/onepass",TRUE,https://github.com/jonlinca/onepass,2782,4,2021-05-28T04:33:15Z,695.5
ONEST,"
    This ONEST software implements the method of assessing the pathologist agreement in reading PD-L1 assays (Reisenbichler et al. (2020 <doi:10.1038/s41379-020-0544-x>)), to determine the minimum number of evaluators needed to estimate agreement involving a large number of raters. Input to the program should be binary(1/0) pathology data, where “0” may stand for negative and “1” for positive. Additional examples were given using the data from Rimm et al. (2017 <doi:10.1001/jamaoncol.2017.0013>). ",2021-07-26,Gang Han,https://github.com/hangangtrue/ONEST,TRUE,https://github.com/hangangtrue/onest,4171,0,2021-07-27T14:28:44Z,NA
onion,"
  Quaternions and Octonions are four- and eight- dimensional
  extensions of the complex numbers.  They are normed division
  algebras over the real numbers and find applications in spatial
  rotations (quaternions), and string theory and relativity
  (octonions).  The quaternions are noncommutative and the octonions
  nonassociative.  See the package vignette for more details.",2021-02-11,Robin K. S. Hankin,https://github.com/RobinHankin/onion,TRUE,https://github.com/robinhankin/onion,28592,2,2021-05-06T20:37:02Z,14296
onlineretail,"Transactions occurring for a UK-based and registered, non-store online 
    retail between 01/12/2010 and 09/12/2011 (Chen et. al., 2012, <doi:10.1145/1835804.1835882>). 
    This dataset is included in this package with the donor's permission, Dr. Daqing Chen.",2021-05-15,Allan Quadros,"https://github.com/allanvc/onlineretail/,
https://doi.org/10.1057/dbm.2012.17,
https://www.researchgate.net/profile/Daqing-Chen",TRUE,https://github.com/allanvc/onlineretail,4658,2,2021-05-14T00:28:05Z,2329
onnx,"R Interface to 'ONNX' - Open Neural Network Exchange <https://onnx.ai/>. 
             'ONNX' provides an open source format for machine learning models. 
             It defines an extensible computation graph model, as well as definitions
             of built-in operators and standard data types.",2021-04-16,Yuan Tang,https://github.com/onnx/onnx-r,TRUE,https://github.com/onnx/onnx-r,12992,33,2021-07-24T14:59:53Z,393.6969696969697
onsr,"Client for the 'Office of National Statistics'
    ('ONS') API <https://api.beta.ons.gov.uk/v1>.  ",2021-04-15,Kostas Vasilopoulos,https://kvasilopoulos.github.io/onsr/,TRUE,https://github.com/kvasilopoulos/onsr,1544,1,2021-04-18T16:54:14Z,1544
ontoFAST,Tools for annotating characters (character matrices) with anatomical and phenotype ontologies. Includes functions for visualising character annotations and creating simple queries using ontological relationships.,2021-04-29,Sergei Tarasov,https://github.com/sergeitarasov/ontoFAST,TRUE,https://github.com/sergeitarasov/ontofast,1507,0,2021-05-13T10:06:20Z,NA
oolong,"Intended to create standard human-in-the-loop validity tests for typical automated content analysis such as topic modeling and dictionary-based methods. This package offers a standard workflow with functions to prepare, administer and evaluate a human-in-the-loop validity test. This package provides functions for validating topic models using word intrusion, topic intrusion (Chang et al. 2009,  <https://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models>) and word set intrusion (Ying et al. Forthcoming) tests. This package also provides functions for generating gold-standard data which are useful for validating dictionary-based methods. The default settings of all generated tests match those suggested in Chang et al. (2009) and Song et al. (2020) <doi:10.1080/10584609.2020.1723752>.",2021-05-31,Chung-hong Chan,https://github.com/chainsawriot/oolong,TRUE,https://github.com/chainsawriot/oolong,6691,37,2021-06-18T18:06:20Z,180.83783783783784
OOS,"A comprehensive and cohesive API for the out-of-sample forecasting workflow: 
             data preparation, forecasting - including both traditional econometric time series models and 
             modern machine learning techniques - forecast combination, model and error analysis, and 
             forecast visualization. ",2021-03-17,Tyler J. Pike,"https://github.com/tylerJPike/OOS,
https://tylerjpike.github.io/OOS/",TRUE,https://github.com/tylerjpike/oos,1455,3,2021-03-18T19:14:47Z,485
opalr,"Data integration Web application for biobanks by 'OBiBa'. 'Opal' is
    the core database application for biobanks. Participant data, once
    collected from any data source, must be integrated and stored in a central
    data repository under a uniform model. 'Opal' is such a central repository.
    It can import, process, validate, query, analyze, report, and export data.
    'Opal' is typically used in a research center to analyze the data acquired at
    assessment centres. Its ultimate purpose is to achieve seamless
    data-sharing among biobanks. This 'Opal' client allows to interact with 'Opal'
    web services and to perform operations on the R server side. 'DataSHIELD'
    administration tools are also provided.",2021-08-23,Yannick Marcon,"https://github.com/obiba/opalr/, https://www.obiba.org/opalr/,
https://www.obiba.org/pages/products/opal/,
https://academic.oup.com/ije/article/46/5/1372/4102813,
https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008880,
https://www.datashield.org/",TRUE,https://github.com/obiba/opalr,22577,3,2021-08-23T09:31:35Z,7525.666666666667
opdisDownsampling,An optimized method for distribution-preserving class-proportional down-sampling of bio-medical data.,2021-08-08,Jorn Lotsch,https://github.com/JornLotsch/opdisDownsampling,TRUE,https://github.com/jornlotsch/opdisdownsampling,2077,0,2021-08-31T05:00:27Z,NA
openadds,"'Openaddresses' (<https://openaddresses.io/>) client. Search,
    fetch data, and combine 'datasets'. Outputs are easy to visualize
    with base plots, 'ggplot2', or 'leaflet'.",2017-01-03,Scott Chamberlain,https://github.com/sckott/openadds,TRUE,https://github.com/sckott/openadds,16341,9,2021-03-17T16:35:47Z,1815.6666666666667
openair,"Tools to analyse, interpret and understand air
    pollution data. Data are typically hourly time series
    and both monitoring data and dispersion model output
    can be analysed.  Many functions can also be applied to
    other data, including meteorological and traffic data.",2021-03-16,David Carslaw,https://davidcarslaw.github.io/openair/,TRUE,https://github.com/davidcarslaw/openair,187197,207,2021-03-17T18:10:18Z,904.3333333333334
opencage,"Geocode with the OpenCage API, either from place name to longitude 
    and latitude (forward geocoding) or from longitude and latitude to the name 
    and address of a location (reverse geocoding), see 
    <https://opencagedata.com/>.",2021-02-20,Daniel Possenriede,"https://docs.ropensci.org/opencage/,
https://github.com/ropensci/opencage",TRUE,https://github.com/ropensci/opencage,20932,80,2021-08-02T19:24:24Z,261.65
opendatatoronto,"Access data from the ""City of Toronto
    Open Data Portal"" (<https://open.toronto.ca>) directly from R.",2020-10-14,Sharla Gelfand,"https://sharlagelfand.github.io/opendatatoronto/,
https://github.com/sharlagelfand/opendatatoronto/",TRUE,https://github.com/sharlagelfand/opendatatoronto,16336,56,2021-01-24T02:33:10Z,291.7142857142857
OpenImageR,"Incorporates functions for image preprocessing, filtering and image recognition. The package takes advantage of 'RcppArmadillo' to speed up computationally intensive functions. The histogram of oriented gradients descriptor is a modification of the 'findHOGFeatures' function of the 'SimpleCV' computer vision platform, the average_hash(), dhash() and phash() functions are based on the 'ImageHash' python library. The Gabor Feature Extraction functions are based on 'Matlab' code of the paper, ""CloudID: Trustworthy cloud-based and cross-enterprise biometric identification"" by M. Haghighat, S. Zonouz, M. Abdel-Mottaleb, Expert Systems with Applications, vol. 42, no. 21, pp. 7905-7916, 2015, <doi:10.1016/j.eswa.2015.06.025>. The 'SLIC' and 'SLICO' superpixel algorithms were explained in detail in (i) ""SLIC Superpixels Compared to State-of-the-art Superpixel Methods"", Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Suesstrunk, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, num. 11, p. 2274-2282, May 2012, <doi:10.1109/TPAMI.2012.120> and (ii) ""SLIC Superpixels"", Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Suesstrunk, EPFL Technical Report no. 149300, June 2010.",2021-05-21,Lampros Mouselimis,https://github.com/mlampros/OpenImageR,TRUE,https://github.com/mlampros/openimager,105635,45,2021-05-21T07:59:13Z,2347.4444444444443
openintro,"Supplemental functions and data for 'OpenIntro' resources, which 
    includes open-source textbooks and resources for introductory statistics 
    (<https://www.openintro.org/>). The package contains data sets used in our 
    open-source textbooks along with custom plotting functions for reproducing 
    book figures. Note that many functions and examples include color 
    transparency; some plotting elements may not show up properly (or at all) 
    when run in some versions of Windows operating system.",2021-06-22,Mine Çetinkaya-Rundel,"http://openintrostat.github.io/openintro/,
https://github.com/OpenIntroStat/openintro/",TRUE,https://github.com/openintrostat/openintro,204476,193,2021-09-02T15:20:22Z,1059.461139896373
openmetrics,"Provides a client for the open-source monitoring and alerting
  toolkit, 'Prometheus', that emits metrics in the 'OpenMetrics' format. Allows
  users to automatically instrument 'Plumber' and 'Shiny' applications, collect
  standard process metrics, as well as define custom counter, gauge, and
  histogram metrics of their own.",2020-11-09,Aaron Jacobs,https://github.com/atheriel/openmetrics,TRUE,https://github.com/atheriel/openmetrics,8068,24,2021-07-05T16:10:22Z,336.1666666666667
OpenML,"We provide an R interface to 'OpenML.org' which is an online machine learning platform where researchers can access open data, download and upload data sets, share their machine learning tasks and experiments and organize them online to work and collaborate with other researchers. 
    The R interface allows to query for data sets with specific properties, and allows the downloading and uploading of data sets, tasks, flows and runs. 
    See <https://www.openml.org/guide/api> for more information.",2019-09-21,Giuseppe Casalicchio,https://github.com/openml/openml-r,TRUE,https://github.com/openml/openml-r,23931,84,2020-10-02T13:28:10Z,284.89285714285717
openMSE,"The 'openMSE' package is designed for building operating models, 
    doing simulation modelling and management strategy evaluation for fisheries.
    'openMSE' is an umbrella package for the 'MSEtool' (Management Strategy Evaluation
    toolkit), 'DLMtool' (Data-Limited Methods toolkit), and 
    SAMtool (Stock Assessment Methods toolkit) packages. By loading and installing
    'openMSE', users have access to the full functionality contained within
    these packages. Learn more about 'openMSE' at <https://openmse.com/>.",2021-02-08,Adrian Hordyk,"https://openmse.com/, https://github.com/Blue-Matter/openMSE",TRUE,https://github.com/blue-matter/openmse,1836,0,2021-08-25T22:35:34Z,NA
OpenMx,"Create structural equation models that can be manipulated programmatically.
    Models may be specified with matrices or paths (LISREL or RAM)
    Example models include confirmatory factor, multiple group, mixture
    distribution, categorical threshold, modern test theory, differential
    Fit functions include full information maximum likelihood, maximum likelihood, and weighted least squares.
    equations, state space, and many others.
	Support and advanced package binaries available at <http://openmx.ssri.psu.edu>.
    The software is described in Neale, Hunter, Pritikin, Zahery, Brick,
    Kirkpatrick, Estabrook, Bates, Maes, & Boker (2016) <doi:10.1007/s11336-014-9435-8>.",2021-06-28,Joshua N. Pritikin,"http://openmx.ssri.psu.edu, https://github.com/OpenMx/OpenMx",TRUE,https://github.com/openmx/openmx,491401,62,2021-09-02T23:29:24Z,7925.822580645161
OpenRepGrid,"Analyze repertory grids, a qualitative-quantitative 
    data collection technique devised by George A. Kelly in the 1950s. Today, grids are used across 
    various domains ranging from clinical psychology to marketing. The package contains
    functions to quantitatively analyze and visualize repertory grid data 
    (see e.g. Bell, 2005, <doi:10.1002/0470013370.ch9>; 
    Fransella, Bell, & Bannister, 2004, ISBN: 978-0-470-09080-0).",2018-05-31,Mark Heckmann,"http://openrepgrid.org,
https://github.com/markheckmann/OpenRepGrid",TRUE,https://github.com/markheckmann/openrepgrid,18110,14,2021-04-29T07:31:38Z,1293.5714285714287
OpenRepGrid.ic,"Shiny UI to identify cliques of related constructs in repertory grid data. 
    See Burr, King, & Heckmann (2020) <doi:10.1080/14780887.2020.1794088> for a description 
    of the interpretive clustering (IC) method.",2021-06-26,Mark Heckmann,https://github.com/markheckmann/OpenRepGrid.ic,TRUE,https://github.com/markheckmann/openrepgrid.ic,5671,1,2021-06-27T09:52:22Z,5671
openSkies,"Provides functionalities and data structures to retrieve, analyze and visualize aviation 
    data. It includes a client interface to the 'OpenSky' API <https://opensky-network.org>. It allows 
    retrieval of flight information, as well as aircraft state vectors.",2021-08-23,Rafael Ayala,NA,TRUE,https://github.com/rafael-ayala/openskies,4848,2,2021-08-23T08:55:06Z,2424
OpenSpecy,"Raman and (FT)IR spectral analysis tool for plastic particles and
    other environmental samples (Cowger et al. 2021,
    <doi:10.1021/acs.analchem.1c00123>). Supported features include reading
    spectral data files (.asp, .csv, .jdx, .spc, .spa, .0), Savitzky-Golay
    smoothing of spectral intensities with smooth_intens(), correcting
    background noise with subtr_bg() in accordance with Zhao et al. (2007)
    <doi:10.1366/000370207782597003>, and identifying spectra using an onboard
    reference library (Cowger et al. 2020, <doi:10.1177/0003702820929064>).
    Analyzed spectra can be shared with the Open Specy community. A Shiny app is
    available via run_app() or online at
    <https://wincowger.shinyapps.io/OpenSpecy/>.",2021-05-20,Win Cowger,https://github.com/wincowgerDEV/OpenSpecy,TRUE,https://github.com/wincowgerdev/openspecy,1814,5,2021-06-01T18:04:47Z,362.8
openssl,"Bindings to OpenSSL libssl and libcrypto, plus custom SSH key parsers.
    Supports RSA, DSA and EC curves P-256, P-384, P-521, and curve25519. Cryptographic
    signatures can either be created and verified manually or via x509 certificates. 
    AES can be used in cbc, ctr or gcm mode for symmetric encryption; RSA for asymmetric
    (public key) encryption or EC for Diffie Hellman. High-level envelope functions 
    combine RSA and AES for encrypting arbitrary sized data. Other utilities include key
    generators, hash functions (md5, sha1, sha256, etc), base64 encoder, a secure random
    number generator, and 'bignum' math methods for manually performing crypto 
    calculations on large multibyte integers.",2021-09-02,Jeroen Ooms,https://github.com/jeroen/openssl,TRUE,https://github.com/jeroen/openssl,18836407,52,2021-09-01T13:45:31Z,362238.5961538461
openSTARS,"An open source implementation of the 'STARS' toolbox
    (Peterson & Ver Hoef, 2014, <doi:10.18637/jss.v056.i02>) using 'R' and 'GRASS GIS'.
    It prepares the *.ssn object needed for the 'SSN' package.
    A Digital Elevation Model (DEM) is used to derive stream networks 
    (in contrast to 'STARS' that can clean an existing stream network).",2020-10-17,Mira Kattwinkel,https://github.com/MiKatt/openSTARS,TRUE,https://github.com/mikatt/openstars,16078,28,2020-10-22T08:49:41Z,574.2142857142857
opentimsr,"A free, open-source package designed for
    handling .tdf data files produced by Bruker's 'timsTOF' mass
    spectrometers.
    Fast, free, crossplatform, with no reading through
    EULAs or messing with binary .dll files involved.",2021-07-06,Michał Piotr Startek,https://github.com/michalsta/opentims,TRUE,https://github.com/michalsta/opentims,3757,10,2021-08-31T07:29:40Z,375.7
opentripplanner,"Setup and connect to 'OpenTripPlanner' (OTP) <http://www.opentripplanner.org/>.
    OTP is an open source platform for multi-modal and multi-agency
    journey planning written in 'Java'. The package allows you to manage a local version or 
    connect to remote OTP server to find walking, cycling, driving, or transit routes.
    This package has been peer-reviewed by rOpenSci (v. 0.2.0.0).",2020-11-09,Malcolm Morgan,"https://github.com/ropensci/opentripplanner,
https://docs.ropensci.org/opentripplanner/",TRUE,https://github.com/ropensci/opentripplanner,13041,57,2021-08-20T14:57:04Z,228.78947368421052
openVA,"Implements multiple existing open-source algorithms for coding cause of death from verbal autopsies. The methods implemented include 'InterVA4' by Byass et al (2012) <doi:10.3402/gha.v5i0.19281>, 'InterVA5' by Byass at al (2019) <doi:10.1186/s12916-019-1333-6>, 'InSilicoVA' by McCormick et al (2016) <doi:10.1080/01621459.2016.1152191>, 'NBC' by Miasnikof et al (2015) <doi:10.1186/s12916-015-0521-2>, and a replication of 'Tariff' method by James et al (2011) <doi:10.1186/1478-7954-9-31> and Serina, et al. (2015) <doi:10.1186/s12916-015-0527-9>. It also provides tools for data manipulation tasks commonly used in Verbal Autopsy analysis and implements easy graphical visualization of individual and population level statistics. The 'NBC' method is implemented by the 'nbc4va' package that can be installed from <https://github.com/rrwen/nbc4va>. Note that this package was not developed by authors affiliated with the Institute for Health Metrics and Evaluation and thus unintentional discrepancies may exist in the implementation of the 'Tariff' method.",2021-02-03,Zehang Richard Li,https://github.com/verbal-autopsy-software/openVA,TRUE,https://github.com/verbal-autopsy-software/openva,22740,1,2021-08-02T15:52:35Z,22740
openxlsx,"Simplifies the creation of Excel .xlsx files by
    providing a high level interface to writing, styling and editing
    worksheets. Through the use of 'Rcpp', read/write times are comparable
    to the 'xlsx' and 'XLConnect' packages with the added benefit of
    removing the dependency on Java.",2021-06-16,Philipp Schauberger,"https://ycphs.github.io/openxlsx/index.html,
https://github.com/ycphs/openxlsx",TRUE,https://github.com/ycphs/openxlsx,9103600,127,2021-09-02T15:19:49Z,71681.88976377953
opera,"Misc methods to form online predictions, for regression-oriented 
    time-series, by combining a finite set of forecasts provided by the user. See 
             Cesa-Bianchi and Lugosi (2006) <doi:10.1017/CBO9780511546921> for an overview. ",2021-06-15,Pierre Gaillard,http://pierre.gaillard.me/opera.html,TRUE,https://github.com/dralliag/opera,39582,34,2021-06-23T13:42:42Z,1164.1764705882354
opitools,"Designed for performing impact analysis of
  opinions in a digital text document (DTD). The 
  package allows a user to assess the extent to which a theme
  or subject within a document impacts the overall opinion 
  expressed in the document. The package can be applied to a wide 
  range of opinion-based DTD, including commentaries on social media
  platforms (such as 'Facebook', 'Twitter' and 'Youtube'), 
  online products reviews, and so on. 
  The utility of 'opitools' was originally demonstrated 
  in Adepeju and Jimoh (2021) <doi:10.31235/osf.io/c32qh> in the 
  assessment of COVID-19 impacts on neighbourhood policing using 
  Twitter data. Further examples can be found in the vignette of 
  the package.",2021-07-29,Monsuru Adepeju,https://github.com/MAnalytics/opitools,TRUE,https://github.com/manalytics/opitools,1801,10,2021-08-31T12:33:32Z,180.1
oppr,"A decision support tool for prioritizing conservation projects.
    Prioritizations can be developed by maximizing expected feature richness,
    expected phylogenetic diversity, the number of features that meet
    persistence targets, or identifying a set of projects that meet persistence
    targets for minimal cost. Constraints (e.g. lock in specific actions) and
    feature weights can also be specified to further customize prioritizations.
    After defining a project prioritization problem, solutions can be obtained
    using exact algorithms, heuristic algorithms, or random processes. In
    particular, it is recommended to install the 'Gurobi' optimizer (available
    from <https://www.gurobi.com>) because it can identify optimal solutions
    very quickly. Finally, methods are provided for comparing different
    prioritizations and evaluating their benefits. For more information, see
    Hanson et al. (2019) <doi:10.1111/2041-210X.13264>.",2021-05-12,Jeffrey O Hanson,https://prioritizr.github.io/oppr/,TRUE,https://github.com/prioritizr/oppr,15859,4,2021-05-31T03:58:43Z,3964.75
optimall,"Functions for the design process of survey sampling, with specific tools for multi-wave and multi-phase designs. Perform optimum allocation using Neyman (1934) <doi:10.2307/2342192> or Wright (2012) <doi:10.1080/00031305.2012.733679> allocation, split strata based on quantiles or values of known variables, randomly select samples from strata, allocate sampling waves iteratively, and organize a complex survey design. Also includes a Shiny application for observing the effects of different strata splits.",2021-07-21,Jasper Yang,https://github.com/yangjasp/optimall,TRUE,https://github.com/yangjasp/optimall,570,0,2021-08-06T16:14:40Z,NA
OptimalRerandExpDesigns,"This is a tool to find the optimal rerandomization threshold in non-sequential experiments. We offer three procedures based 
	on assumptions made on the residuals distribution: (1) normality assumed (2) excess kurtosis assumed (3) entire distribution assumed.
	Illustrations are included. Also included is a routine to unbiasedly estimate Frobenius norms of variance-covariance matrices.
	Details of the method can be found in ""Optimal Rerandomization via a Criterion that Provides Insurance Against Failed Experiments""
	Adam Kapelner, Abba M. Krieger, Michael Sklar and David Azriel (2020) <arXiv:1905.03337>.",2021-01-28,Adam Kapelner,https://github.com/kapelner/OptimalRerandExpDesigns,TRUE,https://github.com/kapelner/optimalrerandexpdesigns,2279,0,2021-01-22T00:25:35Z,NA
optimParallel,"Provides a parallel version of the L-BFGS-B method of optim(). The main function of the package is optimParallel(), which has the same usage and output as optim(). Using optimParallel() can significantly reduce the optimization time.",2021-02-11,Florian Gerber,https://github.com/florafauna/optimParallel-R,TRUE,https://github.com/florafauna/optimparallel-r,57232,3,2021-02-10T18:51:29Z,19077.333333333332
optmatch,"Distance based bipartite matching using the RELAX-IV minimum cost flow solver,
    oriented to matching of treatment and control groups in observational
    studies. Routines are provided to generate distances from generalised linear models (propensity
    score matching), formulas giving variables on which to limit matched distances, stratified or
    exact matching directives, or calipers, alone or in combination.",2021-08-17,Ben B. Hansen,https://github.com/markmfredrickson/optmatch,TRUE,https://github.com/markmfredrickson/optmatch,193872,31,2021-08-23T14:51:47Z,6253.935483870968
orderly,"Order, create and store reports from R.  By defining a
    lightweight interface around the inputs and outputs of an
    analysis, a lot of the repetitive work for reproducible research
    can be automated.  We define a simple format for organising and
    describing work that facilitates collaborative reproducible
    research and acknowledges that all analyses are run multiple
    times over their lifespans.",2021-06-17,Rich FitzJohn,https://github.com/vimc/orderly,TRUE,https://github.com/vimc/orderly,10759,94,2021-09-01T11:39:44Z,114.45744680851064
ore,"Provides an alternative to R's built-in functionality for handling
    regular expressions, based on the Onigmo library. Offers first-class
    compiled regex objects, partial matching and function-based substitutions,
    amongst other features.",2019-11-02,Jon Clayden,https://github.com/jonclayden/ore,TRUE,https://github.com/jonclayden/ore,143979,55,2021-08-14T16:06:19Z,2617.8
orf,"An implementation of the Ordered Forest estimator as developed 
    in Lechner & Okasa (2019) <arXiv:1907.02436>. The Ordered Forest flexibly
    estimates the conditional probabilities of models with ordered categorical
    outcomes (so-called ordered choice models). Additionally to common machine 
    learning algorithms the 'orf' package provides functions for estimating
    marginal effects as well as statistical inference thereof and thus provides
    similar output as in standard econometric models for ordered choice. The
    core forest algorithm relies on the fast C++ forest implementation from
    the 'ranger' package (Wright & Ziegler, 2017) <arXiv:1508.04409>.",2020-01-31,Gabriel Okasa,https://github.com/okasag/orf,TRUE,https://github.com/okasag/orf,8407,8,2021-07-19T08:50:05Z,1050.875
origami,"A general framework for the application of cross-validation schemes
    to particular functions. By allowing arbitrary lists of results, origami
    accommodates a range of cross-validation applications.",2020-01-16,Jeremy Coyle,https://tlverse.org/origami,TRUE,https://github.com/tlverse/origami,22403,24,2021-04-30T21:13:55Z,933.4583333333334
oro.nifti,"Functions for the input/output and visualization of
    medical imaging data that follow either the 'ANALYZE', 'NIfTI' or 'AFNI'
    formats.  This package is part of the Rigorous Analytics bundle.",2020-09-08,Brandon Whitcher,"http://rig.oro.us.com, http://rigorousanalytics.blogspot.com",TRUE,https://github.com/bjw34032/oro.nifti,76491,2,2020-11-12T16:50:38Z,38245.5
orsifronts,"A data set package with the ""Orsi"" and ""Park/Durand"" fronts as
    'SpatialLinesDataFrame' objects. The Orsi et al. (1995) fronts are published at
    the Southern Ocean Atlas Database Page, and the Park et al. (2019) fronts are published at the 'SEANOE' 
    Altimetry-derived Antarctic Circumpolar Current fronts page, please see package CITATION for details.",2021-05-04,Michael D. Sumner,https://australianantarcticdivision.github.io/orsifronts/,TRUE,https://github.com/australianantarcticdivision/orsifronts,16189,2,2021-05-04T11:21:53Z,8094.5
ORTSC,Connects to Google cloud vision <https://cloud.google.com/vision> to perform label detection and repurpose this feature for image classification.,2021-02-10,Mohamed Soudy,https://github.com/MohmedSoudy/ORTSC,TRUE,https://github.com/mohmedsoudy/ortsc,2268,1,2021-02-06T14:45:53Z,2268
osfr,"An interface for interacting with 'OSF' (<https://osf.io>). 'osfr' 
    enables you to access open research materials and data, or create and
    manage your own private or public projects.",2020-02-17,Aaron Wolen,"https://docs.ropensci.org/osfr, https://github.com/ropensci/osfr",TRUE,https://github.com/ropensci/osfr,11115,124,2021-01-03T22:38:43Z,89.63709677419355
osmextract,"Match, download, convert and import Open Street Map data extracts 
    obtained from several providers. ",2021-07-27,Andrea Gilardi,"https://docs.ropensci.org/osmextract/,
https://github.com/ropensci/osmextract",TRUE,https://github.com/ropensci/osmextract,4241,95,2021-08-31T08:11:48Z,44.642105263157895
osmplotr,"Bespoke images of 'OpenStreetMap' ('OSM') data and data
    visualisation using 'OSM' objects.",2021-03-28,Mark Padgham,"https://docs.ropensci.org/osmplotr/,
https://github.com/ropensci/osmplotr",TRUE,https://github.com/ropensci/osmplotr,25147,122,2021-05-21T12:03:55Z,206.12295081967213
osrm,"An interface between R and the 'OSRM' API. 'OSRM' is a routing
    service based on 'OpenStreetMap' data. See <http://project-osrm.org/> for more
    information. This package allows to compute routes, trips, isochrones and
    travel distances matrices (travel time and kilometric distance).",2021-03-19,Timothée Giraud,https://github.com/riatelab/osrm,TRUE,https://github.com/riatelab/osrm,51746,169,2021-04-20T15:34:27Z,306.18934911242604
Ostats,"O-statistics, or overlap statistics, measure the degree of community-level trait overlap. 
    They are estimated by fitting nonparametric kernel density functions to each species’ trait 
    distribution and calculating their areas of overlap. For instance, the median pairwise overlap 
    for a community is calculated by first determining the overlap of each species pair 
    in trait space, and then taking the median overlap of each species pair in a community. 
    This median overlap value is called the O-statistic (O for overlap).
    The Ostats() function calculates separate univariate overlap statistics for each trait, 
    while the Ostats_multivariate() function calculates a single multivariate overlap statistic for all traits. 
    O-statistics can be evaluated against null models to obtain standardized effect sizes. 
    Ostats is part of the collaborative Macrosystems Biodiversity Project ""Local- to continental-scale 
    drivers of biodiversity across the National Ecological Observatory Network (NEON)."" 
    For more information on this project, see the Macrosystems Biodiversity Website 
    (<https://neon-biodiversity.github.io/>). Calculation of O-statistics is described in
    Read et al. (2018) <doi:10.1111/ecog.03641>, and a teaching module for introducing the
    underlying biological concepts at an undergraduate level is described in Grady et al.
    (2018) <http://tiee.esa.org/vol/v14/issues/figure_sets/grady/abstract.html>.",2021-07-26,Quentin D. Read,NA,TRUE,https://github.com/neon-biodiversity/ostats,871,6,2021-07-26T12:51:39Z,145.16666666666666
otp,"Generating and validating One-time Password based on 
    Hash-based Message Authentication Code (HOTP) 
    and Time Based One-time Password (TOTP)
    according to RFC 4226 <https://tools.ietf.org/html/rfc4226> and
    RFC 6238 <https://tools.ietf.org/html/rfc6238>.",2020-05-05,Randy Lai,https://github.com/randy3k/otp,TRUE,https://github.com/randy3k/otp,4729,10,2021-06-05T08:21:52Z,472.9
otvPlots,"Enables automated visualization of variable 
    distribution and changes over time for predictive model building.
    It efficiently computes summary statistics aggregated by time for 
    large datasets, and create plots for variable level monitoring.  ",2018-06-26,Yingbo Li,https://github.com/capitalone/otvPlots,TRUE,https://github.com/capitalone/otvplots,13939,12,2021-08-25T19:50:23Z,1161.5833333333333
ouch,Fit and compare Ornstein-Uhlenbeck models for evolution along a phylogenetic tree.,2021-05-16,Aaron A. King,https://kingaa.github.io/ouch/,TRUE,https://github.com/kingaa/ouch,32980,9,2021-09-01T08:15:35Z,3664.4444444444443
outbreaks,"Empirical or simulated disease outbreak data, provided either as
    RData or as text files.",2020-09-28,Finlay Campbell,https://github.com/reconhub/outbreaks,TRUE,https://github.com/reconhub/outbreaks,36095,47,2021-08-06T09:03:21Z,767.9787234042553
outcomerate,"Standardized survey outcome rate functions, including the response rate, contact rate, cooperation rate, and refusal rate. These outcome rates allow survey researchers to measure the quality of survey data using definitions published by the American Association of Public Opinion Research (AAPOR). For details on these standards, see AAPOR (2016) <https://www.aapor.org/Standards-Ethics/Standard-Definitions-(1).aspx>. ",2018-10-06,Rafael Pilliard Hellwig,https://github.com/ropensci/outcomerate,TRUE,https://github.com/ropensci/outcomerate,12161,5,2020-12-31T20:34:10Z,2432.2
outForest,"Provides a random forest based implementation of
    the method described in Chapter 7.1.2 (Regression model based anomaly
    detection) of Chandola et al. (2009) <doi:10.1145/1541880.1541882>. It
    works as follows: Each numeric variable is regressed onto all other
    variables by a random forest. If the scaled absolute difference
    between observed value and out-of-bag prediction of the corresponding
    random forest is suspiciously large, then a value is considered an
    outlier. The package offers different options to replace such
    outliers, e.g. by realistic values found via predictive mean matching.
    Once the method is trained on a reference data, it can be applied to
    new data.",2021-01-07,Michael Mayer,https://github.com/mayer79/outForest,TRUE,https://github.com/mayer79/outforest,9104,6,2021-04-24T18:37:51Z,1517.3333333333333
outliertree,"Outlier detection method that flags suspicious values within observations,
  constrasting them against the normal values in a user-readable format, potentially
  describing conditions within the data that make a given outlier more rare.
  Full procedure is described in Cortes (2020) <arXiv:2001.00636>.
  Loosely based on the 'GritBot' <https://www.rulequest.com/gritbot-info.html> software.",2021-08-22,David Cortes,https://github.com/david-cortes/outliertree,TRUE,https://github.com/david-cortes/outliertree,18968,37,2021-09-02T01:08:04Z,512.6486486486486
outsider.base,"Base package for 'outsider' <https://github.com/ropensci/outsider>.
    The 'outsider' package and its sister packages enable the installation and
    running of external, command-line software within R. This base package is a
    key dependency of the user-facing 'outsider' package as it provides the
    utilities for interfacing between 'Docker' <https://www.docker.com> and R.
    It is intended that end-users of 'outsider' do not directly work with this
    base package.",2021-04-18,Dom Bennett,"https://docs.ropensci.org/outsider.base/,
https://github.com/ropensci/outsider.base",TRUE,https://github.com/ropensci/outsider.base,16835,1,2021-04-17T10:23:10Z,16835
overlap,"Provides functions to fit kernel density functions to
 data on temporal activity patterns of animals; estimate coefficients
 of overlapping of densities for two species; and calculate bootstrap
 estimates of confidence intervals.",2021-05-17,Mike Meredith and Martin Ridout,NA,TRUE,https://github.com/mikemeredith/overlap,39156,1,2021-05-17T04:11:12Z,39156
overviewR,"Makes it easy to display descriptive information on
    a data set.  Getting an easy overview of a data set by displaying and
    visualizing sample information in different tables (e.g., time and
    scope conditions).  The package also provides publishable 'LaTeX' code
    to present the sample information.",2020-11-23,Cosima Meyer,https://github.com/cosimameyer/overviewR,TRUE,https://github.com/cosimameyer/overviewr,6002,17,2021-08-13T08:04:58Z,353.05882352941177
owd,Open the current working directory (or a given directory path) in your computer's file manager.,2020-08-05,Benjamin G. Feakins,https://github.com/Feakster/owd,TRUE,https://github.com/feakster/owd,6098,0,2021-02-01T11:29:05Z,NA
ows4R,"Provides an Interface to Web-Services defined as standards by the Open Geospatial Consortium (OGC), including Web Feature Service
 (WFS) for vector data, Catalogue Service (CSW) for ISO/OGC metadata and associated standards such as the common web-service specification (OWS) and
 OGC Filter Encoding. The long-term purpose is to add support for additional OGC service standards such as Web Coverage Service (WCS) and
 Web Processing Service (WPS).",2020-05-27,Emmanuel Blondel,"https://github.com/eblondel/ows4R,
http://www.opengeospatial.org/standards",TRUE,https://github.com/eblondel/ows4r,22022,15,2021-07-15T09:52:02Z,1468.1333333333334
oxcgrt,"The Oxford COVID-19 Government Response Tracker (OxCGRT) tracks and 
    compares worldwide government responses to the COVID-19 pandemic rigorously 
    and consistently. OxCGRT makes available systematic information in a 
    consistent way, aiding those who require information have access to it
    efficiently for their purposes. This package facilitates access to the 
    OxCGRT data via its API <https://covidtracker.bsg.ox.ac.uk/> and 
    includes functions to calculate the various OxCGRT indices in R.",2020-11-27,Ernest Guevarra,"https://como-ph.github.io/oxcgrt/,
https://github.com/como-ph/oxcgrt",TRUE,https://github.com/como-ph/oxcgrt,3135,6,2020-11-27T13:59:38Z,522.5
oysteR,"Collects a list of your third party R packages, and
    scans them with the 'OSS' Index provided by 'Sonatype', reporting back
    on any vulnerabilities that are found in the third party packages you
    use.",2021-01-10,Colin Gillespie,https://github.com/sonatype-nexus-community/oysteR,TRUE,https://github.com/sonatype-nexus-community/oyster,5650,24,2021-05-18T11:49:16Z,235.41666666666666
ozmaps,"Maps of Australian coastline and administrative regions. Data 
 can be drawn or accessed directly as simple features objects. Includes
 simple functions for country or state maps of Australia and in-built data
 sets of administrative regions from the Australian Bureau of Statistics 
 <https://www.abs.gov.au/>. Layers include electoral divisions and local 
 government areas, simplified from the original sources but with sufficient 
 detail to allow mapping of a local municipality. ",2021-08-03,Michael Sumner,https://github.com/mdsumner/ozmaps,TRUE,https://github.com/mdsumner/ozmaps,16382,11,2021-07-25T11:42:46Z,1489.2727272727273
packagefinder,"Search for R packages on CRAN directly from the R console, based on the packages' titles, short and long descriptions, or other fields. Combine multiple keywords with logical operators ('and', 'or'), view detailed information on any package and keep track of the latest package contributions to CRAN. If you don't want to search from the R console, use the comfortable R Studio add-in.",2020-10-26,Joachim Zuckarelli,"https://github.com/jsugarelli/packagefinder/,
http://www.zuckarelli.de/packagefinder/tutorial.html,
https://youtu.be/B96NMSo3nJI",TRUE,https://github.com/jsugarelli/packagefinder,23358,35,2020-10-26T21:46:26Z,667.3714285714286
packageRank,"Compute and visualize the cross-sectional and longitudinal number
    and rank percentile of package downloads from RStudio's CRAN mirror.",2021-05-03,Peter Li,https://github.com/lindbrook/packageRank,TRUE,https://github.com/lindbrook/packagerank,12416,12,2021-08-24T18:09:46Z,1034.6666666666667
packcircles,Algorithms to find arrangements of non-overlapping circles.,2020-12-12,Michael Bedward,https://github.com/mbedward/packcircles,TRUE,https://github.com/mbedward/packcircles,88626,43,2020-12-11T07:39:39Z,2061.0697674418607
packDAMipd,"A collection of functions to construct Markov model for model-based 
    cost-effectiveness analysis. This includes creating Markov model (both time
    homogenous and time dependent models), decision analysis, sensitivity 
    analysis (deterministic and probabilistic). The package allows estimation 
    of parameters for the Markov model from a given individual patient level 
    data, provided the data file follows some standard data entry rules. ",2021-03-03,Sheeja Manchira Krishnan,https://github.com/sheejamk/packDAMipd,TRUE,https://github.com/sheejamk/packdamipd,2990,0,2021-08-11T10:46:46Z,NA
packer,"
  Enforces good practice and provides convenience functions to make work with 'JavaScript' 
  not just easier but also scalable. It is a robust wrapper to 'NPM', 'yarn', and 'webpack' that 
  enables to compartmentalize 'JavaScript' code, leverage 'NPM' and 'yarn' packages, include
  'TypeScript', 'React', or 'Vue' in web applications, and much more.",2021-08-14,John Coene,"https://github.com/JohnCoene/packer, https://packer.john-coene.com",TRUE,https://github.com/johncoene/packer,4563,97,2021-09-01T18:48:17Z,47.04123711340206
packrat,"Manage the R packages your project depends
    on in an isolated, portable, and reproducible way.",2021-08-20,Kevin Ushey,https://github.com/rstudio/packrat/,TRUE,https://github.com/rstudio/packrat,2342990,368,2021-08-20T20:39:41Z,6366.820652173913
pacotest,"Routines for two different test types, the Constant Conditional Correlation (CCC) test and the Vectorial Independence (VI) test are provided (Kurz and Spanhel (2017) <arXiv:1706.02338>). The tests can be applied to check whether a conditional copula coincides with its partial copula. Functions to test whether a regular vine copula satisfies the so-called simplifying assumption or to test a single copula within a regular vine copula to be a (j-1)-th order partial copula are available. The CCC test comes with a decision tree approach to allow testing in high-dimensional settings.",2020-12-15,Malte S. Kurz,NA,TRUE,https://github.com/maltekurz/pacotest,16169,2,2021-05-04T08:41:03Z,8084.5
pacs,"
  Supplementary utils for CRAN maintainers and R packages developers.
  Validating the library or packages.
  Exploring a complexity of a specific package like evaluating sizes in bytes of all its dependencies.
  Assessing the life duration of a specific package version.
  Checking a package CRAN check page status for any errors and warnings.
  Retrieving a DESCRIPTION or NAMESPACE file for any package version. 
  Comparing DESCRIPTION or NAMESPACE files between different package versions.
  Getting a list of all releases for a specific package.
  The Bioconductor is partly supported.",2021-08-17,Maciej Nasinski,https://github.com/Polkas/pacs,TRUE,https://github.com/polkas/pacs,662,2,2021-09-02T21:20:58Z,331
Pade,"Given a vector of Taylor series coefficients of sufficient length
    as input, the function returns the numerator and denominator coefficients
    for the Padé approximant of appropriate order (Baker, 1975)
    <ISBN:9780120748556>.",2020-11-10,Avraham Adler,https://github.com/aadler/Pade,TRUE,https://github.com/aadler/pade,18463,0,2020-11-23T22:52:48Z,NA
padr,"Transforms datetime data into a format ready for analysis.
    It offers two core functionalities; aggregating data to a higher level interval
    (thicken) and imputing records where observations were absent (pad). ",2020-09-12,Edwin Thoen,https://github.com/EdwinTh/padr,TRUE,https://github.com/edwinth/padr,855123,112,2021-07-23T15:03:10Z,7635.026785714285
PAFit,Statistical methods for estimating preferential attachment and node fitness generative mechanisms in temporal complex networks are provided. Thong Pham et al. (2015) <doi:10.1371/journal.pone.0137796>. Thong Pham et al. (2016) <doi:10.1038/srep32558>. Thong Pham et al. (2020) <doi:10.18637/jss.v092.i03>. ,2021-07-22,Thong Pham,https://github.com/thongphamthe/PAFit,TRUE,https://github.com/thongphamthe/pafit,30938,10,2021-07-22T15:13:12Z,3093.8
pafr,"Provides functions to read, process and visualize pairwise sequence 
 alignments in the 'PAF' format used by 'minimap2' and other whole-genome aligners. 
 'minimap2' is described by Li H. (2018) <doi:10.1093/bioinformatics/bty191>.",2020-12-08,David Winter,https://dwinter.github.io/pafr/,TRUE,https://github.com/dwinter/pafr,3145,31,2020-12-06T19:40:11Z,101.45161290322581
pagedown,"Use the paged media properties in CSS and the JavaScript
  library 'paged.js' to split the content of an HTML document into discrete
  pages. Each page can have its page size, page numbers, margin boxes, and
  running headers, etc. Applications of this package include books, letters,
  reports, papers, business cards, resumes, and posters.",2021-06-23,Yihui Xie,https://github.com/rstudio/pagedown,TRUE,https://github.com/rstudio/pagedown,74676,683,2021-08-18T15:32:32Z,109.33528550512445
pagemap,Quickly and easily add a mini map to your 'rmarkdown' html documents.,2021-09-02,Wei Su,https://github.com/swsoyee/pagemapR,TRUE,https://github.com/swsoyee/pagemapr,4431,12,2021-08-27T16:26:24Z,369.25
pagenum,A simple way to add page numbers to base/ggplot/lattice graphics.,2021-04-17,Kevin Wright,https://kwstat.github.io/pagenum/,TRUE,https://github.com/kwstat/pagenum,15472,0,2021-04-17T02:08:20Z,NA
pagoda2,"Analyzing and interactively exploring large-scale single-cell RNA-seq datasets. 'pagoda2' primarily performs normalization and differential gene expression analysis, with an interactive application for exploring single-cell RNA-seq datasets. It performs basic tasks such as cell size normalization, gene variance normalization, and can be used to identify subpopulations and run differential expression within individual samples. 'pagoda2' was written to rapidly process modern large-scale scRNAseq datasets of approximately 1e6 cells. The companion web application allows users to explore which gene expression patterns form the different subpopulations within your data. The package also serves as the primary method for preprocessing data for conos, <https://github.com/kharchenkolab/conos>. This package interacts with data available through the 'p2data' package, which is available in a 'drat' repository. To access this data package, see the instructions at <https://github.com/kharchenkolab/pagoda2>. The size of the 'p2data' package is approximately 6 MB.",2021-08-11,Evan Biederstedt,https://github.com/kharchenkolab/pagoda2,TRUE,https://github.com/kharchenkolab/pagoda2,5271,110,2021-08-12T00:38:13Z,47.91818181818182
pagoo,"Provides an encapsulated, object-oriented class system for
  analyzing bacterial pangenomes. For a definition of this concept, see
  Tettelin, et al. (2005) <doi:10.1073/pnas.0506758102>. It uses the R6
  package as backend. It was designed in order to facilitate and speed-up
  the comparative analysis of multiple bacterial genomes, standardizing and
  optimizing routine tasks performed everyday. There are a handful of things
  done everyday when working with bacterial pangenomes: subset, summarize,
  extract, visualize and store data. So, 'pagoo' is intended to facilitate these
  tasks as much as possible. For a description of the implemented data structure
  and methods, see Ferres & Iraola (2020), <doi:10.1101/2020.07.29.226951>.",2021-05-13,Ignacio Ferres,"https://iferres.github.io/pagoo/, https://github.com/iferres/pagoo",TRUE,https://github.com/iferres/pagoo,2178,2,2021-06-22T18:39:16Z,1089
pairwiseComparisons,"Multiple pairwise comparison tests on tidy data for
    one-way analysis of variance for both between-subjects and
    within-subjects designs. Currently, it supports only the most common
    types of statistical analyses and tests: parametric (Welch's and
    Student's t-test), nonparametric (Durbin-Conover and Dunn test),
    robust (Yuen’s trimmed means test), and Bayes Factor (Student's
    t-test).",2021-06-01,Indrajeet Patil,"https://indrajeetpatil.github.io/pairwiseComparisons/,
https://github.com/IndrajeetPatil/pairwiseComparisons",TRUE,https://github.com/indrajeetpatil/pairwisecomparisons,111892,44,2021-07-20T17:48:42Z,2543
pak,"The goal of 'pak' is to make package installation faster and
    more reliable. In particular, it performs all HTTP operations in parallel,
    so metadata resolution and package downloads are fast. Metadata and package
    files are cached on the local disk as well. 'pak' has a dependency solver,
    so it finds version conflicts before performing the installation. This
    version of 'pak' supports CRAN, 'Bioconductor' and 'GitHub' packages as well.",2020-11-19,Gábor Csárdi,https://pak.r-lib.org/,TRUE,https://github.com/r-lib/pak,55081,353,2021-08-17T08:49:03Z,156.03682719546742
palaeoSig,"Several tests of quantitative palaeoenvironmental reconstructions 
  from microfossil assemblages, including the null model tests of the 
  statistically significant of reconstructions developed by Telford and Birks
  (2011) <doi:10.1016/j.quascirev.2011.03.002>, and tests of the effect of 
  spatial autocorrelation on transfer function model performance using methods 
  from Telford and Birks (2009) <doi:10.1016/j.quascirev.2008.12.020> and 
  Trachsel and Telford (2016) <doi:10.5194/cp-12-1215-2016>. Age-depth models with 
  generalized mixed-effect regression from Heegaard et al (2005)
  <doi:10.1191/0959683605hl836rr> are also included.",2019-06-28,Richard Telford,https://github.com/richardjtelford/palaeoSig,TRUE,https://github.com/richardjtelford/palaeosig,15281,1,2020-12-10T15:28:20Z,15281
palasso,"Implements sparse regression with paired covariates (Rauschenberger et al. 2020 <doi:10.1007/s11634-019-00375-6>). For the optional shrinkage, install ashr (<https://github.com/stephens999/ashr>) and CorShrink (<https://github.com/kkdey/CorShrink>) from GitHub (see README).",2021-04-19,Armin Rauschenberger,https://github.com/rauschenberger/palasso,TRUE,https://github.com/rauschenberger/palasso,18250,1,2021-04-19T15:18:14Z,18250
paleopop,"This extension of the poems pattern-oriented modeling (POM) framework 
    provides a collection of modules and functions customized for paleontological 
    time-scales, and optimized for single-generation transitions and large populations, 
    across multiple generations.",2021-04-23,Julia Pilowsky,https://github.com/GlobalEcologyLab/paleopop/,TRUE,https://github.com/globalecologylab/paleopop,1458,2,2021-04-23T05:46:36Z,729
paleotree,"Provides tools for transforming, a posteriori time-scaling, and
    modifying phylogenies containing extinct (i.e. fossil) lineages. In particular,
    most users are interested in the functions timePaleoPhy, bin_timePaleoPhy,
    cal3TimePaleoPhy and bin_cal3TimePaleoPhy, which date cladograms of
    fossil taxa using stratigraphic data. This package also contains a large number
    of likelihood functions for estimating sampling and diversification rates from
    different types of data available from the fossil record (e.g. range data,
    occurrence data, etc). paleotree users can also simulate diversification and
    sampling in the fossil record using the function simFossilRecord, which is a
    detailed simulator for branching birth-death-sampling processes composed of
    discrete taxonomic units arranged in ancestor-descendant relationships. Users
    can use simFossilRecord to simulate diversification in incompletely sampled
    fossil records, under various models of morphological differentiation (i.e.
    the various patterns by which morphotaxa originate from one another), and
    with time-dependent, longevity-dependent and/or diversity-dependent rates of
    diversification, extinction and sampling. Additional functions allow users to
    translate simulated ancestor-descendant data from simFossilRecord into standard
    time-scaled phylogenies or unscaled cladograms that reflect the relationships
    among taxon units.",2019-12-12,David W. Bapst,https://github.com/dwbapst/paleotree,TRUE,https://github.com/dwbapst/paleotree,42182,15,2021-08-05T00:43:27Z,2812.133333333333
paletteer,"The choices of color palettes in R can be quite
    overwhelming with palettes spread over many packages with many
    different API's. This packages aims to collect all color palettes
    across the R ecosystem under the same package with a streamlined API.",2021-07-20,See AUTHORS file.,https://github.com/EmilHvitfeldt/paletteer,TRUE,https://github.com/emilhvitfeldt/paletteer,162971,598,2021-07-20T21:18:57Z,272.5267558528428
palm,"Functions to fit point process models using the Palm likelihood. First proposed by Tanaka, Ogata, and Stoyan (2008) <DOI:10.1002/bimj.200610339>, maximisation of the Palm likelihood can provide computationally efficient parameter estimation for point process models in situations where the full likelihood is intractable. This package is chiefly focused on Neyman-Scott point processes, but can also fit the void processes proposed by Jones-Todd et al. (2019) <DOI:10.1002/sim.8046>. The development of this package was motivated by the analysis of capture-recapture surveys on which individuals cannot be identified---the data from which can conceptually be seen as a clustered point process (Stevenson, Borchers, and Fewster, 2019 <DOI:10.1111/biom.12983>). As such, some of the functions in this package are specifically for the estimation of cetacean density from two-camera aerial surveys.",2020-09-25,Ben Stevenson,https://github.com/b-steve/palm,TRUE,https://github.com/b-steve/palm,17135,1,2020-09-25T06:13:30Z,17135
palr,"Colour palettes for data, based on some well known public data
    sets. Includes helper functions to map absolute values to known palettes, and 
    capture the work of image colour mapping as raster data sets. ",2021-05-05,Michael D. Sumner,https://github.com/AustralianAntarcticDivision/palr,TRUE,https://github.com/australianantarcticdivision/palr,82595,2,2021-05-05T02:50:10Z,41297.5
pals,"A comprehensive collection of color palettes, colormaps, and tools to evaluate them.",2021-04-17,Kevin Wright,https://kwstat.github.io/pals/,TRUE,https://github.com/kwstat/pals,116083,60,2021-04-16T22:41:08Z,1934.7166666666667
pammtools,"The Piece-wise exponential (Additive Mixed) Model
    (PAMM; Bender and others (2018) <doi: 10.1177/1471082X17748083>) is a
    powerful model class for the analysis of survival (or time-to-event) data,
    based on Generalized Additive (Mixed) Models (GA(M)Ms). It offers intuitive specification and robust estimation of complex survival models with stratified baseline hazards, random effects, time-varying effects, time-dependent covariates and cumulative effects (Bender and others (2019)), as well as support for left-truncated, competing risks and recurrent events data.
    pammtools provides tidy workflow for survival analysis with PAMMs,
    including data simulation, transformation and other functions for data
    preprocessing and model post-processing as well as visualization.",2021-06-21,Andreas Bender,https://adibender.github.io/pammtools/,TRUE,https://github.com/adibender/pammtools,21423,29,2021-06-24T17:57:31Z,738.7241379310345
pander,"Contains some functions catching all messages, 'stdout' and other
    useful information while evaluating R code and other helpers to return user
    specified text elements (like: header, paragraph, table, image, lists etc.)
    in 'pandoc' markdown or several type of R objects similarly automatically
    transformed to markdown format. Also capable of exporting/converting (the
    resulting) complex 'pandoc' documents to e.g. HTML, 'PDF', 'docx' or 'odt'. This
    latter reporting feature is supported in brew syntax or with a custom reference
    class with a smarty caching 'backend'.",2021-06-13,Gergely Daróczi,https://rapporter.github.io/pander/,TRUE,https://github.com/rapporter/pander,1309365,274,2021-06-13T12:15:30Z,4778.704379562044
pandocfilters,"The document converter 'pandoc' <http://pandoc.org/> is widely used
    in the R community. One feature of 'pandoc' is that it can produce and consume
    JSON-formatted abstract syntax trees (AST). This allows to transform a given
    source document into JSON-formatted AST, alter it by so called filters and pass
    the altered JSON-formatted AST back to 'pandoc'. This package provides functions
    which allow to write such filters in native R code. 
    Although this package is inspired by the Python package 'pandocfilters' 
    <https://github.com/jgm/pandocfilters/>, it provides additional convenience functions which make it simple to use the 'pandocfilters' package as a 
    report generator. Since 'pandocfilters' inherits most of it's functionality
    from 'pandoc' it can create documents in many formats 
    (for more information see <http://pandoc.org/>) but is also bound to the same
    limitations as 'pandoc'.",2019-11-26,Florian Schwendinger,"http://pandoc.org/, https://github.com/jgm/pandocfilters/",TRUE,https://github.com/jgm/pandocfilters,29332,384,2021-07-08T08:39:34Z,76.38541666666667
PanelMatch,"Implements a set of methodological tools
	     that enable researchers to apply matching methods to
	     time-series cross-sectional data. Imai, Kim, and Wang
	     (2021) <http://web.mit.edu/insong/www/pdf/tscs.pdf> 
	     proposes a nonparametric generalization of the
	     difference-in-differences estimator, which does not rely
	     on the linearity assumption as often done in
	     practice. Researchers first select a method of matching
	     each treated observation for a given unit in a
	     particular time period with control observations from
	     other units in the same time period that have a similar
	     treatment and covariate history. These methods include
	     standard matching methods based on propensity score and
	     Mahalanobis distance, as well as weighting methods. Once 
	     matching is done, both short-term and long-term average 
	     treatment effects for the treated can be estimated with 
	     standard errors. The package also offers a visualization 
	     technique that allows researchers to assess the quality 
	     of matches by examining the resulting covariate balance.",2021-09-02,In Song Kim,NA,TRUE,https://github.com/insongkim/panelmatch,8762,52,2021-07-12T18:36:41Z,168.5
panelr,"Provides an object type and associated tools for storing and 
  wrangling panel data. Implements several methods for creating regression
  models that take advantage of the unique aspects of 
  panel data. Among other capabilities, automates the ""within-between"" 
  (also known as ""between-within"" and ""hybrid"") panel regression specification
  that combines the desirable aspects of both fixed effects and random effects 
  econometric models and fits them as multilevel models 
  (Allison, 2009 <doi:10.4135/9781412993869.d33>; 
  Bell & Jones, 2015 <doi:10.1017/psrm.2014.7>). These models can also be 
  estimated via generalized estimating equations 
  (GEE; McNeish, 2019 <doi:10.1080/00273171.2019.1602504>) and Bayesian 
  estimation is (optionally) supported via 'Stan'. 
  Supports estimation of asymmetric effects models via first differences
  (Allison, 2019 <doi:10.1177/2378023119826441>) as well as a generalized
  linear model extension thereof using GEE. ",2021-01-18,Jacob A. Long,https://panelr.jacob-long.com,TRUE,https://github.com/jacob-long/panelr,45602,76,2021-01-17T22:54:49Z,600.0263157894736
pangaear,"Tools to interact with the 'Pangaea' Database
    (<https://www.pangaea.de>), including functions for searching for data,
    fetching 'datasets' by 'dataset' 'ID', and working with the 'Pangaea'
    'OAI-PMH' service.",2021-05-14,Scott Chamberlain,"https://github.com/ropensci/pangaear (devel),
https://docs.ropensci.org/pangaear/ (documentation)",TRUE,https://github.com/ropensci/pangaear,21645,13,2021-05-14T04:43:54Z,1665
papeR,"A toolbox for writing 'knitr', 'Sweave' or other 'LaTeX'- or 'markdown'-based
	     reports and to prettify the output of various estimated models.",2021-03-22,Benjamin Hofner,https://github.com/hofnerb/papeR,TRUE,https://github.com/hofnerb/paper,42662,24,2021-03-22T13:15:33Z,1777.5833333333333
paradox,"Define parameter spaces, constraints and
    dependencies for arbitrary algorithms, to program on such spaces. Also
    includes statistical designs and random samplers. Objects are
    implemented as 'R6' classes.",2021-03-07,Michel Lang,"https://paradox.mlr-org.com, https://github.com/mlr-org/paradox",TRUE,https://github.com/mlr-org/paradox,223883,21,2021-09-01T20:52:33Z,10661.095238095239
parallelDist,"A fast parallelized alternative to R's native 'dist' function to
    calculate distance matrices for continuous, binary, and multi-dimensional
    input matrices, which supports a broad variety of 41 predefined distance
    functions from the 'stats', 'proxy' and 'dtw' R packages, as well as user-
    defined functions written in C++. For ease of use, the 'parDist' function
    extends the signature of the 'dist' function and uses the same parameter
    naming conventions as distance methods of existing R packages. The package
    is mainly implemented in C++ and leverages the 'RcppParallel' package to
    parallelize the distance computations with the help of the 'TinyThread'
    library. Furthermore, the 'Armadillo' linear algebra library is used for
    optimized matrix operations during distance calculations. The curiously
    recurring template pattern (CRTP) technique is applied to avoid virtual
    functions, which improves the Dynamic Time Warping calculations while
    the implementation stays flexible enough to support different DTW step
    patterns and normalization methods.",2018-12-12,Alexander Eckert,"https://github.com/alexeckert/parallelDist,
https://www.alexandereckert.com/R",TRUE,https://github.com/alexeckert/paralleldist,38940,38,2021-03-13T13:10:06Z,1024.7368421052631
ParallelLogger,"Support for parallel computation with progress bar, and option to stop or proceed on errors. Also provides logging to console and disk,
  and the logging persists in the parallel threads. Additional functions support function call automation with delayed execution (e.g. for executing functions in
  parallel).",2021-07-16,Martijn Schuemie,"https://ohdsi.github.io/ParallelLogger/,
https://github.com/OHDSI/ParallelLogger",TRUE,https://github.com/ohdsi/parallellogger,37669,9,2021-07-15T13:18:09Z,4185.444444444444
parallelly,"Utility functions that enhance the 'parallel' package and support the built-in parallel backends of the 'future' package.  For example, availableCores() gives the number of CPU cores available to your R process as given by the operating system, 'cgroups' and Linux containers, R options, and environment variables, including those set by job schedulers on high-performance compute clusters. If none is set, it will fall back to parallel::detectCores(). Another example is makeClusterPSOCK(), which is backward compatible with parallel::makePSOCKcluster() while doing a better job in setting up remote cluster workers without the need for configuring the firewall to do port-forwarding to your local computer.",2021-07-19,Henrik Bengtsson,"https://parallelly.futureverse.org,
https://github.com/HenrikBengtsson/parallelly",TRUE,https://github.com/henrikbengtsson/parallelly,1687990,80,2021-09-03T08:39:52Z,21099.875
parallelMap,"Unified parallelization framework for multiple
    back-end, designed for internal package and interactive usage.  The
    main operation is parallel mapping over lists.  Supports 'local',
    'multicore', 'mpi' and 'BatchJobs' mode.  Allows tagging of the
    parallel operation with a level name that can be later selected by the
    user to switch on parallel execution for exactly this operation.",2021-06-28,Bernd Bischl,"https://parallelmap.mlr-org.com,
https://github.com/mlr-org/parallelMap",TRUE,https://github.com/mlr-org/parallelmap,647510,57,2021-06-28T12:59:08Z,11359.82456140351
param6,"By making use of 'set6', alongside the S3 and R6 paradigms, this package provides a fast and lightweight R6 interface for parameters and parameter sets.",2021-07-29,Raphael Sonabend,"https://xoopR.github.io/param6/, https://github.com/xoopR/param6/",TRUE,https://github.com/xoopr/param6,2013,7,2021-07-28T20:23:31Z,287.57142857142856
parameters,"Utilities for processing the parameters of various
    statistical models. Beyond computing p values, CIs, and other indices
    for a wide variety of models (see support list of insight; Lüdecke,
    Waggoner & Makowski (2019) <doi:10.21105/joss.01412>), this package
    implements features like bootstrapping or simulating of parameters and
    models, feature reduction (feature extraction and variable selection)
    as well as functions to describe data and variable characteristics
    (e.g. skewness, kurtosis, smoothness or distribution).",2021-05-29,Daniel Lüdecke  (<https://orcid.org/0000-0002-8895-3206>,https://easystats.github.io/parameters/,TRUE,https://github.com/easystats/parameters,731918,226,2021-09-03T06:19:53Z,3238.575221238938
ParamHelpers,"Functions for parameter descriptions and operations
    in black-box optimization, tuning and machine learning. Parameters can
    be described (type, constraints, defaults, etc.), combined to
    parameter sets and can in general be programmed on. A useful OptPath
    object (archive) to log function evaluations is also provided.",2020-03-24,Bernd Bischl,"https://paramhelpers.mlr-org.com,
https://github.com/mlr-org/ParamHelpers",TRUE,https://github.com/mlr-org/paramhelpers,622027,26,2021-07-24T14:16:38Z,23924.115384615383
paramlink2,"Parametric linkage analysis of monogenic traits in medical 
    pedigrees. Features include singlepoint analysis, multipoint analysis via 
    'MERLIN' (Abecasis et al. (2002) <doi:10.1038/ng786>), visualisation of 
    log of the odds (LOD) scores and summaries of linkage peaks. Disease models
    may be specified to  accommodate phenocopies, reduced penetrance and 
    liability classes. 'paramlink2' is part of the 'ped suite' package 
    ecosystem, presented in 'Pedigree Analysis in R' (Vigeland, 2021, 
    ISBN:9780128244302).",2021-05-04,Magnus Dehli Vigeland,https://github.com/magnusdv/paramlink2,TRUE,https://github.com/magnusdv/paramlink2,1508,1,2021-05-04T18:52:21Z,1508
params,"An interface to simplify organizing parameters used in a package,
    using external configuration files. This attempts to provide a cleaner
    alternative to options().",2021-03-01,Sahil Seth,https://github.com/sahilseth/params,TRUE,https://github.com/sahilseth/params,20172,3,2021-03-10T15:42:41Z,6724
ParBayesianOptimization,"Fast, flexible framework for implementing Bayesian optimization of model 
	hyperparameters according to the methods described in Snoek et al. <arXiv:1206.2944>.
	The package allows the user to run scoring function in parallel, save intermediary 
	results, and tweak other aspects of the process to fully utilize the computing resources
	available to the user.",2021-02-11,Samuel Wilson,https://github.com/AnotherSamWilson/ParBayesianOptimization,TRUE,https://github.com/anothersamwilson/parbayesianoptimization,49804,68,2021-05-12T13:11:25Z,732.4117647058823
parlitools,"Provides various tools for analysing UK political data, including 
    election result datasets, hexagonal cartograms and functions to 
    retrieve council member data.",2020-01-12,Evan Odell,"https://docs.evanodell.com/parlitools,
https://github.com/EvanOdell/parlitools/",TRUE,https://github.com/evanodell/parlitools,20093,22,2020-11-29T10:40:49Z,913.3181818181819
paropt,"Enable optimization of parameters of ordinary differential equations. Therefore, using 'SUNDIALS' to solve the ODE-System (see Hindmarsh, Alan C., Peter N. Brown, Keith E. Grant, Steven L. Lee, Radu Serban, Dan E. Shumaker, and Carol S. Woodward. (2005) <doi:10.1145/1089014.1089020>). Furthermore, for optimization the particle swarm algorithm is used (see: Akman, Devin, Olcay Akman, and Elsa Schaefer. (2018) <doi:10.1155/2018/9160793> and Sengupta, Saptarshi, Sanchita Basak, and Richard Peters. (2018) <doi:10.3390/make1010010>). The ODE-System has to be passed as 'Rcpp'-function. The information for the parameter boundaries and states are conveyed using data.frames.  ",2021-06-14,Krämer Konrad,NA,TRUE,https://github.com/konrad1991/paropt,4601,2,2021-07-28T10:30:30Z,2300.5
parqr,"Reads in multi-part parquet files. Will read in parquet files that have not been previously coalesced into one file. Convenient for reading in moderately sized, but split files. ",2021-07-15,John Waller,https://github.com/jhnwllr/parqr,TRUE,https://github.com/jhnwllr/parqr,656,0,2021-07-14T12:30:09Z,NA
parsedate,"Parse dates automatically, without the need of
    specifying a format. Currently it includes the git date parser.
    It can also recognize and parse all ISO 8601 formats.",2021-04-20,Gábor Csárdi,https://github.com/gaborcsardi/parsedate,TRUE,https://github.com/gaborcsardi/parsedate,531616,52,2021-04-20T16:24:16Z,10223.384615384615
parsermd,"An implementation of a formal grammar and parser for R Markdown documents
    using the Boost Spirit X3 library. It also includes a collection of high level
    functions for working with the resulting abstract syntax tree.",2021-05-20,Colin Rundel,"https://rundel.github.io/parsermd/,
https://github.com/rundel/parsermd",TRUE,https://github.com/rundel/parsermd,2519,45,2021-07-13T00:59:15Z,55.977777777777774
parsnip,"A common interface is provided to allow users to specify a
    model without having to remember the different argument names across
    different functions or computational engines (e.g. 'R', 'Spark',
    'Stan', etc).",2021-07-21,Max Kuhn,"https://parsnip.tidymodels.org,
https://github.com/tidymodels/parsnip",TRUE,https://github.com/tidymodels/parsnip,497349,414,2021-08-27T14:03:22Z,1201.3260869565217
particles,"Simulating particle movement in 2D space has many application. The
    'particles' package implements a particle simulator based on the ideas 
    behind the 'd3-force' 'JavaScript' library. 'particles' implements all 
    forces defined in 'd3-force' as well as others such as vector fields, traps, 
    and attractors.",2019-01-14,Thomas Lin Pedersen,https://github.com/thomasp85/particles,TRUE,https://github.com/thomasp85/particles,13183,112,2021-08-02T10:41:37Z,117.70535714285714
partition,"A fast and flexible framework for agglomerative
    partitioning. 'partition' uses an approach called
    Direct-Measure-Reduce to create new variables that maintain the
    user-specified minimum level of information. Each reduced variable is
    also interpretable: the original variables map to one and only one
    variable in the reduced data set. 'partition' is flexible, as well:
    how variables are selected to reduce, how information loss is
    measured, and the way data is reduced can all be customized.
    'partition' is based on the Partition framework discussed in Millstein
    et al. (2020) <doi: 10.1093/bioinformatics/btz661>.",2021-01-07,Malcolm Barrett,"https://uscbiostats.github.io/partition/,
https://github.com/USCbiostats/partition",TRUE,https://github.com/uscbiostats/partition,13004,16,2021-03-19T18:22:31Z,812.75
partitions,"Additive partitions of integers.  Enumerates the
  partitions, unequal partitions, and restricted partitions of an
  integer; the three corresponding partition functions are also
  given.  Set partitions and now compositions are included.",2021-02-23,Robin K. S. Hankin,https://github.com/RobinHankin/partitions,TRUE,https://github.com/robinhankin/partitions,279406,5,2021-08-30T08:58:25Z,55881.2
partools,"Miscellaneous utilities for parallelizing large
   computations.  Alternative to MapReduce.
   File splitting and distributed operations such as sort and aggregate.
   ""Software Alchemy"" method for parallelizing most statistical methods,
   presented in N. Matloff, Parallel Computation for Data Science,
   Chapman and Hall, 2015.  Includes a debugging aid.",2017-04-10,Norm Matloff,https://github.com/matloff/partools,TRUE,https://github.com/matloff/partools,19709,38,2021-05-21T20:25:06Z,518.6578947368421
partR2,"Partitioning the R2 of GLMMs into variation explained by each 
    predictor and combination of predictors using semi-partial (part) R2 and
    inclusive R2. Methods are based on the R2 for GLMMs described in
    Nakagawa & Schielzeth (2013) <doi:10.1111/j.2041-210x.2012.00261.x> and
    Nakagawa, Johnson & Schielzeth (2017) <doi:10.1098/rsif.2017.0213>.",2021-01-18,Martin A. Stoffel,https://github.com/mastoffel/partR2,TRUE,https://github.com/mastoffel/partr2,3288,15,2021-08-05T13:29:17Z,219.2
partsm,"Basic functions to fit and predict periodic autoregressive time series models. These models are discussed in the book P.H. Franses (1996) ""Periodicity and Stochastic Trends in Economic Time Series"", Oxford University Press. Data set analyzed in that book is also provided. NOTE: the package was orphaned during several years. It is now only maintained, but no major enhancements are expected, and the maintainer cannot provide any support. ",2020-11-25,Matthieu Stigler,https://github.com/MatthieuStigler/partsm,TRUE,https://github.com/matthieustigler/partsm,23954,3,2020-11-25T03:48:30Z,7984.666666666667
parzer,"Parse geographic coordinates from various formats
    to decimal degree numeric values. Parse coordinates into
    their parts (degree, minutes, seconds); calculate hemisphere
    from coordinates; pull out individually degrees,
    minutes, or seconds; add and subtract degrees, minutes,
    and seconds. C++ code herein originally inspired from code
    written by Jeffrey D. Bogan, but then completely re-written.",2021-02-16,Scott Chamberlain,"https://github.com/ropensci/parzer (devel)
https://docs.ropensci.org/parzer/ (docs)",TRUE,https://github.com/ropensci/parzer,9646,51,2021-02-17T00:50:07Z,189.13725490196077
pasadr,"Anomaly detection method based on the paper ""Truth will out: Departure-based process-level detection of stealthy attacks on control systems"" from Wissam Aoudi, Mikel Iturbe, and Magnus Almgren (2018) <DOI:10.1145/3243734.3243781>. Also referred to the following implementation: <https://github.com/rahulrajpl/PyPASAD>.",2021-06-30,Donghwan Kim,https://github.com/ainsuotain/pasadr,TRUE,https://github.com/ainsuotain/pasadr,803,3,2021-06-30T12:44:29Z,267.6666666666667
PAsso,"An implementation of the unified framework for assessing partial association 
            between ordinal variables after adjusting for a set of covariates (Dungang Liu, Shaobo 
            Li, Yan Yu and Irini Moustaki (2020), accepted by the Journal of the American 
            Statistical Association). This package provides a set of tools to quantify, visualize, 
            and test partial associations between multiple ordinal variables. It can produce a number
            of $phi$ measures, partial regression plots, 3-D plots, and $p$-values for testing 
            $H_0: phi=0$ or $H_0: phi <= delta$.",2021-06-18,Xiaorui (Jeremy) Zhu,GitHub: https://github.com/XiaoruiZhu/PAsso,TRUE,https://github.com/xiaoruizhu/passo,5610,2,2021-07-28T04:58:38Z,2805
passport,"Smooths the process of working with country names and codes via 
    powerful parsing, standardization, and conversion utilities arranged in a 
    simple, consistent API. Country name formats include multiple sources 
    including the Unicode Common Locale Data 
    Repository (CLDR, <http://cldr.unicode.org/>) common-sense standardized 
    names in hundreds of languages.",2020-11-07,Edward Visel,"https://github.com/alistaire47/passport,
https://alistaire47.github.io/passport/",TRUE,https://github.com/alistaire47/passport,16104,34,2020-12-03T04:33:39Z,473.6470588235294
passt,"Simulates judgments of frequency and duration based on
    the Probability Associator Time (PASS-T) model. PASS-T is a memory
    model based on a simple competitive artificial neural network. It 
    can imitate human judgments of frequency and duration, which have
    been extensively studied in cognitive psychology
    (e.g. Hintzman (1970) <doi:10.1037/h0028865>, Betsch et al. (2010)
    <https://psycnet.apa.org/record/2010-18204-003>). The PASS-T model
    is an extension of the PASS model (Sedlmeier, 2002,
    ISBN:0198508638). The package provides an easy way to run
    simulations, which can then be compared with empirical data in
    human judgments of frequency and duration.",2021-05-03,Johannes Titz,https://github.com/johannes-titz/passt,TRUE,https://github.com/johannes-titz/passt,7040,0,2021-05-03T12:01:19Z,NA
pastecs,"Regularisation, decomposition and analysis of space-time series.
  The pastecs R package is a PNEC-Art4 and IFREMER (Benoit Beliaeff
  <Benoit.Beliaeff@ifremer.fr>) initiative to bring PASSTEC 2000 functionalities to R.",2018-03-15,Philippe Grosjean,https://github.com/phgrosjean/pastecs,TRUE,https://github.com/phgrosjean/pastecs,530561,2,2021-08-08T15:42:49Z,265280.5
patchwork,"The 'ggplot2' package provides a strong API for sequentially 
    building up a plot, but does not concern itself with composition of multiple
    plots. 'patchwork' is a package that expands the API to allow for 
    arbitrarily complex composition of plots by, among others, providing 
    mathematical operators for combining multiple plots. Other packages that try 
    to address this need (but with a different approach) are 'gridExtra' and 
    'cowplot'.",2020-12-17,Thomas Lin Pedersen,"https://patchwork.data-imaginist.com,
https://github.com/thomasp85/patchwork",TRUE,https://github.com/thomasp85/patchwork,903832,1924,2020-12-15T08:32:39Z,469.76715176715174
patentr,"Converts TXT and XML data curated by the United States Patent and
    Trademark Office (USPTO). Allows conversion of bulk data after downloading
    directly from the USPTO bulk data website, eliminating need for users to
    wrangle multiple data formats to get large patent databases in tidy,
    rectangular format. Data details can be found on the USPTO website
    <https://bulkdata.uspto.gov/>. Currently, all 3 formats: 1. TXT data
    (1976-2001); 2. XML format 1 data (2002-2004); and 3. XML format 2 data
    (2005-current) can be converted to rectangular, CSV format.
    Relevant literature that uses data from USPTO includes Wada (2020)
    <doi:10.1007/s11192-020-03674-4> and Plaza & Albert (2008)
    <doi:10.1007/s11192-007-1763-3>.",2021-07-17,Raoul Wadhwa,https://JYProjs.github.io/patentr/,TRUE,https://github.com/jyprojs/patentr,2108,4,2021-08-10T16:45:21Z,527
patentsview,"Provides functions to simplify the 'PatentsView' API
    (<http://www.patentsview.org/api/doc.html>) query language,
    send GET and POST requests to the API's seven endpoints, and parse the data
    that comes back.",2019-01-28,Christopher Baker,https://ropensci.github.io/patentsview/index.html,TRUE,https://github.com/ropensci/patentsview,17475,23,2021-08-21T02:02:54Z,759.7826086956521
path.chain,"Provides path_chain class and functions, which facilitates loading and saving 
             directory structure in YAML configuration files via 'config' package. 
             The file structure you created during exploration can be transformed 
             into legible section in the config file, and then easily loaded for further usage.",2020-09-23,Krzysztof Joachimiak,"https://github.com/krzjoa/path.chain,
https://krzjoa.github.io/path.chain/",TRUE,https://github.com/krzjoa/path.chain,3929,8,2020-09-23T13:11:22Z,491.125
pathfindR,"Enrichment analysis enables researchers to uncover mechanisms 
    underlying a phenotype. However, conventional methods for enrichment 
    analysis do not take into account protein-protein interaction information, 
    resulting in incomplete conclusions. pathfindR is a tool for enrichment 
    analysis utilizing active subnetworks. The main function identifies active 
    subnetworks in a protein-protein interaction network using a user-provided 
    list of genes and associated p values. It then performs enrichment analyses 
    on the identified subnetworks, identifying enriched terms (i.e. pathways or, 
    more broadly, gene sets) that possibly underlie the phenotype of interest.
    pathfindR also offers functionalities to cluster the enriched terms and 
    identify representative terms in each cluster, to score the enriched terms 
    per sample and to visualize analysis results. The enrichment, clustering and 
    other methods implemented in pathfindR are described in detail in 
    Ulgen E, Ozisik O, Sezerman OU. 2019. pathfindR: An R Package for 
    Comprehensive Identification of Enriched Pathways in Omics Data Through 
    Active Subnetworks. Front. Genet. <doi:10.3389/fgene.2019.00858>.",2021-08-21,Ege Ulgen,"https://egeulgen.github.io/pathfindR/,
https://github.com/egeulgen/pathfindR",TRUE,https://github.com/egeulgen/pathfindr,25923,106,2021-08-22T08:08:51Z,244.5566037735849
pathfindR.data,"This is a data-only package, containing data needed to run the CRAN 
    package 'pathfindR', a package for enrichment analysis utilizing active 
    subnetworks. This package contains protein-protein interaction network data, 
    data related to gene sets and example input/output data.",2021-08-21,Ege Ulgen,https://github.com/egeulgen/pathfindR.data,TRUE,https://github.com/egeulgen/pathfindr.data,9664,0,2021-08-21T07:44:51Z,NA
pathviewr,"Tools to import, clean, and visualize movement data,
    particularly from motion capture systems such as Optitrack's 
    'Motive', the Straw Lab's 'Flydra', or from other sources. We provide 
    functions to remove artifacts, standardize tunnel position and tunnel 
    axes, select a region of interest, isolate specific trajectories, fill
    gaps in trajectory data, and calculate 3D and per-axis velocity. For 
    experiments of visual guidance, we also provide functions that use 
    subject position to estimate perception of visual stimuli. ",2021-05-06,Vikram B. Baliga,"https://github.com/ropensci/pathviewr/,
https://docs.ropensci.org/pathviewr/",TRUE,https://github.com/ropensci/pathviewr,1648,4,2021-07-28T18:16:35Z,412
patientProfilesVis,"Creation of patient profile visualizations for
  exploration, diagnostic or monitoring purposes during a clinical trial.
  These static visualizations display a patient-specific overview
  of the evolution during the trial time frame of 
  parameters of interest (as laboratory, ECG, vital signs),
  presence of adverse events, exposure to a treatment; 
  associated with metadata patient information, 
  as demography, concomitant medication.
  The visualizations can be tailored for specific domain(s) or endpoint(s) of interest.
  Visualizations are exported into patient profile report(s)
  or can be embedded in custom report(s).",2021-07-13,Laure Cougnaud,https://github.com/openanalytics/patientProfilesVis,TRUE,https://github.com/openanalytics/patientprofilesvis,923,0,2021-07-13T15:24:23Z,NA
patrick,"This is an extension of the 'testthat' package that
    lets you add parameters to your unit tests. Parameterized unit tests
    are often easier to read and more reliable, since they follow the DNRY
    (do not repeat yourself) rule.",2020-10-27,Michael Quinn,https://github.com/google/patrick,TRUE,https://github.com/google/patrick,22421,79,2020-11-04T00:05:21Z,283.8101265822785
patternize,"Quantification of variation in organismal color patterns as
    obtained from image data. Patternize defines homology between pattern positions
    across images either through fixed landmarks or image registration. Pattern
    identification is performed by categorizing the distribution of colors using RGB
    thresholds or image segmentation.",2018-11-23,Steven Van Belleghem,https://github.com/StevenVB12/patternize,TRUE,https://github.com/stevenvb12/patternize,15401,17,2021-08-25T19:52:48Z,905.9411764705883
Patterns,"A modeling tool dedicated to biological network modeling (Bertrand and others 2020, <doi:10.1093/bioinformatics/btaa855>). It allows for single or joint modeling of, for instance, genes and proteins. It starts with the selection of the actors that will be the used in the reverse engineering upcoming step. An actor can be included in that selection based on its differential measurement (for instance gene expression or protein abundance) or on its time course profile. Wrappers for actors clustering functions and cluster analysis are provided. It also allows reverse engineering of biological networks taking into account the observed time course patterns of the actors. Many inference functions are provided and dedicated to get specific features for the inferred network such as sparsity, robust links, high confidence links or stable through resampling links. Some simulation and prediction tools are also available for cascade networks (Jung and others 2014, <doi:10.1093/bioinformatics/btt705>). Example of use with microarray or RNA-Seq data are provided.",2021-03-21,Frederic Bertrand,"https://fbertran.github.io/Patterns/,
https://github.com/fbertran/Patterns/",TRUE,https://github.com/fbertran/patterns,6414,3,2021-03-21T01:09:12Z,2138
PAutilities,"A collection of utilities that are useful for a broad range of
    tasks that are common in physical activity research, including the
    following: creation of Bland-Altman plots, formatted descriptive
    statistics, metabolic calculations (e.g. basal metabolic rate predictions)
    and conversions, demographic calculations (age and age-for-body-mass-index
    percentile), bout analysis of moderate-to-vigorous intensity physical
    activity, and analysis of bout detection algorithm performance.",2020-05-17,Paul R. Hibbing,https://github.com/paulhibbing/PAutilities,TRUE,https://github.com/paulhibbing/pautilities,16212,0,2021-05-17T05:10:20Z,NA
pavo,"A cohesive framework for parsing, analyzing and
    organizing colour from spectral data.",2021-03-23,Thomas White,"http://pavo.colrverse.com, https://github.com/rmaia/pavo/",TRUE,https://github.com/rmaia/pavo,35523,26,2021-06-23T10:30:47Z,1366.2692307692307
paws,"Interface to Amazon Web Services <https://aws.amazon.com>,
    including storage, database, and compute services, such as 'Simple
    Storage Service' ('S3'), 'DynamoDB' 'NoSQL' database, and 'Lambda'
    functions-as-a-service.",2021-09-03,David Kretch,https://github.com/paws-r/paws,TRUE,https://github.com/paws-r/paws,40314,176,2021-08-27T01:35:18Z,229.0568181818182
paws.analytics,"Interface to 'Amazon Web Services' 'analytics' services,
    including 'Elastic MapReduce' 'Hadoop' and 'Spark' big data service,
    'Elasticsearch' search engine, and more <https://aws.amazon.com/>.",2021-08-23,David Kretch,https://github.com/paws-r/paws,TRUE,https://github.com/paws-r/paws,37456,176,2021-08-27T01:35:18Z,212.8181818181818
paws.application.integration,"Interface to 'Amazon Web Services' application integration
    services, including 'Simple Queue Service' ('SQS') message queue,
    'Simple Notification Service' ('SNS') publish/subscribe messaging, and
    more <https://aws.amazon.com/>.",2021-08-23,David Kretch,https://github.com/paws-r/paws,TRUE,https://github.com/paws-r/paws,37371,176,2021-08-27T01:35:18Z,212.33522727272728
paws.compute,"Interface to 'Amazon Web Services' compute services,
    including 'Elastic Compute Cloud' ('EC2'), 'Lambda'
    functions-as-a-service, containers, batch processing, and more
    <https://aws.amazon.com/>.",2021-08-23,David Kretch,https://github.com/paws-r/paws,TRUE,https://github.com/paws-r/paws,38801,176,2021-08-27T01:35:18Z,220.46022727272728
paws.cost.management,"Interface to 'Amazon Web Services' cost management services,
    including cost and usage reports, budgets, pricing, and more
    <https://aws.amazon.com/>.",2021-08-23,David Kretch,https://github.com/paws-r/paws,TRUE,https://github.com/paws-r/paws,38270,176,2021-08-27T01:35:18Z,217.4431818181818
paws.customer.engagement,"Interface to 'Amazon Web Services' customer engagement
    services, including 'Simple Email Service', 'Connect' contact center
    service, and more <https://aws.amazon.com/>.",2021-08-23,David Kretch,https://github.com/paws-r/paws,TRUE,https://github.com/paws-r/paws,37234,176,2021-08-27T01:35:18Z,211.5568181818182
paws.database,"Interface to 'Amazon Web Services' database services,
    including 'Relational Database Service' ('RDS'), 'DynamoDB' 'NoSQL'
    database, and more <https://aws.amazon.com/>.",2021-08-23,David Kretch,https://github.com/paws-r/paws,TRUE,https://github.com/paws-r/paws,37548,176,2021-08-27T01:35:18Z,213.3409090909091
paws.developer.tools,"Interface to 'Amazon Web Services' developer tools services,
    including version control, continuous integration and deployment, and
    more <https://aws.amazon.com/products/developer-tools/>.",2021-08-24,David Kretch,https://github.com/paws-r/paws,TRUE,https://github.com/paws-r/paws,120,176,2021-08-27T01:35:18Z,0.6818181818181818
paws.end.user.computing,"Interface to 'Amazon Web Services' end user computing
    services, including collaborative document editing, mobile intranet,
    and more <https://aws.amazon.com/>.",2021-08-24,David Kretch,https://github.com/paws-r/paws,TRUE,https://github.com/paws-r/paws,118,176,2021-08-27T01:35:18Z,0.6704545454545454
paws.machine.learning,"Interface to 'Amazon Web Services' machine learning services,
    including 'SageMaker' managed machine learning service, natural
    language processing, speech recognition, translation, and more
    <https://aws.amazon.com/machine-learning/>.",2021-08-23,David Kretch,https://github.com/paws-r/paws,TRUE,https://github.com/paws-r/paws,38569,176,2021-08-27T01:35:18Z,219.14204545454547
paws.management,"Interface to 'Amazon Web Services' management and governance
    services, including 'CloudWatch' application and infrastructure
    monitoring, 'Auto Scaling' for automatically scaling resources, and
    more <https://aws.amazon.com/>.",2021-08-23,David Kretch,https://github.com/paws-r/paws,TRUE,https://github.com/paws-r/paws,41173,176,2021-08-27T01:35:18Z,233.9375
paws.networking,"Interface to 'Amazon Web Services' networking and content
    delivery services, including 'Route 53' Domain Name System service,
    'CloudFront' content delivery, load balancing, and more
    <https://aws.amazon.com/>.",2021-08-23,David Kretch,https://github.com/paws-r/paws,TRUE,https://github.com/paws-r/paws,37533,176,2021-08-27T01:35:18Z,213.2556818181818
paws.security.identity,"Interface to 'Amazon Web Services' security, identity, and
    compliance services, including the 'Identity & Access Management'
    ('IAM') service for managing access to services and resources, and
    more <https://aws.amazon.com/>.",2021-08-23,David Kretch,https://github.com/paws-r/paws,TRUE,https://github.com/paws-r/paws,38698,176,2021-08-27T01:35:18Z,219.875
paws.storage,"Interface to 'Amazon Web Services' storage services,
    including 'Simple Storage Service' ('S3') and more
    <https://aws.amazon.com/>.",2021-08-22,David Kretch,https://github.com/paws-r/paws,TRUE,https://github.com/paws-r/paws,39017,176,2021-08-27T01:35:18Z,221.6875
pbapply,"A lightweight package that adds
  progress bar to vectorized R functions
  ('*apply'). The implementation can easily be added
  to functions where showing the progress is
  useful (e.g. bootstrap). The type and style of the
  progress bar (with percentages or remaining time)
  can be set through options.
  Supports several parallel processing backends.",2020-08-18,Peter Solymos,https://github.com/psolymos/pbapply,TRUE,https://github.com/psolymos/pbapply,1499068,101,2021-08-24T02:24:14Z,14842.257425742575
pbdSLAP,"Utilizing scalable linear algebra packages mainly
        including 'BLACS', 'PBLAS', and 'ScaLAPACK' in double precision via
        'pbdMPI' based on 'ScaLAPACK' version 2.0.2.",2021-06-14,Wei-Chen Chen,https://pbdr.org/,TRUE,https://github.com/snoweye/pbdslap,27940,0,2021-06-13T01:23:06Z,NA
pbdZMQ,"'ZeroMQ' is a well-known library for high-performance
    asynchronous messaging in scalable, distributed applications.  This
    package provides high level R wrapper functions to easily utilize
    'ZeroMQ'. We mainly focus on interactive client/server programming
    frameworks. For convenience, a minimal 'ZeroMQ' library (4.2.2)
    is shipped with 'pbdZMQ', which can be used if no system installation
    of 'ZeroMQ' is available.  A few wrapper functions compatible with
    'rzmq' are also provided.",2021-02-10,Wei-Chen Chen,https://pbdr.org/,TRUE,https://github.com/snoweye/pbdzmq,422949,15,2021-04-18T21:41:13Z,28196.6
pbixr,"Access data and metadata from 'Microsoft' 'Power BI' ('.pbix', <https://powerbi.microsoft.com>) documents with R. The 'pbixr' package enables one to extract 'Power Query M' formulas (<https://docs.microsoft.com/en-us/power-query/>) 'Data Analysis Expressions' queries ('DAX', <https://docs.microsoft.com/en-us/dax/>) and their properties, report layout and style, and data and data models.",2020-10-27,Don Diproto,https://github.com/pbixr/pbixr,TRUE,https://github.com/pbixr/pbixr,5701,5,2020-10-27T09:58:33Z,1140.2
pbm,"Binding models which are useful when analysing protein-ligand interactions by techniques such as Biolayer Interferometry (BLI) or Surface Plasmon Resonance (SPR). Naman B. Shah, Thomas M. Duncan (2014) <doi:10.3791/51383>. Hoang H. Nguyen et al. (2015) <doi:10.3390/s150510481>. After initial binding parameters are known, binding curves can be simulated and parameters can be varied. The models within this package may also be used to fit a curve to measured binding data using non-linear regression.",2021-03-28,Jonathan Davies,https://github.com/jonathanrd/pbm,TRUE,https://github.com/jonathanrd/pbm,11776,2,2021-03-31T08:32:21Z,5888
pcadapt,"Methods to detect genetic markers involved in biological
    adaptation. 'pcadapt' provides statistical tools for outlier detection based on
    Principal Component Analysis. Implements the method described in (Luu, 2016)
    <DOI:10.1111/1755-0998.12592>.",2020-05-05,Florian Privé,https://github.com/bcm-uga/pcadapt,TRUE,https://github.com/bcm-uga/pcadapt,28408,25,2020-11-18T16:32:32Z,1136.32
pcal,"Calibrate p-values under a robust perspective using the methods developed
    by Sellke, Bayarri, and Berger (2001) <doi:10.1198/000313001300339950> and obtain 
    measures of the evidence provided by the data in favor of point null hypotheses 
    which are safer and more straightforward to interpret.",2020-07-06,Pedro Fonseca,"https://pedro-teles-fonseca.github.io/pcal/,
https://github.com/pedro-teles-fonseca/pcal",TRUE,https://github.com/pedro-teles-fonseca/pcal,4507,0,2020-11-17T23:53:39Z,NA
PCAmatchR,"Matches cases to controls based on genotype principal components (PC). 
      In order to produce better results, matches are based on the weighted 
      distance of PCs where the weights are equal to the % variance explained 
      by that PC. A weighted Mahalanobis distance metric (Kidd et al. (1987)
      <DOI:10.1016/0031-3203(87)90066-5>) is used to determine matches. ",2021-01-10,Derek W. Brown,https://github.com/machiela-lab/PCAmatchR,TRUE,https://github.com/machiela-lab/pcamatchr,6582,5,2021-01-19T12:34:02Z,1316.4
pccc,"An implementation of the pediatric complex chronic conditions (CCC)
    classification system using R and C++.",2020-06-02,Seth Russell,https://github.com/CUD2V/pccc,TRUE,https://github.com/cud2v/pccc,15895,4,2021-04-19T16:19:07Z,3973.75
pcFactorStan,"Provides convenience functions and pre-programmed
    Stan models related to the paired comparison factor model. Its purpose
    is to make fitting paired comparison data using Stan easy. This
    package is described in Pritikin (2020) <doi:10.1016/j.heliyon.2020.e04821>.",2020-09-15,Joshua N. Pritikin,https://github.com/jpritikin/pcFactorStan,TRUE,https://github.com/jpritikin/pcfactorstan,17435,2,2021-01-27T14:45:38Z,8717.5
PCMBase,"Phylogenetic comparative methods represent models of continuous trait 
  data associated with the tips of a phylogenetic tree. Examples of such models 
  are Gaussian continuous time branching stochastic processes such as Brownian 
  motion (BM) and Ornstein-Uhlenbeck (OU) processes, which regard the data at the 
  tips of the tree as an observed (final) state of a Markov process starting from 
  an initial state at the root and evolving along the branches of the tree. The 
  PCMBase R package provides a general framework for manipulating such models. 
  This framework consists of an application programming interface for specifying 
  data and model parameters, and efficient algorithms for simulating trait evolution 
  under a model and calculating the likelihood of model parameters for an assumed
  model and trait data. The package implements a growing collection of models, 
  which currently includes BM, OU, BM/OU with jumps, two-speed OU as well as mixed 
  Gaussian models, in which different types of the above models can be associated 
  with different branches of the tree. The PCMBase package is limited to 
  trait-simulation and likelihood calculation of (mixed) Gaussian phylogenetic 
  models. The PCMFit package provides functionality for inference of 
  these models to tree and trait data. The package web-site 
  <https://venelin.github.io/PCMBase/>
  provides access to the documentation and other resources. ",2021-06-07,Venelin Mitov,"https://venelin.github.io/PCMBase/, https://venelin.github.io",TRUE,https://github.com/venelin/pcmbase,14573,4,2021-07-22T17:19:21Z,3643.25
pcoxtime,Fits penalized models for both time-independent and time-dependent survival data. It fully implements elastic net and uses proximal gradient descent to solve the optimization problem. The package is an implementation of Steve Cygu and Benjamin M. Bolker. (2021) <arXiv:2102.02297>.,2021-07-06,Bicko Cygu,https://github.com/CYGUBICKO/pcoxtime-pkg,TRUE,https://github.com/cygubicko/pcoxtime-pkg,2645,2,2021-07-13T09:45:59Z,1322.5
PCRedux,"Extracts features from amplification curve data of quantitative 
    Polymerase Chain Reactions (qPCR) (Pabinger S. et al. (2014) 
    <doi:10.1016/j.bdq.2014.08.002>) for machine learning purposes. Helper 
    functions prepare the amplification curve data for processing as functional 
    data (e.g., Hausdorff distance) or enable the plotting of amplification 
    curve classes (negative, ambiguous, positive). The hookreg() and hookregNL() 
    functions (Burdukiewicz M. et al. (2018) <doi:10.1016/j.bdq.2018.08.001>) 
    can be used to predict amplification curves with an hook effect-like 
    curvature. The pcrfit_single() function can be used to extract features 
    from an amplification curve.",2021-03-16,Stefan Roediger,https://CRAN.R-project.org/package=PCRedux,TRUE,https://github.com/pcruniversum/pcredux,14310,5,2021-03-14T13:59:21Z,2862
pcsstools,"Defines functions to describe regression models using only
    pre-computed summary statistics (i.e. means, variances, and covariances)
    in place of individual participant data.
    Possible models include linear models for linear combinations, products, 
    and logical combinations of phenotypes.
    Implements methods presented in 
    Wolf et al. (2021) <doi:10.1101/2021.03.08.433979>
    Wolf et al. (2020) <doi:10.1142/9789811215636_0063> and 
    Gasdaska et al. (2019) <doi:10.1142/9789813279827_0036>.",2021-03-23,Jack Wolf,https://github.com/jackmwolf/pcsstools/,TRUE,https://github.com/jackmwolf/pcsstools,1663,2,2021-03-23T20:28:43Z,831.5
pct,"Functions and example data to teach and
  increase the reproducibility of the methods and code underlying 
  the Propensity to Cycle Tool (PCT), a research project and web application 
  hosted at <https://www.pct.bike/>. 
  For an academic paper on the methods,
  see Lovelace et al (2017) <doi:10.5198/jtlu.2016.862>.",2021-06-25,Robin Lovelace,"https://itsleeds.github.io/pct/, https://github.com/ITSLeeds/pct",TRUE,https://github.com/itsleeds/pct,20412,12,2021-08-25T14:56:00Z,1701
pdi,"Oak declines are complex disease syndromes and consist of many visual indicators that include aspects of tree size, crown condition and trunk condition. This can cause difficulty in the manual classification of symptomatic and non-symptomatic trees from what is in reality a broad spectrum of oak tree health condition. Two phenotypic oak decline indexes have been developed to quantitatively describe and differentiate oak decline syndromes in Quercus robur. This package provides a toolkit to generate these decline indexes from phenotypic descriptors using the machine learning algorithm random forest. The methodology for generating these indexes is outlined in Finch et al. (2121) <doi:10.1016/j.foreco.2021.118948>.",2021-02-09,Jasen Finch,https://jasenfinch.github.io/pdi,TRUE,https://github.com/jasenfinch/pdi,4650,0,2021-02-09T13:27:22Z,NA
pdp,"A general framework for constructing partial dependence (i.e., 
  marginal effect) plots from various types machine learning models in R.",2018-08-27,Brandon Greenwell,"https://bgreenwell.github.io/pdp/index.html,
https://github.com/bgreenwell/pdp",TRUE,https://github.com/bgreenwell/pdp,217464,73,2021-08-13T15:24:29Z,2978.958904109589
pdqr,"Create, transform, and summarize custom random
    variables with distribution functions (analogues of 'p*()', 'd*()',
    'q*()', and 'r*()' functions from base R). Two types of distributions
    are supported: ""discrete"" (random variable has finite number of output
    values) and ""continuous"" (infinite number of values in the form of
    continuous random variable). Functions for distribution
    transformations and summaries are available. Implemented approaches
    often emphasize approximate and numerical solutions: all distributions
    assume finite support and finite values of density function; some
    methods implemented with simulation techniques.",2021-01-21,Evgeni Chasnovski,"https://github.com/echasnovski/pdqr,
https://echasnovski.github.io/pdqr/",TRUE,https://github.com/echasnovski/pdqr,9030,12,2021-01-22T19:04:31Z,752.5
PDQutils,"A collection of tools for approximating the 'PDQ' functions
    (respectively, the cumulative distribution, density, and quantile) of
    probability distributions via classical expansions involving moments and
    cumulants.",2017-03-18,Steven E. Pav,https://github.com/shabbychef/PDQutils,TRUE,https://github.com/shabbychef/pdqutils,23003,5,2021-04-01T19:20:15Z,4600.6
pdynmc,"Linear dynamic panel data modeling based on linear and
    nonlinear moment conditions as proposed by
    Holtz-Eakin, Newey, and Rosen (1988) <doi:10.2307/1913103>,
    Ahn and Schmidt (1995) <doi:10.1016/0304-4076(94)01641-C>,
    and Arellano and Bover (1995) <doi:10.1016/0304-4076(94)01642-D>.
    Estimation of the model parameters relies on the Generalized
    Method of Moments (GMM), numerical optimization (when nonlinear
    moment conditions are employed) and the computation of closed
    form solutions (when estimation is based on linear moment
    conditions). One-step, two-step and iterated estimation is
    available. of closed form solutions. For inference and specification
    testing, Windmeijer (2005) <doi:10.1016/j.jeconom.2004.02.005>
    corrected standard errors, serial correlation tests, tests for
    overidentification, and Wald tests are available. Functions for
    visualizing panel data structures and modeling results obtained
    from GMM estimation are also available. The plot methods include
    functions to plot unbalanced panel structure, coefficient ranges
    and coefficient paths across GMM iterations (the latter is
    implemented according to the plot shown in
    Hansen and Lee, 2021 <doi:10.3982/ECTA16274>).",2021-08-13,Markus Fritsch,https://github.com/markusfritsch/pdynmc,TRUE,https://github.com/markusfritsch/pdynmc,9340,1,2021-09-01T13:08:56Z,9340
peacesciencer,"These are useful tools and data sets for the study of quantitative 
    peace science. The goal for this package is to include tools and data sets
    for doing original research that mimics well what a user would have to previously
    get from a software package that may not be well-sourced or well-supported.
    Those software bundles were useful the extent to which they encourage replications 
    of long-standing analyses by starting the data-generating process from scratch. However, 
    a lot of the functionality can be done relatively quickly and more transparently
    in the R programming language.",2021-06-21,Steve Miller,https://github.com/svmiller/peacesciencer/,TRUE,https://github.com/svmiller/peacesciencer,3957,10,2021-07-28T13:12:57Z,395.7
peakRAM,"When working with big data sets, RAM conservation is critically
    important. However, it is not always enough to just monitor the
    size of the objects created. So-called ""copy-on-modify"" behavior,
    characteristic of R, means that some expressions or functions may
    require an unexpectedly large amount of RAM overhead. For example,
    replacing a single value in a matrix duplicates that matrix in the
    back-end, making this task require twice as much RAM as that used
    by the matrix itself. This package makes it easy to monitor the total
    and peak RAM used so that developers can quickly identify and
    eliminate RAM hungry code.",2017-01-16,Thomas Quinn,http://github.com/tpq/peakRAM,TRUE,https://github.com/tpq/peakram,27741,14,2021-08-27T04:55:16Z,1981.5
pedbuildr,"Reconstruct pedigrees from genotype data, by optimising the likelihood 
    over all possible pedigrees subject to given restrictions. Tailor-made plots
    facilitate evaluation of the output. This package is part of the 'ped suite'
    ecosystem for pedigree analysis. In particular, it imports 'pedprobr' for 
    calculating pedigree likelihoods and 'forrel' for estimating pairwise relatedness.",2021-03-16,Magnus Dehli Vigeland,https://github.com/magnusdv/pedbuildr,TRUE,https://github.com/magnusdv/pedbuildr,1513,1,2021-06-07T13:22:13Z,1513
pedigreeTools,"Tools to sort, edit and prune pedigrees and to extract the inbreeding coefficients
    and the relationship matrix (includes code for pedigrees from self-pollinated species). 
    The use of pedigree data is central to genetics research within the animal and plant breeding communities to predict 
    breeding values. The relationship matrix between the individuals can be derived from pedigree structure following
    the algorithms described for example in Vazquez et al., 2010 <doi:10.2527/jas.2009-1952>.",2018-12-09,Ana Ines Vazquez,https://github.com/Rpedigree/pedigreeTools/,TRUE,https://github.com/rpedigree/pedigreetools,11452,5,2020-10-13T18:34:24Z,2290.4
pedmut,"A collection of functions for modeling mutations in pedigrees with 
    marker data, as used e.g. in likelihood computations with microsatellite data.
    Implemented models include proportional and stepwise models, as well as random 
    models for experimental work, and custom models allowing the user to apply any 
    valid mutation matrix. Allele lumping is done following the lumpability criteria 
    of Kemeny and Snell (1976), ISBN:0387901922.",2020-07-30,Magnus Dehli Vigeland,https://github.com/magnusdv/pedmut,TRUE,https://github.com/magnusdv/pedmut,9941,1,2020-11-13T13:28:00Z,9941
pedprobr,"An implementation of the Elston-Stewart algorithm for calculating
    pedigree likelihoods given genetic marker data (Elston and Stewart (1971) 
    <doi:10.1159/000152448>). The standard algorithm is extended to allow inbred 
    founders. 'pedprobr' is part of the ped suite, a collection of packages for 
    pedigree analysis in R, based on 'pedtools' for handling pedigrees and 
    markers. Mutation modelling is supported by the 'pedmut' package.",2021-03-27,Magnus Dehli Vigeland,https://github.com/magnusdv/pedprobr,TRUE,https://github.com/magnusdv/pedprobr,12466,3,2021-08-27T13:02:01Z,4155.333333333333
pedquant,"
    Provides an interface to access public economic and financial data for 
    economic research and quantitative analysis. The data sources including 
    NBS, FRED, Yahoo Finance, 163 Finance and etc. ",2020-08-27,Shichen Xie,https://github.com/ShichenXie/pedquant,TRUE,https://github.com/shichenxie/pedquant,16178,26,2021-08-19T14:27:06Z,622.2307692307693
pedSimulate,"Simulate pedigree, genetic merits and phenotypes with random/non-random matings followed by random/non-random selection with different intensities and patterns in males and females.
   Bijma, P. & Rutten, M. (2002) <https://www.wur.nl/en/Research-Results/Chair-groups/Animal-Sciences/Animal-Breeding-and-Genomics-Group/Research/Software.htm>.",2021-08-18,Mohammad Ali Nilforooshan,https://github.com/nilforooshan/pedSimulate,TRUE,https://github.com/nilforooshan/pedsimulate,12814,0,2021-08-18T01:12:16Z,NA
pedsuite,"The 'ped suite' is a collection of packages for pedigree
    analysis, covering applications in forensic genetics, medical genetics
    and more. A detailed presentation of the 'ped suite' is given in the
    book 'Pedigree Analysis in R' (Vigeland, 2021, ISBN: 9780128244302).",2021-06-21,Magnus Dehli Vigeland,"https://magnusdv.github.io/pedsuite/,
https://github.com/magnusdv/pedsuite",TRUE,https://github.com/magnusdv/pedsuite,917,2,2021-08-27T07:20:45Z,458.5
pedtools,"A comprehensive collection of tools for creating,
    manipulating and visualising pedigrees and genetic marker data.
    Pedigrees can be read from text files or created on the fly with
    built-in functions. A range of utilities enable modifications like
    adding or removing individuals, breaking loops, and merging pedigrees.
    Pedigree plots are produced by wrapping the plotting functionality of
    the 'kinship2' package. A Shiny app for creating pedigrees, based on
    'pedtools', is available at <https://magnusdv.shinyapps.io/quickped>.
    'pedtools' is the hub of the 'ped suite', a collection of packages for
    pedigree analysis. A detailed presentation of the 'ped suite' is given
    in the book 'Pedigree Analysis in R' (Vigeland, 2021,
    ISBN:9780128244302).",2021-06-22,Magnus Dehli Vigeland,"https://github.com/magnusdv/pedtools,
https://magnusdv.github.io/pedsuite/",TRUE,https://github.com/magnusdv/pedtools,13192,10,2021-06-27T20:01:34Z,1319.2
PeerPerformance,"Provides functions to perform the peer performance
    analysis of funds' returns as described in Ardia and Boudt (2018) <doi:10.1016/j.jbankfin.2017.10.014>.",2021-05-16,David Ardia,https://github.com/ArdiaD/PeerPerformance,TRUE,https://github.com/ardiad/peerperformance,21599,8,2021-05-16T15:39:58Z,2699.875
penaltyLearning,"Implementations of algorithms from 
 Learning Sparse Penalties for Change-point Detection
 using Max Margin Interval Regression, by
 Hocking, Rigaill, Vert, Bach
 <http://proceedings.mlr.press/v28/hocking13.html>
 published in proceedings of ICML2013.",2020-05-14,Toby Dylan Hocking,https://github.com/tdhock/penaltyLearning,TRUE,https://github.com/tdhock/penaltylearning,18416,12,2021-04-21T21:44:15Z,1534.6666666666667
penfa,"Fits single- and multiple-group penalized factor analysis models 
    via a trust-region algorithm with integrated automatic multiple tuning 
    parameter selection (Geminiani et al., 2021 <doi:10.1007/s11336-021-09751-8>). 
    Available penalties include lasso, adaptive lasso, scad, mcp, and ridge. ",2021-07-17,Elena Geminiani,https://github.com/egeminiani/penfa,TRUE,https://github.com/egeminiani/penfa,755,2,2021-07-20T11:46:25Z,377.5
pense,"Robust penalized (adaptive) elastic net S and M estimators for
    linear regression. The methods are proposed in
    Cohen Freue, G. V., Kepplinger, D., Salibián-Barrera, M., and Smucler, E.
    (2019) <https://projecteuclid.org/euclid.aoas/1574910036>.
    The package implements the extensions and algorithms described in
    Kepplinger, D. (2020) <doi:10.14288/1.0392915>.",2021-07-07,David Kepplinger,"https://dakep.github.io/pense-rpkg/,
https://github.com/dakep/pense-rpkg",TRUE,https://github.com/dakep/pense-rpkg,19447,1,2021-07-07T21:30:33Z,19447
pensim,"Simulation of continuous, correlated high-dimensional data with 
    time to event or binary response, and parallelized functions for Lasso, 
    Ridge, and Elastic Net penalized regression with repeated starts and 
    two-dimensional tuning of the Elastic Net.",2021-05-11,Levi Waldron,https://waldronlab.io/pensim,TRUE,https://github.com/waldronlab/pensim,19845,0,2021-05-11T11:08:56Z,NA
peperr,"Designed for prediction error estimation
        through resampling techniques, possibly accelerated by parallel
        execution on a compute cluster. Newly developed model fitting
        routines can be easily incorporated. Methods used in the package are detailed in
        Porzelius Ch., Binder H. and Schumacher M. (2009) <doi:10.1093/bioinformatics/btp062>
        and were used, for instance, in
        Porzelius Ch., Schumacher M.and  Binder H. (2011) <doi:10.1007/s00180-011-0236-6>.",2021-03-16,Christine Porzelius,"https://github.com/fbertran/peperr/,
https://fbertran.github.io/peperr/",TRUE,https://github.com/fbertran/peperr,27160,1,2021-04-12T17:13:35Z,27160
pepr,"A PEP, or Portable Encapsulated Project, is a dataset that 
    subscribes to the PEP structure for organizing metadata. It is written using
    a simple YAML + CSV format, it is your one-stop solution to metadata 
    management across data analysis environments. This package reads this 
    standardized project configuration structure into R.",2020-10-16,Michal Stolarczyk,NA,TRUE,https://github.com/pepkit/pepr,5461,3,2020-10-15T23:56:49Z,1820.3333333333333
Peptides,Includes functions to calculate several physicochemical properties and indices for amino-acid sequences as well as to read and plot 'XVG' output files from the 'GROMACS' molecular dynamics package.,2021-05-14,Daniel Osorio,https://github.com/dosorio/Peptides/,TRUE,https://github.com/dosorio/peptides,41788,47,2021-05-14T13:49:02Z,889.1063829787234
performance,"Utilities for computing measures to assess model quality,
    which are not directly provided by R's 'base' or 'stats' packages.
    These include e.g. measures like r-squared, intraclass correlation
    coefficient (Nakagawa, Johnson & Schielzeth (2017)
    <doi:10.1098/rsif.2017.0213>), root mean squared error or functions to
    check models for overdispersion, singularity or zero-inflation and
    more. Functions apply to a large variety of regression models,
    including generalized linear models, mixed effects models and Bayesian
    models.",2021-07-21,Daniel Lüdecke  (<https://orcid.org/0000-0002-8895-3206>,https://easystats.github.io/performance/,TRUE,https://github.com/easystats/performance,701554,447,2021-09-02T07:26:58Z,1569.4720357941835
PerformanceAnalytics,"Collection of econometric functions for performance and risk 
    analysis. In addition to standard risk and performance metrics, this 
    package aims to aid practitioners and researchers in utilizing the latest
    research in analysis of non-normal return streams.  In general, it is most 
    tested on return (rather than price) data on a regular scale, but most 
    functions will work with irregular return data as well, and increasing
    numbers of functions will work with P&L or price data where possible.",2020-02-06,Brian G. Peterson,https://github.com/braverock/PerformanceAnalytics,TRUE,https://github.com/braverock/performanceanalytics,1236388,153,2021-07-17T23:51:49Z,8080.967320261438
periscope,"An enterprise-targeted scalable and UI-standardized 'shiny' framework 
    including a variety of developer convenience functions with the goal of both 
    streamlining robust application development while assisting with creating a 
    consistent user experience regardless of application or developer.",2021-04-07,Constance Brett,"https://github.com/cb4ds/periscope, http://periscopeapps.org:3838",TRUE,https://github.com/cb4ds/periscope,18922,14,2021-04-07T18:24:42Z,1351.5714285714287
permutations,Manipulates invertible functions from a finite set to itself.  Can transform from word form to cycle form and back.,2020-11-12,Robin K. S. Hankin,https://github.com/RobinHankin/permutations,TRUE,https://github.com/robinhankin/permutations,26278,1,2021-08-27T09:44:03Z,26278
permutes,"Helps you determine the analysis window to use when analyzing densely-sampled
    time-series data, such as EEG data, using permutation testing (Maris & Oostenveld, 2007)
    <doi:10.1016/j.jneumeth.2007.03.024>. These permutation tests can help identify the timepoints
    where significance of an effect begins and ends, and the results can be plotted in various
    types of heatmap for reporting. Mixed-effects models are supported using an implementation of
    the approach by Lee & Braun (2012) <doi:10.1111/j.1541-0420.2011.01675.x>.",2021-09-02,Cesko C. Voeten,NA,TRUE,https://github.com/cvoeten/permutes,14247,2,2021-09-02T18:32:01Z,7123.5
personalized,"Provides functions for fitting and validation of models for subgroup
    identification and personalized medicine / precision medicine under the general subgroup
    identification framework of Chen et al. (2017) <doi:10.1111/biom.12676>.
    This package is intended for use for both randomized controlled trials and
    observational studies and is described in detail in Huling and Yu (2021) 
    <doi:10.18637/jss.v098.i05>.",2021-05-28,Jared Huling,"https://jaredhuling.org/personalized/,
https://arxiv.org/abs/1809.07905",TRUE,https://github.com/jaredhuling/personalized,20613,18,2021-05-31T18:34:02Z,1145.1666666666667
personalized2part,"Implements the methodology of Huling, Smith, and 
    Chen (2020) <doi:10.1080/01621459.2020.1801449>, which allows for subgroup identification 
    for semi-continuous outcomes by estimating individualized treatment rules. It uses a two-part 
    modeling framework to handle semi-continuous data by separately modeling the positive part 
    of the outcome and an indicator of whether each outcome is positive, but still results in a 
    single treatment rule. High dimensional data is handled with a cooperative lasso penalty, 
    which encourages the coefficients in the two models to have the same sign. ",2020-09-10,Jared Huling,https://github.com/jaredhuling/personalized2part,TRUE,https://github.com/jaredhuling/personalized2part,3986,1,2020-09-12T15:47:34Z,3986
personalr,"Functions to setup a personal R package that
    attaches given libraries and exports personal helper functions.",2020-11-23,Sebastian Carl,https://github.com/mrcaseb/personalr,TRUE,https://github.com/mrcaseb/personalr,3054,5,2021-07-03T14:49:37Z,610.8
personr,"An R-package-version of an open online science-based personality
    test from <https://openpsychometrics.org/tests/IPIP-BFFM/>,
    providing a better-designed interface and a more detailed report.
    The core command launch_test() opens a personality test in your browser,
    and generates a report after you click ""Submit"". In this report, your results
    are compared with other people's, to show what these results mean.
    Other people's data is from <https://openpsychometrics.org/_rawdata/BIG5.zip>.",2020-06-24,Renfei Mao,https://github.com/flujoo/personr,TRUE,https://github.com/flujoo/personr,4143,0,2020-11-17T17:56:52Z,NA
peRspective,"Interface to the 'Perspective' API, which can be found at the following URL: <https://github.com/conversationai/perspectiveapi#perspective-comment-analyzer-api>. 
    The 'Perspective' API uses machine learning models to score the perceived impact a comment might have on a conversation (i.e. TOXICITY, INFLAMMATORY, etc.).     
    'peRspective' provides access to the API and returns tidy data frames with results of the specified machine learning model(s).",2021-07-14,Fabio Votta,"https://favstats.github.io/peRspective/,
https://github.com/favstats/peRspective",TRUE,https://github.com/favstats/perspective,10163,36,2021-07-13T13:41:57Z,282.30555555555554
peruse,A friendly API for sequence iteration and set comprehension.,2021-03-08,Jacob Goldsmith,"https://github.com/jacgoldsm/peruse,
https://jacgoldsm.github.io/peruse/",TRUE,https://github.com/jacgoldsm/peruse,3486,1,2021-03-08T07:05:46Z,3486
pesel,"Automatic estimation of number of principal components in PCA
    with PEnalized SEmi-integrated Likelihood (PESEL). See Piotr Sobczyk, Malgorzata Bogdan, Julie Josse
    'Bayesian dimensionality reduction with PCA using penalized semi-integrated likelihood' 
    (2017) <doi:10.1080/10618600.2017.1340302>.",2020-03-04,Piotr Sobczyk,https://github.com/psobczyk/pesel,TRUE,https://github.com/psobczyk/pesel,15625,4,2021-06-17T20:34:35Z,3906.25
pestr,"Set of tools to automatize extraction of data on pests from 'EPPO
    Data Services' and 'EPPO Global Database' and to put them into tables with
    human readable format. Those function use 'EPPO database API', thus you 
    first need to register on <https://data.eppo.int> (free of charge).
    Additional helpers allow to download, check and connect to
    'SQLite EPPO database'.",2021-01-20,Michal Jan Czyz,https://github.com/mczyzj/pestr,TRUE,https://github.com/mczyzj/pestr,2533,0,2021-01-23T15:26:09Z,NA
pewdata,"Reproducible, programmatic retrieval of survey datasets from the
    Pew Research Center.",2021-04-08,Frederick Solt,https://github.com/fsolt/pewdata,TRUE,https://github.com/fsolt/pewdata,16236,8,2021-04-14T05:54:40Z,2029.5
pfica,Performs penalized independent component analysis for univariate functional data [<doi:10.3390/math9111243>].,2021-06-03,Marc Vidal,https://github.com/m-vidal/pfica,TRUE,https://github.com/m-vidal/pfica,4382,0,2021-06-16T15:38:02Z,NA
PGRdup,"Provides functions to aid the identification of probable/possible
    duplicates in Plant Genetic Resources (PGR) collections using
    'passport databases' comprising of information records of each constituent
    sample. These include methods for cleaning the data, creation of a
    searchable Key Word in Context (KWIC) index of keywords associated with
    sample records and the identification of nearly identical records with
    similar information by fuzzy, phonetic and semantic matching of keywords.",2021-02-17,J. Aravind,"https://cran.r-project.org/package=PGRdup,
https://github.com/aravind-j/PGRdup,
https://doi.org/10.5281/zenodo.841963,
https://aravind-j.github.io/PGRdup/,
https://www.rdocumentation.org/packages/PGRdup",TRUE,https://github.com/aravind-j/pgrdup,20423,1,2021-05-20T13:33:07Z,20423
pguIMP,"Reproducible cleaning of bio-medical laboratory data using methods of visualization,error correction and transformation implemented as interactive R-notebooks.",2021-07-22,Sebastian Malkusch,https://github.com/SMLMS/pguIMP,TRUE,https://github.com/smlms/pguimp,920,0,2021-07-19T10:34:40Z,NA
ph2rand,"Provides functions to assist with the design of randomized
    comparative phase II oncology trials that assume their primary outcome
    variable is Bernoulli distributed. Specifically, support is provided to (a)
    perform a sample size calculation when using one of several published
    designs, (b) evaluate the operating characteristics of a given design (both
    analytically and via simulation), and (c) produce informative plots.",2021-03-02,Michael Grayling,https://github.com/mjg211/ph2rand,TRUE,https://github.com/mjg211/ph2rand,1925,0,2021-03-01T13:39:34Z,NA
phangorn,"Allows for estimation of phylogenetic trees and networks
    using Maximum Likelihood, Maximum Parsimony, distance methods and
    Hadamard conjugation. Offers methods for tree comparison, model
    selection and visualization of phylogenetic networks as described in
    Schliep et al. (2017) <doi:10.1111/2041-210X.12760>.",2021-07-13,Klaus Schliep,https://github.com/KlausVigo/phangorn,TRUE,https://github.com/klausvigo/phangorn,436617,140,2021-08-25T17:32:34Z,3118.692857142857
pharmaRTF,"Enhanced RTF wrapper written in R for use with existing R tables
    packages such as 'Huxtable' or 'GT'. This package fills a gap where tables in
    certain packages can be written out to RTF, but cannot add certain metadata
    or features to the document that are required/expected in a report for a
    regulatory submission, such as multiple levels of titles and footnotes,
    making the document landscape, and controlling properties such as margins.",2021-05-27,Michael Stackhouse,NA,TRUE,https://github.com/atorus-research/pharmartf,7445,17,2021-05-27T16:21:55Z,437.94117647058823
phaseR,"Performs a qualitative analysis of one- and two-dimensional
   autonomous ordinary differential equation systems, using phase plane methods.
   Programs are available to identify and classify equilibrium points, plot the
   direction field, and plot trajectories for multiple initial conditions. In
   the one-dimensional case, a program is also available to plot the phase
   portrait. Whilst in the two-dimensional case, programs are additionally
   available to plot nullclines and stable/unstable manifolds of saddle points.
   Many example systems are provided for the user. For further details can be
   found in Grayling (2014) <doi:10.32614/RJ-2014-023>.",2019-10-12,Michael J Grayling,https://github.com/mjg211/phaseR,TRUE,https://github.com/mjg211/phaser,29162,9,2020-10-06T07:57:59Z,3240.222222222222
PheCAP,"Implement surrogate-assisted feature extraction (SAFE) and
             common machine learning approaches to train and validate phenotyping models.
             Background and details about the methods can be found at 
             Zhang et al. (2019) <doi:10.1038/s41596-019-0227-6>,
             Yu et al. (2017) <doi:10.1093/jamia/ocw135>, and 
             Liao et al. (2015) <doi:10.1136/bmj.h1885>.",2020-09-17,PARSE LTD,"https://celehs.github.io/PheCAP/, https://github.com/celehs/PheCAP",TRUE,https://github.com/celehs/phecap,4621,11,2021-02-23T06:01:23Z,420.09090909090907
phenesse,"Generates Weibull-parameterized estimates of phenology for any percentile of 
    a distribution using the framework established in Cooke (1979)
    <doi:10.1093/biomet/66.2.367>. Extensive testing against other 
    estimators suggest the weib_percentile() function is especially useful in 
    generating more accurate and less biased estimates of onset and offset 
    (Belitz et al. 2020 <doi.org:10.1111/2041-210X.13448>. Non-parametric 
    bootstrapping can be used to generate confidence intervals around those 
    estimates, although this is computationally expensive. Additionally, this 
    package offers an easy way to perform non-parametric bootstrapping to 
    generate confidence intervals for quantile estimates, mean estimates, 
    or any statistical function of interest.",2020-07-28,Michael Belitz,https://github.com/mbelitz/phenesse,TRUE,https://github.com/mbelitz/phenesse,9101,4,2021-02-03T16:54:59Z,2275.25
phenofit,"
    The merits of 'TIMESAT' and 'phenopix' are adopted. Besides, a simple and 
    growing season dividing method and a practical snow elimination method 
    based on Whittaker were proposed. 7 curve fitting methods and 4 phenology 
    extraction methods were provided. Parameters boundary are considered for 
    every curve fitting methods according to their ecological meaning. 
    And 'optimx' is used to select best optimization method for different 
    curve fitting methods.
    Reference:
    Dongdong Kong, R package: A state-of-the-art Vegetation Phenology extraction package, 
    phenofit version 0.2.3, <https://github.com/kongdd/phenofit>;
    Zhang, Q., Kong, D., Shi, P., Singh, V.P., Sun, P., 2018. Vegetation phenology 
    on the Qinghai-Tibetan Plateau and its response to climate change (1982–2013). 
    Agric. For. Meteorol. 248, 408–417. <doi:10.1016/j.agrformet.2017.10.026>.",2020-04-02,Dongdong Kong,https://github.com/kongdd/phenofit,TRUE,https://github.com/kongdd/phenofit,12593,38,2021-07-31T11:59:53Z,331.39473684210526
PheNorm,"The algorithm combines the most predictive variable, such as count of the main International Classification of Diseases (ICD) codes, and other Electronic Health Record (EHR) features (e.g. health utilization and processed clinical note data), to obtain a score for accurate risk prediction and disease classification. In particular, it normalizes the surrogate to resemble gaussian mixture and leverages the remaining features through random corruption denoising. Background and details about the method can be found at Yu et al. (2018) <doi:10.1093/jamia/ocx111>.",2021-01-07,Clara-Lea Bonzel,https://github.com/celehs/PheNorm,TRUE,https://github.com/celehs/phenorm,2572,2,2021-02-23T06:08:30Z,1286
PhenotypeSimulator,"Simulation is a critical part of method development and assessment
    in quantitative genetics. 'PhenotypeSimulator' allows for the flexible 
    simulation of phenotypes under different models, including genetic variant 
    and  infinitesimal genetic effects (reflecting population structure) as well 
    as non-genetic covariate effects, observational noise and additional 
    correlation effects. The different phenotype components are combined into a 
    final phenotype while controlling for the proportion of variance explained 
    by each of the components. For each effect component, the number of 
    variables, their distribution and the design of their effect across traits 
    can be customised. For the simulation of the genetic effects, external 
    genotype data from a number of standard software ('plink', 'hapgen2'/
    'impute2', 'genome', 'bimbam', simple text files) can be imported. The final 
    simulated phenotypes and its components can be automatically saved into .rds 
    or .csv files. In addition, they can be saved in formats compatible with 
    commonly used genetic association software ('gemma', 'bimbam', 'plink', 
    'snptest', 'LiMMBo'). ",2021-07-16,Hannah Meyer,https://github.com/HannahVMeyer/PhenotypeSimulator,TRUE,https://github.com/hannahvmeyer/phenotypesimulator,17710,21,2021-07-16T14:46:35Z,843.3333333333334
philentropy,"Computes 46 optimized distance and similarity measures for comparing probability functions (Drost (2018) <doi:10.21105/joss.00765>). These comparisons between probability functions have their foundations in a broad range of scientific disciplines from mathematics to ecology. The aim of this package is to provide a core framework for clustering, classification, statistical inference, goodness-of-fit, non-parametric statistics, information theory, and machine learning tasks that are based on comparing univariate or multivariate probability functions.",2021-05-12,Hajk-Georg Drost,https://github.com/drostlab/philentropy,TRUE,https://github.com/drostlab/philentropy,96815,81,2021-08-21T11:12:05Z,1195.2469135802469
phonenumber,"Convert English letters to numbers or numbers to English
    letters as on a telephone keypad. When converting letters to numbers,
    a character vector is returned with ""A,"" ""B,"" or ""C"" becoming 2, ""D,""
    ""E"", or ""F"" becoming 3, etc. When converting numbers to letters, a
    character vector is returned with multiple elements (i.e., ""2"" becomes
    a vector of ""A,"" ""B,"" and ""C"").",2021-05-01,Steve Myles,"https://stevemyles.site/phonenumber/,
https://github.com/scumdogsteev/phonenumber",TRUE,https://github.com/scumdogsteev/phonenumber,15753,1,2021-05-02T00:31:11Z,15753
phonfieldwork,"There are a lot of different typical tasks that have to be solved during phonetic research and experiments. This includes creating a presentation that will contain all stimuli, renaming and concatenating multiple sound files recorded during a session, automatic annotation in 'Praat' TextGrids (this is one of the sound annotation standards provided by 'Praat' software, see Boersma & Weenink 2020 <https://www.fon.hum.uva.nl/praat/>), creating an html table with annotations and spectrograms, and converting multiple formats ('Praat' TextGrid, 'ELAN', 'EXMARaLDA', 'Audacity', subtitles '.srt', and 'FLEx' flextext). All of these tasks can be solved by a mixture of different tools (any programming language has programs for automatic renaming, and Praat contains scripts for concatenating and renaming files, etc.). 'phonfieldwork' provides a functionality that will make it easier to solve those tasks independently of any additional tools. You can also compare the functionality with other packages: 'rPraat' <https://CRAN.R-project.org/package=rPraat>, 'textgRid' <https://CRAN.R-project.org/package=textgRid>.",2021-03-02,George Moroz,"https://CRAN.R-project.org/package=phonfieldwork,
https://docs.ropensci.org/phonfieldwork/",TRUE,https://github.com/ropensci/phonfieldwork,11584,12,2021-08-21T19:43:27Z,965.3333333333334
phonics,"Provides a collection of phonetic algorithms including
    Soundex, Metaphone, NYSIIS, Caverphone, and others.  The package is
    documented in <doi:10.18637/jss.v095.i08>.",2021-07-11,James Howard,https://jameshoward.us/phonics-in-r/,TRUE,https://github.com/k3jph/phonics-in-r,27073,22,2021-07-11T12:51:56Z,1230.590909090909
photobiology,"Definitions of classes, methods, operators and functions for use 
    in photobiology and radiation meteorology and climatology. Calculation of
    effective (weighted) and not-weighted irradiances/doses, fluence rates,
    transmittance, reflectance, absorptance, absorbance and diverse ratios and 
    other derived quantities from spectral data. Local maxima and minima: peaks,
    valleys and spikes. Conversion between energy-and photon-based units. 
    Wavelength interpolation. Astronomical calculations related solar angles and 
    day length. Colours and vision. This package is part of the 'r4photobiology' 
    suite, Aphalo, P. J. (2015) <doi:10.19232/uv4pb.2015.1.14>.",2021-04-04,Pedro J. Aphalo,"https://docs.r4photobiology.info/photobiology/,
https://github.com/aphalo/photobiology",TRUE,https://github.com/aphalo/photobiology,41635,0,2021-07-27T22:24:49Z,NA
PhotosynQ,"Connect R to the PhotosynQ platform (<https://photosynq.org>). It allows to login and logout,
    as well as receive project information and project data. Further it transforms the received JSON objects
    into a data frame, which can be used for the final data analysis.",2021-07-13,Sebastian Kuhlgert,https://github.com/Photosynq/PhotosynQ-R,TRUE,https://github.com/photosynq/photosynq-r,656,4,2021-07-14T20:13:00Z,164
photosynthesis,"Contains modeling and analytical tools for plant ecophysiology.
    MODELING: Simulate C3 photosynthesis using the Farquhar, von Caemmerer,
    Berry (1980) <doi:10.1007/BF00386231> model as described in Buckley and
    Diaz-Espejo (2015) <doi:10.1111/pce.12459>. It uses units to ensure that
    parameters are properly specified and transformed before calculations.
    Temperature response functions get automatically ""baked"" into all
    parameters based on leaf temperature following Bernacchi et al. (2002)
    <doi:10.1104/pp.008250>. The package includes boundary layer, cuticular,
    stomatal, and mesophyll conductances to CO2, which each can vary on the
    upper and lower portions of the leaf. Use straightforward functions to
    simulate photosynthesis over environmental gradients such as Photosynthetic
    Photon Flux Density (PPFD) and leaf temperature, or over trait gradients
    such as CO2 conductance or photochemistry. 
    ANALYTICAL TOOLS: Fit ACi (Farquhar et al. 1980 <doi:10.1007/BF00386231>)
    and AQ curves (Marshall & Biscoe 1980 <doi:10.1093/jxb/31.1.29>),
    temperature responses (Heskel et al. 2016 <doi:10.1073/pnas.1520282113>;
    Kruse et al. 2008 <doi:10.1111/j.1365-3040.2008.01809.x>, Medlyn et al.
    2002 <doi:10.1046/j.1365-3040.2002.00891.x>, Hobbs et al. 2013
    <doi:10.1021/cb4005029>), respiration in the light (Kok 1956
    <doi:10.1016/0006-3002(56)90003-8>, Walker & Ort 2015 <doi:10.1111/pce.12562>,
    Yin et al. 2009 <doi:10.1111/j.1365-3040.2009.01934.x>, Yin et al. 2011
    <doi:10.1093/jxb/err038>), mesophyll conductance (Harley et al. 1992
    <doi:10.1104/pp.98.4.1429>), pressure-volume curves (Koide et al. 2000
    <doi:10.1007/978-94-009-2221-1_9>, Sack et al. 2003
    <doi:10.1046/j.0016-8025.2003.01058.x>, Tyree et al. 1972
    <doi:10.1093/jxb/23.1.267>), hydraulic vulnerability curves (Ogle et al. 2009
    <doi:10.1111/j.1469-8137.2008.02760.x>, Pammenter et al. 1998
    <doi:10.1093/treephys/18.8-9.589>), and tools for running sensitivity
    analyses particularly for variables with uncertainty (e.g. g_mc, gamma_star,
    R_d).",2021-07-01,Chris Muir,https://github.com/cdmuir/photosynthesis,TRUE,https://github.com/cdmuir/photosynthesis,10855,7,2021-07-01T00:23:17Z,1550.7142857142858
phyclust,"Phylogenetic clustering (phyloclustering) is an evolutionary
        Continuous Time Markov Chain model-based approach to identify
        population structure from molecular data without assuming
        linkage equilibrium. The package phyclust (Chen 2011) provides a
        convenient implementation of phyloclustering for DNA and SNP data,
        capable of clustering individuals into subpopulations and identifying
        molecular sequences representative of those subpopulations. It is
        designed in C for performance, interfaced with R for visualization,
        and incorporates other popular open source programs including
        ms (Hudson 2002) <doi:10.1093/bioinformatics/18.2.337>,
        seq-gen (Rambaut and Grassly 1997)
        <doi:10.1093/bioinformatics/13.3.235>,
        Hap-Clustering (Tzeng 2005) <doi:10.1002/gepi.20063> and
        PAML baseml (Yang 1997, 2007) <doi:10.1093/bioinformatics/13.5.555>,
        <doi:10.1093/molbev/msm088>,
        for simulating data, additional analyses, and searching the best tree.
        See the phyclust website for more information, documentations and
        examples.",2021-02-10,Wei-Chen Chen,https://snoweye.github.io/phyclust/,TRUE,https://github.com/snoweye/phyclust,45580,5,2021-02-10T02:13:54Z,9116
phylin,"The spatial interpolation of genetic distances between
	     samples is based on a modified kriging method that
	     accepts a genetic distance matrix and generates a map of
	     probability of lineage presence. This package also offers
	     tools to generate a map of  potential contact zones
	     between groups with user-defined thresholds in the tree
	     to account for old and recent divergence. Additionally,
	     it has functions for IDW interpolation using genetic data
	     and midpoints.",2019-12-12,Pedro Tarroso,"https://www.r-project.org, https://github.com/ptarroso/phylin",TRUE,https://github.com/ptarroso/phylin,19511,3,2021-03-10T17:13:51Z,6503.666666666667
phylocomr,"Interface to 'Phylocom' (<http://phylodiversity.net/phylocom/>),
    a library for analysis of 'phylogenetic' community structure and
    character evolution. Includes low level methods for interacting with
    the three executables, as well as higher level interfaces for methods
    like 'aot', 'ecovolve', 'bladj', 'phylomatic', and more.",2019-12-20,Scott Chamberlain,"https://docs.ropensci.org/phylocomr,
https://github.com/ropensci/phylocomr",TRUE,https://github.com/ropensci/phylocomr,22733,15,2021-03-17T16:40:37Z,1515.5333333333333
phylolm,Provides functions for fitting phylogenetic linear models and phylogenetic generalized linear models. The computation uses an algorithm that is linear in the number of tips in the tree. The package also provides functions for simulating continuous or binary traits along the tree. Other tools include functions to test the adequacy of a population tree.,2020-06-22,Lam Si Tung Ho,https://github.com/lamho86/phylolm,TRUE,https://github.com/lamho86/phylolm,36536,21,2021-08-11T19:01:27Z,1739.8095238095239
phylopath,"A comprehensive and easy to use R implementation of confirmatory
    phylogenetic path analysis as described by Von Hardenberg and Gonzalez-Voyer
    (2012) <doi:10.1111/j.1558-5646.2012.01790.x>.",2019-12-07,Wouter van der Bijl,http://Ax3man.github.io/phylopath/,TRUE,https://github.com/ax3man/phylopath,18555,8,2021-01-19T18:29:10Z,2319.375
phyloregion,"Computational infrastructure for biogeography, community ecology,
    and biodiversity conservation (Daru et al. 2020) <doi:10.1111/2041-210X.13478>.
    It is based on the methods described in Daru et al. (2020) <doi:10.1038/s41467-020-15921-6>.
    The original conceptual work is described in Daru et al. (2017) <doi:10.1016/j.tree.2017.08.013>
    on patterns and processes of biogeographical regionalization. Additionally, the package
    contains fast and efficient functions to compute more standard conservation measures
    such as phylogenetic diversity, phylogenetic endemism, evolutionary distinctiveness
    and global endangerment, as well as compositional turnover (e.g., beta diversity).",2021-05-01,Barnabas H. Daru,"https://github.com/darunabas/phyloregion,
https://darunabas.github.io/phyloregion/index.html",TRUE,https://github.com/darunabas/phyloregion,7279,12,2021-08-23T10:28:17Z,606.5833333333334
phylotools,"A collection of tools for building RAxML supermatrix using
             PHYLIP or aligned FASTA files. These functions will be
             useful for building large phylogenies using multiple markers.",2017-12-10,Jinlong Zhang,https://github.com/helixcn/phylotools,TRUE,https://github.com/helixcn/phylotools,29821,4,2021-03-24T15:55:30Z,7455.25
phyr,"A collection of functions to do model-based phylogenetic analysis. 
    It includes functions to calculate community phylogenetic diversity,
    to estimate correlations among functional traits while accounting for 
    phylogenetic relationships, and to fit phylogenetic generalized linear
    mixed models. The Bayesian phylogenetic generalized linear mixed models
    are fitted with the 'INLA' package (<https://www.r-inla.org>).",2020-12-18,Daijiang Li,"https://daijiang.github.io/phyr/,
https://github.com/daijiang/phyr/",TRUE,https://github.com/daijiang/phyr,13178,21,2021-05-20T15:52:02Z,627.5238095238095
phytools,"A wide range of functions for phylogenetic analysis. Functionality is concentrated in phylogenetic comparative biology, but also includes numerous methods for visualizing, manipulating, reading or writing, and even inferring phylogenetic trees and data. Included among the functions in phylogenetic comparative biology are various for ancestral state reconstruction, model-fitting, simulation of phylogenies and data, and multivariate analysis. There are a broad range of plotting methods for phylogenies and comparative data which include, but are not restricted to, methods for mapping trait evolution on trees, for projecting trees into phenotypic space or a geographic map, and for visualizing correlated speciation between trees. Finally, there are a number of functions for reading, writing, analyzing, inferring, simulating, and manipulating phylogenetic trees and comparative data not covered by other packages. For instance, there are functions for randomly or non-randomly attaching species or clades to a phylogeny, for estimating supertrees or consensus phylogenies from a set, for simulating trees and phylogenetic data under a range of models, and for a wide variety of other manipulations and analyses that phylogenetic biologists might find useful in their research.",2021-06-03,Liam J. Revell,https://github.com/liamrevell/phytools,TRUE,https://github.com/liamrevell/phytools,254452,125,2021-08-10T21:39:02Z,2035.616
piecemaker,"Tokenizers break text into pieces that are more usable by machine 
    learning models. Many tokenizers share some preparation steps. This package
    provides those shared steps, along with a simple tokenizer.",2021-08-06,Jon Harmon,https://github.com/macmillancontentscience/piecemaker,TRUE,https://github.com/macmillancontentscience/piecemaker,340,0,2021-08-07T20:00:58Z,NA
piecepackr,"Functions to make board game graphics.  Specializes in game diagrams, animations, and ""Print & Play"" layouts for the 'piecepack' <https://www.ludism.org/ppwiki> but can make graphics for other board game systems.  Includes configurations for several public domain game systems.",2021-08-11,Trevor L Davis,"https://trevorldavis.com/piecepackr/ (blog),
https://trevorldavis.com/R/piecepackr/ (pkgdown),
https://groups.google.com/forum/#!forum/piecepackr (forum)",TRUE,https://github.com/piecepackr/piecepackr,13287,22,2021-08-11T17:33:00Z,603.9545454545455
piggyback,"Because larger (> 50 MB) data files cannot easily be committed to git,
  a different approach is required to manage data associated with an analysis in a 
  GitHub repository.  This package provides a simple work-around by allowing larger
  (up to 2 GB) data files to piggyback on a repository as assets attached to individual
  GitHub releases.  These files are not handled by git in any way, but instead are
  uploaded, downloaded, or edited directly by calls through the GitHub API. These
  data files can be versioned manually by creating different releases.  This approach
  works equally well with public or private repositories.  Data can be uploaded
  and downloaded programmatically from scripts. No authentication is required to
  download data from public repositories.",2021-08-06,Carl Boettiger,https://github.com/ropensci/piggyback,TRUE,https://github.com/ropensci/piggyback,19068,129,2021-08-19T04:49:31Z,147.8139534883721
pillar,"Provides 'pillar' and 'colonnade' generics designed
    for formatting columns of data using the full range of colours
    provided by modern terminals.",2021-07-29,Kirill Müller,"https://pillar.r-lib.org/, https://github.com/r-lib/pillar",TRUE,https://github.com/r-lib/pillar,34371933,131,2021-07-29T09:43:43Z,262381.1679389313
pinp,"A 'PNAS'-alike style for 'rmarkdown', derived from the
 'Proceedings of the National Academy of Sciences of the United States
 of America' ('PNAS', see <https://www.pnas.org>) 'LaTeX' style, and
 adapted for use with 'markdown' and 'pandoc'.",2020-10-01,Dirk Eddelbuettel and James Balamuta,http://dirk.eddelbuettel.com/code/pinp.html,TRUE,https://github.com/eddelbuettel/pinp,135424,135,2021-07-29T22:06:02Z,1003.1407407407407
pins,"Pin remote resources into a local cache to work offline,
  improve speed and avoid recomputing; discover and share resources
  in local folders, 'GitHub', 'Kaggle' or 'RStudio Connect'. Resources can 
  be anything from 'CSV', 'JSON', or image files to arbitrary R objects.",2021-01-05,Hadley Wickham,https://github.com/rstudio/pins,TRUE,https://github.com/rstudio/pins,61229,180,2021-08-31T19:53:01Z,340.1611111111111
pipenostics,"Functions representing some useful empirical and data-driven 
    models of heat losses, corrosion diagnostics, reliability and predictive 
    maintenance of pipeline systems. The package is an option for digital 
    transformation of technical engineering departments of heat generating and 
    heat transferring companies. Methods are described in 
    Timashev et al. (2016) <doi:10.1007/978-3-319-25307-7>, 
    A.C.Reddy (2017) <doi:10.1016/j.matpr.2017.07.081>,
    Minenergo (2008) <https://docs.cntd.ru/document/902148459>,
    Minenergo (2005) <http://www.complexdoc.ru/ntdtext/547103>.",2021-03-02,Yuri Possokhov,https://omega1x.github.io/pipenostics/,TRUE,https://github.com/omega1x/pipenostics,1544,0,2021-03-24T06:01:36Z,NA
piRF,"Implements multiple state-of-the-art prediction interval methodologies for random forests. 
	These include: quantile regression intervals, out-of-bag intervals, bag-of-observations intervals, 
	one-step boosted random forest intervals, bias-corrected intervals, high-density intervals, and 
	split-conformal intervals. The implementations include a combination of novel adjustments to the 
	original random forest methodology and novel prediction interval methodologies. All of these 
	methodologies can be utilized using solely this package, rather than a collection of separate 
	packages. Currently, only regression trees are supported. Also capable of handling high dimensional data. 
	Roy, Marie-Helene and Larocque, Denis (2019) <doi:10.1177/0962280219829885>.
	Ghosal, Indrayudh and Hooker, Giles (2018) <arXiv:1803.08000>.
	Zhu, Lin and Lu, Jiaxin and Chen, Yihong (2019) <arXiv:1905.10101>.
	Zhang, Haozhe and Zimmerman, Joshua and Nettleton, Dan and Nordman, Daniel J. (2019) <doi:10.1080/00031305.2019.1585288>.
	Meinshausen, Nicolai (2006) <http://www.jmlr.org/papers/volume7/meinshausen06a/meinshausen06a.pdf>.
	Romano, Yaniv and Patterson, Evan and Candes, Emmanuel (2019) <arXiv:1905.03222>.
	Tung, Nguyen Thanh and Huang, Joshua Zhexue and Nguyen, Thuy Thi and Khan, Imran (2014) <doi:10.13140/2.1.2500.8002>.",2020-05-12,Chancellor Johnstone,http://github.com/chancejohnstone/piRF,TRUE,https://github.com/chancejohnstone/pirf,4673,6,2020-12-02T19:35:44Z,778.8333333333334
piton,"A wrapper around the 'Parsing Expression Grammar Template Library', a C++11 library for generating
    Parsing Expression Grammars, that makes it accessible within Rcpp. With this, developers can implement
    their own grammars and easily expose them in R packages.",2020-11-15,Os Keyes,https://github.com/Ironholds/piton,TRUE,https://github.com/ironholds/piton,46533,17,2020-11-02T18:59:31Z,2737.235294117647
pivmet,"Collection of pivotal algorithms 
             for: relabelling the MCMC chains in order to undo the label 
             switching problem in Bayesian mixture models,
             as proposed in Egidi, Pappadà, Pauli and Torelli (2018a)<doi:10.1007/s11222-017-9774-2>;
             initializing the centers of the classical k-means algorithm 
             in order to obtain a better clustering solution. For further details see
             Egidi, Pappadà, Pauli and Torelli (2018b)<ISBN:9788891910233>.",2021-04-30,Leonardo Egidi,https://github.com/leoegidi/pivmet,TRUE,https://github.com/leoegidi/pivmet,12624,5,2021-08-31T18:09:19Z,2524.8
pivotaltrackR,"'Pivotal Tracker' <https://www.pivotaltracker.com> is a project
    management software-as-a-service that provides a REST API. This package
    provides an R interface to that API, allowing you to query it and work with
    its responses.",2021-01-10,Neal Richardson,"https://enpiar.com/r/pivotaltrackR/,
https://github.com/nealrichardson/pivotaltrackR/",TRUE,https://github.com/nealrichardson/pivotaltrackr,13191,0,2021-01-10T21:20:00Z,NA
pivottabler,"Create regular pivot tables with just a few lines of R.  
    More complex pivot tables can also be created, e.g. pivot tables
    with irregular layouts, multiple calculations and/or derived 
    calculations based on multiple data frames.  Pivot tables are
    constructed using R only and can be written to a range of
    output formats (plain text, 'HTML', 'Latex' and 'Excel'), 
    including with styling/formatting.",2021-06-27,Christopher Bailiss,"http://www.pivottabler.org.uk/,
https://github.com/cbailiss/pivottabler",TRUE,https://github.com/cbailiss/pivottabler,70081,91,2021-06-27T08:48:19Z,770.1208791208791
piwikproR,Run Queries against the API of 'Piwik Pro' <https://developers.piwik.pro/en/latest/custom_reports/http_api/http_api.html>. The result is a tibble.,2021-06-08,Martin Stingl,https://github.com/dfv-ms/piwikproR,TRUE,https://github.com/dfv-ms/piwikpror,1016,0,2021-07-09T10:18:29Z,NA
pixarfilms,"Data about Disney Pixar films provided by Wikipedia. This
    package contains data about the films, the people involved, and their
    awards.",2021-07-27,Eric Leung,"https://github.com/erictleung/pixarfilms,
https://erictleung.com/pixarfilms/",TRUE,https://github.com/erictleung/pixarfilms,2763,10,2021-07-27T04:58:43Z,276.3
pixelpuzzle,"Puzzle game that can be played in the R console.
  Restore the pixel art by shifting rows.",2021-02-01,Roland Krasser,https://github.com/rolkra/pixelpuzzle,TRUE,https://github.com/rolkra/pixelpuzzle,2466,2,2021-01-27T11:50:17Z,1233
pixels,"Provides tools to show and draw image pixels using 'HTML' widgets 
  and 'Shiny' applications.  It can be used to visualize the 'MNIST' dataset
  for handwritten digit recognition or to create new image recognition datasets.",2020-12-04,Daniel Falbel,https://github.com/javierluraschi/pixels,TRUE,https://github.com/javierluraschi/pixels,14760,28,2020-12-03T21:56:08Z,527.1428571428571
pkgbuild,"Provides functions used to build R packages. Locates compilers
  needed to build R packages on various platforms and ensures the PATH is
  configured appropriately so R can use them.",2020-12-15,Jim Hester,https://github.com/r-lib/pkgbuild,TRUE,https://github.com/r-lib/pkgbuild,15925609,44,2021-01-07T15:13:59Z,361945.6590909091
pkgcond,"This provides utilities for creating classed error and warning
  conditions based on where the error originated.",2021-04-28,Andrew Redd,https://github.com/RDocTaskForce/pkgcond,TRUE,https://github.com/rdoctaskforce/pkgcond,23256,2,2021-04-27T21:37:51Z,11628
pkgdown,"Generate an attractive and useful website from a source package.
    'pkgdown' converts your documentation, vignettes, 'README', and more to 
    'HTML' making it easy to share information about your package online.",2020-09-12,Hadley Wickham,"https://pkgdown.r-lib.org, https://github.com/r-lib/pkgdown",TRUE,https://github.com/r-lib/pkgdown,10269403,571,2021-09-01T13:49:30Z,17984.943957968477
pkgfilecache,"Manage optional data for your package. The data can be hosted anywhere, and you have to give a Uniform Resource Locator (URL) for each file. File integrity checks are supported. This is useful for package authors who need to ship more than the 5 Megabyte of data currently allowed by the the Comprehensive R Archive Network (CRAN).",2021-05-17,Tim Schäfer,https://github.com/dfsp-spirit/pkgfilecache,TRUE,https://github.com/dfsp-spirit/pkgfilecache,16730,5,2021-05-18T07:21:50Z,3346
pkgKitten,"Provides a function kitten() which creates cute little 
 packages which pass R package checks. This sets it apart from 
 package.skeleton() which it calls, and which leaves imperfect files 
 behind. As this is not exactly helpful for beginners, kitten() offers 
 an alternative. Unit test support can be added via the 'tinytest'
 package (if present), and documentation-creation support can be
 added via 'roxygen2' (if present).",2021-07-20,Dirk Eddelbuettel,"https://github.com/eddelbuettel/pkgkitten,
https://dirk.eddelbuettel.com/code/pkgkitten.html",TRUE,https://github.com/eddelbuettel/pkgkitten,253879,31,2021-07-19T22:41:21Z,8189.645161290323
pkglite,"A tool, grammar, and standard to represent and exchange
    R package source code as text files. Converts one or more source
    packages to a text file and restores the package structures from the file.",2021-05-22,Nan Xiao,"https://merck.github.io/pkglite/, https://github.com/Merck/pkglite",TRUE,https://github.com/merck/pkglite,5083,15,2021-08-19T05:42:53Z,338.8666666666667
pkgload,"Simulates the process of installing a package
    and then attaching it. This is a key part of the 'devtools' package as it
    allows you to rapidly iterate while developing a package.",2021-04-06,Jim Hester,https://github.com/r-lib/pkgload,TRUE,https://github.com/r-lib/pkgload,15458851,42,2021-06-14T12:17:55Z,368067.88095238095
pkgmaker,"Provides some low-level utilities to use for package
    development. It currently provides managers for multiple package specific
    options and registries, vignette, unit test and bibtex related utilities.
    It serves as a base package for packages like NMF, RcppOctave, doRNG, and
    as an incubator package for other general purposes utilities, that will
    eventually be packaged separately.
    It is still under heavy development and changes in the interface(s) are
    more than likely to happen.",2020-10-20,Renaud Gaujoux,https://renozao.github.io/pkgmaker/,TRUE,https://github.com/renozao/pkgmaker,495600,6,2020-10-20T19:42:01Z,82600
pkgndep,"It checks the heaviness of the packages that user's
    package depends on. For each package listed in the ""Depends"", ""Imports"" and
    ""Suggests"" fields in the DESCRIPTION file, it opens a new R session, loads the
    package and counts the number of namespaces that are loaded. The summary of
    the dependencies is visualized by a customized heatmap. Examples of dependency
    analysis can be found at <https://jokergoo.github.io/pkgndep/stat/>.",2021-03-05,Zuguang Gu,https://github.com/jokergoo/pkgndep,TRUE,https://github.com/jokergoo/pkgndep,6099,24,2021-07-01T13:11:36Z,254.125
pkgnet,"Tools from the domain of graph theory can be used to quantify the complexity
             and vulnerability to failure of a software package. That is the guiding philosophy
             of this package. 'pkgnet' provides tools to analyze the dependencies between functions
             in an R package and between its imported packages.  See the pkgnet website for vignettes 
             and other supplementary information.",2020-04-06,Brian Burns,"https://github.com/uptake/pkgnet, https://uptake.github.io/pkgnet/",TRUE,https://github.com/uptake/pkgnet,36279,115,2020-11-28T03:10:27Z,315.4695652173913
pkgnews,"Read R package news files, regardless of whether or not the package
  is installed.",2021-03-12,Owen Jones,https://github.com/owenjonesuob/pkgnews,TRUE,https://github.com/owenjonesuob/pkgnews,2235,3,2021-03-12T16:53:16Z,745
pkgsearch,"Search CRAN metadata about packages by keyword, popularity,
    recent activity, package name and more. Uses the 'R-hub' search server,
    see <https://r-pkg.org> and the CRAN metadata database, that
    contains information about CRAN packages. Note that this is _not_
    a CRAN project.",2020-10-08,Gábor Csárdi,"https://github.com/r-hub/pkgsearch,
https://r-hub.github.io/pkgsearch/",TRUE,https://github.com/r-hub/pkgsearch,28214,79,2021-04-11T10:09:06Z,357.1392405063291
PKNCA,"Compute standard Non-Compartmental Analysis (NCA) parameters for
    typical pharmacokinetic analyses and summarize them.",2020-06-01,Bill Denney,https://github.com/billdenney/pknca,TRUE,https://github.com/billdenney/pknca,32956,25,2021-08-24T00:34:11Z,1318.24
pksensi,"Applying the global sensitivity analysis workflow to investigate 
    the parameter uncertainty and sensitivity in physiologically based 
    kinetic (PK) models, especially the physiologically based 
    pharmacokinetic/toxicokinetic model with multivariate outputs. 
    The package also provides some functions to check the convergence 
    and sensitivity of model parameters. The workflow was first mentioned 
    in Hsieh et al., (2018) <doi:10.3389/fphar.2018.00588>, then further 
    refined (Hsieh et al., 2020 <doi:10.1016/j.softx.2020.100609>).                ",2021-07-04,Nan-Hung Hsieh,"https://github.com/nanhung/pksensi,
https://nanhung.github.io/pksensi/",TRUE,https://github.com/nanhung/pksensi,15741,4,2021-07-17T21:54:44Z,3935.25
PL94171,"Tools to process legacy format summary redistricting data files
    produced by the United States Census Bureau pursuant to P.L. 94-171. These
    files are generally available earlier but are difficult to work with as-is.",2021-08-06,Cory McCartan,"https://corymccartan.github.io/PL94171/,
https://github.com/CoryMcCartan/PL94171/",TRUE,https://github.com/corymccartan/pl94171,1116,3,2021-08-27T19:38:16Z,372
PlackettLuce,"Functions to prepare rankings data and fit the Plackett-Luce model
    jointly attributed to Plackett (1975) <doi:10.2307/2346567> and Luce
    (1959, ISBN:0486441369). The standard Plackett-Luce model is generalized
    to accommodate ties of any order in the ranking. Partial rankings, in which
    only a subset of items are ranked in each ranking, are also accommodated in
    the implementation. Disconnected/weakly connected networks implied by the
    rankings may be handled by adding pseudo-rankings with a hypothetical item.
    Optionally, a multivariate normal prior may be set on the log-worth
    parameters and ranker reliabilities may be incorporated as proposed by
    Raman and Joachims (2014) <doi:10.1145/2623330.2623654>. Maximum a
    posteriori estimation is used when priors are set. Methods are provided to
    estimate standard errors or quasi-standard errors for inference as well as
    to fit Plackett-Luce trees. See the package website or vignette for further
    details.",2021-08-16,Heather Turner,https://hturner.github.io/PlackettLuce/,TRUE,https://github.com/hturner/plackettluce,22987,8,2021-08-24T11:10:22Z,2873.375
planar,Solves the electromagnetic problem of reflection and transmission at a planar multilayer interface. Also computed are the decay rates and emission profile for a dipolar emitter.,2016-02-29,Baptiste Auguie,https://github.com/baptiste/planar,TRUE,https://github.com/baptiste/planar,18298,6,2021-05-25T21:07:38Z,3049.6666666666665
PlanetNICFI,It includes functions to download and process the 'Planet NICFI' (Norway's International Climate and Forest Initiative) Satellite Imagery utilizing the Planet Mosaics API <https://developers.planet.com/docs/basemaps/reference/#tag/Basemaps-and-Mosaics>. 'GDAL' (library for raster and vector geospatial data formats) and 'aria2c' (paralleled download utility) must be installed and configured in the user's Operating System.,2021-07-22,Lampros Mouselimis,https://github.com/mlampros/PlanetNICFI,TRUE,https://github.com/mlampros/planetnicfi,1262,2,2021-07-22T08:11:12Z,631
plater,"Tools for interacting with data from experiments done in microtiter
    plates. Easily read in plate-shaped data and convert it to tidy format, 
    combine plate-shaped data with tidy data, and view tidy data in plate shape.  ",2021-01-06,Sean Hughes,"https://docs.ropensci.org/plater/,
https://github.com/ropensci/plater",TRUE,https://github.com/ropensci/plater,17549,20,2021-01-04T23:18:36Z,877.45
platetools,"Collection of functions for working with multi-well microtitre
    plates, mainly 96, 384 and 1536 well plates.",2021-06-03,Scott Warchal,https://github.com/swarchal/platetools,TRUE,https://github.com/swarchal/platetools,19836,40,2021-08-02T15:06:00Z,495.9
platowork,"Data and analysis from an experiment with improving touch 
    typing speed, using the tDCS PlatoWork headset produced by PlatoScience.",2021-05-04,Lasse Hjorth Madsen,https://github.com/lassehjorthmadsen/platowork,TRUE,https://github.com/lassehjorthmadsen/platowork,1393,0,2021-05-03T14:33:59Z,NA
pleiotest,It performs a fast multi-trait genome-wide association analysis based on seemingly unrelated regressions. It tests for pleiotropic effects based on a series of Intersection-Union Wald tests. The package can handle large and unbalanced data and plot results.,2021-03-18,Fernando Aguate,https://github.com/FerAguate/pleiotest,TRUE,https://github.com/feraguate/pleiotest,1686,1,2021-07-19T19:35:00Z,1686
pliman,"Provides tools for image manipulation that will help you to
    quantify plant leaf area, disease severity, number of disease lesions,
    and obtain statistics of image objects such as grains, pods, pollen,
    leaves, and more. Tools for segment images and create binary images
    using the method of automatic threshold selection method proposed by
    Otsu (1979) <doi:10.1109/tsmc.1979.4310076> are also provided.",2021-06-10,Tiago Olivoto,https://github.com/TiagoOlivoto/pliman,TRUE,https://github.com/tiagoolivoto/pliman,1271,8,2021-07-06T20:26:39Z,158.875
plinkQC,"Genotyping arrays enable the direct measurement of an individuals
    genotype at thousands of markers. 'plinkQC' facilitates genotype quality
    control for genetic association studies as described by Anderson and
    colleagues (2010) <doi:10.1038/nprot.2010.116>. It makes 'PLINK' basic
    statistics (e.g. missing genotyping rates per individual, allele frequencies
    per genetic marker) and relationship functions accessible from 'R' and
    generates a per-individual and per-marker quality control report.
    Individuals and markers that fail the quality control can subsequently be
    removed to generate a new, clean dataset. Removal of individuals based on
    relationship status is optimised to retain as many individuals as possible
    in the study.",2021-07-15,Hannah Meyer,https://meyer-lab-cshl.github.io/plinkQC/,TRUE,https://github.com/meyer-lab-cshl/plinkqc,18342,30,2021-07-15T18:08:09Z,611.4
plm,"A set of estimators and tests for panel data
             econometrics, as described in Baltagi (2013), Econometric Analysis of Panel Data, ISBN-13:978-1-118-67232-7,
             Hsiao (2014), Analysis of Panel Data <doi:10.1017/CBO9781139839327>, and
             Croissant and Millo (2018), Panel Data Econometrics with R, ISBN-13:978-1-118-94918-4.",2021-03-02,Yves Croissant,"https://cran.r-project.org/package=plm,
https://r-forge.r-project.org/projects/plm/",TRUE,https://github.com/ycroissant/plm,1132873,3,2021-08-30T17:05:23Z,377624.3333333333
PLNmodels,"The Poisson-lognormal model and variants (Chiquet, Mariadassou and Robin, 
    2020 <doi:10.1101/2020.10.07.329383>) can be used for 
    a variety of multivariate problems when count data are at play, including 
    principal component analysis for count data, discriminant analysis, model-based clustering and 
    network inference. Implements variational algorithms to fit such models accompanied with a set of 
    functions for visualization and diagnostic. ",2021-03-16,Julien Chiquet,https://pln-team.github.io/PLNmodels/,TRUE,https://github.com/pln-team/plnmodels,13441,36,2021-06-04T13:09:16Z,373.3611111111111
plot3logit,"An implementation of the ternary plot for interpreting regression
    coefficients of trinomial regression models, as proposed in Santi, Dickson
    and Espa (2019) <doi:10.1080/00031305.2018.1442368>. Ternary plots can be
    drawn using either 'ggtern' package (based on 'ggplot2') or 'Ternary'
    package (based on standard graphics).",2021-02-26,Flavio Santi,https://www.flaviosanti.it/software/plot3logit,TRUE,https://github.com/f-santi/plot3logit,14462,2,2021-02-27T22:00:50Z,7231
plotdap,"Easily visualize and animate 'tabledap' and 'griddap' objects obtained via the 'rerddap' package in a simple one-line command,  using either base graphics or 'ggplot2' graphics. 'plotdap' handles extracting and reshaping the data,  map projections and continental outlines.  Optionally the data can be animated through time using the 'gganmiate' package.",2020-10-28,Carson Sievert,https://github.com/ropensci/plotdap,TRUE,https://github.com/ropensci/plotdap,14362,8,2020-10-24T22:10:01Z,1795.25
plotKML,"Writes sp-class, spacetime-class, raster-class and similar spatial and spatio-temporal objects to KML following some basic cartographic rules.",2021-04-29,Tomislav Hengl,https://github.com/Envirometrix/plotKML,TRUE,https://github.com/envirometrix/plotkml,110201,14,2021-04-30T09:32:21Z,7871.5
plotmm,"The main function, plot_mm(), is used for plotting output from mixture models, 
    including both densities and overlaying mixture weight component curves from the fit models. In line with the
    tidyverse, the package also includes the plot_cut_point() function to visualize the cutpoint (mu) from the model
    over a histogram of the data density with several color options. Finally, the package includes the plot_mix_comps() 
    helper function, which is used for both added customization as well as in the plot_mm() function. 
    Supported model objects include: 'mixtools', 'EMCluster', and 'flexmix', with more from each forthcoming. 
    Supported mixture model specifications include mixtures of univariate Gaussians, multivariate Gaussians, Gammas, 
    logistic regressions, linear regressions, and Poisson regressions.",2020-07-10,Philip Waggoner,NA,TRUE,https://github.com/pdwaggoner/plotmm,5387,20,2021-08-03T16:14:32Z,269.35
plotROC,"Most ROC curve plots obscure the cutoff values and inhibit
    interpretation and comparison of multiple curves. This attempts to address
    those shortcomings by providing plotting and interactive tools. Functions
    are provided to generate an interactive ROC curve plot for web use, and
    print versions. A Shiny application implementing the functions is also
    included.",2018-06-23,Michael C. Sachs,http://sachsmc.github.io/plotROC,TRUE,https://github.com/sachsmc/plotroc,146495,75,2020-10-13T14:46:23Z,1953.2666666666667
pls,"Multivariate regression methods
	Partial Least Squares Regression (PLSR), Principal Component
	Regression (PCR) and Canonical Powered Partial Least Squares (CPPLS).",2021-09-03,Kristian Hovde Liland,https://github.com/khliland/pls,TRUE,https://github.com/khliland/pls,1502450,24,2021-09-02T18:46:01Z,62602.083333333336
plsdof,"The plsdof package provides Degrees of Freedom estimates
        for Partial Least Squares (PLS) Regression. Model selection for
        PLS is based on various information criteria (aic, bic, gmdl)
        or on cross-validation. Estimates for the mean and covariance
        of the PLS regression coefficients are available. They allow
        the construction of approximate confidence intervals and the
        application of test procedures (Kramer and Sugiyama 
        2012 <doi:10.1198/jasa.2011.tm10107>).
        Further, cross-validation procedures for Ridge Regression and 
        Principal Components Regression are available.",2021-03-14,Nicole Kraemer,"https://github.com/fbertran/plsdof/,
https://fbertran.github.io/plsdof/",TRUE,https://github.com/fbertran/plsdof,21239,1,2021-07-09T22:56:49Z,21239
plsmod,"Bindings for additional regression models for use
    with the 'parsnip' package, including ordinary and spare partial least 
    squares models for regression and classification (Rohart et al (2017) 
    <doi:10.1371/journal.pcbi.1005752>).",2020-10-28,Max Kuhn,https://github.com/tidymodels/plsmod,TRUE,https://github.com/tidymodels/plsmod,7005,9,2021-06-29T17:41:09Z,778.3333333333334
plsRbeta,"Provides Partial least squares Regression for (weighted) beta regression models (Bertrand 2013,  <http://journal-sfds.fr/article/view/215>) and k-fold cross-validation of such models using various criteria. It allows for missing data in the explanatory variables. Bootstrap confidence intervals constructions are also available.",2021-03-18,Frederic Bertrand,"https://fbertran.github.io/plsRbeta/,
https://github.com/fbertran/plsRbeta/",TRUE,https://github.com/fbertran/plsrbeta,17139,2,2021-07-15T02:10:31Z,8569.5
plsRcox,"Provides Partial least squares Regression and various regular, sparse or kernel, techniques for fitting Cox models in high dimensional settings <doi:10.1093/bioinformatics/btu660>, Bastien, P., Bertrand, F., Meyer N., Maumy-Bertrand, M. (2015), Deviance residuals-based sparse PLS and sparse kernel PLS regression for censored data, Bioinformatics, 31(3):397-404. Cross validation criteria were studied in <arXiv:1810.02962>, Bertrand, F., Bastien, Ph. and Maumy-Bertrand, M. (2018), Cross validating extensions of kernel, sparse or regular partial least squares regression models to censored data.",2021-03-19,Frederic Bertrand,"http://fbertran.github.io/plsRcox/,
https://github.com/fbertran/plsRcox/",TRUE,https://github.com/fbertran/plsrcox,20676,1,2021-04-12T17:16:18Z,20676
plsRglm,Provides (weighted) Partial least squares Regression for generalized linear models and repeated k-fold cross-validation of such models using various criteria <arXiv:1810.01005>. It allows for missing data in the explanatory variables. Bootstrap confidence intervals constructions are also available.,2021-03-15,Frederic Bertrand,"https://fbertran.github.io/plsRglm/,
https://github.com/fbertran/plsRglm/",TRUE,https://github.com/fbertran/plsrglm,38297,12,2021-07-13T23:05:11Z,3191.4166666666665
plu,"Converts English phrases to singular or plural form based on
    the length of an associated vector.  Contains helper functions to
    create natural language lists from vectors and to include the length
    of a vector in natural language.",2021-04-07,Alexander Rossell Hayes,"https://plu.rossellhayes.com, https://github.com/rossellhayes/plu",TRUE,https://github.com/rossellhayes/plu,8407,1,2021-07-06T23:09:53Z,8407
plumber,"Gives the ability to automatically generate and serve an HTTP API
    from R functions using the annotations in the R documentation around your
    functions.",2021-03-24,Barret Schloerke,"https://www.rplumber.io, https://github.com/rstudio/plumber",TRUE,https://github.com/rstudio/plumber,586049,1214,2021-06-04T13:55:52Z,482.74217462932455
plumberDeploy,"Gives the ability to automatically deploy a plumber API
    from R functions on 'DigitalOcean' and other cloud-based servers.",2021-03-22,Bruno Tremblay,https://github.com/meztez/plumberDeploy,TRUE,https://github.com/meztez/plumberdeploy,2045,22,2021-07-08T00:49:31Z,92.95454545454545
plumbertableau,"Build 'Plumber' APIs that can be used in 'Tableau' workbooks.
    Annotations in R comments allow APIs to conform to the 'Tableau Analytics
    Extension' specification, so that R code can be used to power 'Tableau'
    workbooks.",2021-08-06,James Blair,"https://rstudio.github.io/plumbertableau/,
https://github.com/rstudio/plumbertableau",TRUE,https://github.com/rstudio/plumbertableau,367,3,2021-08-09T15:17:20Z,122.33333333333333
pmc,"Monte Carlo based model choice for applied phylogenetics of
    continuous traits. Method described in  Carl Boettiger, Graham Coop,
    Peter Ralph (2012) Is your phylogeny informative? Measuring
    the power of comparative methods, Evolution 66 (7)
    2240-51. <doi:10.1111/j.1558-5646.2011.01574.x>.",2021-01-07,Carl Boettiger,https://github.com/cboettig/pmc,TRUE,https://github.com/cboettig/pmc,21165,2,2021-01-06T21:03:43Z,10582.5
pmclust,"Aims to utilize model-based clustering (unsupervised)
        for high dimensional and ultra large data, especially in a distributed
        manner. The code employs 'pbdMPI' to perform a
        expectation-gathering-maximization algorithm
        for finite mixture Gaussian
        models. The unstructured dispersion matrices are assumed in the
        Gaussian models. The implementation is default in the single program
        multiple data programming model. The code can be executed
        through 'pbdMPI' and MPI' implementations such as 'OpenMPI'
        and 'MPICH'.
        See the High Performance Statistical Computing website
	<https://snoweye.github.io/hpsc/>
	for more information, documents and examples.",2021-02-11,Wei-Chen Chen,https://pbdr.org/,TRUE,https://github.com/snoweye/pmclust,16070,3,2021-02-10T03:15:21Z,5356.666666666667
pmd,"Paired mass distance (PMD) analysis proposed in Yu, Olkowicz and Pawliszyn (2018) <doi:10.1016/j.aca.2018.10.062> for gas/liquid chromatography–mass spectrometry (GC/LC-MS) based non-targeted analysis. PMD analysis including GlobalStd algorithm and structure/reaction directed analysis. GlobalStd algorithm could found independent peaks in m/z-retention time profiles based on retention time hierarchical cluster analysis and frequency analysis of paired mass distances within retention time groups. Structure directed analysis could be used to find potential relationship among those independent peaks in different retention time groups based on frequency of paired mass distances. Reactomics analysis could also be performed to build PMD network, assign sources and make biomarker reaction discovery. GUIs for PMD analysis is also included as 'shiny' applications.",2021-01-21,Miao YU,https://yufree.github.io/pmd/,TRUE,https://github.com/yufree/pmd,11379,5,2021-03-17T18:21:53Z,2275.8
pmdplyr,"Using the 'dplyr' package as a base, adds a family of functions designed to make manipulating panel data easier. Allows the addition of indexing variables to a tibble to create a pibble, and the manipulation of data based on those indexing variables.",2020-05-30,Nick Huntington-Klein,"https://nickch-k.github.io/pmdplyr,
https://github.com/NickCH-K/pmdplyr",TRUE,https://github.com/nickch-k/pmdplyr,12039,25,2021-08-07T03:33:19Z,481.56
pmetar,"Allows to download current and historical METAR weather reports
  extract and parse basic parameters and present main weather information. 
  Current reports are downloaded from Aviation Weather Center 
  <https://www.aviationweather.gov/metar> and historical reports from
  Iowa Environmental Mesonet web page of Iowa State University
  ASOS-AWOS-METAR <http://mesonet.agron.iastate.edu/AWOS/>.",2021-03-03,Pawel Cwiek,https://github.com/prcwiek/pmetar,TRUE,https://github.com/prcwiek/pmetar,4403,3,2021-04-10T16:53:55Z,1467.6666666666667
pmlbr,"Check available classification and regression data sets from the PMLB repository and download them.
    The PMLB repository (<https://github.com/EpistasisLab/pmlbr>) contains a curated collection of data sets for evaluating and comparing machine learning algorithms.
    These data sets cover a range of applications, and include binary/multi-class classification problems and 
    regression problems, as well as combinations of categorical, ordinal, and continuous features.
    There are currently over 150 datasets included in the PMLB repository.",2020-10-02,Trang Le,https://github.com/EpistasisLab/pmlbr,TRUE,https://github.com/epistasislab/pmlbr,3802,7,2021-08-25T17:50:33Z,543.1428571428571
pmml,"The Predictive Model Markup Language (PMML) is an XML-based language which provides a way for applications to define machine learning, statistical and data mining models and to share models between PMML compliant applications. More information about the PMML industry standard and the Data Mining Group can be found at <http://dmg.org/>. The generated PMML can be imported into any PMML consuming application, such as Zementis Predictive Analytics products, which integrate with web services, relational database systems and deploy natively on Hadoop in conjunction with Hive, Spark or Storm, as well as allow predictive analytics to be executed for IBM z Systems mainframe applications and real-time, streaming analytics platforms. The package isofor (used for anomaly detection) can be installed with devtools::install_github(""Zelazny7/isofor"").",2021-01-15,Dmitriy Bolotov,"https://softwareag.github.io/r-pmml/,
https://github.com/SoftwareAG/r-pmml,
https://www.softwareag.com/corporate/products/az/zementis/default.html",TRUE,https://github.com/softwareag/r-pmml,187867,18,2021-07-27T22:33:43Z,10437.055555555555
pmwg,"Provides an R implementation of the Particle Metropolis within
    Gibbs sampler for model parameter, covariance matrix and random effect
    estimation. A more general implementation of the sampler based on the paper
    by Gunawan, D., Hawkins, G. E., Tran, M. N., Kohn, R., & Brown, S. D.
    (2020) <doi:10.1016/j.jmp.2020.102368>. An HTML tutorial document describing
    the package is available at <https://newcastlecl.github.io/samplerDoc/> and
    includes several detailed examples, some background and troubleshooting
    steps.",2021-02-17,Gavin Cooper,https://github.com/newcastlecl/pmwg,TRUE,https://github.com/newcastlecl/pmwg,4365,2,2021-02-17T12:00:38Z,2182.5
PMwR,"Functions and examples for 'Portfolio
  Management with R': backtesting investment and
  trading strategies, computing profit/loss and
  returns, analysing trades, handling lists of
  transactions, reporting, and more.",2021-01-19,Enrico Schumann,"http://enricoschumann.net/PMwR/,
https://github.com/enricoschumann/PMwR,
https://gitlab.com/enricoschumann/PMwR",TRUE,https://github.com/enricoschumann/pmwr,14972,36,2021-08-13T19:36:41Z,415.8888888888889
PNWColors,"PNW-Inspired Palettes for 'R' data visualizations. Palettes are variable 
	in length and checked for colorblind accessibility from hue, saturation, 
	and lightness value scaling using the 'Chroma.js  Color Palette Helper' 
	<https://gka.github.io/palettes/>.",2020-06-12,Jake Lawlor,https://github.com/jakelawlor/PNWColors,TRUE,https://github.com/jakelawlor/pnwcolors,5565,142,2021-07-14T16:14:59Z,39.190140845070424
pocketapi,"Functions that interface with the 'Pocket' API (<https://getpocket.com/developer/>). Allows the user to get, add, and modify items in their own 'Pocket' account.",2020-11-20,Frie Preu,https://github.com/CorrelAid/pocketapi/,TRUE,https://github.com/correlaid/pocketapi,3071,12,2021-02-22T16:42:35Z,255.91666666666666
podr,"Make it easy to connect, access and review datasets in Pharmaceutical
  User Software Exchange ('PHUSE') open data repository ('PODR'). The R Shiny 
  application included in this package allow users to connect to 'PODR' and access to
  over 260 open data sets. ",2020-10-27,Hanming Tu,https://github.com/TuCai/podr,TRUE,https://github.com/tucai/podr,3497,1,2021-06-05T15:25:56Z,3497
poems,"The poems package provides a framework of interoperable R6 classes for 
    building ensembles of viable models via the pattern-oriented modeling (POM) 
    approach. The package includes classes for encapsulating and generating model 
    parameters, and managing the POM workflow. The workflow includes: model setup; 
    generating model parameters via Latin hyper-cube sampling; running multiple sampled
    model simulations; collating summary results; and validating and selecting an 
    ensemble of models that best match known patterns. By default, model validation and 
    selection utilizes an approximate Bayesian computation (ABC) approach, although 
    alternative user-defined functionality could be employed. The package includes 
    a spatially explicit demographic population model simulation engine, which 
    incorporates default functionality for density dependence, correlated environmental 
    stochasticity, stage-based transitions, and distance-based dispersal. The user may 
    customize the simulator by defining functionality for translocations, harvesting, 
    mortality, and other processes, as well as defining the sequence order for the 
    simulator processes. The framework could also be adapted for use with other model
    simulators by utilizing its extendable (inheritable) base classes.",2021-03-29,Sean Haythorne,https://github.com/GlobalEcologyLab/poems,TRUE,https://github.com/globalecologylab/poems,4199,1,2021-08-25T03:10:32Z,4199
pointblank,"Validate data in data frames, 'tibble' objects, 'Spark'
    'DataFrames', and database tables. Validation pipelines can be made using
    easily-readable, consecutive validation steps. Upon execution of the
    validation plan, several reporting options are available. User-defined
    thresholds for failure rates allow for the determination of appropriate
    reporting actions. Many other workflows are available including an
    information management workflow, where the aim is to record, collect, and
    generate useful information on data tables.",2021-07-25,Richard Iannone,"https://rich-iannone.github.io/pointblank/,
https://github.com/rich-iannone/pointblank",TRUE,https://github.com/rich-iannone/pointblank,87223,535,2021-09-03T03:54:43Z,163.03364485981308
pointr,"R has no built-in pointer functionality. The 'pointr' package fills this gap and lets you create pointers to R objects, including subsets of dataframes. This makes your R code more readable and maintainable.",2021-01-06,Joachim Zuckarelli,https://github.com/jsugarelli/pointr/,TRUE,https://github.com/jsugarelli/pointr,5039,3,2020-12-21T10:40:27Z,1679.6666666666667
poismf,"Creates a low-rank factorization of a sparse counts matrix by maximizing Poisson likelihood with l1/l2 regularization
	with all non-negative latent factors (e.g. for recommender systems or topic modeling) (Cortes, (2018) <arXiv:1811.01908>).
	Similar to hierarchical Poisson factorization, but follows an optimization-based approach with regularization instead of a
	hierarchical structure, and is fit through gradient-based methods instead of variational inference.",2021-09-02,David Cortes,https://github.com/david-cortes/poismf,TRUE,https://github.com/david-cortes/poismf,13131,36,2021-09-01T01:28:15Z,364.75
poissonreg,"Bindings for Poisson regression models for use with the
    'parsnip' package. Models include simple generalized linear models,
    Bayesian models, and zero-inflated Poisson models (Zeileis, Kleiber,
    and Jackman (2008) <doi:10.18637/jss.v027.i08>).",2021-08-07,Max Kuhn,"https://github.com/tidymodels/poissonreg,
https://poissonreg.tidymodels.org/",TRUE,https://github.com/tidymodels/poissonreg,16299,12,2021-08-17T16:29:08Z,1358.25
polaroid,"Create hexagonal shape sticker image.
  'polaroid' can be used in user's web browser.
  'polaroid' can be used in 'shinyapps.io'.
  In both way, user can download created 'hexSticker' as 'PNG' image.
  'polaroid' is built based on 'argonDash', 'colourpicker' and 'hexSticker' R package.",2020-03-30,Jinhwan Kim,https://github.com/jhk0530/polaroid,TRUE,https://github.com/jhk0530/polaroid,5178,8,2020-11-13T01:50:37Z,647.25
PolicyPortfolios,"Tools for simplifying the creation and management of data structures
    suitable for dealing with policy portfolios, that is, two-dimensional spaces 
    of policy instruments and policy targets. The package also allows to generate measures of
    portfolio characteristics and facilitates their visualization.",2021-03-01,Xavier Fernández i Marín,"http://xavier-fim.net/packages/PolicyPortfolios/,
https://github.com/xfim/PolicyPortfolios",TRUE,https://github.com/xfim/policyportfolios,1974,0,2021-03-12T05:21:15Z,NA
policytree,"Learn optimal policies via doubly robust empirical
 welfare maximization over trees. This package implements the multi-action doubly
 robust approach of Zhou, Athey and Wager (2018) <arXiv:1810.04778> in the case where
 we want to learn policies that belong to the class of depth k decision trees.",2021-07-07,Erik Sverdrup,https://github.com/grf-labs/policytree,TRUE,https://github.com/grf-labs/policytree,12189,43,2021-08-25T02:31:12Z,283.4651162790698
polished,"Easily add modern authentication and user administration to your 'shiny' apps. 
  Customize user sign in and registration pages to match your brand.  Control who can
  access one or more of your 'shiny' apps.",2021-03-09,Andy Merlino,"https://github.com/tychobra/polished, https://polished.tech",TRUE,https://github.com/tychobra/polished,6637,159,2021-09-02T19:17:36Z,41.742138364779876
polmineR,"Package for corpus analysis using the Corpus Workbench 
    ('CWB', <http://cwb.sourceforge.net/>) as an efficient back end for indexing
    and querying large corpora. The package offers functionality to flexibly create
    subcorpora and to carry out basic statistical operations (count, co-occurrences
    etc.). The original full text of documents can be reconstructed and inspected at
    any time. Beyond that, the package is intended to serve as an interface to 
    packages implementing advanced statistical procedures. Respective data structures
    (document-term matrices, term-co-occurrence matrices etc.) can be created based 
    on the indexed corpora.",2020-09-22,Andreas Blaette,https://github.com/PolMine/polmineR,TRUE,https://github.com/polmine/polminer,23386,34,2021-02-12T17:54:28Z,687.8235294117648
polyCub,"Numerical integration of continuously differentiable
    functions f(x,y) over simple closed polygonal domains.
    The following cubature methods are implemented:
    product Gauss cubature (Sommariva and Vianello, 2007,
    <doi:10.1007/s10543-007-0131-2>),
    the simple two-dimensional midpoint rule
    (wrapping 'spatstat.geom' functions),
    adaptive cubature for radially symmetric functions via line
    integrate() along the polygon boundary (Meyer and Held, 2014,
    <doi:10.1214/14-AOAS743>, Supplement B),
    and integration of the bivariate Gaussian density based on
    polygon triangulation.
    For simple integration along the axes, the 'cubature' package
    is more appropriate.",2021-01-27,Sebastian Meyer,https://github.com/bastistician/polyCub,TRUE,https://github.com/bastistician/polycub,42017,4,2021-07-22T08:14:57Z,10504.25
polyglot,Use the R console as an interactive learning environment to memorize any two columns dataset.,2020-05-18,Félix Luginbuhl,"https://felixluginbuhl.com/polyglot,
https://github.com/lgnbhl/polyglot",TRUE,https://github.com/lgnbhl/polyglot,13487,1,2021-02-23T20:21:49Z,13487
polylabelr,"A wrapper around the C++ library 'polylabel' from 'Mapbox', 
    providing an efficient routine for finding the approximate pole of 
    inaccessibility of a polygon, which usually serves as an excellent candidate
    for labeling of a polygon.",2020-04-19,Johan Larsson,"https://github.com/jolars/polylabelr,
https://jolars.github.io/polylabelr",TRUE,https://github.com/jolars/polylabelr,46441,15,2021-01-27T18:39:54Z,3096.0666666666666
polyMatrix,"
  Implementation of class ""polyMatrix"" for storing a matrix of polynomials and implements 
  basic matrix operations; including a determinant and characteristic polynomial.
  It is based on the package 'polynom' and uses a lot of its methods to implement matrix operations.
  This package includes 3 methods of triangularization of polynomial matrices:
  Extended Euclidean algorithm which is most classical but numerically unstable;
  Sylvester algorithm based on LQ decomposition;
  Interpolation algorithm is based on LQ decomposition and Newton interpolation.
  Both methods are described in 
  D. Henrion & M. Sebek, Reliable numerical methods for polynomial matrix triangularization,
  IEEE Transactions on Automatic Control (Volume 44, Issue 3, Mar 1999, Pages 497-508) <doi:10.1109/9.751344>
  and in 
  Salah Labhalla, Henri Lombardi & Roger Marlin, 
  Algorithmes de calcule de la reduction de Hermite d'une matrice a coefficients polynomeaux,
  Theoretical Computer Science (Volume 161, Issue 1-2, July 1996, Pages 69-92) <doi:10.1016/0304-3975(95)00090-9>.",2021-07-18,Nikolai Ryzhkov,https://github.com/namezys/polymatrix,TRUE,https://github.com/namezys/polymatrix,9075,0,2021-07-11T21:42:44Z,NA
polypharmacy,"Analyse prescription drug deliveries to calculate several indicators of polypharmacy corresponding to the various definitions found in the literature.
  Bjerrum, L., Rosholm, J. U., Hallas, J., & Kragstrup, J. (1997) <doi:10.1007/s002280050329>.
  Chan, D.-C., Hao, Y.-T., & Wu, S.-C. (2009a) <doi:10.1002/pds.1712>.
  Fincke, B. G., Snyder, K., Cantillon, C., Gaehde, S., Standring, P., Fiore, L., ... Gagnon, D.R. (2005) <doi:10.1002/pds.966>.
  Hovstadius, B., Astrand, B., & Petersson, G. (2009) <doi:10.1186/1472-6904-9-11>.
  Hovstadius, B., Astrand, B., & Petersson, G. (2010) <doi:10.1002/pds.1921>.
  Kennerfalk, A., Ruigómez, A., Wallander, M.-A., Wilhelmsen, L., & Johansson, S. (2002) <doi:10.1345/aph.1A226>.
  Masnoon, N., Shakib, S., Kalisch-Ellett, L., & Caughey, G. E. (2017) <doi:10.1186/s12877-017-0621-2>.
  Narayan, S. W., & Nishtala, P. S. (2015) <doi:10.1007/s40801-015-0020-y>.
  Nishtala, P. S., & Salahudeen, M. S. (2015) <doi:10.1159/000368191>.
  Park, H. Y., Ryu, H. N., Shim, M. K., Sohn, H. S., & Kwon, J. W. (2016) <doi:10.5414/cp202484>.
  Veehof, L., Stewart, R., Haaijer-Ruskamp, F., & Jong, B. M. (2000) <doi:10.1093/fampra/17.3.261>.",2021-07-12,Guillaume Boucher,NA,TRUE,https://github.com/guiboucher/polypharmacy,624,3,2021-06-22T06:08:31Z,208
polypoly,"Tools for reshaping, plotting, and manipulating matrices of orthogonal polynomials.",2017-05-27,Tristan Mahr,https://github.com/tjmahr/polypoly,TRUE,https://github.com/tjmahr/polypoly,11878,13,2021-06-01T15:12:36Z,913.6923076923077
polyRAD,"Read depth data from genotyping-by-sequencing (GBS) or restriction 
  site-associated DNA sequencing (RAD-seq) are imported and used to make Bayesian
  probability estimates of genotypes in polyploids or diploids.  The genotype 
  probabilities, posterior mean genotypes, or most probable genotypes can then
  be exported for downstream analysis.  'polyRAD' is described by Clark et al.
  (2019) <doi:10.1534/g3.118.200913>.  A variant calling pipeline for highly
  duplicated genomes is also included and is described by Clark et al. (2020)
  <doi:10.1101/2020.01.11.902890>.",2021-09-02,Lindsay V. Clark,https://github.com/lvclark/polyRAD,TRUE,https://github.com/lvclark/polyrad,14125,12,2021-09-02T15:53:40Z,1177.0833333333333
polyreg,"Automate formation and evaluation of polynomial regression models. The motivation for this package is described in 'Polynomial Regression As an Alternative to Neural Nets' by Xi Cheng, Bohdan Khomtchouk, Norman Matloff, and Pete Mohanty (<arXiv:1806.06850>).",2020-04-04,Norm Matloff,https://github.com/matloff/polyreg,TRUE,https://github.com/matloff/polyreg,10857,172,2021-09-03T02:14:13Z,63.122093023255815
pomdp,Provides the infrastructure to define and analyze the solutions of Partially Observable Markov Decision Processes (POMDP) models. The package includes pomdp-solve to solve POMDPs using a variety of exact and approximate value iteration algorithms. Smallwood and Sondik (1973) <doi:10.1287/opre.21.5.1071>.,2021-08-05,Michael Hahsler,https://github.com/mhahsler/pomdp,TRUE,https://github.com/mhahsler/pomdp,15454,4,2021-08-05T12:14:33Z,3863.5
pomp,"Tools for data analysis with partially observed Markov process (POMP) models (also known as stochastic dynamical systems, hidden Markov models, and nonlinear, non-Gaussian, state-space models).  The package provides facilities for implementing POMP models, simulating them, and fitting them to time series data by a variety of frequentist and Bayesian methods.  It is also a versatile platform for implementation of inference methods for general POMP models.",2021-07-28,Aaron A. King,https://kingaa.github.io/pomp/,TRUE,https://github.com/kingaa/pomp,70425,78,2021-09-03T15:03:39Z,902.8846153846154
pool,"Enables the creation of object pools, which make it
    less computationally expensive to fetch a new object. Currently the
    only supported pooled objects are 'DBI' connections.",2021-01-14,Joe Cheng,https://github.com/rstudio/pool,TRUE,https://github.com/rstudio/pool,263862,177,2021-01-18T18:43:21Z,1490.7457627118645
PooledCohort,"The 2017 American College of Cardiology and American Heart
  Association blood pressure guideline recommends using 10-year predicted 
  atherosclerotic cardiovascular disease risk to guide the decision to 
  initiate or intensify antihypertensive medication. The guideline recommends 
  using the Pooled Cohort risk prediction equations to predict 10-year 
  atherosclerotic cardiovascular disease risk. This package implements the 
  original Pooled Cohort risk prediction equations and also incorporates 
  updated versions based on more contemporary data and statistical methods.
  References:
  Goff DC, Lloyd-Jones DM, Bennett G, Coady S, D’Agostino RB, Gibbons R, 
  Greenland P, Lackland DT, Levy D, O’Donnell CJ, and Robinson JG (2014) 
  <doi:10.1016/j.jacc.2014.03.006>
  Yadlowsky S, Hayward RA, Sussman JB, McClelland RL, Min YI, and Basu S (2018)
  <doi:10.7326/m17-3011>.",2021-04-19,Byron Jaeger,https://github.com/bcjaeger/PooledCohort,TRUE,https://github.com/bcjaeger/pooledcohort,1446,1,2020-10-25T17:41:24Z,1446
PoolTestR,"An easy-to-use tool for working with presence/absence tests on 'pooled'
    or 'grouped' samples. The primary application is for estimating prevalence of 
    a marker in a population based on the results of tests on pooled specimens.
    This sampling method is often employed in surveillance of rare conditions in
    humans or animals (e.g. molecular xenomonitoring). The package was initially
    conceived as an R-based alternative to the molecular xenomonitoring software,
    'PoolScreen' <https://sites.uab.edu/statgenetics/software/>. However, it goes
    further, allowing for estimates of prevalence to be adjusted for hierarchical
    sampling frames, and perform flexible mixed-effect regression analyses
    (McLure et al. pre-print <arXiv:2012.05405>). The package is currently in
    early stages, however more features are planned or in the works: e.g.
    adjustments for imperfect test specificity/sensitivity, functions for helping
    with optimal experimental design, and functions for spatial modelling.",2021-02-15,Angus McLure,https://github.com/AngusMcLure/PoolTestR,TRUE,https://github.com/angusmclure/pooltestr,2467,0,2021-06-04T06:28:44Z,NA
poorman,A replication of key functionality from 'dplyr' and the wider 'tidyverse' using only 'base'.,2021-03-28,Nathan Eastwood,"https://nathaneastwood.github.io/poorman/,
https://github.com/nathaneastwood/poorman",TRUE,https://github.com/nathaneastwood/poorman,21544,236,2021-04-18T14:59:30Z,91.28813559322033
PopED,"Optimal experimental designs for both population and individual
    studies based on nonlinear mixed-effect models. Often this is based on a
    computation of the Fisher Information Matrix. This package was developed
    for pharmacometric problems, and examples and predefined models are available
    for these types of systems. The methods are described in Nyberg et al. 
    (2012) <doi:10.1016/j.cmpb.2012.05.005>, and Foracchia et al. (2004) 
    <doi:10.1016/S0169-2607(03)00073-7>.",2021-05-21,Andrew C. Hooker,"https://andrewhooker.github.io/PopED/,
https://github.com/andrewhooker/PopED,",TRUE,https://github.com/andrewhooker/poped,24366,21,2021-05-24T09:05:56Z,1160.2857142857142
popkin,"Provides functions to estimate the kinship matrix of individuals from a large set of biallelic SNPs, and extract inbreeding coefficients and the generalized FST (Wright's fixation index).  Method described in Ochoa and Storey (2021) <doi:10.1371/journal.pgen.1009241>.",2021-07-27,Alejandro Ochoa,https://github.com/StoreyLab/popkin/,TRUE,https://github.com/storeylab/popkin,15805,16,2021-07-27T20:37:44Z,987.8125
poppr,"Population genetic analyses for hierarchical analysis of partially
    clonal populations built upon the architecture of the 'adegenet' package. 
    Originally described in Kamvar, Tabima, and Grünwald (2014) 
    <doi:10.7717/peerj.281> with version 2.0 described in Kamvar, Brooks, and 
    Grünwald (2015) <doi:10.3389/fgene.2015.00208>.",2021-05-22,Zhian N. Kamvar,"https://grunwaldlab.github.io/poppr/,
https://github.com/grunwaldlab/poppr/,
https://grunwaldlab.github.io/Population_Genetics_in_R/",TRUE,https://github.com/grunwaldlab/poppr,114264,50,2021-06-07T13:59:21Z,2285.28
popRF,"Disaggregating census-based areal population counts to finer 
    gridded population surfaces using Random Forest algorithm to determine 
    the target area weights 
    (see _Stevens, et al._ (2015) <doi:10.1371/journal.pone.0107042>). ",2021-07-26,Maksym Bondarenko,https://github.com/wpgp/popRF,TRUE,https://github.com/wpgp/poprf,561,13,2021-08-02T15:36:18Z,43.15384615384615
popsom,"Kohonen's self-organizing maps with a number of distinguishing features:
    (1) A very efficient, single threaded, stochastic training algorithm based on ideas from tensor algebra.  Up to 60x faster than traditional single-threaded training algorithms. No special accelerator hardware required.
    (2) Automatic centroid detection and visualization using starbursts.
    (3) Two models of the data: (a) a self-organizing map model, (b) a centroid based clustering model.
    (4) A number of easily accessible quality metrics for the self-organizing map and the centroid based cluster model.",2021-07-09,Lutz Hamel,https://github.com/lutzhamel/popsom,TRUE,https://github.com/lutzhamel/popsom,20351,4,2021-08-19T03:19:42Z,5087.75
PortalHacienda,"Obtener listado de datos, acceder y extender series del Portal de 
    Datos de Hacienda.Las proyecciones se realizan con 'forecast', 
    Hyndman RJ, Khandakar Y (2008) <doi:10.18637/jss.v027.i03>. 
    Search, download and forecast time-series from the Ministry of Economy 
    of Argentina. Forecasts are built with the 'forecast' package, 
    Hyndman RJ, Khandakar Y (2008) <doi:10.18637/jss.v027.i03>. ",2020-11-19,Fernando Garcia Diaz,https://github.com/fmgarciadiaz/PortalHacienda-CRAN,TRUE,https://github.com/fmgarciadiaz/portalhacienda-cran,5659,9,2020-11-19T01:17:22Z,628.7777777777778
portalr,"Download and generate summaries for the rodent,
    plant, ant, and weather data from the Portal Project. Portal is a
    long-term (and ongoing) experimental monitoring site in the Chihuahua
    desert. The raw data files can be found at
    <https://github.com/weecology/portaldata>.",2021-05-23,Glenda M. Yenni,"https://weecology.github.io/portalr/,
https://github.com/weecology/portalr",TRUE,https://github.com/weecology/portalr,20090,10,2021-08-05T21:31:28Z,2009
portfolio,"Classes for analysing and implementing equity portfolios,
    including routines for generating tradelists and calculating
    exposures to user-specified risk factors.",2021-07-10,Daniel Gerlanc,https://github.com/dgerlanc/portfolio,TRUE,https://github.com/dgerlanc/portfolio,26380,10,2021-07-09T13:03:12Z,2638
PortfolioAnalytics,Portfolio optimization and analysis routines and graphics.,2018-05-17,Brian G. Peterson,https://github.com/braverock/PortfolioAnalytics,TRUE,https://github.com/braverock/portfolioanalytics,75364,53,2021-05-09T14:05:45Z,1421.9622641509434
portfolioBacktest,"Automated backtesting of multiple portfolios over multiple 
    datasets of stock prices in a rolling-window fashion. Intended for 
    researchers and practitioners to backtest a set of different portfolios, 
    as well as by a course instructor to assess the students in their portfolio 
    design in a fully automated and convenient manner, with results conveniently 
    formatted in tables and plots. Each portfolio design is easily defined as a
    function that takes as input a window of the stock prices and outputs the 
    portfolio weights. Multiple portfolios can be easily specified as a list 
    of functions or as files in a folder. Multiple datasets can be conveniently 
    extracted randomly from different markets, different time periods, and 
    different subsets of the stock universe. The results can be later assessed 
    and ranked with tables based on a number of performance criteria (e.g., 
    expected return, volatility, Sharpe ratio, drawdown, turnover rate, return 
    on investment, computational time, etc.), as well as plotted in a number of 
    ways with nice barplots and boxplots.",2021-01-12,Daniel P. Palomar,"https://CRAN.R-project.org/package=portfolioBacktest,
https://github.com/dppalomar/portfolioBacktest",TRUE,https://github.com/dppalomar/portfoliobacktest,14644,23,2021-02-16T06:26:27Z,636.695652173913
postcards,"A collection of R Markdown templates for creating simple and easy 
  to personalize single page websites.",2021-07-31,Sean Kross,https://github.com/seankross/postcards,TRUE,https://github.com/seankross/postcards,5763,290,2021-08-01T00:08:29Z,19.872413793103448
PostcodesioR,"Free UK geocoding using data from Office for National Statistics.
    It is using several functions to get information about post codes, outward
    codes, reverse geocoding, nearest post codes/outward codes, validation, or
    randomly generate a post code. API wrapper around <https://postcodes.io>.",2021-01-07,Eryk Walczak,https://docs.ropensci.org/PostcodesioR/,TRUE,https://github.com/ropensci/postcodesior,11218,36,2021-01-07T21:51:29Z,311.6111111111111
posterior,"Provides useful tools for both users and developers of packages 
  for fitting Bayesian models or working with output from Bayesian models. 
  The primary goals of the package are to: 
  (a) Efficiently convert between many different useful formats of
  draws (samples) from posterior or prior distributions.
  (b) Provide consistent methods for operations commonly performed on draws, 
  for example, subsetting, binding, or mutating draws.
  (c) Provide various summaries of draws in convenient formats.
  (d) Provide lightweight implementations of state of the art posterior 
  inference diagnostics. References: Vehtari et al. (2021) 
  <doi:10.1214/20-BA1221>.",2021-07-14,Paul-Christian Bürkner,"https://mc-stan.org/posterior/, https://discourse.mc-stan.org/",TRUE,https://github.com/stan-dev/posterior,13101,92,2021-08-29T10:34:49Z,142.40217391304347
PosteriorBootstrap,"An implementation of a non-parametric statistical model using a
    parallelised Monte Carlo sampling scheme. The method implemented in this
    package allows non-parametric inference to be regularized for small sample
    sizes, while also being more accurate than approximations such as
    variational Bayes. The concentration parameter is an effective sample size
    parameter, determining the faith we have in the model versus the data. When
    the concentration is low, the samples are close to the exact Bayesian
    logistic regression method; when the concentration is high, the samples are
    close to the simplified variational Bayes logistic regression. The method is
    described in full in the paper Lyddon, Walker, and Holmes (2018),
    ""Nonparametric learning from Bayesian models with randomized objective
    functions"" <arXiv:1806.11544>.",2021-05-14,James Robinson,https://github.com/alan-turing-institute/PosteriorBootstrap/,TRUE,https://github.com/alan-turing-institute/posteriorbootstrap,8612,4,2021-05-14T14:12:02Z,2153
postpack,"The aim of 'postpack' is to provide the infrastructure for a standardized workflow for 'mcmc.list' objects.
    These objects can be used to store output from models fitted with Bayesian inference using
    'JAGS', 'WinBUGS', 'OpenBUGS', 'NIMBLE', 'Stan', or even custom MCMC algorithms. Although the 'coda' R package provides
    some methods for these objects, it is somewhat limited in easily performing post-processing tasks for
    specific nodes. Models are ever increasing in their complexity and the number of tracked nodes, and oftentimes
    a user may wish to summarize/diagnose sampling behavior for only a small subset of nodes at a time
    for a particular question or figure. Thus, many 'postpack' functions support performing tasks on a
    subset of nodes, where the subset is specified with regular expressions. The functions in 'postpack'
    streamline the extraction, summarization, and diagnostics of specific monitored nodes after model fitting.
    Further, because there is rarely only ever one model under consideration, 'postpack' scales efficiently 
    to perform the same tasks on output from multiple models simultaneously, facilitating rapid assessment 
    of model sensitivity to changes in assumptions.",2021-06-02,Ben Staton,https://bstaton1.github.io/postpack/,TRUE,https://github.com/bstaton1/postpack,4308,0,2020-09-28T02:39:05Z,NA
potential,"Provides functions to compute the potential model as defined by
    Stewart (1941) <doi:10.1126/science.93.2404.89>. Several options are available
    to customize the model, such as the possibility to fine-tune the distance
    friction functions or to use custom distance matrices. Some computations are
    parallelized to improve their efficiency.",2020-11-18,Timothée Giraud,https://github.com/riatelab/potential,TRUE,https://github.com/riatelab/potential,2642,18,2021-09-01T11:03:20Z,146.77777777777777
potools,"Translating messages in R packages is managed using the po top-level directory and the 'gettext' program. This package provides some helper functions for building this support in R packages, e.g. common validation & I/O tasks.",2021-07-12,Michael Chirico,https://github.com/MichaelChirico/potools,TRUE,https://github.com/michaelchirico/potools,1387,24,2021-07-12T07:48:19Z,57.791666666666664
POUMM,"The Phylogenetic Ornstein-Uhlenbeck Mixed Model (POUMM) allows to 
    estimate the phylogenetic heritability of continuous traits, to test 
    hypotheses of neutral evolution versus stabilizing selection, to quantify 
    the strength of stabilizing selection, to estimate measurement error and to
    make predictions about the evolution of a phenotype and phenotypic variation 
    in a population. The package implements combined maximum likelihood and 
    Bayesian inference of the univariate Phylogenetic Ornstein-Uhlenbeck Mixed 
    Model, fast parallel likelihood calculation, maximum likelihood 
    inference of the genotypic values at the tips, functions for summarizing and
    plotting traces and posterior samples, functions for simulation of a univariate 
    continuous trait evolution model along a phylogenetic tree. So far, the 
    package has been used for estimating the heritability of quantitative traits
    in macroevolutionary and epidemiological studies, see e.g. 
    Bertels et al. (2017) <doi:10.1093/molbev/msx246> and 
    Mitov and Stadler (2018) <doi:10.1093/molbev/msx328>. The algorithm for 
    parallel POUMM likelihood calculation has been published in 
    Mitov and Stadler (2019) <doi:10.1111/2041-210X.13136>.",2020-10-27,Venelin Mitov,"https://venelin.github.io/POUMM/index.html,
https://github.com/venelin/POUMM",TRUE,https://github.com/venelin/poumm,14033,3,2020-10-21T08:35:32Z,4677.666666666667
POV,"An implementation of the Partition Of variation (POV) method as
    developed by Dr. Thomas A Little <https://thomasalittleconsulting.com> in
    1993 for the analysis of semiconductor data for hard drive manufacturing.
    POV is based on sequential sum of squares and is an exact method that
    explains all observed variation. It quantitates both the between and within
    factor variation effects and can quantitate the influence of both continuous
    and categorical factors.",2020-11-16,Paul Deen,"https://github.com/PaulAntonDeen/POV-R-Package,
https://thomasalittleconsulting.com",TRUE,https://github.com/paulantondeen/pov-r-package,3105,0,2020-11-11T18:03:53Z,NA
povcalnetR,"Provides an interface to compute poverty and inequality indicators for more than 160 countries and regions from the World Bank's database of household surveys. It has the same functionality as the 'Povcalnet' website (<http://iresearch.worldbank.org/PovcalNet/>). 'Povcalnet' is a computational tool that allows users to estimate poverty rates for regions, sets of countries or individual countries, over time and at any poverty line. 'Povcalnet' is managed jointly by the data and research group in the World Bank's development economics division. It draws heavily upon a strong collaboration with the poverty and equity global practice, which is responsible for the gathering and harmonization of the underlying survey data.",2020-05-19,Tony Fujs,https://github.com/worldbank/povcalnetR,TRUE,https://github.com/worldbank/povcalnetr,9515,9,2020-09-14T08:06:14Z,1057.2222222222222
powdR,"Full pattern summation of X-ray powder diffraction data as
  described in Chipera and Bish (2002) <doi:10.1107/S0021889802017405> and
  Butler and Hillier (2021) <doi:10.1016/j.cageo.2020.104662>.
  Derives quantitative estimates of crystalline and amorphous phase
  concentrations in complex mixtures.",2021-08-13,Benjamin Butler,https://github.com/benmbutler/powdR,TRUE,https://github.com/benmbutler/powdr,15887,5,2021-08-20T13:52:07Z,3177.4
Power2Stage,"Contains functions to obtain the operational characteristics of 
    bioequivalence studies in Two-Stage Designs (TSD) via simulations.",2021-01-16,Detlew Labes,https://github.com/Detlew/Power2Stage,TRUE,https://github.com/detlew/power2stage,20600,1,2021-05-05T16:42:33Z,20600
powerLATE,"An implementation of the generalized power analysis for the local average treatment effect (LATE), proposed by Bansak (2020) <doi:10.1214/19-STS732>. Power analysis is in the context of estimating the LATE (also known as the complier average causal effect, or CACE), with calculations based on a test of the null hypothesis that the LATE equals 0 with a two-sided alternative. The method uses standardized effect sizes to place a conservative bound on the power under minimal assumptions. Package allows users to recover power, sample size requirements, or minimum detectable effect sizes. Package also allows users to work with absolute effects rather than effect sizes, to specify an additional assumption to narrow the bounds, and to incorporate covariate adjustment.",2020-09-27,Eddie Yang,https://github.com/kbansak/powerLATE,TRUE,https://github.com/kbansak/powerlate,4569,1,2020-09-26T01:10:26Z,4569
PowerTOST,"Contains functions to calculate power and sample size for
    various study designs used in bioequivalence studies. Use known.designs() to
    see the designs supported. Power and sample size can be obtained based on
    different methods, amongst them prominently the TOST procedure (two one-sided t-tests).
    See README and NEWS for further information.",2021-01-18,Detlew Labes,https://github.com/Detlew/PowerTOST,TRUE,https://github.com/detlew/powertost,43056,8,2021-09-02T14:58:14Z,5382
PP,"The PP package includes estimation of (MLE, WLE, MAP, EAP, ROBUST)
    person parameters for the 1,2,3,4-PL model and the GPCM (generalized
    partial credit model). The parameters are estimated under the assumption
    that the item parameters are known and fixed. The package is useful e.g. in
    the case that items from an item pool / item bank with known item parameters
    are administered to a new population of test-takers and an ability
    estimation for every test-taker is needed.",2021-05-27,Jan Steinfeld,https://github.com/jansteinfeld/PP,TRUE,https://github.com/jansteinfeld/pp,23358,1,2021-05-28T15:15:35Z,23358
ppdiag,"A suite of diagnostic tools for univariate point processes.
    This includes tools for simulating and fitting both common and more
    complex temporal point processes. We also include functions to 
    visualise these point processes and collect existing diagnostic
    tools of Brown et al. (2002) <doi:10.1162/08997660252741149> and
    Wu et al. (2021) <doi:10.1002/9781119821588.ch7>,
    which can be used to assess the fit of a chosen point process
    model.",2021-08-12,Owen G. Ward,https://owenward.github.io/ppdiag/,TRUE,https://github.com/owenward/ppdiag,2030,2,2021-08-12T13:15:52Z,1015
ppitables,"The Poverty Probability Index (PPI) is a poverty measurement tool 
    for organizations and businesses with a mission to serve the poor. The PPI 
    is statistically-sound, yet simple to use: the answers to 10 questions about 
    a household’s characteristics and asset ownership are scored to compute the 
    likelihood that the household is living below the poverty line – or above by 
    only a narrow margin. This package contains country-specific lookup data tables
    used as reference to determine the poverty likelihood of a household based
    on their score from the country-specific PPI questionnaire. These lookup 
    tables have been extracted from documentation of the PPI found at 
    <https://www.povertyindex.org> and managed by Innovations for Poverty Action 
    <https://www.poverty-action.org>.",2020-10-25,Ernest Guevarra,https://github.com/katilingban/ppitables,TRUE,https://github.com/katilingban/ppitables,17118,3,2020-10-25T19:21:09Z,5706
PPQplan,"Assessment for statistically-based PPQ sampling plan, including calculating the passing probability, optimizing the baseline and high performance cutoff points, visualizing the PPQ plan and power dynamically. The analytical idea is based on the simulation methods from the textbook Burdick, R. K., LeBlond, D. J., Pfahler, L. B., Quiroz, J., Sidor, L., Vukovinsky, K., & Zhang, L. (2017). Statistical Methods for CMC Applications. In Statistical Applications for Chemistry, Manufacturing and Controls (CMC) in the Pharmaceutical Industry (pp. 227-250). Springer, Cham.",2020-10-08,Yalin Zhu,"https://allenzhuaz.github.io/PPQplan/,
https://github.com/allenzhuaz/PPQplan",TRUE,https://github.com/allenzhuaz/ppqplan,12889,0,2020-10-08T03:30:48Z,NA
PPTcirc,Provides functionality for the prior and posterior projected Polya tree for the analysis of circular data (Nieto-Barajas and Nunez-Antonio (2019) <arXiv:1902.06020>).,2021-05-14,Karla Mayra Perez,https://github.com/Karlampm/PPTcirc,TRUE,https://github.com/karlampm/pptcirc,3312,0,2021-05-13T23:25:08Z,NA
prais,"The Prais-Winsten estimator (Prais & Winsten, 1954) takes into account AR(1) serial correlation of the errors in a linear regression model. The procedure recursively estimates the coefficients and the error autocorrelation of the specified model until sufficient convergence of the AR(1) coefficient is attained.",2019-03-10,Franz X. Mohr,https://github.com/franzmohr/prais,TRUE,https://github.com/franzmohr/prais,30472,1,2021-07-10T15:21:56Z,30472
praise,"Build friendly R packages that
    praise their users if they have done something
    good, or they just need it to feel better.",2015-08-11,Gabor Csardi,https://github.com/gaborcsardi/praise,TRUE,https://github.com/gaborcsardi/praise,11333397,131,2021-07-22T12:19:22Z,86514.48091603053
prcbench,A testing workbench for evaluating precision-recall curves under various conditions.,2021-04-07,Takaya Saito,"https://evalclass.github.io/prcbench/,
https://github.com/evalclass/prcbench",TRUE,https://github.com/evalclass/prcbench,17784,4,2021-04-06T11:38:35Z,4446
PRDA,"An implementation of the ""Design Analysis"" proposed by 
    Gelman and Carlin (2014) <doi:10.1177/1745691614551642>. It combines 
    the evaluation of Power-Analysis with other inferential-risks as 
    Type-M error (i.e. Magnitude) and Type-S error (i.e. Sign). See also
    Altoè et al. (2020) <doi:10.3389/fpsyg.2019.02893> and 
    Bertoldo et al. (2020) <doi:10.31234/osf.io/q9f86>.",2020-12-08,Claudio Zandonella Callegher,"https://claudiozandonella.github.io/PRDA/,
https://github.com/ClaudioZandonella/PRDA",TRUE,https://github.com/claudiozandonella/prda,2900,5,2021-02-22T10:18:01Z,580
pre,"Derives prediction rule ensembles (PREs). Largely follows the
    procedure for deriving PREs as described in Friedman & Popescu (2008; 
    <DOI:10.1214/07-AOAS148>), with adjustments and improvements. The 
    main function pre() derives prediction rule ensembles consisting of 
    rules and/or linear terms for continuous, binary, count, multinomial, 
    and multivariate continuous responses. Function gpe() derives 
    generalized prediction ensembles, consisting of rules, hinge and linear 
    functions of the predictor variables.",2021-07-05,Marjolein Fokkema,https://github.com/marjoleinF/pre,TRUE,https://github.com/marjoleinf/pre,58550,44,2021-07-04T20:25:40Z,1330.6818181818182
pRecipe,"An open-access tool/framework to download, validate, visualize, and analyze multi-source precipitation data across various spatio-temporal scales. Ultimately providing the hydrology science community with the tools for consistent and reproducible analysis regarding precipitation.",2021-06-18,Mijael R. Vargas G.,https://github.com/MiRoVaGo/pRecipe,TRUE,https://github.com/mirovago/precipe,757,1,2021-06-19T03:57:56Z,757
precisely,"Estimate sample size based on precision rather than
    power. 'precisely' is a study planning tool to calculate sample size
    based on precision. Power calculations are focused
    on whether or not an estimate will be statistically significant;
    calculations of precision are based on the same principles as power
    calculation but turn the focus to the width of the confidence
    interval. 'precisely' is based on the work of Rothman and Greenland 
    (2018) <doi: 10.1097/EDE.0000000000000876>.",2021-01-12,Malcolm Barrett,https://github.com/malcolmbarrett/precisely,TRUE,https://github.com/malcolmbarrett/precisely,5343,79,2021-01-12T16:23:24Z,67.63291139240506
precisePlacement,"Provides a selection of tools that make it easier to place elements onto a (base R) plot exactly where you want them. It allows users to identify points and distances on a plot in terms of inches, pixels, margin lines, data units, and proportions of the plotting space, all in a manner more simple than manipulating par().",2021-06-15,Jasper Watson,NA,TRUE,https://github.com/rntq472/preciseplacement,949,1,2021-06-20T10:42:42Z,949
precommit,"Useful git hooks for R building on top of the
    multi-language framework 'pre-commit' for hook management. This
    package provides git hooks for common tasks like formatting files with
    'styler' or spell checking as well as wrapper functions to access the
    'pre-commit' executable.",2020-10-10,Lorenz Walthert,"https://lorenzwalthert.github.io/precommit/,
https://github.com/lorenzwalthert/precommit",TRUE,https://github.com/lorenzwalthert/precommit,7147,148,2021-07-02T14:38:01Z,48.29054054054054
precrec,"Accurate calculations and visualization of precision-recall and ROC (Receiver Operator Characteristics)
    curves. Saito and Rehmsmeier (2015) <doi:10.1371/journal.pone.0118432>.",2021-05-31,Takaya Saito,"https://evalclass.github.io/precrec/,
https://github.com/evalclass/precrec/",TRUE,https://github.com/evalclass/precrec,50825,42,2021-05-31T01:23:47Z,1210.1190476190477
predict3d,"Draw 2 dimensional and three dimensional plot for multiple regression models using package 'ggplot2' and 'rgl'.
   Supports linear models (lm), generalized linear models (glm) and local polynomial regression fittings (loess).  ",2019-09-03,Keon-Woong Moon,https://github.com/cardiomoon/predict3d,TRUE,https://github.com/cardiomoon/predict3d,22974,5,2021-04-13T15:09:21Z,4594.8
predictrace,"Predicts the most common race of a surname and based on U.S. Census
    data, and the most common first named based on U.S. Social Security Administration data.",2021-04-23,Jacob Kaplan,https://github.com/jacobkap/predictrace,TRUE,https://github.com/jacobkap/predictrace,11247,1,2021-04-23T19:38:30Z,11247
preferably,"This is an accessible template for 'pkgdown'. It uses two
    bootstrap themes, Flatly and Darkly and utilizes the
    'prefers-color-scheme' CSS variable to automatically serve either of
    the two based on user’s operating system setting, or allowing them to
    manually toggle between them.",2021-05-02,Amir Masoud Abdol,https://preferably.amirmasoudabdol.name,TRUE,https://github.com/amirmasoudabdol/preferably,1736,35,2021-06-13T14:43:54Z,49.6
preference,"Design and analyze two-stage randomized trials with a continuous
    outcome measure. The package contains functions to compute the required 
    sample size needed to detect a given preference, treatment, and selection 
    effect; alternatively, the package contains functions that can report the 
    study power given a fixed sample size. Finally, analysis functions are 
    provided to test each effect using either summary data (i.e. means, 
    variances) or raw study data <doi:10.18637/jss.v094.c02>.",2020-09-09,Michael Kane,https://github.com/kaneplusplus/preference,TRUE,https://github.com/kaneplusplus/preference,16020,2,2020-09-08T18:35:02Z,8010
prenoms,"A database containing the names 
  of the babies born in Quebec between 1980 and 2020.",2021-04-27,Marc-Andre Desautels,<https://github.com/desautm/prenoms>,TRUE,https://github.com/desautm/prenoms,3389,0,2021-04-30T13:26:45Z,NA
prereg,Provides a collection of templates to author preregistration documents for scientific studies in PDF format.,2020-12-02,Frederik Aust,https://github.com/crsh/prereg,TRUE,https://github.com/crsh/prereg,17791,42,2021-03-01T12:46:13Z,423.5952380952381
presize,"Bland (2009) <doi:10.1136/bmj.b3985> recommended to
    base study sizes on the width of the confidence interval rather the power of 
    a statistical test. The goal of 'presize' is to provide functions for such 
    precision based sample size calculations. For a given sample size, the 
    functions will return the precision (width of the confidence interval), and 
    vice versa. ",2021-04-08,Alan G. Haynes,"https://github.com/CTU-Bern/presize,
https://ctu-bern.github.io/presize/",TRUE,https://github.com/ctu-bern/presize,4700,6,2021-09-03T06:59:51Z,783.3333333333334
PressPurt,"This is a computational package designed to identify the most sensitive interactions within a network which must be estimated most accurately in order to produce qualitatively robust predictions to a press perturbation. This is accomplished by enumerating the number of sign switches (and their magnitude) in the net effects matrix when an edge experiences uncertainty. The package produces data and visualizations when uncertainty is associated to one or more edges in the network and according to a variety of distributions. The software requires the network to be described by a system of differential equations but only requires as input a numerical Jacobian matrix evaluated at an equilibrium point. This package is based on Koslicki, D., & Novak, M. (2017) <doi:10.1007/s00285-017-1163-0>.",2020-10-19,David Koslicki,https://github.com/dkoslicki/PressPurt,TRUE,https://github.com/dkoslicki/presspurt,3495,2,2021-05-07T19:15:16Z,1747.5
prettifyAddins,"Provides 'RStudio' addins to prettify 'HTML', 'CSS', 'SCSS', 'JavaScript', 'JSX', 'Markdown', 'C(++)', 'LaTeX', 'Python', 'Julia', 'XML', 'Java', 'JSON', 'Ruby', and to reindent 'C(++)', 'Fortran', 'Java', 'Julia', 'Python', 'SAS', 'Scala', 'Shell' and 'SQL'. Two kinds of addins are provided: 'Prettify' and 'Indent'. The 'Indent' addins only reindent the code, while the 'Prettify' addins also modify the code, e.g. trailing semi-colons are added to 'JavaScript' code when they are missing. ",2021-09-02,Stéphane Laurent,https://github.com/stla/prettifyAddins,TRUE,https://github.com/stla/prettifyaddins,4979,8,2021-09-01T10:22:32Z,622.375
prettyB,"Drop-in replacements for standard base graphics
    functions. The replacements are prettier versions of the originals.",2021-02-10,Colin Gillespie,https://github.com/jumpingrivers/prettyB/,TRUE,https://github.com/jumpingrivers/prettyb,10387,55,2021-02-10T15:51:49Z,188.85454545454544
prettydoc,"Creating tiny yet beautiful documents and vignettes from R
    Markdown. The package provides the 'html_pretty' output format as an
    alternative to the 'html_document' and 'html_vignette' engines that
    convert R Markdown into HTML pages. Various themes and syntax highlight
    styles are supported.",2021-01-10,Yixuan Qiu,https://github.com/yixuan/prettydoc,TRUE,https://github.com/yixuan/prettydoc,353930,357,2021-01-26T02:31:20Z,991.4005602240896
prettyGraphs,"Simple and crisp publication-quality graphics for the ExPosition family of packages.
  See An ExPosition of the Singular Value Decomposition in R (Beaton et al 2014) <doi:10.1016/j.csda.2013.11.006>.",2018-12-18,Derek Beaton,NA,TRUE,https://github.com/derekbeaton/exposition-family_old,79938,3,2020-11-20T04:12:09Z,26646
prettyunits,"Pretty, human readable formatting of quantities.
    Time intervals: '1337000' -> '15d 11h 23m 20s'.
    Vague time intervals: '2674000' -> 'about a month ago'.
    Bytes: '1337' -> '1.34 kB'.",2020-01-24,Gabor Csardi,https://github.com/gaborcsardi/prettyunits,TRUE,https://github.com/gaborcsardi/prettyunits,17019182,109,2021-05-12T08:13:29Z,156139.28440366974
prevR,"Spatial estimation of a prevalence surface
    or a relative risks surface, using data from a Demographic and Health
    Survey (DHS) or an analog survey, see Larmarange et al. (2011)
    <doi:10.4000/cybergeo.24606>.",2020-08-28,Joseph Larmarange,https://github.com/larmarange/prevR/,TRUE,https://github.com/larmarange/prevr,26453,4,2020-11-28T01:44:04Z,6613.25
prewas,"Standardize the pre-processing of genomic variants before 
    performing a bacterial genome-wide association study (bGWAS). 'prewas'
    creates a variant matrix (where each row is a variant, each column is a 
    sample, and the entries are presence - 1 - or absence - 0 - of the variant) 
    that can be used as input for bGWAS tools. When creating the binary variant
    matrix, 'prewas' can perform 3 pre-processing steps including: dealing with 
    multiallelic SNPs, (optional) dealing with SNPs in overlapping genes, and 
    choosing a reference allele. 'prewas' can output matrices for use with both 
    SNP-based bGWAS and gene-based bGWAS. This method is described in Saund et 
    al. (2020) <doi:10.1099/mgen.0.000368>. 'prewas' can also provide 
    gene matrices for variants with specific annotations from the 'SnpEff' 
    software (Cingolani et al. 2012).",2021-04-02,Katie Saund,https://github.com/Snitkin-Lab-Umich/prewas,TRUE,https://github.com/snitkin-lab-umich/prewas,5141,3,2021-07-29T10:22:56Z,1713.6666666666667
priceR,"Functions to aid in micro and macro economic analysis and handling of price and
    currency data. Includes extraction of relevant inflation and exchange rate data from World Bank
    API, data cleaning/parsing, and standardisation. Inflation adjustment
    calculations as found in Principles of Macroeconomics by Gregory Mankiw et al (2014). Current
    and historical end of day exchange rates for 171 currencies from the European Central Bank
    Statistical Data Warehouse (2020) <https://sdw.ecb.europa.eu/curConverter.do>.",2021-05-25,Steve Condylios,https://github.com/stevecondylios/priceR,TRUE,https://github.com/stevecondylios/pricer,16210,17,2021-08-26T14:33:01Z,953.5294117647059
pricesensitivitymeter,"An implementation of the van Westendorp Price
    Sensitivity Meter in R, which is a survey-based approach
	to analyze consumer price preferences and sensitivity
    (van Westendorp 1976, isbn:9789283100386).",2021-03-21,Max Alletsee,https://github.com/max-alletsee/pricesensitivitymeter,TRUE,https://github.com/max-alletsee/pricesensitivitymeter,16641,11,2021-06-30T20:28:14Z,1512.8181818181818
primes,"Fast functions for dealing with prime numbers, such as testing
    whether a number is prime and generating a sequence prime numbers.
    Additional functions include finding prime factors and Ruth-Aaron pairs,
    finding next and previous prime numbers in the series, finding or estimating
    the nth prime, estimating the number of primes less than or equal to an
    arbitrary number, computing primorials, prime k-tuples (e.g., twin primes),
    finding the greatest common divisor and smallest (least) common multiple,
    testing whether two numbers are coprime, and computing Euler's totient
    function. Most functions are vectorized for speed and convenience.",2020-09-03,Os Keyes,https://github.com/ironholds/primes,TRUE,https://github.com/ironholds/primes,31828,5,2020-10-28T19:52:04Z,6365.6
princurve,"Fitting a principal curve to a data matrix in arbitrary dimensions. 
  Hastie and Stuetzle (1989) <doi:10.2307/2289936>.",2021-01-18,Kurt Hornik,https://github.com/rcannood/princurve,TRUE,https://github.com/rcannood/princurve,114023,32,2021-01-18T18:31:57Z,3563.21875
printr,"Extends the S3 generic function knit_print() in 'knitr'
    to automatically print some objects using an appropriate format such as
    Markdown or LaTeX. For example, data frames are automatically printed as
    tables, and the help() pages can also be rendered in 'knitr' documents.",2021-01-27,Yihui Xie,https://yihui.org/printr/,TRUE,https://github.com/yihui/printr,58261,118,2021-01-27T15:38:48Z,493.73728813559325
prioritizr,"
    Systematic conservation prioritization using mixed integer linear
    programming (MILP). It provides a flexible interface for building and
    solving conservation planning problems. Once built, conservation planning
    problems can be solved using a variety of commercial and open-source exact
    algorithm solvers. By using exact algorithm solvers, solutions can be
    generated that are guaranteed to be optimal (or within a pre-specified
    optimality gap). Furthermore, conservation problems can be constructed to
    optimize the spatial allocation of different management actions or zones,
    meaning that conservation practitioners can identify solutions that benefit
    multiple stakeholders. To solve large-scale or complex conservation
    planning problems, users should install the Gurobi optimization software
    (available from <https://www.gurobi.com/>) and the 'gurobi' R package (see
    Gurobi Installation Guide vignette for details). Additionally, the 'rcbc' R
    package (available at <https://github.com/dirkschumacher/rcbc>) can be used
    to generate solutions using the CBC optimization software
    (<https://projects.coin-or.org/Cbc>).",2021-03-31,Jeffrey O Hanson,"https://prioritizr.net, https://github.com/prioritizr/prioritizr",TRUE,https://github.com/prioritizr/prioritizr,29928,67,2021-08-17T02:22:47Z,446.6865671641791
prioritizrdata,"Conservation planning datasets for learning how to use the
    'prioritizr' package <https://CRAN.R-project.org/package=prioritizr>.",2020-08-05,Jeffrey O Hanson,"https://prioritizr.github.io/prioritizrdata/,
https://github.com/prioritizr/prioritizrdata",TRUE,https://github.com/prioritizr/prioritizrdata,18036,1,2021-05-31T03:57:32Z,18036
prism,"Allows users to access the Oregon State Prism climate data
    (<https://prism.nacse.org/>). Using the web service API data
    can easily downloaded in bulk and loaded into R for spatial analysis.
    Some user friendly visualizations are also provided.",2020-12-05,Hart Edmund,"https://docs.ropensci.org/prism/,
https://github.com/ropensci/prism",TRUE,https://github.com/ropensci/prism,20438,42,2021-04-23T20:20:00Z,486.6190476190476
PRISM.forecast,Implements Penalized Regression with Inferred Seasonality Module (PRISM) to generate forecast estimation of weekly unemployment initial claims using 'Google Trends' data. It includes required data and tools for backtesting the performance in 2007-2020.,2020-10-21,Dingdong Yi,https://github.com/ryanddyi/prism,TRUE,https://github.com/ryanddyi/prism,12551,2,2020-10-20T05:48:40Z,6275.5
prismatic,"Manipulate and visualize colors in a intuitive,
    low-dependency and functional way.",2021-01-05,Emil Hvitfeldt,https://github.com/EmilHvitfeldt/prismatic,TRUE,https://github.com/emilhvitfeldt/prismatic,117028,95,2021-01-06T07:17:08Z,1231.8736842105263
probably,"Models can be improved by post-processing class probabilities, by: recalibration, conversion to hard probabilities, assessment of equivocal zones, and other activities. 'probably' contains tools for conducting these operations. ",2020-06-05,Davis Vaughan,"https://github.com/tidymodels/probably/,
https://probably.tidymodels.org",TRUE,https://github.com/tidymodels/probably,17358,71,2021-06-30T23:37:21Z,244.4788732394366
ProbBayes,"Functions and datasets  to accompany J. Albert and J. Hu, ""Probability and Bayesian Modeling"", CRC Press, (2019, ISBN: 1138492566).",2020-03-06,Jim Albert,https://github.com/bayesball/ProbBayes,TRUE,https://github.com/bayesball/probbayes,7366,2,2020-11-25T03:24:18Z,3683
ProbReco,"Training of reconciliation weights for probabilistic forecasts to optimise total energy (or variogram) score using Stochastic Gradient Descent with automatically differentiated gradients. See Panagiotelis, Gamakumara, Athanasopoulos and Hyndman, (2020) <https://www.monash.edu/business/ebs/research/publications/ebs/wp26-2020.pdf> for a description of the methods.",2020-09-24,Anastasios Panagiotelis,https://github.com/anastasiospanagiotelis/ProbReco,TRUE,https://github.com/anastasiospanagiotelis/probreco,5603,4,2020-09-24T05:19:11Z,1400.75
pROC,"Tools for visualizing, smoothing and comparing receiver operating characteristic (ROC curves). (Partial) area under the curve (AUC) can be compared with statistical tests based on U-statistics or bootstrap. Confidence intervals can be computed for (p)AUC or ROC curves.",2021-09-03,Xavier Robin,http://expasy.org/tools/pROC/,TRUE,https://github.com/xrobin/proc,3491833,94,2021-09-02T11:23:33Z,37147.15957446808
ProcData,"Provides tools for exploratory process data analysis. Process data refers to the data describing
    participants' problem-solving processes in computer-based assessments. It is often recorded in computer
    log files. This package provides functions to read, process, and write process data. It also implements
    two feature extraction methods to compress the information stored in process data into standard 
    numerical vectors. This package also provides recurrent neural network based models that relate response processes 
    with other binary or scale variables of interest. The functions that involve training and evaluating neural networks 
    are wrappers of functions in 'keras'.",2021-04-01,Xueying Tang,NA,TRUE,https://github.com/xytangtang/procdata,7592,1,2021-05-29T18:18:14Z,7592
proceduralnames,"A small, dependency-free way to generate random names. Methods 
    provided include the adjective-surname approach of Docker containers 
    ('<https://github.com/moby/moby/blob/master/pkg/namesgenerator/names-generator.go'),
    and combinations of common English or Spanish words.",2021-05-11,Michael Mahoney,"https://mikemahoney218.github.io/proceduralnames/,
https://github.com/mikemahoney218/proceduralnames",TRUE,https://github.com/mikemahoney218/proceduralnames,5276,0,2021-05-11T12:20:04Z,NA
processanimateR,"Provides animated process maps based on the 'procesmapR' package.
  Cases stored in event logs created with with 'bupaR' S3 class eventlog() are
  rendered as tokens (SVG shapes) and animated according to their occurrence 
  times on top of the process map. For rendering SVG animations ('SMIL') and the
  'htmlwidget' package are used.",2020-03-13,Felix Mannhardt,https://github.com/bupaverse/processanimateR/,TRUE,https://github.com/bupaverse/processanimater,43647,49,2020-11-10T18:32:59Z,890.7551020408164
processmapR,"Visualize event logs using directed graphs, i.e. process maps. Part of the 'bupaR' framework.",2020-03-11,Gert Janssenswillen,"https://www.bupar.net, https://github.com/bupaverse/processmapr",TRUE,https://github.com/bupaverse/processmapr,61743,5,2021-06-01T11:09:14Z,12348.6
processR,"Perform moderation, mediation, moderated mediation and moderated moderation. 
   Inspired from famous 'PROCESS' macro for 'SPSS' and 'SAS' created by Andrew Hayes. ",2021-01-07,Keon-Woong Moon,https://github.com/cardiomoon/processR,TRUE,https://github.com/cardiomoon/processr,20279,30,2021-01-07T23:51:11Z,675.9666666666667
procmaps,"Portable '/proc/self/maps' as a data frame.
    Determine which library or other region is mapped to a specific
    address of a process. --
    R packages can contain native code, compiled to shared libraries at build or
    installation time.
    When loaded, each shared library occupies a portion of the address space of
    the main process.
    When only a machine instruction pointer is available (e.g. from a backtrace
    during error inspection or profiling), the address space map determines
    which library this instruction pointer corresponds to.",2020-09-22,Kirill Müller,"https://r-prof.github.io/procmaps/,
https://github.com/r-prof/procmaps",TRUE,https://github.com/r-prof/procmaps,5103,2,2021-07-29T04:14:12Z,2551.5
prodigenr,"Create a project directory structure, along with typical files
    for that project.  This allows projects to be quickly and easily created,
    as well as for them to be standardized. Designed specifically with scientists
    in mind (mainly bio-medical researchers, but likely applies to other fields).",2021-06-02,Luke Johnston,"https://github.com/rostools/prodigenr,
https://rostools.github.io/prodigenr/",TRUE,https://github.com/rostools/prodigenr,17912,27,2021-06-02T12:07:33Z,663.4074074074074
proffer,"Like similar profiling tools,
  the 'proffer' package automatically detects
  sources of slowness in R code.
  The distinguishing feature of 'proffer' is its utilization of
  'pprof', which supplies interactive visualizations
  that are efficient and easy to interpret.
  Behind the scenes, the 'profile' package converts
  native Rprof() data to a protocol buffer
  that 'pprof' understands.
  For the documentation of 'proffer',
  visit <https://r-prof.github.io/proffer/>.
  To learn about the implementations and methodologies of
  'pprof', 'profile', and protocol buffers,
  visit <https://github.com/google/pprof>.
  <https://developers.google.com/protocol-buffers>,
  and <https://github.com/r-prof/profile>, respectively.",2021-07-26,William Michael Landau,"https://github.com/r-prof/proffer,
https://r-prof.github.io/proffer/",TRUE,https://github.com/r-prof/proffer,10752,64,2021-07-26T16:32:39Z,168
profile,"Defines a data structure for profiler data, and methods to read and
    write from the 'Rprof' and 'pprof' file formats.",2020-05-11,Kirill Müller,"https://github.com/r-prof/profile,
https://r-prof.github.io/profile",TRUE,https://github.com/r-prof/profile,15743,9,2021-07-29T04:14:12Z,1749.2222222222222
profileModel,"Provides tools that can be used to calculate, evaluate, plot and use for inference the profiles of *arbitrary* inference functions for *arbitrary* 'glm'-like fitted models with linear predictors. More information on the methods that are implemented can be found in Kosmidis (2008) <https://www.r-project.org/doc/Rnews/Rnews_2008-2.pdf>.",2021-01-08,Ioannis Kosmidis,https://github.com/ikosmidis/profileModel,TRUE,https://github.com/ikosmidis/profilemodel,266291,0,2021-01-08T16:35:37Z,NA
ProFit,Get data / Define model / ??? / Profit! 'ProFit' is a Bayesian galaxy fitting tool that uses a fast 'C++' image generation library and a flexible interface to a large number of likelihood samplers.,2019-11-11,Aaron Robotham,https://github.com/ICRAR/ProFit,TRUE,https://github.com/icrar/profit,17567,22,2021-09-01T08:41:33Z,798.5
profmem,"A simple and light-weight API for memory profiling of R expressions.  The profiling is built on top of R's built-in memory profiler ('utils::Rprofmem()'), which records every memory allocation done by R (also native code).",2020-12-13,Henrik Bengtsson,https://github.com/HenrikBengtsson/profmem,TRUE,https://github.com/henrikbengtsson/profmem,265749,26,2020-12-13T20:41:18Z,10221.115384615385
profoc,"Combine probabilistic forecasts using CRPS learning algorithms proposed in Berrisch, Ziel (2021) <arXiv:2102.00968>. The package implements multiple online learning algorithms like Bernstein online aggregation; see Wintenberger (2014) <arXiv:1404.1356>. Quantile regression is also implemented for comparison purposes. Model parameters can be tuned automatically with respect to the loss of the forecast combination. Methods like predict(), update(), plot() and print() are available for convenience. This package utilizes the optim C++ library for numeric optimization <https://github.com/kthohr/optim>.",2021-08-15,Jonathan Berrisch,"https://profoc.berrisch.biz/, https://github.com/BerriJ/profoc",TRUE,https://github.com/berrij/profoc,643,2,2021-08-15T11:19:59Z,321.5
progressr,"A minimal, unifying API for scripts and packages to report progress updates from anywhere including when using parallel processing.  The package is designed such that the developer can to focus on what progress should be reported on without having to worry about how to present it.  The end user has full control of how, where, and when to render these progress updates, e.g. in the terminal using utils::txtProgressBar() or progress::progress_bar(), in a graphical user interface using utils::winProgressBar(), tcltk::tkProgressBar() or shiny::withProgress(), via the speakers using beep::beepr(), or on a file system via the size of a file. Anyone can add additional, customized, progression handlers. The 'progressr' package uses R's condition framework for signaling progress updated. Because of this, progress can be reported from almost anywhere in R, e.g. from classical for and while loops, from map-reduce APIs like the lapply() family of functions, 'purrr', 'plyr', and 'foreach'. It will also work with parallel processing via the 'future' framework, e.g. future.apply::future_lapply(), furrr::future_map(), and 'foreach' with 'doFuture'. The package is compatible with Shiny applications.",2021-06-10,Henrik Bengtsson,"https://progressr.futureverse.org,
https://github.com/HenrikBengtsson/progressr",TRUE,https://github.com/henrikbengtsson/progressr,238101,211,2021-08-09T22:21:26Z,1128.4407582938388
PROJ,"Currently non-operational, a harmless wrapper to allow package 'reproj' to install and 
  function while relying on the 'proj4' package. ",2020-10-19,Michael D. Sumner,https://github.com/hypertidy/PROJ,TRUE,https://github.com/hypertidy/proj,29054,13,2021-07-27T02:21:30Z,2234.923076923077
ProjectionBasedClustering,"A clustering approach applicable to every projection method is proposed here. The two-dimensional scatter plot of any projection method can construct a topographic map which displays unapparent data structures by using distance and density information of the data. The generalized U*-matrix renders this visualization in the form of a topographic map, which can be used to automatically define the clusters of high-dimensional data. The whole system is based on Thrun and Ultsch, ""Using Projection based Clustering to Find Distance and Density based Clusters in High-Dimensional Data"" <DOI:10.1007/s00357-020-09373-2>. Selecting the correct projection method will result in a visualization in which mountains surround each cluster. The number of clusters can be determined by counting valleys on the topographic map. Most projection methods are wrappers for already available methods in R. By contrast, the neighbor retrieval visualizer (NeRV) is based on C++ source code of the 'dredviz' software package, and the Curvilinear Component Analysis (CCA) is translated from 'MATLAB' ('SOM Toolbox' 2.0) to R.",2020-12-11,Michael Thrun,http://www.deepbionics.org,TRUE,https://github.com/mthrun/projectionbasedclustering,20123,2,2021-08-03T10:04:45Z,10061.5
projections,"Provides functions and graphics for projecting daily incidence based on past incidence, and estimates of the serial interval and reproduction number. Projections are based on a branching process using a Poisson-distributed number of new cases per day, similar to the model used for estimating R in 'EpiEstim' or in 'earlyR', and described by Nouvellet et al. (2017) <doi:10.1016/j.epidem.2017.02.012>. The package provides the S3 class 'projections' which extends 'matrix', with accessors and additional helpers for handling, subsetting, merging, or adding these objects, as well as dedicated printing and plotting methods.",2021-04-22,Thibaut Jombart,https://www.repidemicsconsortium.org/projections/,TRUE,https://github.com/reconhub/projections,26937,12,2021-04-22T09:17:47Z,2244.75
ProjectTemplate,"Provides functions to
    automatically build a directory structure for a new R
    project. Using this structure, 'ProjectTemplate'
    automates data loading, preprocessing, library
    importing and unit testing.",2021-07-31,Aleksandar Blagotic [ctb,http://projecttemplate.net,TRUE,https://github.com/kentonwhite/projecttemplate,45646,585,2021-07-30T16:01:37Z,78.02735042735043
projpred,"
    Performs projection predictive feature selection for generalized linear
    models and generalized linear and additive multilevel models
    (see, Piironen, Paasiniemi and Vehtari, 2020, <https://projecteuclid.org/euclid.ejs/1589335310>,
    Catalina, Bürkner and Vehtari, 2020, <arXiv:2010.06994>).
    The package is compatible with the 'rstanarm' and 'brms' packages, but other
    reference models can also be used. See the package vignette for more
    information and examples.",2020-10-28,Alejandro Catalina,"https://mc-stan.org/projpred/, https://discourse.mc-stan.org",TRUE,https://github.com/stan-dev/projpred,193891,86,2021-01-22T08:17:58Z,2254.546511627907
promises,"Provides fundamental abstractions for doing asynchronous programming
    in R using promises. Asynchronous programming is useful for allowing a single
    R process to orchestrate multiple tasks in the background while also attending
    to something else. Semantics are similar to 'JavaScript' promises, but with a
    syntax that is idiomatic R.",2021-02-11,Joe Cheng,"https://rstudio.github.io/promises/,
https://github.com/rstudio/promises",TRUE,https://github.com/rstudio/promises,14110187,165,2021-02-25T21:06:56Z,85516.28484848485
prompt,"Set the 'R' prompt dynamically, from a function. The package
    contains some examples to include various useful dynamic information
    in the prompt: the status of the last command (success or failure);
    the amount of memory allocated by the current 'R' process; the name of
    the R package(s) loaded by 'pkgload' and/or 'devtools'; various 'git'
    information: the name of the active branch, whether it is dirty,
    if it needs pushes pulls. You can also create your own prompt if you
    don't like the predefined examples.",2021-03-12,Gábor Csárdi,https://github.com/gaborcsardi/prompt,TRUE,https://github.com/gaborcsardi/prompt,3112,193,2021-03-12T17:17:45Z,16.124352331606218
prompter,"In 'Shiny' apps, it is sometimes useful to store information
    on a particular item in a tooltip. 'Prompter' allows you to easily 
    create such tooltips, using 'Hint.css'.",2021-01-11,Etienne Bacher,https://github.com/etiennebacher/prompter,TRUE,https://github.com/etiennebacher/prompter,2615,14,2021-08-23T10:19:00Z,186.78571428571428
PROsetta,"Perform scale linking to establish relationships between instruments
    that measure similar constructs according to the PROsetta Stone methodology, as in Choi, Schalet, Cook, & Cella (2014) <doi:10.1037/a0035768>.",2021-07-20,Seung W. Choi,"http://prosettastone.org (project description),
https://choi-phd.github.io/PROsetta/ (documentation)",TRUE,https://github.com/choi-phd/prosetta,5802,0,2021-07-20T00:56:19Z,NA
ProSGPV,"Implementation of penalized regression with second-generation p-values for variable
    selection. The algorithm can handle linear regression, GLM, and Cox regression. S3 methods print(), summary(), coef(), predict(), and plot() are available for the algorithm. Technical details
    can be found at Zuo et al. (2021) <doi:10.1080/00031305.2021.1946150>. ",2021-08-06,Yi Zuo,https://github.com/zuoyi93/ProSGPV,TRUE,https://github.com/zuoyi93/prosgpv,2916,3,2021-08-24T12:43:35Z,972
prospectr,"Functions to preprocess spectroscopic data 
    and conduct (representative) sample selection/calibration sampling.",2020-10-23,Antoine Stevens,https://github.com/l-ramirez-lopez/prospectr,TRUE,https://github.com/l-ramirez-lopez/prospectr,35092,10,2020-11-02T06:49:54Z,3509.2
protolite,"Pure C++ implementations for reading and writing several common data 
    formats based on Google protocol-buffers. Currently supports 'rexp.proto' for 
    serialized R objects, 'geobuf.proto' for binary geojson, and 'mvt.proto' for 
    vector tiles. This package uses the auto-generated C++ code by protobuf-compiler, 
    hence the entire serialization is optimized at compile time. The 'RProtoBuf' 
    package on the other hand uses the protobuf runtime library to provide a general-
    purpose toolkit for reading and writing arbitrary protocol-buffer data in R.",2021-07-28,Jeroen Ooms,https://github.com/jeroen/protolite,TRUE,https://github.com/jeroen/protolite,328737,43,2021-07-26T10:11:47Z,7645.046511627907
protr,"Comprehensive toolkit for generating various numerical
    features of protein sequences described in Xiao et al. (2015)
    <DOI:10.1093/bioinformatics/btv042>. For full functionality,
    the software 'ncbi-blast+' is needed, see
    <https://blast.ncbi.nlm.nih.gov/Blast.cgi?PAGE_TYPE=BlastDocs&DOC_TYPE=Download>
    for more information.",2019-05-18,Nan Xiao,"https://nanx.me/protr/, https://github.com/nanxstats/protr,
http://protr.org",TRUE,https://github.com/nanxstats/protr,36780,34,2021-08-20T20:01:29Z,1081.764705882353
protti,"Useful functions and workflows for proteomics quality control and data analysis of both limited proteolysis-coupled mass spectrometry (LiP-MS) (Feng et. al. (2014) <doi:10.1038/nbt.2999>) and regular bottom-up proteomics experiments. Data generated with search tools such as 'Spectronaut', 'MaxQuant' and 'Proteome Discover' can be easily used due to flexibility of functions.",2021-05-10,Jan-Philipp Quast,"https://github.com/jpquast/protti,
https://jpquast.github.io/protti/",TRUE,https://github.com/jpquast/protti,1462,3,2021-06-28T16:27:11Z,487.3333333333333
protViz,"Helps with quality checks, visualizations 
    and analysis of mass spectrometry data, coming from proteomics 
    experiments. The package is developed, tested and used at the Functional 
    Genomics Center Zurich <https://fgcz.ch>. We use this package
    mainly for prototyping, teaching, and having fun with proteomics data.
    But it can also be used to do data analysis for small scale data sets.",2020-05-02,Christian Panse,https://github.com/cpanse/protViz/,TRUE,https://github.com/cpanse/protviz,28303,6,2020-11-26T14:10:18Z,4717.166666666667
proxyC,"
    Computes proximity between rows or columns of large matrices efficiently in C++.
    Functions are optimised for large sparse matrices using the Armadillo and Intel TBB libraries.
    Among several built-in similarity/distance measures, computation of correlation,
    cosine similarity and Euclidean distance is particularly fast.",2021-09-02,Kohei Watanabe,https://github.com/koheiw/proxyC,TRUE,https://github.com/koheiw/proxyc,251804,13,2021-09-02T12:30:51Z,19369.53846153846
prozor,"Determine minimal protein set explaining
    peptide spectrum matches. Utility functions for creating fasta amino acid databases with decoys and contaminants.
    Peptide false discovery rate estimation for target decoy search results on psm, precursor, peptide and protein
    level.",2018-07-26,Witold Wolski,https://github.com/protviz/prozor,TRUE,https://github.com/protviz/prozor,15038,5,2021-08-30T13:56:01Z,3007.6
prt,"Intended for larger-than-memory tabular data, 'prt' objects provide an interface to read row and/or column subsets into memory as data.table objects. Data queries, constructed as 'R' expressions, are evaluated using the non-standard evaluation framework provided by 'rlang' and file-backing is powered by the fast and efficient 'fst' package.",2021-05-16,Nicolas Bennett,https://nbenn.github.io/prt/,TRUE,https://github.com/nbenn/prt,5749,3,2021-05-16T07:04:33Z,1916.3333333333333
pryr,"Useful tools to pry back the covers of R and understand the
    language at a deeper level.",2021-07-26,Hadley Wickham,https://github.com/hadley/pryr,TRUE,https://github.com/hadley/pryr,916176,190,2021-07-25T22:40:30Z,4821.978947368421
PSCBS,Segmentation of allele-specific DNA copy number data and detection of regions with abnormal copy number within each parental chromosome.  Both tumor-normal paired and tumor-only analyses are supported.,2019-05-05,Henrik Bengtsson,https://github.com/HenrikBengtsson/PSCBS,TRUE,https://github.com/henrikbengtsson/pscbs,50963,6,2021-08-18T20:55:33Z,8493.833333333334
psd,"Produces power spectral density estimates through iterative
    refinement of the optimal number of sine-tapers at each frequency. This
    optimization procedure is based on the method of Riedel and Sidorenko
    (1995), which minimizes the Mean Square Error (sum of variance and bias)
    at each frequency, but modified for computational stability. The same
    procedure can now be used to calculate the cross spectrum.",2020-06-29,Andrew J. Barbour,"https://github.com/abarbour/psd, Barbour and Parker (2014):
https://doi.org/10.1016/j.cageo.2013.09.015, Riedel and
Sidorenko (1995): https://doi.org/10.1109/78.365298",TRUE,https://github.com/abarbour/psd,29131,9,2020-09-12T17:42:36Z,3236.777777777778
psda,"A toolbox in symbolic data framework as a statistical learning and data mining solution for symbolic polygonal data analysis. This study is a new approach in data analysis and it was proposed by 
            Silva et al. (2019) <doi:10.1016/j.knosys.2018.08.009>. The package presents the estimation of main descriptive statistical measures, e.g, mean, covariance, variance, correlation and coefficient of variation. 
            In addition, a method to obtain polygonal data from classical data is presented. Empirical probability distribution function based on symbolic polygonal histogram and a regression model with its main measures are presented.",2020-05-24,Wagner Silva,https://github.com/wagnerjorge/psda,TRUE,https://github.com/wagnerjorge/psda,17379,0,2021-01-25T00:44:21Z,NA
pseudorank,"Efficient calculation of pseudo-ranks and (pseudo)-rank based test statistics. In case of equal sample sizes, pseudo-ranks and mid-ranks are equal. When used for inference mid-ranks may lead to paradoxical results. Pseudo-ranks are in general not affected by such a problem <doi:10.18637/jss.v095.c01>.",2020-10-02,Martin Happ,https://github.com/happma/pseudorank/,TRUE,https://github.com/happma/pseudorank,16763,1,2020-10-01T18:32:47Z,16763
psfmi,"
	Pooling, backward and forward selection of logistic and Cox regression models in 
	multiply imputed datasets. Backward and forward selection can be done 
	from the pooled model using Rubin's Rules (RR), the D1, D2, D3 and 
	the median p-values method. This is also possible for Mixed models. 
	The models can contain continuous, dichotomous, categorical and restricted 
	cubic spline predictors and interaction terms between	all these type of predictors. 
	The stability of the models	can be evaluated using bootstrapping and cluster 
	bootstrapping. The package further contains functions to pool the model performance 
	as ROC/AUC, R-squares, scaled Brier score and calibration	plots for logistic 
	regression models. Internal validation can be	done with cross-validation or bootstrapping. 
	The adjusted intercept after shrinkage of pooled regression coefficients can be obtained. 
	Backward and forward selection as part of internal validation is possible. 
	A function to externally validate logistic prediction models in multiple imputed 
	datasets is available and a function to compare models.
	Eekhout (2017) <doi:10.1186/s12874-017-0404-7>.
	Wiel (2009) <doi:10.1093/biostatistics/kxp011>.
	Marshall (2009) <doi:10.1186/1471-2288-9-57>.",2021-01-13,Martijn Heymans,https://mwheymans.github.io/psfmi/,TRUE,https://github.com/mwheymans/psfmi,12643,1,2021-08-31T06:48:45Z,12643
psidR,"Makes it easy to build panel data in wide format from Panel Survey
    of Income Dynamics ('PSID') delivered raw data. Downloads data directly from
    the PSID server using the 'SAScii' package. 'psidR' takes care of merging
    data from each wave onto a cross-period index file, so that individuals can be
    followed over time. The user must specify which years they are interested in,
    and the 'PSID' variable names (e.g. ER21003) for each year (they differ in each
    year). The package offers helper functions to retrieve variable names from different
    waves. There are different panel data designs and sample subsetting criteria
    implemented (""SRC"", ""SEO"", ""immigrant"" and ""latino"" samples).",2021-05-07,Florian Oswald,https://github.com/floswald/psidR,TRUE,https://github.com/floswald/psidr,26633,40,2021-07-13T09:24:37Z,665.825
pspline.inference,"Inference of infectious disease outcomes using generalized additive
    (mixed) models with penalized basis splines (P-Splines). See 
    <https://medrxiv.org/cgi/content/short/2020.07.14.20138180v1>.",2021-01-19,Ben Artin,https://github.com/weinbergerlab/pspline.inference,TRUE,https://github.com/weinbergerlab/pspline.inference,5739,0,2021-01-19T13:27:50Z,NA
psqn,"Provides quasi-Newton methods to minimize partially separable
    functions. The methods are largely described by  
    Nocedal and Wright (2006) <doi:10.1007/978-0-387-40065-5>.",2021-05-04,Benjamin Christoffersen,https://github.com/boennecd/psqn,TRUE,https://github.com/boennecd/psqn,4714,1,2021-09-02T14:43:54Z,4714
PSSMCOOL,"Returns almost all features that has been extracted from Position Specific 
             Scoring Matrix (PSSM) so far, which is a matrix of L rows (L is protein length) 
             and 20 columns produced by 'PSI-BLAST' which is a program to produce
             PSSM Matrix from multiple sequence alignment of proteins
             see <https://www.ncbi.nlm.nih.gov/books/NBK2590/> for mor details. some 
             of these features are described in Zahiri, J., et al.(2013)
             <DOI:10.1016/j.ygeno.2013.05.006>,
             Saini, H., et al.(2016)
             <DOI:10.17706/jsw.11.8.756-767>,
             Ding, S., et al.(2014)
             <DOI:10.1016/j.biochi.2013.09.013>,
             Cheng, C.W., et al.(2008)
             <DOI:10.1186/1471-2105-9-S12-S6>,
             Juan, E.Y., et al.(2009)
             <DOI:10.1109/CISIS.2009.194>. ",2020-10-25,Alireza mohammadi,https://github.com/Alireza9651501005/PSSMCOOL,TRUE,https://github.com/alireza9651501005/pssmcool,5880,2,2021-08-29T00:00:13Z,2940
pssmooth,"Implements estimation and testing procedures for evaluating an intermediate biomarker response as a principal surrogate of a clinical response to treatment (i.e., principal stratification effect modification analysis), as described in Juraska M, Huang Y, and Gilbert PB (2020), Inference on treatment effect modification by biomarker response in a three-phase sampling design, Biostatistics, 21(3): 545-560 <doi:10.1093/biostatistics/kxy074>. The methods avoid the restrictive 'placebo structural risk' modeling assumption common to past methods and further improve robustness by the use of nonparametric kernel smoothing for biomarker density estimation. A randomized controlled two-group clinical efficacy trial is assumed with an ordered categorical or continuous univariate biomarker response measured at a fixed timepoint post-randomization and with a univariate baseline surrogate measure allowed to be observed in only a subset of trial participants with an observed biomarker response (see the flexible three-phase sampling design in the paper for details). Bootstrap-based procedures are available for pointwise and simultaneous confidence intervals and testing of four relevant hypotheses. Summary and plotting functions are provided for estimation results.",2020-11-18,Michal Juraska,https://github.com/mjuraska/pssmooth,TRUE,https://github.com/mjuraska/pssmooth,12490,0,2020-11-18T01:09:16Z,NA
PSweight,"Supports propensity score weighting analysis of observational studies and randomized trials. Enables the estimation and inference of average causal effects with binary and multiple treatments using overlap weights (ATO), inverse probability of treatment weights (ATE), average treatment effect among the treated weights (ATT), matching weights (ATM) and entropy weights (ATEN), with and without propensity score trimming. These weights are members of the family of balancing weights introduced in Li, Morgan and Zaslavsky (2018) <doi:10.1080/01621459.2016.1260466> and Li and Li (2019) <doi:10.1214/19-AOAS1282>.",2021-04-01,Tianhui Zhou,https://github.com/thuizhou/PSweight,TRUE,https://github.com/thuizhou/psweight,14838,6,2021-05-18T18:04:10Z,2473
psychmeta,"Tools for computing bare-bones and psychometric meta-analyses and for generating psychometric data for use in meta-analysis simulations. Supports bare-bones, individual-correction, and artifact-distribution methods for meta-analyzing correlations and d values. Includes tools for converting effect sizes, computing sporadic artifact corrections, reshaping meta-analytic databases, computing multivariate corrections for range variation, and more. Bugs can be reported to <https://github.com/psychmeta/psychmeta/issues> or <issues@psychmeta.com>.",2021-06-01,Jeffrey A. Dahlke,NA,TRUE,https://github.com/psychmeta/psychmeta,36197,32,2021-08-05T17:41:45Z,1131.15625
psycho,"The main goal of the psycho package is to provide tools for psychologists, neuropsychologists and neuroscientists, 
   to facilitate and speed up the time spent on data analysis. It aims at supporting best practices and tools to format the output 
   of statistical methods to directly paste them into a manuscript, ensuring statistical reporting standardization and conformity.",2021-01-19,Dominique Makowski,https://github.com/neuropsychology/psycho.R,TRUE,https://github.com/neuropsychology/psycho.r,85887,122,2021-01-19T01:59:11Z,703.9918032786885
psychonetrics,"Multi-group (dynamical) structural equation models in combination with confirmatory network models from cross-sectional, time-series and panel data <doi:10.31234/osf.io/8ha93>. Allows for confirmatory testing and fit as well as exploratory model search.",2021-02-23,Sacha Epskamp,http://psychonetrics.org/,TRUE,https://github.com/sachaepskamp/psychonetrics,20653,16,2021-07-26T15:25:30Z,1290.8125
psychrolib,"
    Implementation of 'PsychroLib'
    <https://github.com/psychrometrics/psychrolib> library which contains
    functions to enable the calculation properties of moist and dry air in both
    metric (SI) and imperial (IP) systems of units. References: Meyer, D. and
    Thevenard, D (2019) <doi:10.21105/joss.01137>.",2021-05-29,Hongyuan Jia,https://github.com/psychrometrics/psychrolib,TRUE,https://github.com/psychrometrics/psychrolib,8836,101,2021-02-01T18:29:28Z,87.48514851485149
ptspotter,"Utility functions produced specifically for (but not limited to) 
    working with 'ProjectTemplate' data pipelines. This package helps to quickly
    create and manage sequentially numbered scripts, quickly set up logging with
    'log4r' and functions to help debug and monitor procedures.",2021-05-03,Rich Leyshon,https://github.com/r-leyshon/ptspotter,TRUE,https://github.com/r-leyshon/ptspotter,1574,0,2021-07-20T19:49:02Z,NA
ptvapi,"Access the 'Public Transport Victoria' Timetable API 
    <https://www.ptv.vic.gov.au/footer/data-and-reporting/datasets/ptv-timetable-api/>,
    with results returned as familiar R data structures. Retrieve information on
    stops, routes, disruptions, departures, and more.",2021-05-02,David Neuzerling,https://github.com/mdneuzerling/ptvapi,TRUE,https://github.com/mdneuzerling/ptvapi,5799,14,2021-05-10T03:23:54Z,414.2142857142857
PTXQC,"Generates Proteomics (PTX) quality control (QC) reports for shotgun LC-MS data analyzed with the 
             MaxQuant software suite (from .txt files) or mzTab files (ideally from OpenMS 'QualityControl' tool).
             Reports are customizable (target thresholds, subsetting) and available in HTML or PDF format.
             Published in J. Proteome Res., Proteomics Quality Control: Quality Control Software for MaxQuant Results (2015)
             <doi:10.1021/acs.jproteome.5b00780>.",2021-06-02,Chris Bielow,https://github.com/cbielow/PTXQC,TRUE,https://github.com/cbielow/ptxqc,26038,26,2021-07-13T11:57:51Z,1001.4615384615385
pubchunks,"Get chunks of XML scholarly articles without
    having to know how to work with XML. Custom mappers
    for each publisher and for each article section pull
    out the information you want. Works with outputs from
    package 'fulltext', 'xml2' package documents, and file
    paths to XML documents.",2020-09-04,Scott Chamberlain,"https://docs.ropensci.org/pubchunks/,
https://github.com/ropensci/pubchunks",TRUE,https://github.com/ropensci/pubchunks,15084,7,2021-01-08T02:37:26Z,2154.8571428571427
pubh,"A toolbox for making R functions and capabilities more
    accessible to students and professionals from Epidemiology and
    Public Health related disciplines. Includes a function to report 
    coefficients and confidence intervals from models using robust
    standard errors (when available), functions that expand 'ggplot2'
    plots and functions relevant for introductory papers in Epidemiology 
    or Public Health. Please note that use of the 
    provided data sets is for educational purposes only.",2021-02-16,Josie Athens,https://github.com/josie-athens/pubh,TRUE,https://github.com/josie-athens/pubh,17926,4,2021-02-15T20:46:33Z,4481.5
PUlasso,"Efficient algorithm for solving PU (Positive and Unlabeled) problem in low or high dimensional setting with lasso or group lasso penalty. The algorithm uses Maximization-Minorization and (block) coordinate descent. Sparse calculation and parallel computing are supported for the computational speed-up. See Hyebin Song, Garvesh Raskutti (2018) <arXiv:1711.08129>.",2021-01-17,Hyebin Song,https://arxiv.org/abs/1711.08129,TRUE,https://github.com/hsong1/pulasso,20054,5,2021-01-15T21:18:06Z,4010.8
pullword,"R Interface to Pullword Service for natural language processing
    in Chinese. It enables users to extract valuable words from text by deep learning models. 
    For more details please visit the official site (in Chinese) <http://www.pullword.com/>.",2021-07-13,Tong He,NA,TRUE,https://github.com/hetong007/pullword,15622,19,2021-07-13T07:46:29Z,822.2105263157895
puls,"A method of clustering functional data using
    subregion information of the curves. It is intended to supplement the
    'fda' and 'fda.usc' packages in functional data object clustering. It
    also facilitates the printing and plotting of the results in a tree
    format and limits the partitioning candidates into a specific set of
    subregions.",2021-02-16,Mark Greenwood,"https://vinhtantran.github.io/puls/,
https://github.com/vinhtantran/puls",TRUE,https://github.com/vinhtantran/puls,3502,0,2021-02-22T00:28:50Z,NA
puniform,"Provides meta-analysis methods that correct for
    publication bias and outcome reporting bias. Four methods and a visual tool 
    are currently included in the package. The p-uniform method as described in 
    van Assen, van Aert, and Wicherts (2015) <doi:10.1037/met0000025> can be used
    for estimating the average effect size, testing the null hypothesis of no 
    effect, and testing for publication bias using only the statistically 
    significant effect sizes of primary studies. The second method in the package 
    is the p-uniform* method as described in van Aert and van Assen (2019) 
    <doi:10.31222/osf.io/zqjr9>. This method is an extension of the p-uniform 
    method that allows for estimation of the average effect size and the 
    between-study variance in a meta-analysis, and uses both the statistically 
    significant and nonsignificant effect sizes. The third method in the package 
    is the hybrid method as described in van Aert and van Assen (2017) 
    <doi:10.3758/s13428-017-0967-6>. The hybrid method is a meta-analysis method 
    for combining an original study and replication and while taking into account 
    statistical significance of the  original study. The p-uniform and hybrid method 
    are based on the statistical theory that the distribution of p-values is 
    uniform conditional on the population effect size. The fourth method in the 
    package is the Snapshot Bayesian Hybrid Meta-Analysis Method as described in 
    van Aert and van Assen (2018) <doi:10.1371/journal.pone.0175302>. This method 
    computes posterior probabilities for four true effect sizes (no, small, medium, 
    and large) based on an original study and replication while taking into account 
    publication bias in the original study. The method can also be used for computing 
    the required sample size of the replication akin to power analysis in null 
    hypothesis significance testing. The meta-plot is a visual tool for meta-analysis 
    that provides information on the primary studies in the meta-analysis, the 
    results of the meta-analysis, and characteristics of the research on the effect 
    under study (van Assen et al., 2021). Helper functions to apply the 
    Correcting for Outcome Reporting Bias (CORB) method to correct for outcome 
    reporting bias in a meta-analysis (van Aert & Wicherts, 2021).",2021-01-06,Robbie C.M. van Aert,https://github.com/RobbievanAert/puniform,TRUE,https://github.com/robbievanaert/puniform,16317,3,2020-12-21T14:27:44Z,5439
PupillometryR,"Provides a unified pipeline to clean, prepare, plot,
    and run basic analyses on pupillometry experiments.",2020-06-13,Samuel Forbes,NA,TRUE,https://github.com/samhforbes/pupillometryr,9989,32,2021-09-01T14:46:34Z,312.15625
pureseqtmr,"Proteins reside in either the cell plasma or in the
    cell membrane. A membrane protein goes through the 
    membrane at least once. Given the amino acid sequence of a
    membrane protein, the tool
    'PureseqTM' (<https://github.com/PureseqTM/pureseqTM_package>,
    as described in ""Efficient And Accurate Prediction Of Transmembrane 
    Topology From Amino acid sequence only."", Wang, Qing, et al (2019), 
    <doi:10.1101/627307>),
    can predict the topology of a membrane protein. This package
    allows one to use 'PureseqTM' from R.",2020-07-30,Richèl J.C. Bilderbeek,https://github.com/richelbilderbeek/pureseqtmr,TRUE,https://github.com/richelbilderbeek/pureseqtmr,4811,0,2021-02-27T14:45:12Z,NA
purrr,"A complete and consistent functional programming
    toolkit for R.",2020-04-17,Lionel Henry,"http://purrr.tidyverse.org, https://github.com/tidyverse/purrr",TRUE,https://github.com/tidyverse/purrr,24587467,986,2021-04-12T07:24:19Z,24936.57910750507
purrrlyr,"Some functions at the intersection of 'dplyr' and
    'purrr' that formerly lived in 'purrr'.",2020-12-16,Lionel Henry,https://github.com/hadley/purrrlyr,TRUE,https://github.com/hadley/purrrlyr,127517,103,2020-12-17T19:02:02Z,1238.0291262135922
pushbar,"Create sliders from left, right, top and bottom which may include any html or 'Shiny' input or output.",2019-03-15,John Coene,https://github.com/JohnCoene/pushbar,TRUE,https://github.com/johncoene/pushbar,11670,52,2020-11-11T09:04:58Z,224.42307692307693
pvaluefunctions,"Contains functions to compute and plot confidence distributions, confidence densities, p-value functions and s-value (surprisal) functions for several commonly used estimates. Instead of just calculating one p-value and one confidence interval, p-value functions display p-values and confidence intervals for many levels thereby allowing to gauge the compatibility of several parameter values with the data. These methods are discussed by Infanger D, Schmidt-Trucksäss A. (2019) <doi:10.1002/sim.8293>; Poole C. (1987) <doi:10.2105/AJPH.77.2.195>; Schweder T, Hjort NL. (2002) <doi:10.1111/1467-9469.00285>; Bender R, Berg G, Zeeb H. (2005) <doi:10.1002/bimj.200410104> ; Singh K, Xie M, Strawderman WE. (2007) <doi:10.1214/074921707000000102>; Rothman KJ, Greenland S, Lash TL. (2008, ISBN:9781451190052); Amrhein V, Trafimow D, Greenland S. (2019) <doi:10.1080/00031305.2018.1543137>; Greenland S. (2019) <doi:10.1080/00031305.2018.1529625> and Rafi Z, Greenland S. (2020) <doi:10.1186/s12874-020-01105-9>.",2020-12-09,Denis Infanger,https://github.com/DInfanger/pvaluefunctions,TRUE,https://github.com/dinfanger/pvaluefunctions,14699,8,2021-07-28T21:32:44Z,1837.375
PWFSLSmoke,"Utilities for working with air quality monitoring data
    with a focus on small particulates (PM2.5) generated by wildfire
    smoke. Functions are provided for downloading available data from
    the United States 'EPA' <https://www.epa.gov/outdoor-air-quality-data> and
    it's 'AirNow' air quality site <https://www.airnow.gov>.
    Additional sources of PM2.5 data made accessible by the package include:
    'AIRSIS' (password protected) <https://www.oceaneering.com/data-management/>
    and 'WRCC' <https://wrcc.dri.edu/cgi-bin/smoke.pl>.
    Data compilations are provided by 'PWFSL'
    <https://www.fs.fed.us/pnw/pwfsl/>.",2020-07-02,Jonathan Callahan,https://github.com/MazamaScience/PWFSLSmoke,TRUE,https://github.com/mazamascience/pwfslsmoke,21311,15,2021-07-26T20:35:02Z,1420.7333333333333
pxR,"Provides a set of functions for reading and writing PC-Axis files, used by different statistical organizations around the globe for data dissemination.",2020-06-07,Carlos J. Gil Bellosta,https://github.com/cjgb/pxR,TRUE,https://github.com/cjgb/pxr,33764,25,2020-09-12T17:00:27Z,1350.56
pxweb,"Generic interface for the PX-Web/PC-Axis API. The PX-Web/PC-Axis
    API is used by organizations such as Statistics Sweden and Statistics
    Finland to disseminate data. The R package can interact with all
    PX-Web/PC-Axis APIs to fetch information about the data hierarchy, extract
    metadata and extract and parse statistics to R data.frame format. PX-Web is
    a solution to disseminate PC-Axis data files in dynamic tables on the web.
    Since 2013 PX-Web contains an API to disseminate PC-Axis files.",2021-07-08,Mans Magnusson,https://github.com/rOpenGov/pxweb/,TRUE,https://github.com/ropengov/pxweb,25037,50,2021-07-08T08:04:15Z,500.74
PxWebApiData,"Function to read PX-Web data into R via API. The example code reads data from the three national statistical institutes, Statistics Norway, Statistics Sweden and Statistics Finland.",2021-07-18,Øyvind Langsrud,https://github.com/statisticsnorway/PxWebApiData,TRUE,https://github.com/statisticsnorway/pxwebapidata,15289,2,2021-08-04T08:40:49Z,7644.5
pyinit,"Deterministic Pena-Yohai initial estimator for robust S estimators
    of regression. The procedure is described in detail in
    Pena, D., & Yohai, V. (1999) <doi:10.2307/2670164>.",2020-12-01,David Kepplinger,https://github.com/dakep/pyinit,TRUE,https://github.com/dakep/pyinit,31374,1,2021-04-25T21:35:10Z,31374
pzfx,Read and write 'GraphPad Prism' '.pzfx' files in R.,2020-07-04,Yue Jiang,https://github.com/Yue-Jiang/pzfx,TRUE,https://github.com/yue-jiang/pzfx,27028,7,2020-11-07T21:12:39Z,3861.1428571428573
qCBA,"CBA postprocessing algorithm that creates smaller models for datasets containing quantitative (numerical) attributes. Article describing QCBA is published in Tomas Kliegr (2017) <arXiv:1711.10166>. The package can also postprocess results of the SBRL package, which is no longer in CRAN, but can be obtained from <https://github.com/cran/sbrl>.",2020-11-19,Tomas Kliegr,https://github.com/kliegr/QCBA,TRUE,https://github.com/kliegr/qcba,11835,7,2021-06-17T14:31:47Z,1690.7142857142858
qcc,"Shewhart quality control charts for continuous, attribute and count data. Cusum and EWMA charts. Operating characteristic curves. Process capability analysis. Pareto chart and cause-and-effect chart. Multivariate control charts.",2017-07-11,Luca Scrucca,https://github.com/luca-scr/qcc,TRUE,https://github.com/luca-scr/qcc,377247,32,2021-08-31T15:05:32Z,11788.96875
qdap,"Automates many of the tasks associated with quantitative discourse analysis of transcripts containing discourse
              including frequency counts of sentence types, words, sentences, turns of talk, syllables and other assorted
              analysis tasks. The package provides parsing tools for preparing transcript data. Many functions enable the user
              to aggregate data by any number of grouping variables, providing analysis and seamless integration with other R
              packages that undertake higher level analysis and visualization of text. This affords the user a more efficient
              and targeted analysis. 'qdap' is designed for transcript analysis, however, many functions are applicable to other
              areas of Text Mining/ Natural Language Processing.",2020-09-27,Tyler Rinker,http://trinker.github.io/qdap/,TRUE,https://github.com/trinker/qdap,257889,150,2020-09-27T17:19:41Z,1719.26
QDiabetes,Calculate the risk of developing type 2 diabetes using risk prediction algorithms derived by 'ClinRisk'.,2021-02-11,Benjamin G. Feakins,https://github.com/Feakster/qdiabetes,TRUE,https://github.com/feakster/qdiabetes,3628,3,2021-02-07T12:59:31Z,1209.3333333333333
QFASA,"Accurate estimates of the diets of predators are required
    in many areas of ecology, but for many species current methods are
    imprecise, limited to the last meal, and often biased. The diversity
    of fatty acids and their patterns in organisms, coupled with the
    narrow limitations on their biosynthesis, properties of digestion in
    monogastric animals, and the prevalence of large storage reservoirs of
    lipid in many predators, led to the development of quantitative
    fatty acid signature analysis (QFASA) to study predator diets.",2021-07-20,Connie Stewart,https://CRAN.R-project.org/package=QFASA,TRUE,https://github.com/cstewartgh/qfasa,15827,0,2021-07-20T18:51:02Z,NA
QGameTheory,"General purpose toolbox for simulating quantum versions of game theoretic models (Flitney and Abbott 2002) <arXiv:quant-ph/0208069>. Quantum (Nielsen and Chuang 2010, ISBN:978-1-107-00217-3) versions of models that have been handled are: Penny Flip Game (David A. Meyer 1998) <arXiv:quant-ph/9804010>, Prisoner's Dilemma (J. Orlin Grabbe 2005) <arXiv:quant-ph/0506219>, Two Person Duel (Flitney and Abbott 2004) <arXiv:quant-ph/0305058>, Battle of the Sexes (Nawaz and Toor 2004) <arXiv:quant-ph/0110096>, Hawk and Dove Game (Nawaz and Toor 2010) <arXiv:quant-ph/0108075>, Newcomb's Paradox (Piotrowski and Sladkowski 2002) <arXiv:quant-ph/0202074> and Monty Hall Problem (Flitney and Abbott 2002) <arXiv:quant-ph/0109035>.",2020-06-12,Indranil Ghosh,https://github.com/indrag49/QGameTheory,TRUE,https://github.com/indrag49/qgametheory,4202,7,2021-02-21T13:06:35Z,600.2857142857143
qgg,"Provides an infrastructure for efficient processing of large-scale genetic and phenotypic data including core functions for: 1) fitting linear mixed models, 2) constructing marker-based genomic relationship matrices, 3) estimating genetic parameters (heritability and correlation), 4) performing genomic prediction and genetic risk profiling, and 5) single or multi-marker association analyses.
    Rohde et al. (2019) <doi:10.1101/503631>.",2020-06-29,Peter Soerensen,https://github.com/psoerensen/qgg,TRUE,https://github.com/psoerensen/qgg,9790,10,2021-09-02T11:29:15Z,979
QGglmm,"Compute various quantitative genetics parameters from a Generalised Linear Mixed Model (GLMM) estimates. Especially, it yields the observed phenotypic mean, phenotypic variance and additive genetic variance.",2020-01-07,Pierre de Villemereuil,NA,TRUE,https://github.com/devillemereuil/qgglmm,17096,10,2021-03-18T12:59:30Z,1709.6
qgraph,"Weighted network visualization and analysis, as well as Gaussian graphical model computation. See Epskamp et al. (2012) <doi:10.18637/jss.v048.i04>.",2021-01-28,Sacha Epskamp,NA,TRUE,https://github.com/sachaepskamp/qgraph,438398,50,2021-09-03T10:13:57Z,8767.96
qicharts2,"Functions for making run charts, Shewhart control charts and
    Pareto charts for continuous quality improvement. Included control charts
    are: I, MR, Xbar, S, T, C, U, U', P, P', and G charts. Non-random variation
    in the form of minor to moderate persistent shifts in data over time is
    identified by the Anhoej rules for unusually long runs and unusually few
    crossing  [Anhoej, Olesen (2014) <doi:10.1371/journal.pone.0113825>].
    Non-random variation in the form of larger, possibly transient, shifts is
    identified by Shewhart's 3-sigma rule [Mohammed, Worthington, Woodall (2008)
    <doi:10.1136/qshc.2004.012047>].",2021-07-08,Jacob Anhoej,https://github.com/anhoej/qicharts2,TRUE,https://github.com/anhoej/qicharts2,42308,28,2021-07-09T07:43:54Z,1511
qmethod,"Analysis of Q methodology, used to identify distinct perspectives existing within a group.
  This methodology is used across social, health and environmental sciences to understand diversity of attitudes, discourses, or decision-making styles (for more information, see <https://qmethod.org/>).
  A single function runs the full analysis. Each step can be run separately using the corresponding functions: for automatic flagging of Q-sorts (manual flagging is optional), for statement scores, for distinguishing and consensus statements, and for general characteristics of the factors.
  The package allows to choose either principal components or centroid factor extraction, manual or automatic flagging, a number of mathematical methods for rotation (or none), and a number of correlation coefficients for the initial correlation matrix, among many other options.
  Additional functions are available to import and export data (from raw *.CSV, 'HTMLQ' and 'FlashQ' *.CSV, 'PQMethod' *.DAT and 'easy-htmlq' *.JSON files), to print and plot, to import raw data from individual *.CSV files, and to make printable cards.
  The package also offers functions to print Q cards and to generate Q distributions for study administration.
  See further details in the package documentation, and in the web pages below, which include a cookbook, guidelines for more advanced analysis (how to perform manual flagging or change the sign of factors), data management, and a beta graphical user interface for online and offline use.",2021-03-15,Aiora Zabala  (Main author,"https://github.com/aiorazabala/qmethod,
https://github.com/aiorazabala/qmethod/wiki",TRUE,https://github.com/aiorazabala/qmethod,22899,30,2021-03-20T14:56:34Z,763.3
qpdf,"Content-preserving transformations transformations of PDF files such 
    as split, combine, and compress. This package interfaces directly to the 'qpdf' 
    C++ API and does not require any command line utilities. Note that 'qpdf' does
    not read actual content from PDF files: to extract text and data you need the
    'pdftools' package.",2019-03-07,Jeroen Ooms,"https://github.com/ropensci/qpdf (devel),
http://qpdf.sourceforge.net/ (upstream)",TRUE,https://github.com/ropensci/qpdf,464454,36,2021-08-05T08:17:29Z,12901.5
qqman,Create Q-Q and manhattan plots for GWAS data from PLINK results.,2021-04-19,Stephen Turner,https://github.com/stephenturner/qqman,TRUE,https://github.com/stephenturner/qqman,149246,115,2021-04-16T18:50:02Z,1297.791304347826
qqplotr,Extensions of 'ggplot2' Q-Q plot functionalities.,2021-04-23,Adam Loy,https://github.com/aloy/qqplotr,TRUE,https://github.com/aloy/qqplotr,94740,39,2021-04-23T13:25:40Z,2429.230769230769
qrcode,Create QRcode in R.,2021-07-02,Victor Teh,"https://thierryo.github.io/qrcode/,
https://github.com/ThierryO/qrcode,
https://doi.org/10.5281/zenodo.5040088",TRUE,https://github.com/thierryo/qrcode,74896,1,2021-07-02T14:14:30Z,74896
qs,Provides functions for quickly writing and reading any R object to and from disk.  ,2021-07-25,Travers Ching,https://github.com/traversc/qs,TRUE,https://github.com/traversc/qs,128352,241,2021-08-14T02:20:13Z,532.5809128630706
qsimulatR,"A quantum computer simulator framework with up to 24 qubits. It allows to
    define general single qubit gates and general controlled single
    qubit gates. For convenience, it currently provides the
    most common gates (X, Y, Z, H, Z, S, T, Rx, Ry, Rz, CNOT, SWAP, Toffoli or
    CCNOT, Fredkin or CSWAP). 'qsimulatR' supports plotting of circuits and is able to
    export circuits to 'Qiskit' <https://qiskit.org/>, a python package
    which can be used to run on IBM's hardware <https://quantum-computing.ibm.com/>.",2020-12-09,Carsten Urbach,https://github.com/HISKP-LQCD/qsimulatR,TRUE,https://github.com/hiskp-lqcd/qsimulatr,2804,6,2021-08-03T16:32:36Z,467.3333333333333
qsub,"Run lapply() calls in parallel by submitting them to 
    'gridengine' clusters using the 'qsub' command.",2021-05-12,Robrecht Cannoodt  (<https://orcid.org/0000-0003-3641-729X>,https://github.com/rcannood/qsub,TRUE,https://github.com/rcannood/qsub,15302,5,2021-07-15T10:11:46Z,3060.4
qtl,"Analysis of experimental crosses to identify genes
  (called quantitative trait loci, QTLs) contributing to variation in
  quantitative traits.
  Broman et al. (2003) <doi:10.1093/bioinformatics/btg112>.",2021-03-27,Karl W Broman <broman@wisc.edu> and Hao Wu,"https://rqtl.org, https://github.com/kbroman/qtl",TRUE,https://github.com/kbroman/qtl,115918,62,2021-08-05T20:49:35Z,1869.6451612903227
qtl2,"Provides a set of tools to perform quantitative
    trait locus (QTL) analysis in experimental crosses. It is a
    reimplementation of the 'R/qtl' package to better handle
    high-dimensional data and complex cross designs.
    Broman et al. (2018) <doi:10.1534/genetics.118.301595>.",2020-12-18,Karl W Broman,"https://kbroman.org/qtl2/, https://github.com/rqtl/qtl2",TRUE,https://github.com/rqtl/qtl2,9116,27,2021-07-26T14:00:21Z,337.6296296296296
qtl2convert,"Functions to convert data structures among the 'qtl2', 'qtl', and 'DOQTL' packages for mapping quantitative trait loci (QTL).",2021-04-29,Karl W Broman,"https://kbroman.org/qtl2/, https://github.com/rqtl/qtl2convert",TRUE,https://github.com/rqtl/qtl2convert,5177,4,2021-07-23T00:58:20Z,1294.25
qtl2fst,"Uses the 'fst' package to store genotype probabilities on disk for the 'qtl2' package. These genotype probabilities are a central data object for mapping quantitative trait loci (QTL), but they can be quite large. The facilities in this package enable the genotype probabilities to be stored on disk, leading to reduced memory usage with only a modest increase in computation time.",2021-04-28,Karl W Broman,https://github.com/rqtl/qtl2fst,TRUE,https://github.com/rqtl/qtl2fst,4916,2,2021-07-12T15:59:22Z,2458
qtl2pleio,"We implement an
    adaptation of Jiang & Zeng's (1995) <https://www.genetics.org/content/140/3/1111> likelihood ratio test for testing
    the null hypothesis of pleiotropy against the alternative hypothesis,
    two separate quantitative trait loci. The test differs from that in Jiang & Zeng (1995) <https://www.genetics.org/content/140/3/1111> 
    and that in Tian et al. (2016) <doi:10.1534/genetics.115.183624> in
    that our test accommodates multiparental populations.",2020-12-02,Frederick J Boehm,https://github.com/fboehm/qtl2pleio,TRUE,https://github.com/fboehm/qtl2pleio,9551,4,2021-07-13T20:20:18Z,2387.75
qtlbook,"Datasets for the book, A Guide to QTL Mapping with R/qtl.
    Broman and Sen (2009) <doi:10.1007/978-0-387-92125-9>.",2019-06-28,Karl W Broman,"http://rqtl.org/book, https://github.com/kbroman/qtlbook",TRUE,https://github.com/kbroman/qtlbook,18468,5,2021-01-10T06:00:19Z,3693.6
qtlcharts,"Web-based interactive charts (using D3.js) for the analysis of
    experimental crosses to identify genetic loci (quantitative trait
    loci, QTL) contributing to variation in quantitative traits.
    Broman (2015) <doi:10.1534/genetics.114.172742>.",2021-08-06,Karl W Broman,"https://kbroman.org/qtlcharts/,
https://github.com/kbroman/qtlcharts",TRUE,https://github.com/kbroman/qtlcharts,21530,82,2021-08-05T20:45:07Z,262.5609756097561
quadmesh,"Create surface forms from matrix or 'raster' data for flexible plotting and
 conversion to other mesh types. The functions 'quadmesh' or 'triangmesh'
 produce a continuous surface as a 'mesh3d' object as used by the 'rgl'
 package. This is used for plotting raster data in 3D (optionally with
 texture), and allows the application of a map projection without data loss and 
 many processing applications that are restricted by inflexible regular grid rasters.
 There are discrete forms of these continuous surfaces available with
 'dquadmesh' and 'dtriangmesh' functions.",2021-01-11,Michael D. Sumner,https://github.com/hypertidy/quadmesh,TRUE,https://github.com/hypertidy/quadmesh,28571,19,2021-02-03T01:30:16Z,1503.7368421052631
qualtRics,"Provides functions to access survey results directly into R using 
    the 'Qualtrics' API. 'Qualtrics' <https://www.qualtrics.com/about/> is an 
    online survey and data collection software platform. See 
    <https://api.qualtrics.com/> for more information about the 'Qualtrics' API. 
    This package is community-maintained and is not officially supported by 
    'Qualtrics'.",2021-01-14,Julia Silge,"https://docs.ropensci.org/qualtRics/,
https://github.com/ropensci/qualtRics",TRUE,https://github.com/ropensci/qualtrics,47623,156,2021-08-22T22:43:34Z,305.27564102564105
quantdr,"An implementation of dimension reduction techniques
    for conditional quantiles. Nonparametric estimation of 
    conditional quantiles is also available.  ",2021-04-20,Eliana Christou,https://github.com/elianachristou/quantdr,TRUE,https://github.com/elianachristou/quantdr,4085,2,2021-04-19T03:45:15Z,2042.5
quanteda,"A fast, flexible, and comprehensive framework for 
    quantitative text analysis in R.  Provides functionality for corpus management,
    creating and manipulating tokens and ngrams, exploring keywords in context, 
    forming and manipulating sparse matrices
    of documents by features and feature co-occurrences, analyzing keywords, computing feature similarities and
    distances, applying content dictionaries, applying supervised and unsupervised machine learning, 
    visually representing text and text analyses, and more. ",2021-08-17,Kenneth Benoit,https://quanteda.io,TRUE,https://github.com/quanteda/quanteda,576765,681,2021-08-17T16:48:00Z,846.9383259911895
quanteda.textmodels,"Scaling models and classifiers for sparse matrix objects representing 
    textual data in the form of a document-feature matrix.  Includes original 
    implementations of 'Laver', 'Benoit', and Garry's (2003) <doi:10.1017/S0003055403000698>,
    'Wordscores' model, Perry and 'Benoit's' (2017) <arXiv:1710.08963> class affinity scaling model, 
    and 'Slapin' and 'Proksch's' (2008) <doi:10.1111/j.1540-5907.2008.00338.x> 'wordfish'
    model, as well as methods for correspondence analysis, latent semantic analysis,
    and fast Naive Bayes and linear 'SVMs' specially designed for sparse textual data.",2021-04-06,Kenneth Benoit,https://github.com/quanteda/quanteda.textmodels,TRUE,https://github.com/quanteda/quanteda.textmodels,50433,35,2021-09-02T15:49:04Z,1440.942857142857
quanteda.textplots,"Plotting functions for visualising textual data.  Extends 'quanteda' and 
   related packages with plot methods designed specifically for text data, textual statistics, 
   and models fit to textual data. Plot types include word clouds, lexical dispersion plots, 
   scaling plots, network visualisations, and word 'keyness' plots.",2021-04-06,Kenneth Benoit,NA,TRUE,https://github.com/quanteda/quanteda.textplots,21769,2,2021-03-30T10:33:49Z,10884.5
quanteda.textstats,"Textual statistics functions formerly in the 'quanteda' package.
    Textual statistics for characterizing and comparing textual data. Includes 
    functions for measuring term and document frequency, the co-occurrence of 
    words, similarity and distance between features and documents, feature entropy, 
    keyword occurrence, readability, and lexical diversity.  These functions 
    extend the 'quanteda' package and are specially designed for sparse textual data.",2021-05-11,Kenneth Benoit,https://quanteda.io,TRUE,https://github.com/quanteda/quanteda.textstats,26543,5,2021-04-28T10:18:21Z,5308.6
quantities,"Integration of the 'units' and 'errors' packages for a complete
    quantity calculus system for R vectors, matrices and arrays, with automatic
    propagation, conversion, derivation and simplification of magnitudes and
    uncertainties. Documentation about 'units' and 'errors' is provided in the
    papers by Pebesma, Mailund & Hiebert (2016, <doi:10.32614/RJ-2016-061>) and
    by Ucar, Pebesma & Azcorra (2018, <doi:10.32614/RJ-2018-075>), included in
    those packages as vignettes; see 'citation(""quantities"")' for details.",2021-02-21,Iñaki Ucar,https://github.com/r-quantities/quantities,TRUE,https://github.com/r-quantities/quantities,14321,19,2021-08-15T23:24:17Z,753.7368421052631
quantmod,"Specify, build, trade, and analyse quantitative financial trading strategies.",2020-12-09,Joshua M. Ulrich,http://www.quantmod.com https://github.com/joshuaulrich/quantmod,TRUE,https://github.com/joshuaulrich/quantmod,7977853,612,2021-08-10T12:10:48Z,13035.70751633987
Quartet,"Calculates the number of four-taxon subtrees consistent with a pair
  of cladograms, calculating the symmetric quartet distance of Bandelt & Dress (1986),
  Reconstructing the shape of a tree from observed dissimilarity data,
  Advances in Applied Mathematics, 7, 309-343 <doi:10.1016/0196-8858(86)90038-2>, 
  and using the tqDist algorithm of Sand et al. (2014), tqDist: a library for
  computing the quartet and triplet distances between binary or general trees,
  Bioinformatics, 30, 2079–2080 <doi:10.1093/bioinformatics/btu157>
  for pairs of binary trees.",2020-12-09,Martin R. Smith,"https://ms609.github.io/Quartet/,
https://github.com/ms609/Quartet/",TRUE,https://github.com/ms609/quartet,15210,4,2021-08-23T06:09:13Z,3802.5
quarto,"Convert R Markdown documents and 'Jupyter' notebooks to a variety of
  output formats using 'Quarto'.",2021-08-06,JJ Allaire,https://github.com/quarto-dev/quarto-r,TRUE,https://github.com/quarto-dev/quarto-r,2932,16,2021-09-03T13:10:05Z,183.25
queryparser,Translate 'SQL' 'SELECT' statements into lists of 'R' expressions.,2021-01-17,Ian Cook,https://github.com/ianmcook/queryparser,TRUE,https://github.com/ianmcook/queryparser,12903,47,2021-08-07T18:01:55Z,274.531914893617
questionr,"Set of functions to make the processing and analysis of
    surveys easier : interactive shiny apps and addins for data recoding,
    contingency tables, dataset metadata handling, and several convenience
    functions.",2020-11-30,Julien Barnier,https://juba.github.io/questionr/,TRUE,https://github.com/juba/questionr,1622108,60,2021-03-04T16:07:57Z,27035.133333333335
queuecomputer,"Implementation of a computationally efficient method for
    simulating queues with arbitrary arrival and service times. 
    Please see Ebert, Wu, Mengersen & Ruggeri (2020, <doi:10.18637/jss.v095.i05>) 
    for further details. ",2021-04-09,Anthony Ebert,https://github.com/AnthonyEbert/queuecomputer,TRUE,https://github.com/anthonyebert/queuecomputer,18146,26,2021-04-11T10:14:36Z,697.9230769230769
quickerstats,"Provides several convenience functions for searching and pulling data from
    the 'USDA NASS Quick Stats API' <https://quickstats.nass.usda.gov/api>. 
    Users can easily search for specific data items, and then download county-level or 
    state-level Census of Agricultural data from a specified year.",2020-10-08,Aaron Anderson,NA,TRUE,https://github.com/anderaa/quickerstats,3576,0,2020-10-23T13:28:36Z,NA
qwraps2,"A collection of (wrapper) functions the creator found useful
    for quickly placing data summaries and formatted regression results into
    '.Rnw' or '.Rmd' files. Functions for generating commonly used graphics,
    such as receiver operating curves or Bland-Altman plots, are also provided
    by 'qwraps2'.  'qwraps2' is a updated version of a package 'qwraps'. The
    original version 'qwraps' was never submitted to CRAN but can be found at
    <https://github.com/dewittpe/qwraps/>. The implementation and limited scope
    of the functions within 'qwraps2' <https://github.com/dewittpe/qwraps2/> is
    fundamentally different from 'qwraps'.",2021-03-07,Peter DeWitt,https://github.com/dewittpe/qwraps2/,TRUE,https://github.com/dewittpe/qwraps2,110016,28,2021-03-30T16:37:41Z,3929.1428571428573
R.cache,"Memoization can be used to speed up repetitive and computational expensive function calls.  The first time a function that implements memoization is called the results are stored in a cache memory.  The next time the function is called with the same set of parameters, the results are momentarily retrieved from the cache avoiding repeating the calculations.  With this package, any R object can be cached in a key-value storage where the key can be an arbitrary set of R objects.  The cache memory is persistent (on the file system).",2021-04-30,Henrik Bengtsson,https://github.com/HenrikBengtsson/R.cache,TRUE,https://github.com/henrikbengtsson/r.cache,1037959,31,2021-07-28T20:20:20Z,33482.54838709677
R.devices,"Functions for creating plots and image files in a unified way
    regardless of output format (EPS, PDF, PNG, SVG, TIFF, WMF, etc.). Default
    device options as well as scales and aspect ratios are controlled in a uniform
    way across all device types. Switching output format requires minimal changes
    in code. This package is ideal for large-scale batch processing, because it
    will never leave open graphics devices or incomplete image files behind, even on
    errors or user interrupts.",2021-01-19,Henrik Bengtsson,https://github.com/HenrikBengtsson/R.devices,TRUE,https://github.com/henrikbengtsson/r.devices,167986,14,2021-01-19T18:40:09Z,11999
R.filesets,"A file set refers to a set of files located in one or more directories on the file system.  This package provides classes and methods to locate, setup, subset, navigate and iterate such sets.  The API is designed such that these classes can be extended via inheritance to provide a richer API for special file formats.  Moreover, a specific name format is defined such that filenames and directories can be considered to have full names which consists of a name followed by comma-separated tags.  This adds additional flexibility to identify file sets and individual files.  NOTE: This package's API should be considered to be in an beta stage.  Its main purpose is currently to support the aroma.* packages, where it is one of the main core components; if you decide to build on top of this package, please contact the author first.",2020-12-08,Henrik Bengtsson,"https://github.com/HenrikBengtsson/R.filesets,
https://www.aroma-project.org/",TRUE,https://github.com/henrikbengtsson/r.filesets,48066,1,2020-12-08T07:21:33Z,48066
R.methodsS3,"Methods that simplify the setup of S3 generic functions and S3 methods.  Major effort has been made in making definition of methods as simple as possible with a minimum of maintenance for package developers.  For example, generic functions are created automatically, if missing, and naming conflict are automatically solved, if possible.  The method setMethodS3() is a good start for those who in the future may want to migrate to S4.  This is a cross-platform package implemented in pure R that generates standard S3 methods.",2020-08-26,Henrik Bengtsson,https://github.com/HenrikBengtsson/R.methodsS3,TRUE,https://github.com/henrikbengtsson/r.methodss3,3043134,0,2021-08-09T11:16:28Z,NA
R.rsp,"The RSP markup language makes any text-based document come alive.  RSP provides a powerful markup for controlling the content and output of LaTeX, HTML, Markdown, AsciiDoc, Sweave and knitr documents (and more), e.g. 'Today's date is <%=Sys.Date()%>'.  Contrary to many other literate programming languages, with RSP it is straightforward to loop over mixtures of code and text sections, e.g. in month-by-month summaries.  RSP has also several preprocessing directives for incorporating static and dynamic contents of external files (local or online) among other things.  Functions rstring() and rcat() make it easy to process RSP strings, rsource() sources an RSP file as it was an R script, while rfile() compiles it (even online) into its final output format, e.g. rfile('report.tex.rsp') generates 'report.pdf' and rfile('report.md.rsp') generates 'report.html'.  RSP is ideal for self-contained scientific reports and R package vignettes.  It's easy to use - if you know how to write an R script, you'll be up and running within minutes.",2020-07-09,Henrik Bengtsson,https://github.com/HenrikBengtsson/R.rsp,TRUE,https://github.com/henrikbengtsson/r.rsp,443732,27,2021-03-10T23:12:27Z,16434.51851851852
R.temis,"An integrated solution to perform
    a series of text mining tasks such as importing and cleaning a corpus, and
    analyses like terms and documents counts, lexical summary, terms
    co-occurrences and documents similarity measures, graphs of terms,
    correspondence analysis and hierarchical clustering. Corpora can be imported
    from spreadsheet-like files, directories of raw text files,
    as well as from 'Dow Jones Factiva', 'LexisNexis', 'Europresse' and 'Alceste' files.",2021-05-12,Milan Bouchet-Valat,https://github.com/nalimilan/R.TeMiS,TRUE,https://github.com/nalimilan/r.temis,15109,20,2021-05-12T18:07:40Z,755.45
R.utils,Utility functions useful when programming and developing R packages.,2020-08-26,Henrik Bengtsson,https://github.com/HenrikBengtsson/R.utils,TRUE,https://github.com/henrikbengtsson/r.utils,2932630,49,2021-04-27T22:49:52Z,59849.5918367347
R2BEAT,"Multivariate optimal allocation for different domains in one and two stages stratified sample design. R2BEAT extends the Neyman (1934) – Tschuprow (1923) allocation method to the case of several variables, adopting a generalization of the Bethel’s proposal (1989).R2BEAT develops this methodology but, moreover, it allows to determine the sample allocation in the multivariate and multi-domains case of estimates for two-stage stratified samples. It also allows to perform both Primary Stage Units and Secondary Stage Units selection. This package requires the availability of ReGenesees, that can be installed from <https://github.com/DiegoZardetto/ReGenesees>.",2021-06-24,Andrea Fasulo,https://barcaroli.github.io/R2BEAT/,TRUE,https://github.com/barcaroli/r2beat,13321,1,2021-09-01T07:40:56Z,13321
r2d3,"Suite of tools for using 'D3', a library for producing dynamic, interactive data
  visualizations. Supports translating objects into 'D3' friendly data structures, rendering
  'D3' scripts, publishing 'D3' visualizations, incorporating 'D3' in R Markdown, creating
  interactive 'D3' applications with Shiny, and distributing 'D3' based 'htmlwidgets' in R
  packages.",2020-12-18,Nick Strayer,https://github.com/rstudio/r2d3,TRUE,https://github.com/rstudio/r2d3,1541943,457,2020-12-15T18:57:43Z,3374.054704595186
r2dictionary,"Despite the predominant use of 'R' for data manipulation and various robust statistical calculations, in recent years, more people from various disciplines are beginning to use 'R' for other purposes. A critical milestone that has enabled large influx of users to the 'R' community is the development of the 'Tidyverse' family of packages and 'Rmarkdown'. With the latter, one can write all kinds of documents and produce output in formats such 'HTML' and 'PDF' very easily. In doing this seemlessly, further tools are needed for such users to easily and freely write in 'R' for all kinds of purposes. The 'r2dictionary' introduces a means for users to directly search for definitions of terms from within the 'R' environment. The source dictionary is an original work of 'The Online Plain Text English Dictionary (OPTED)'.",2020-06-24,Obinna Obianom,https://github.com/oobianom/r2dictionary,TRUE,https://github.com/oobianom/r2dictionary,4142,1,2020-12-08T21:47:55Z,4142
r2dii.analysis,"These tools help you to assess if a corporate lending
    portfolio aligns with climate goals. They summarize key climate
    indicators attributed to the portfolio (e.g. production, emission
    factors), and calculate alignment targets based on climate scenarios.
    They implement in R the last step of the free software 'PACTA' (Paris
    Agreement Capital Transition Assessment;
    <https://2degrees-investing.org/>). Financial institutions use 'PACTA'
    to study how their capital allocation decisions align with climate
    change mitigation goals.",2021-08-18,Jackson Hoffart,https://github.com/2DegreesInvesting/r2dii.analysis,TRUE,https://github.com/2degreesinvesting/r2dii.analysis,8479,7,2021-08-18T16:32:51Z,1211.2857142857142
r2dii.data,"These datasets support the implementation in R of the
    software 'PACTA' (Paris Agreement Capital Transition Assessment),
    which is a free tool that calculates the alignment between corporate
    lending portfolios and climate scenarios
    (<https://2degrees-investing.org/>). Financial institutions use
    'PACTA' to study how their capital allocation decisions align with
    climate change mitigation goals. Because both financial institutions
    and market data providers keep their data private, this package
    provides fake, public data to enable the development and use of
    'PACTA' in R.",2021-07-09,Mauro Lepore,"https://2degreesinvesting.github.io/r2dii.data/,
https://github.com/2DegreesInvesting/r2dii.data",TRUE,https://github.com/2degreesinvesting/r2dii.data,16149,4,2021-09-03T08:38:10Z,4037.25
r2dii.match,"These tools implement in R a fundamental part of the software
    'PACTA' (Paris Agreement Capital Transition Assessment), which is a
    free tool that calculates the alignment between financial portfolios
    and climate scenarios (<https://2degrees-investing.org/>). Financial
    institutions use 'PACTA' to study how their capital allocation
    decisions align with climate change mitigation goals. This package
    matches data from corporate lending portfolios to asset level data
    from market-intelligence databases (e.g. power plant capacities,
    emission factors, etc.). This is the first step to assess if a
    financial portfolio aligns with climate goals.",2021-06-29,Mauro Lepore,"https://2degreesinvesting.github.io/r2dii.match/,
https://github.com/2DegreesInvesting/r2dii.match",TRUE,https://github.com/2degreesinvesting/r2dii.match,12416,1,2021-08-13T08:52:45Z,12416
r2dii.plot,"Create plots to visualize the alignment of a corporate
    lending portfolio to climate change scenarios based on climate
    indicators (production and emission intensities) across key climate
    relevant sectors of the 'PACTA' methodology (Paris Agreement Capital
    Transition Assessment;
    <https://www.transitionmonitor.com/pacta-for-banks-2020/methodology-and-supporting-materials/>).
    Financial institutions use 'PACTA' to study how their capital
    allocation decisions align with climate change mitigation goals.",2021-06-29,Monika Furdyna,https://github.com/2DegreesInvesting/r2dii.plot,TRUE,https://github.com/2degreesinvesting/r2dii.plot,1109,1,2021-07-30T12:49:42Z,1109
R2jags,"Providing wrapper functions to implement Bayesian analysis in JAGS.  Some major features include monitoring convergence of a MCMC model using Rubin and Gelman Rhat statistics, automatically running a MCMC model till it converges, and implementing parallel processing of a MCMC model for multiple chains.",2021-08-05,Yu-Sung Su,NA,TRUE,https://github.com/suyusung/r2jags,337856,2,2021-08-04T23:14:37Z,168928
r2mlm,"Generates both total- and level-specific R-squared measures from
    Rights and Sterba’s (2019) <doi:10.1037/met0000184> framework of R-squared measures for multilevel
    models with random intercepts and/or slopes, which is based on a complete
    decomposition of variance. Additionally generates graphical 
    representations of these R-squared measures to allow visualizing and 
    interpreting all measures in the framework together as an integrated set.
    This framework subsumes 10 previously-developed R-squared measures for 
    multilevel models as special cases of 5 measures from the framework, and it
    also includes several newly-developed measures. Measures in the framework 
    can be used to compute R-squared differences when comparing multilevel 
    models (following procedures in Rights & Sterba (2020) <doi:10.1080/00273171.2019.1660605>).",2021-06-04,Mairead Shaw,https://github.com/mkshaw/r2mlm,TRUE,https://github.com/mkshaw/r2mlm,5472,16,2021-08-11T17:22:16Z,342
r2pmml,"R wrapper for the JPMML-R library <https://github.com/jpmml/jpmml-r>,
    which converts R models to Predictive Model Markup Language (PMML).",2021-03-19,Villu Ruusmann,https://github.com/jpmml/r2pmml,TRUE,https://github.com/jpmml/r2pmml,14090,69,2021-03-19T11:11:08Z,204.20289855072463
r2r,"
	Implementation of hash tables (hash sets and hash maps) in R, 
	featuring arbitrary R objects as keys, 
	arbitrary hash and key-comparison functions, 
	and customizable behaviour upon queries of missing keys.",2021-07-06,Valerio Gherardi,https://github.com/vgherard/r2r,TRUE,https://github.com/vgherard/r2r,718,1,2021-07-07T13:21:05Z,718
r2rtf,Create production-ready Rich Text Format (RTF) table and figure with flexible format.,2021-06-01,Yilong Zhang,"https://merck.github.io/r2rtf/, https://github.com/Merck/r2rtf",TRUE,https://github.com/merck/r2rtf,6800,20,2021-08-26T18:30:51Z,340
r2shortcode,"When creating a package, authors may sometimes struggle with coming up with easy and straightforward function names, and at the same time hoping that other packages do not already have the same function names. In trying to meet this goal, sometimes, function names are not descriptive enough and may confuse the potential users. The purpose of this package is to serve as a package function short form generator and also provide shorthand names for other functions. Having this package will entice authors to create long function names without the fear of users not wanting to use their packages because of the long names. In a way, everyone wins - the authors can use long descriptive function names, and the users can use this package to make short functions names while still using the package in question.",2020-06-25,Obinna Obianom,https://github.com/oobianom/r2shortcode,TRUE,https://github.com/oobianom/r2shortcode,4061,0,2020-12-16T17:14:59Z,NA
r2sundials,"Wrapper for widely used 'SUNDIALS' software (SUite of Nonlinear and DIfferential/ALgebraic Equation Solvers) and more precisely to its 'CVODES' solver. It is aiming to solve ordinary differential equations (ODE) and optionally pending forward sensitivity problem. The wrapper is made 'R' friendly by allowing to pass custom parameters to user's callback functions. Such functions can be both written in 'R' and in 'C++' ('RcppArmadillo' flavor). In case of 'C++', performance is greatly improved so this option is highly advisable when performance matters. If provided, Jacobian matrix can be calculated either in dense or sparse format. In the latter case 'rmumps' package is used to solve corresponding linear systems. Root finding and pending event management are optional and can be specified as 'R' or 'C++' functions too. This makes them a very flexible tool for controlling the ODE system during the time course simulation. 'SUNDIALS' library was published in Hindmarsh et al. (2005) <doi:10.1145/1089014.1089020>.",2021-05-17,Serguei Sokol,NA,TRUE,https://github.com/sgsokol/r2sundials,8445,1,2021-05-17T15:02:34Z,8445
R2SWF,"Using the 'Ming' library
    <http://www.libming.org/> to create Flash animations.
    Users can either use the 'SWF' device swf() to generate 'SWF' file
    directly through plotting functions like plot() and lines(),
    or convert images of other formats ('SVG', 'PNG', 'JPEG') into 'SWF'.",2021-01-12,Yixuan Qiu,https://github.com/yixuan/R2SWF,TRUE,https://github.com/yixuan/r2swf,22836,2,2021-08-14T05:53:38Z,11418
r2symbols,"Direct insertion of symbols (e.g. currencies, letters, arrows, mathematical symbols and so on) into 'Rmarkdown' documents and 'Shiny' applications by incorporating 'HTML' hex codes.",2020-09-09,Obinna Obianom,https://github.com/oobianom/r2symbols,TRUE,https://github.com/oobianom/r2symbols,4982,1,2021-01-29T18:06:28Z,4982
R2ucare,Performs goodness-of-fit tests for capture-recapture models. Also contains several functions to process capture-recapture data.,2017-04-13,Olivier Gimenez,https://github.com/oliviergimenez/R2ucare,TRUE,https://github.com/oliviergimenez/r2ucare,14621,3,2021-08-30T07:37:19Z,4873.666666666667
r3dmol,"Create rich and fully interactive 3D visualizations of molecular data.
    Visualizations can be included in Shiny apps and R markdown documents, or viewed
    from the R console and 'RStudio' Viewer. 'r3dmol' includes an extensive API
    to manipulate the visualization after creation, and supports getting data out of
    the visualization into R. Based on the '3dmol.js' and the 'htmlwidgets' R package.",2021-03-14,Wei Su,https://github.com/swsoyee/r3dmol,TRUE,https://github.com/swsoyee/r3dmol,4000,35,2021-09-03T13:45:43Z,114.28571428571429
r3PG,"Provides a flexible and easy-to-use interface for the Physiological Processes Predicting Growth (3-PG) model written in Fortran. The r3PG serves as a flexible and easy-to-use interface for the 3-PGpjs (monospecific, evenaged and evergreen forests) described in Landsberg & Waring (1997) <doi:10.1016/S0378-1127(97)00026-1> and the 3-PGmix (deciduous, uneven-aged or mixed-species forests) described in Forrester & Tang (2016) <doi:10.1016/j.ecolmodel.2015.07.010>.",2021-02-20,Volodymyr Trotsiuk,https://github.com/trotsiuk/r3PG,TRUE,https://github.com/trotsiuk/r3pg,6026,12,2021-06-24T05:59:11Z,502.1666666666667
R3port,"Create and combine HTML and PDF reports from within R.
    Possibility to design tables and listings for reporting and also include R plots.",2020-09-17,Richard Hooijmaijers,https://github.com/RichardHooijmaijers/R3port,TRUE,https://github.com/richardhooijmaijers/r3port,14589,8,2020-09-17T08:16:01Z,1823.625
r4ss,"A collection of R functions for use with Stock Synthesis, a
    fisheries stock assessment modeling platform written in ADMB by Dr. Richard
    D.  Methot at the NOAA Northwest Fisheries Science Center. The functions
    include tools for summarizing and plotting results, manipulating files,
    visualizing model parameterizations, and various other common stock
    assessment tasks.",2019-10-18,Ian G. Taylor,https://github.com/r4ss/r4ss,TRUE,https://github.com/r4ss/r4ss,26056,23,2021-09-01T21:22:26Z,1132.8695652173913
r5r,"Rapid realistic routing on multimodal transport networks (walk, 
             bike, public transport and car) using 'R5', the Rapid Realistic 
             Routing on Real-world and Reimagined networks 
             <https://github.com/conveyal/r5>. The package allows users to 
             generate detailed routing analysis or calculate travel time matrices
             using seamless parallel computing on top of the R5 Java machine.",2021-07-02,Marcus Saraiva,https://github.com/ipeaGIT/r5r,TRUE,https://github.com/ipeagit/r5r,6338,67,2021-08-25T02:06:41Z,94.59701492537313
R6,"Creates classes with reference semantics, similar to R's built-in
    reference classes. Compared to reference classes, R6 classes are simpler
    and lighter-weight, and they are not built on S4 classes so they do not
    require the methods package. These classes allow public and private
    members, and they support inheritance, even when the classes are defined in
    different packages.",2021-08-19,Winston Chang,"https://r6.r-lib.org, https://github.com/r-lib/R6/",TRUE,https://github.com/r-lib/r6,23722161,320,2021-08-06T20:17:07Z,74131.753125
R62S3,"After defining an R6 class, R62S3 is used to automatically generate optional S3/S4 generics and methods for dispatch. Also allows piping for R6 objects.",2020-03-09,Raphael Sonabend,https://github.com/RaphaelS1/R62S3/,TRUE,https://github.com/raphaels1/r62s3,93661,24,2020-11-10T12:34:01Z,3902.5416666666665
r6methods,"Generate boilerplate code for R6 classes. Given R6 class create getters 
    and/or setters for selected class fields or use RStudio addins to insert methods 
    straight into class definition.",2021-03-16,Jakub Sobolewski,https://github.com/jakubsob/r6methods,TRUE,https://github.com/jakubsob/r6methods,1781,6,2021-03-21T12:03:11Z,296.8333333333333
R6P,"Build robust and maintainable software with object-oriented design 
    patterns in R. Design patterns abstract and present in neat, well-defined 
    components and interfaces the experience of many software designers and 
    architects over many years of solving similar problems. These are solutions 
    that have withstood the test of time with respect to re-usability,
    flexibility, and maintainability. 'R6P' provides abstract base classes with 
    examples for a few known design patterns. The patterns were selected by 
    their applicability to analytic projects in R. Using these patterns in R 
    projects have proven effective in dealing with the complexity that 
    data-driven applications possess.",2021-08-03,Harel Lustiger,"https://tidylab.github.io/R6P/, https://github.com/tidylab/R6P",TRUE,https://github.com/tidylab/r6p,2818,4,2021-08-05T05:29:01Z,704.5
RABR,"Conduct simulations of the Response Adaptive Block Randomization (RABR) design to evaluate its type I error rate, power and operating characteristics for binary and continuous endpoints. For more details of the proposed method, please refer to Zhan et al. (2020) <arXiv:2004.07356>. ",2021-05-26,Tianyu Zhan,https://github.com/tian-yu-zhan/RABR,TRUE,https://github.com/tian-yu-zhan/rabr,1125,1,2021-05-25T13:36:07Z,1125
raceland,"Implements a computational framework for a pattern-based, 
    zoneless analysis, and visualization of (ethno)racial topography 
    (Dmowska, Stepinski, and Nowosad (2020) <doi:10.1016/j.apgeog.2020.102239>).
    It is a reimagined
    approach for analyzing residential segregation and racial diversity based on 
    the concept of 'landscape’ used in the domain of landscape ecology.",2020-10-19,Jakub Nowosad,https://nowosad.github.io/raceland/,TRUE,https://github.com/nowosad/raceland,12121,8,2020-10-19T17:38:42Z,1515.125
RadData,"Nuclear Decay Data for Dosimetric Calculations from the 
    International Commission on Radiological Protection from ICRP 
    Publication 107. Ann. ICRP 38 (3). Eckerman, Keith and Endo, Akira 2008 
    <doi:10.1016/j.icrp.2008.10.004> 
    <https://www.icrp.org/publication.asp?id=ICRP%20Publication%20107>. 
    This is a database of the physical data needed in calculations of 
    radionuclide-specific protection and operational quantities. The 
    data is prescribed by the ICRP, the international authority on 
    radiation dose standards, for estimating dose from the intake of or 
    exposure to radionuclides in the workplace and the environment. 
    The database contains information on the half-lives, decay chains, 
    and yields and energies of radiations emitted in nuclear transformations 
    of 1252 radionuclides of 97 elements. ",2021-04-13,Mark Hogue,https://github.com/markhogue/RadData,TRUE,https://github.com/markhogue/raddata,10148,2,2021-04-12T16:43:07Z,5074
RadialVisGadgets,Shiny-based interactive gadgets of radial visualization methods and extensions thereof.,2020-12-11,José Matute,https://github.com/jmatute/RadialShinyGadgets,TRUE,https://github.com/jmatute/radialshinygadgets,2904,2,2020-12-09T16:21:38Z,1452
radiant,"A platform-independent browser-based interface for business
    analytics in R, based on the shiny package. The application combines the
    functionality of 'radiant.data', 'radiant.design', 'radiant.basics',
    'radiant.model', and 'radiant.multivariate'.",2021-06-08,Vincent Nijs,https://github.com/radiant-rstats/radiant,TRUE,https://github.com/radiant-rstats/radiant,51267,309,2021-06-21T01:01:56Z,165.9126213592233
radiant.basics,"The Radiant Basics menu includes interfaces for probability 
    calculation, central limit theorem simulation, comparing means and proportions, 
    goodness-of-fit testing, cross-tabs, and correlation. The application extends 
    the functionality in 'radiant.data'.",2021-06-03,Vincent Nijs,"https://github.com/radiant-rstats/radiant.basics/,
https://radiant-rstats.github.io/radiant.basics/,
https://radiant-rstats.github.io/docs/",TRUE,https://github.com/radiant-rstats/radiant.basics,43108,8,2021-06-03T05:40:51Z,5388.5
radiant.data,"The Radiant Data menu includes interfaces for loading, saving,
    viewing, visualizing, summarizing, transforming, and combining data. It also
    contains functionality to generate reproducible reports of the analyses
    conducted in the application.",2020-11-27,Vincent Nijs,"https://github.com/radiant-rstats/radiant.data/,
https://radiant-rstats.github.io/radiant.data/,
https://radiant-rstats.github.io/docs/",TRUE,https://github.com/radiant-rstats/radiant.data,58417,43,2021-06-21T01:10:41Z,1358.5348837209303
radiant.model,"The Radiant Model menu includes interfaces for linear and logistic
    regression, naive Bayes, neural networks, classification and regression trees,
    model evaluation, collaborative filtering, decision analysis, and simulation. 
    The application extends the functionality in 'radiant.data'.",2021-05-13,Vincent Nijs,"https://github.com/radiant-rstats/radiant.model/,
https://radiant-rstats.github.io/radiant.model/,
https://radiant-rstats.github.io/docs/",TRUE,https://github.com/radiant-rstats/radiant.model,46840,16,2021-05-13T20:36:10Z,2927.5
radous,"Generate random user data from the Random User Generator API.
    For more information, see <https://randomuser.me/>.",2021-05-23,Mohamed El Fodil Ihaddaden,https://github.com/feddelegrand7/radous,TRUE,https://github.com/feddelegrand7/radous,3977,0,2021-05-23T18:47:40Z,NA
radsafer,"Provides functions for radiation safety, also known as
    ""radiation protection"" and ""radiological control"". The science of 
    radiation protection is called ""health physics"" and its engineering 
    functions are called ""radiological engineering"". Functions in this 
    package cover many of the computations needed by radiation safety 
    professionals. Examples include: obtaining updated calibration and
    source check values for radiation monitors to account for radioactive 
    decay in a reference source, simulating instrument readings to better
    understand measurement uncertainty, correcting instrument readings 
    for geometry and ambient atmospheric conditions. Many of these 
    functions are described in Johnson and Kirby (2011, ISBN-13:  
    978-1609134198). Utilities are also included for developing inputs 
    and processing outputs with radiation transport codes, such as MCNP, 
    a general-purpose Monte Carlo N-Particle code that can be used for 
    neutron, photon, electron, or coupled neutron/photon/electron transport
    (Werner et. al. (2018) <doi:10.2172/1419730>).",2021-04-12,Mark Hogue,https://github.com/markhogue/radsafer,TRUE,https://github.com/markhogue/radsafer,14247,1,2021-04-12T19:58:10Z,14247
Radviz,"An implementation of the radviz projection in R. It enables the visualization of
    multidimensional data while maintaining the relation to the original dimensions.
    This package provides functions to create and plot radviz projections, and a number of summary
    plots that enable comparison and analysis. For reference see Ankerst *et al.* (1996) 
    (<http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.68.1811>) for original implementation, 
    see Di Caro *et al* (2012) (<http://link.springer.com/chapter/10.1007/978-3-642-13672-6_13>) 
    for the original method for dimensional anchor arrangements, see Demsar *et al.* (2007) 
    (<doi:10.1016/j.jbi.2007.03.010>) for the original Freeviz implementation.",2020-06-24,Yann Abraham,http://github.com/yannabraham/Radviz,TRUE,https://github.com/yannabraham/radviz,19239,7,2020-11-14T14:43:30Z,2748.4285714285716
RAdwords,"Aims at loading Google Adwords data into R. Adwords is an online
    advertising service that enables advertisers to display advertising copy to web
    users (see <https://developers.google.com/adwords/> for more information). 
    Therefore the package implements three main features. First, the package
    provides an authentication process for R with the Google Adwords API (see 
    <https://developers.google.com/adwords/api/> for more information) via OAUTH2.
    Second, the package offers an interface to apply the Adwords query language in
    R and query the Adwords API with ad-hoc reports. Third, the received data are
    transformed into suitable data formats for further data processing and data
    analysis.",2019-01-28,Johannes Burkhardt,"https://github.com/jburkhardt/RAdwords,
https://developers.google.com/adwords,
https://developers.google.com/adwords/api/",TRUE,https://github.com/jburkhardt/radwords,44471,98,2021-03-02T08:09:21Z,453.7857142857143
RAEN,"The Proportional Subdistribution Hazard (PSH) model has been popular for estimating the effects of the covariates on the cause of interest in Competing Risks analysis. The fast accumulation of large scale datasets has posed a challenge to classical statistical methods. Current penalized variable selection methods show unsatisfactory performance in ultra-high dimensional data. We propose a novel method, the Random Approximate Elastic Net (RAEN), with a robust and generalized solution to the variable selection problem for the PSH model. Our method shows improved sensitivity for variable selection compared with current methods.",2021-02-21,Han Sun and Xiaofeng Wang,https://github.com/saintland/RAEN,TRUE,https://github.com/saintland/raen,2585,0,2021-01-15T19:21:12Z,NA
Rage,Functions for calculating life history metrics using matrix population models ('MPMs'). Described in Jones et al. (2021) <doi:10.1101/2021.04.26.441330>.,2021-07-09,Pol Capdevila,https://github.com/jonesor/Rage,TRUE,https://github.com/jonesor/rage,2759,5,2021-08-18T15:06:58Z,551.8
ragg,"Anti-Grain Geometry (AGG) is a high-quality and high-performance
    2D drawing library. The 'ragg' package provides a set of graphic devices 
    based on AGG to use as alternative to the raster devices provided through
    the 'grDevices' package.",2021-06-09,Thomas Lin Pedersen,"https://ragg.r-lib.org, https://github.com/r-lib/ragg",TRUE,https://github.com/r-lib/ragg,6868118,140,2021-06-09T18:56:49Z,49057.985714285714
rags2ridges,"Proper L2-penalized maximum likelihood estimators for precision 
  matrices and supporting functions to employ these estimators in a graphical 
  modeling setting. For details see van Wieringen & Peeters (2016) 
  <doi:10.1016/j.csda.2016.05.012> and associated publications.",2021-06-07,Carel F.W. Peeters,"https://cfwp.github.io/rags2ridges/,
https://github.com/CFWP/rags2ridges",TRUE,https://github.com/cfwp/rags2ridges,23741,5,2021-06-07T14:14:35Z,4748.2
RAhrefs,"Enables downloading detailed reports from <https://ahrefs.com>
    about backlinks from pointing to website, provides authentication with an 
    API key as well as ordering, grouping and filtering functionalities.",2019-07-28,Leszek Siemiński,https://ahrefs.com/,TRUE,https://github.com/leszek-sieminski/rahrefs,8726,15,2021-05-09T20:51:53Z,581.7333333333333
rainette,"An R implementation of the Reinert text clustering method. For more 
    details about the algorithm see the included vignettes or Reinert (1990) 
    <doi:10.1177/075910639002600103>.",2021-06-25,Julien Barnier,https://juba.github.io/rainette/,TRUE,https://github.com/juba/rainette,7783,38,2021-07-27T09:41:55Z,204.81578947368422
ralger,The goal of 'ralger' is to facilitate web scraping in R. ,2021-03-17,Mohamed El Fodil Ihaddaden,https://github.com/feddelegrand7/ralger,TRUE,https://github.com/feddelegrand7/ralger,14814,132,2021-03-18T12:47:02Z,112.22727272727273
rAmCharts,"Provides an R interface for using 'AmCharts' Library. Based on
    'htmlwidgets', it provides a global architecture to generate 'JavaScript' source
    code for charts. Most of classes in the library have their equivalent in R
    with S4 classes; for those classes, not all properties have been referenced but
    can easily be added in the constructors. Complex properties (e.g. 'JavaScript'
    object) can be passed as named list. See examples at 
    <http://datastorm-open.github.io/introduction_ramcharts/> 
    and <http://www.amcharts.com/> for
    more information about the library. The package includes the free version
    of 'AmCharts' Library. Its only limitation is a small link to the web site
    displayed on your charts. If you enjoy this library, do not hesitate to refer
    to this page <http://www.amcharts.com/online-store/> to purchase a licence,
    and thus support its creators and get a period of Priority Support. See also
    <http://www.amcharts.com/about/> for more information about 'AmCharts' company.",2019-12-06,Benoit Thieurmel,http://datastorm-open.github.io/introduction_ramcharts/,TRUE,https://github.com/datastorm-open/ramcharts,38731,46,2020-11-19T13:50:32Z,841.9782608695652
rAmCharts4,"Creates JavaScript charts. The charts can be included in 'Shiny' apps and R markdown documents, or viewed from the R console and 'RStudio' viewer. Based on the JavaScript library 'amCharts 4' and the R packages 'htmlwidgets' and 'reactR'. Currently available types of chart are: vertical and horizontal bar chart, radial bar chart, stacked bar chart, vertical and horizontal Dumbbell chart, line chart, scatter chart, range area chart, gauge chart, boxplot chart, and pie chart.",2021-06-25,Stéphane Laurent,https://github.com/stla/rAmCharts4,TRUE,https://github.com/stla/ramcharts4,6039,18,2021-06-25T10:56:27Z,335.5
RAMClustR,"A feature clustering algorithm for non-targeted mass spectrometric metabolomics data. This method is compatible with gas and liquid chromatography coupled mass spectrometry, including indiscriminant tandem mass spectrometry <DOI: 10.1021/ac501530d> data. ",2021-08-11,Corey D. Broeckling,https://github.com/cbroeckl/RAMClustR,TRUE,https://github.com/cbroeckl/ramclustr,9317,8,2021-08-11T14:21:45Z,1164.625
ramcmc,"Function for adapting the shape of the random walk Metropolis proposal
    as specified by robust adaptive Metropolis algorithm by Vihola (2012) <DOI:10.1007/s11222-011-9269-5>. 
    The package also includes fast functions for rank-one Cholesky update and downdate.
    These functions can be used directly from R or the corresponding C++ header files 
    can be easily linked to other R packages.",2021-04-18,Jouni Helske,NA,TRUE,https://github.com/helske/ramcmc,45805,4,2021-04-17T19:31:24Z,11451.25
rameritrade,"Use R to interface with the 'TD Ameritrade' API <https://developer.tdameritrade.com/>.
    Functions include authentication, trading, price requests, account information, and option 
    chains. A user will need a TD brokerage account and TD Ameritrade developer app. See README 
    for authentication process and examples.",2021-02-22,Anthony Balentine,https://exploringfinance.github.io/rameritrade/,TRUE,https://github.com/exploringfinance/rameritrade,4003,10,2021-02-22T02:29:45Z,400.3
RaMS,"R-based access to mass-spectrometry (MS) data. While many packages 
  exist to process MS data, many of these make it difficult to 
  access the underlying mass-to-charge ratio (m/z), intensity, and 
  retention time of the files 
  themselves. This package is designed to format MS data in a tidy fashion and 
  allows the user perform the plotting and analysis.",2021-03-22,William Kumler,https://github.com/wkumler/RaMS,TRUE,https://github.com/wkumler/rams,1750,3,2021-03-22T16:40:56Z,583.3333333333334
randgeo,"Generate random positions (latitude/longitude), 
    Well-known text ('WKT') points or polygons, or 'GeoJSON' points or 
    polygons. ",2018-05-18,Scott Chamberlain,https://github.com/ropensci/randgeo,TRUE,https://github.com/ropensci/randgeo,15837,9,2020-11-25T16:30:35Z,1759.6666666666667
rando,"Provides random number generating functions that
    are much more context aware than the built-in functions. The
    functions are also much safer, as they check for incompatible 
    values, and more reproducible. ",2021-02-16,Michael Barrowman,https://github.com/MyKo101/rando,TRUE,https://github.com/myko101/rando,2160,6,2021-01-23T21:06:10Z,360
randomForestExplainer,"A set of tools to help explain which variables are most important in a random forests. Various variable importance measures are calculated and visualized in different settings in order to get an idea on how their importance changes depending on our criteria (Hemant Ishwaran and Udaya B. Kogalur and Eiran Z. Gorodeski and Andy J. Minn and Michael S. Lauer (2010) <doi:10.1198/jasa.2009.tm08622>, Leo Breiman (2001) <doi:10.1023/A:1010933404324>).",2020-07-11,Yue Jiang,https://github.com/ModelOriented/randomForestExplainer,TRUE,https://github.com/modeloriented/randomforestexplainer,40511,186,2021-06-30T22:18:00Z,217.80107526881721
RandomForestsGLS,"Fits non-linear regression models on dependant data with Generalised Least Square (GLS) based Random Forest (RF-GLS) detailed in Saha, Basu and Datta (2020) <arXiv:2007.15421>.",2021-01-31,Arkajyoti Saha,https://github.com/ArkajyotiSaha/RandomForestsGLS,TRUE,https://github.com/arkajyotisaha/randomforestsgls,3124,1,2021-09-02T08:26:09Z,3124
randomizr,"Generates random assignments for common experimental designs and 
	    random samples for common sampling designs.",2019-09-06,Alexander Coppock,"https://declaredesign.org/r/randomizr/,
https://github.com/DeclareDesign/randomizr",TRUE,https://github.com/declaredesign/randomizr,81620,25,2021-06-06T00:27:18Z,3264.8
randomNames,Function for generating random gender and ethnicity correct first and/or last names. Names are chosen proportionally based upon their probability of appearing in a large scale data base of real names.,2021-04-22,Damian W. Betebenner,"https://CenterForAssessment.github.io/randomNames,
https://github.com/CenterForAssessment/randomNames,
https://cran.r-project.org/package=randomNames",TRUE,https://github.com/centerforassessment/randomnames,43412,15,2021-04-25T00:53:37Z,2894.133333333333
Randomuseragent,"Based on data of real user-agent strings, we can set filtering conditions
    and randomly sample user-agent strings from the user-agent string pool.",2021-06-17,Fangzhou Xie,"https://github.com/fangzhou-xie/Randomuseragent,
https://fangzhou-xie.github.io/Randomuseragent/index.html",TRUE,https://github.com/fangzhou-xie/randomuseragent,900,3,2021-06-17T15:04:40Z,300
rangemap,"A collection of tools to create species range maps based on 
    occurrence data, statistics, and spatial objects. Other tools in this 
    collection can be used to analyze the environmental characteristics of 
    the species ranges. Plotting options to represent results in various 
    manners are also available. Results obtained using these tools can be 
    used to explore the distribution of species and define areas of occupancy 
    and extent of occurrence of species. Other packages help to explore species 
    distributions using distinct methods, but options presented in this set of 
    tools (e.g., using trend surface analysis and concave hull polygons) are 
    exclusive. Description of methods, approaches, and comments for some of the 
    tools implemented here can be found in: 
    IUCN (2001) <https://portals.iucn.org/library/node/10315>,
    Peterson et al. (2011) <https://www.degruyter.com/princetonup/view/title/506966>, 
    and Graham and Hijmans (2006) <doi:10.1111/j.1466-8238.2006.00257.x>.",2021-06-26,Marlon E. Cobos,https://github.com/marlonecobos/rangemap,TRUE,https://github.com/marlonecobos/rangemap,2981,12,2021-09-02T04:53:38Z,248.41666666666666
rangeMapper,Tools for generation of (life-history) traits and diversity maps on hexagonal or square grids. Valcu et al.(2012) <doi:10.1111/j.1466-8238.2011.00739.x>.,2021-02-26,Mihai Valcu,https://github.com/mpio-be/rangeMapper,TRUE,https://github.com/mpio-be/rangemapper,25881,7,2021-02-25T17:31:18Z,3697.285714285714
ranger,"A fast implementation of Random Forests, particularly suited for high
          dimensional data. Ensembles of classification, regression, survival and
          probability prediction trees are supported. Data from genome-wide association
          studies can be analyzed efficiently. In addition to data frames, datasets of
          class 'gwaa.data' (R package 'GenABEL') and 'dgCMatrix' (R package 'Matrix') 
          can be directly analyzed.",2021-07-14,Marvin N. Wright,https://github.com/imbs-hl/ranger,TRUE,https://github.com/imbs-hl/ranger,2273171,629,2021-07-30T06:43:25Z,3613.944356120827
Rankcluster,"Implementation of a model-based clustering algorithm for
    ranking data (C. Biernacki, J. Jacques (2013) <doi:10.1016/j.csda.2012.08.008>). 
    Multivariate rankings as well as partial rankings are taken
    into account. This algorithm is based on an extension of the Insertion
    Sorting Rank (ISR) model for ranking data, which is a meaningful and
    effective model parametrized by a position parameter (the modal ranking,
    quoted by mu) and a dispersion parameter (quoted by pi). The heterogeneity
    of the rank population is modelled by a mixture of ISR, whereas conditional
    independence assumption is considered for multivariate rankings.",2021-01-27,Quentin Grimonprez,NA,TRUE,https://github.com/modal-inria/rankcluster,23622,0,2021-08-25T15:03:17Z,NA
RankingProject,"Functions to generate plots and tables for comparing independently-
    sampled populations. Companion package to ""A Primer on Visualizations
    for Comparing Populations, Including the Issue of Overlapping Confidence
    Intervals"" by Wright, Klein, and Wieczorek (2019)
    <DOI:10.1080/00031305.2017.1392359> and ""A Joint Confidence Region for an
    Overall Ranking of Populations"" by Klein, Wright, and Wieczorek (2020)
    <DOI:10.1111/rssc.12402>.",2021-02-07,Jerzy Wieczorek,https://github.com/civilstat/RankingProject,TRUE,https://github.com/civilstat/rankingproject,14817,4,2021-02-07T21:06:36Z,3704.25
rapbase,"Provide common functions and resources for registry specific
    R-packages at Rapporteket
    <https://rapporteket.github.io/rapporteket/articles/short_introduction.html>.
    This package is relevant for developers of packages/registries at
    Rapporteket.",2019-08-07,Are Edvardsen,http://github.com/Rapporteket/rapbase,TRUE,https://github.com/rapporteket/rapbase,8678,0,2021-09-02T13:14:14Z,NA
RApiDatetime,"Access to the C-level R date and datetime code is provided for
 C-level API use by other packages via registration of native functions.
 Client packages simply include a single header 'RApiDatetime.h' provided
 by this package, and also 'import' it.  The R Core group is the original
 author of the code made available with slight modifications by this package. ",2021-08-14,Dirk Eddelbuettel,"https://github.com/eddelbuettel/rapidatetime,
https://dirk.eddelbuettel.com/code/rapidatetime.html",TRUE,https://github.com/eddelbuettel/rapidatetime,118279,10,2021-08-14T12:52:04Z,11827.9
rapidoc,"A collection of 'HTML', 'JavaScript', 'CSS' and fonts
  assets that generate 'RapiDoc' documentation from an 'OpenAPI' Specification:
   <https://mrin9.github.io/RapiDoc/>.",2021-02-05,Bruno Tremblay,https://github.com/meztez/rapidoc,TRUE,https://github.com/meztez/rapidoc,2618,8,2021-04-13T15:04:48Z,327.25
rapidraker,"A 'Java' implementation of the RAKE algorithm ('Rose', S., 'Engel', D., 
  'Cramer', N. and 'Cowley', W. (2010) <doi:10.1002/9780470689646.ch1>), which can 
  be used to extract keywords from documents without any training data.",2021-06-02,Christopher Baker,https://crew102.github.io/slowraker/articles/rapidraker.html,TRUE,https://github.com/crew102/rapidraker,5790,1,2021-06-03T00:26:19Z,5790
rappdirs,"An easy way to determine which directories on the
    users computer you should use to save data, caches and logs. A port of
    Python's 'Appdirs' (<https://github.com/ActiveState/appdirs>) to
    R.",2021-01-31,Hadley Wickham [trl,"https://rappdirs.r-lib.org, https://github.com/r-lib/rappdirs",TRUE,https://github.com/r-lib/rappdirs,6661058,65,2021-01-31T15:58:25Z,102477.81538461539
rapport,"Facilitating the creation of reproducible statistical
    report templates. Once created, rapport templates can be exported to
    various external formats (HTML, LaTeX, PDF, ODT etc.) with pandoc as the
    converter backend.",2021-04-11,Aleksandar Blagotić,https://rapporter.github.io/rapport/,TRUE,https://github.com/rapporter/rapport,44673,49,2021-04-10T22:54:17Z,911.6938775510204
rapsimng,"The Agricultural Production Systems sIMulator ('APSIM') is a widely
    used to simulate the agricultural systems for multiple crops. This package 
    is designed to create, modify and run 'apsimx' files in the 'APSIM' Next 
    Generation <https://www.apsim.info/>.",2021-03-30,Bangyou Zheng,"https://rapsimng.bangyou.me/, https://github.com/byzheng/rapsimng",TRUE,https://github.com/byzheng/rapsimng,3766,3,2021-08-16T05:27:34Z,1255.3333333333333
RAPTOR,"Performs wood cell anatomical data analyses on spatially explicit xylem (tracheids) datasets 
                  derived from thin sections of woody tissue. The package includes functions for visualisation, 
                  detection and alignment of continuous tracheid radial file (defined as rows) and individual tracheid position 
                  within an annual ring of coniferous species. This package is designed to be used with elaborate cell output, 
                  e.g. as provided with ROXAS (von Arx & Carrer, 2014 <doi:10.1016/j.dendro.2013.12.001>). The package has been validated for Picea abies, 
                  Larix Siberica, Pinus cembra and Pinus sylvestris.",2020-03-09,Richard L. Peters,"https://the-hull.github.io/raptor/,
https://github.com/the-hull/RAPTOR",TRUE,https://github.com/the-hull/raptor,11852,1,2020-11-25T09:51:54Z,11852
raptr,"Biodiversity is in crisis. The overarching aim of conservation
    is to preserve biodiversity patterns and processes. To this end, protected
    areas are established to buffer species and preserve biodiversity processes.
    But resources are limited and so protected areas must be cost-effective.
    This package contains tools to generate plans for protected areas
    (prioritizations), using spatially explicit targets for biodiversity
    patterns and processes. To obtain solutions in a feasible amount  of time,
    this package uses the commercial 'Gurobi' software package (obtained from
    <http://www.gurobi.com/>). For more information on using
    this package, see Hanson et al. (2018) <doi:10.1111/2041-210X.12862>.",2020-07-31,Jeffrey O Hanson,"https://jeffrey-hanson.com/raptr,
https://github.com/jeffreyhanson/raptr",TRUE,https://github.com/jeffreyhanson/raptr,19404,4,2021-05-31T04:00:33Z,4851
raster,"Reading, writing, manipulating, analyzing and modeling of spatial data. The package implements basic and high-level functions for raster data and for vector data operations such as intersections. See the manual and tutorials on <https://rspatial.org/> to get started.",2021-06-18,Robert J. Hijmans,https://rspatial.org/raster,TRUE,https://github.com/rspatial/raster,4311557,125,2021-08-12T13:17:38Z,34492.456
rasterdiv,"Providing functions to calculate indices of diversity on numerical matrices based on information theory. The rationale behind the package is described in Rocchini, Marcantonio and Ricotta (2017) <doi:10.1016/j.ecolind.2016.07.039> and Rocchini, Marcantonio,..., Ricotta (2021) <doi:10.1101/2021.01.23.427872>.",2021-02-22,Matteo Marcantonio,NA,TRUE,https://github.com/mattmar/rasterdiv,7400,7,2021-05-31T07:30:18Z,1057.142857142857
rasterDT,"
  Fast alternatives to several relatively slow 'raster' package
  functions. For large rasters, the functions run from 5 to
  approximately 100 times faster than the 'raster' package functions
  they replace. The 'fasterize' package, on which one function in this
  package depends, includes an implementation of the scan line
  algorithm attributed to Wylie et al. (1967)
  <doi:10.1145/1465611.1465619>.",2020-03-04,Joshua OBrien,https://github.com/JoshOBrien/rasterDT/,TRUE,https://github.com/joshobrien/rasterdt,7747,22,2021-06-05T07:20:43Z,352.1363636363636
rasterpdf,"The ability to plot raster graphics in PDF files can be useful
    when one needs multi-page documents, but the plots contain so many
    individual elements that (the usual) use of vector graphics results in
    inconveniently large file sizes. Internally, the package plots each
    individual page as a PNG, and then combines them in one PDF file.",2019-11-22,Ilari Scheinin,"https://ilarischeinin.github.io/rasterpdf,
https://github.com/ilarischeinin/rasterpdf",TRUE,https://github.com/ilarischeinin/rasterpdf,8794,3,2021-08-06T06:22:15Z,2931.3333333333335
rasterVis,"Methods for enhanced visualization and interaction with raster data. It implements visualization methods for quantitative data and categorical data, both for univariate and multivariate rasters. It also provides methods to display spatiotemporal rasters, and vector fields. See the website for examples.",2021-07-22,Oscar Perpinan Lamigueiro,https://oscarperpinan.github.io/rastervis/,TRUE,https://github.com/oscarperpinan/rastervis,372229,65,2021-08-06T10:52:04Z,5726.6
ratematrix,"The Evolutionary Rate Matrix is a variance-covariance matrix which describes both the rates of trait evolution and the evolutionary correlation among multiple traits. This package has functions to estimate these parameters using Bayesian MCMC. It is possible to test if the pattern of evolutionary correlations among traits has changed between predictive regimes painted along the branches of the phylogenetic tree. Regimes can be created a priori or estimated as part of the MCMC under a joint estimation approach. The package has functions to run MCMC chains, plot results, evaluate convergence, and summarize posterior distributions.",2021-02-24,Daniel Caetano,https://github.com/Caetanods/ratematrix,TRUE,https://github.com/caetanods/ratematrix,16603,5,2021-04-15T15:51:51Z,3320.6
rater,"Fit statistical models based on the Dawid-Skene model - Dawid
    and Skene (1979) <doi:10.2307/2346806> - to repeated categorical
    rating data.  Full Bayesian inference for these models is supported
    through the Stan modelling language. 'rater' also allows the user to
    extract and plot key parameters of these models.",2021-07-13,Jeffrey Pullin,"https://jeffreypullin.github.io/rater/,
https://github.com/jeffreypullin/rater",TRUE,https://github.com/jeffreypullin/rater,3965,12,2021-07-14T01:20:22Z,330.4166666666667
RAthena,"Designed to be compatible with the R package 'DBI' (Database Interface)
    when connecting to Amazon Web Service ('AWS') Athena <https://aws.amazon.com/athena/>.
    To do this 'Python' 'Boto3' Software Development Kit ('SDK')
    <https://boto3.amazonaws.com/v1/documentation/api/latest/index.html> is used as a driver.",2021-07-27,Dyfan Jones,https://github.com/DyfanJones/RAthena,TRUE,https://github.com/dyfanjones/rathena,54444,22,2021-08-04T09:33:02Z,2474.7272727272725
rATTAINS,"An R interface to United States Environmental Protection Agency (EPA)  
    Assessment, Total Maximum Daily Load (TMDL) Tracking and Implementation System 
    ('ATTAINS') data. 'ATTAINS' is the EPA database used to track information 
    provided by states about water quality assessments conducted under federal 
    Clean Water Act requirements. ATTAINS information and API information is available at <https://www.epa.gov/waterdata/attains>.",2021-09-02,Michael Schramm,NA,TRUE,https://github.com/mps9506/rattains,107,0,2021-08-27T18:20:01Z,NA
raveio,"Includes multiple cross-platform read/write interfaces for
    'RAVE' project. 'RAVE' stands for ""R analysis and visualization of human 
    intracranial electroencephalography data"". The whole project aims at 
    providing powerful free-source package that analyze brain recordings from 
    patients with electrodes placed on the cortical surface or inserted into 
    the brain. 'raveio' as part of this project provides tools to read/write 
    neurophysiology data from/to 'RAVE' file structure, as well as several 
    popular formats including  'EDF(+)', 'Matlab', 'BIDS-iEEG', and 'HDF5', 
    etc. Documentation and examples about 'RAVE' project are provided at 
    <https://openwetware.org/wiki/RAVE>, and the paper by John F. Magnotti, 
    Zhengjia Wang, Michael S. Beauchamp (2020) 
    <doi:10.1016/j.neuroimage.2020.117341>; see 'citation(""raveio"")' for 
    details.",2021-08-05,Zhengjia Wang,https://beauchamplab.github.io/raveio/,TRUE,https://github.com/beauchamplab/raveio,4812,2,2021-08-19T17:59:33Z,2406
raven.rdf,"Provides an I/O interface between R data.frames and
    Raven DataFrames. Defines functions to both read and write DataFrame
    files, as well as serialize/deserialize data.frames/DataFrames.",2021-03-17,Phil Gaiser,https://github.com/raven-computing/rdf,TRUE,https://github.com/raven-computing/rdf,1688,0,2021-03-17T20:47:51Z,NA
RavenR,"Utilities for processing input and output files associated with the Raven Hydrological Modelling Framework. Includes various plotting functions, model diagnostics, reading output files into extensible time series format, and support for writing Raven input files.",2021-06-10,Robert Chlumsky,https://github.com/rchlumsk/RavenR,TRUE,https://github.com/rchlumsk/ravenr,2467,19,2021-06-14T17:32:13Z,129.8421052631579
rawr,"Retrieves pure R code from popular R websites, including github <https://github.com>, 
    kaggle <https://www.kaggle.com>, 
    and R blogs made using R blogdown <https://github.com/rstudio/blogdown>.",2021-05-17,Steve Condylios,https://github.com/stevecondylios/rawr,TRUE,https://github.com/stevecondylios/rawr,10256,15,2021-05-16T14:35:15Z,683.7333333333333
rayimage,"Uses convolution-based techniques to generate simulated camera bokeh, depth of field, and other camera effects, using an image and an optional depth map. Accepts both filename inputs and in-memory array representations of images and matrices. Includes functions to perform 2D convolutions, reorient and resize images/matrices, add image overlays, generate camera vignette effects, and add titles to images. ",2021-06-27,Tyler Morgan-Wall,"https://www.rayimage.dev,
https://github.com/tylermorganwall/rayimage",TRUE,https://github.com/tylermorganwall/rayimage,42566,33,2021-06-27T04:19:44Z,1289.878787878788
rayrender,"Render scenes using pathtracing. Build 3D scenes out of spheres, cubes, planes, disks, triangles, cones, curves, line segments, cylinders, ellipsoids, and 3D models in the 'Wavefront' OBJ file format or the PLY Polygon File Format. Supports several material types, textures, multicore rendering, and tone-mapping. Based on the ""Ray Tracing in One Weekend"" book series. Peter Shirley (2018) <https://raytracing.github.io>.",2021-04-07,Tyler Morgan-Wall,"https://www.rayrender.net,
https://github.com/tylermorganwall/rayrender",TRUE,https://github.com/tylermorganwall/rayrender,36155,222,2021-06-26T00:57:43Z,162.86036036036037
rayshader,"Uses a combination of raytracing and multiple hill shading methods to produce 2D and 3D data visualizations and maps. Includes water detection and layering functions, programmable color palette generation, several built-in textures for hill shading, 2D and 3D plotting options, a built-in path tracer, 'Wavefront' OBJ file export, and the ability to save 3D visualizations to a 3D printable format.",2021-04-28,Tyler Morgan-Wall,"https://github.com/tylermorganwall/rayshader,
https://www.rayshader.com/",TRUE,https://github.com/tylermorganwall/rayshader,73253,1414,2021-07-16T23:20:59Z,51.8055162659123
raytracing,"Rossby wave ray paths are traced from 
             a determined source, specified wavenumber, and direction
             of propagation. ""raytracing"" also works with a set of 
             experiments changing these parameters, making possible the
             identification of Rossby wave sources automatically. 
             The theory used here is based on classical studies, 
             such as Hoskins and Karoly (1981) <doi:10.1175/1520-0469(1981)038%3C1179:TSLROA%3E2.0.CO;2>,
             Karoly (1983) <doi:10.1016/0377-0265(83)90013-1>, 
             Hoskins and Ambrizzi (1993) <doi:10.1175/1520-0469(1993)050%3C1661:RWPOAR%3E2.0.CO;2>,
             and Yang and Hoskins (1996) <doi:10.1175/1520-0469(1996)053%3C2365:PORWON%3E2.0.CO;2>.",2020-11-14,Amanda Rehbein,https://github.com/salvatirehbein/raytracing/,TRUE,https://github.com/salvatirehbein/raytracing,3177,3,2021-08-27T13:38:12Z,1059
rayvertex,"Rasterize images using a 3D software renderer. 3D scenes are created either by importing external files, building scenes out of the included objects, or by constructing meshes manually. Supports point and directional lights, anti-aliased lines, shadow mapping, transparent objects, translucent objects, multiple materials types, reflection, refraction, environment maps, multicore rendering, bloom, tone-mapping, and screen-space ambient occlusion.",2021-06-17,Tyler Morgan-Wall,"https://www.rayvertex.com,
https://github.com/tylermorganwall/rayvertex",TRUE,https://github.com/tylermorganwall/rayvertex,1873,45,2021-06-27T18:48:57Z,41.62222222222222
rbace,"Interface to the API for the 'Bielefeld' Academic Search
    Engine ('BASE') (<https://www.base-search.net/>). 'BASE' is a
    search engine for more than 150 million scholarly documents from more
    than 7000 sources. Methods are provided for searching for documents,
    as well as getting information on higher level groupings of documents:
    collections and repositories within collections. Search includes
    faceting, so you can get a high level overview of number of documents
    across a given variable (e.g., year). 'BASE' asks users to respect a
    rate limit, but does not enforce it themselves; we enforce that
    rate limit.",2020-10-13,Scott Chamberlain,"https://docs.ropensci.org/rbace/,
https://github.com/ropensci/rbace",TRUE,https://github.com/ropensci/rbace,4311,7,2020-11-25T16:17:09Z,615.8571428571429
rBayesianOptimization,A Pure R implementation of Bayesian Global Optimization with Gaussian Processes.,2021-06-17,Yachen Yan,https://github.com/yanyachen/rBayesianOptimization,TRUE,https://github.com/yanyachen/rbayesianoptimization,48485,74,2021-06-17T07:28:03Z,655.2027027027027
rbefdata,"Basic R package to access data structures offered by any
    BEFdata portal instance.",2013-11-18,Claas-Thido Pfaff,https://github.com/befdata/rbefdata,TRUE,https://github.com/befdata/rbefdata,17246,1,2020-12-15T07:21:00Z,17246
rbenvo,Provides S3 class objects and methods for built environment data to ease the use of working with these data and facilitate other packages that make use of this data structure.,2020-11-18,Adam Peterson,https://github.com/apeterson91/rbenvo,TRUE,https://github.com/apeterson91/rbenvo,3207,0,2020-12-01T16:29:32Z,NA
rbhl,"Interface to 'Biodiversity' 'Heritage' Library ('BHL')
    (<https://www.biodiversitylibrary.org/>) API
    (<https://www.biodiversitylibrary.org/docs/api3.html>). 'BHL' is a
    repository of 'digitized' literature on 'biodiversity'
    studies, including 'floras', research papers, and more.",2021-05-13,Scott Chamberlain,"https://github.com/ropensci/rbhl (devel)
https://docs.ropensci.org/rbhl/ (documentation)",TRUE,https://github.com/ropensci/rbhl,19310,13,2021-05-12T23:19:12Z,1485.3846153846155
rBiasCorrection,"Implementation of the algorithms (with minor modifications)
    to correct bias in quantitative DNA methylation analyses as described
    by Moskalev et al. (2011) <doi:10.1093/nar/gkr213>. Publication:
    Kapsner et al. (2021) <doi:10.1002/ijc.33681>.",2021-08-04,Lorenz A. Kapsner,https://github.com/kapsner/rBiasCorrection,TRUE,https://github.com/kapsner/rbiascorrection,10100,1,2021-08-03T17:54:57Z,10100
rbibutils,"Read and write 'Bibtex' files. Convert between bibliography
    formats, including 'Bibtex', 'Biblatex', 'Endnote', and 'Bibentry'.
    Includes a port of the 'bibutils' utilities by Chris Putnam
    <https://sourceforge.net/projects/bibutils/>. Supports all
    bibliography formats and character encodings implemented in
    'bibutils'.",2021-08-09,Georgi N. Boshnakov  (R port,"https://geobosh.github.io/rbibutils/ (website),
https://github.com/GeoBosh/rbibutils (devel)",TRUE,https://github.com/geobosh/rbibutils,651188,2,2021-08-10T15:29:21Z,325594
rbin,"Manually bin data using weight of evidence and information value. Includes other binning 
    methods such as equal length, quantile and winsorized. Options for combining levels of categorical
    data are also available. Dummy variables can be generated based on the bins created using any of 
    the available binning methods. References: Siddiqi, N. (2006) <doi:10.1002/9781119201731.biblio>.",2020-05-14,Aravind Hebbali,"https://github.com/rsquaredacademy/rbin,
https://rbin.rsquaredacademy.com",TRUE,https://github.com/rsquaredacademy/rbin,41477,11,2020-12-08T07:19:08Z,3770.6363636363635
rbioapi,"Currently fully supports Enrichr, JASPAR, miEAA, PANTHER,
    Reactome, STRING, and UniProt! The goal of rbioapi is to provide a
    user-friendly and consistent interface to biological databases and
    services: In a way that insulates the user from technicalities of
    using web services API and creates a unified and easy-to-use interface
    to biological and medical web services. This an ongoing project; New
    databases and services will be added periodically. Feel free to
    suggest any databases or services you often use.",2021-06-22,Moosa Rezwani,"https://rbioapi.moosa-r.com, https://github.com/moosa-r/rbioapi",TRUE,https://github.com/moosa-r/rbioapi,1707,1,2021-06-22T16:04:36Z,1707
rbiom,"
    A toolkit for working with Biological Observation Matrix ('BIOM') files.
    Features include reading/writing all 'BIOM' formats, rarefaction, alpha
    diversity, beta diversity (including 'UniFrac'), summarizing counts by 
    taxonomic level, and sample subsetting. Standalone functions for 
    reading, writing, and subsetting phylogenetic trees are also provided. 
    All CPU intensive operations are encoded in C with multi-thread support.",2020-05-29,Daniel P. Smith,https://cmmr.github.io/rbiom/index.html,TRUE,https://github.com/cmmr/rbiom,5202,4,2021-09-01T23:37:58Z,1300.5
rbison,"Interface to the 'USGS' 'BISON' (<https://bison.usgs.gov/>)
    API, a 'database' for species occurrence data. Data comes from
    species in the United States from participating data providers. You can get
    data via 'taxonomic' and location based queries. A simple function
    is provided to help visualize data.",2020-06-08,Scott Chamberlain,"https://github.com/ropensci/rbison (devel)
https://docs.ropensci.org/rbison (docs)",TRUE,https://github.com/ropensci/rbison,57589,11,2020-12-22T22:58:58Z,5235.363636363636
RblDataLicense,"R interface to access prices and market data with the 
    'Bloomberg Data License' service from 
    <https://www.bloomberg.com/professional/product/data-license/>. 
    As a prerequisite, a valid Data License from 'Bloomberg' is needed 
    together with the corresponding SFTP credentials and whitelisting 
    of the IP from which accessing the service. 
    This software and its author are in no way affiliated, 
    endorsed, or approved by 'Bloomberg' or any of its affiliates.
    'Bloomberg' is a registered trademark.",2021-07-29,Emanuele Guidotti,https://rbldatalicense.guidotti.dev,TRUE,https://github.com/eguidotti/rbldatalicense,13013,8,2021-07-29T19:19:12Z,1626.625
Rblpapi,An R Interface to 'Bloomberg' is provided via the 'Blp API'.,2021-04-20,Whit Armstrong,"https://dirk.eddelbuettel.com/code/rblpapi.html,
https://github.com/Rblp/Rblpapi",TRUE,https://github.com/rblp/rblpapi,74922,137,2021-04-29T12:07:06Z,546.8759124087592
rblt,"An R-shiny application to plot datalogger time series at a microsecond precision as Acceleration, Temperature, 
  Pressure, Light intensity from CATS, AXY-TREK LUL and WACU bio-loggers. It is possible to link behavioral labels extracted
  from 'BORIS' software <http://www.boris.unito.it> or manually written in a csv file.
  CATS bio-logger are manufactured by <http://www.cats.is>, AXY-TREK are manufactured by <http://www.technosmart.eu> and 
  LUL and WACU are manufactured by <http://www.iphc.cnrs.fr/-MIBE-.html>.",2019-12-05,Sebastien Geiger,https://github.com/sg4r/rblt,TRUE,https://github.com/sg4r/rblt,11471,1,2021-04-24T04:28:31Z,11471
RBMRB,"The Biological Magnetic Resonance Data Bank (BMRB,<http://
    www.bmrb.io/>) collects, annotates, archives, and disseminates (worldwide
    in the public domain) the important spectral and quantitative data derived
    from NMR(Nuclear Magnetic Resonance) spectroscopic investigations of biological
    macromolecules and metabolites. This package provides an interface to BMRB
    database for easy data access and includes a minimal set of data visualization
    functions. Users are encouraged to make their own data visualizations using BMRB
    data.",2021-07-11,Kumaran Baskaran,"https://github.com/uwbmrb/RBMRB,
https://github.com/kumar-physics/RBMRB",TRUE,https://github.com/uwbmrb/rbmrb,16754,3,2020-10-26T01:50:15Z,5584.666666666667
RBNZ,"Provides a convenient way of accessing data published by the Reserve Bank of New Zealand (RBNZ) on their website, <https://www.rbnz.govt.nz/statistics>. A range of financial and economic data is provided in spreadsheet format including exchange and interest rates, commercial lending statistics, Reserve Bank market operations, financial institution statistics, household financial data, New Zealand debt security information, and economic indicators. This package provides a method to download those spreadsheets and read them directly into R.",2020-07-27,Jasper Watson,NA,TRUE,https://github.com/rntq472/rbnz,7251,0,2021-08-27T13:48:07Z,NA
rbokeh,"A native R plotting library that provides a flexible declarative interface for creating interactive web-based graphics, backed by the Bokeh visualization library <https://bokeh.pydata.org/>.",2021-08-04,Ryan Hafen,https://hafen.github.io/rbokeh/ https://github.com/bokeh/rbokeh,TRUE,https://github.com/bokeh/rbokeh,73883,293,2021-08-02T23:12:23Z,252.160409556314
Rborist,"Scalable implementation of classification and regression forests, as described by Breiman (2001), <DOI:10.1023/A:1010933404324>.",2019-10-31,Mark Seligman,"http://www.suiji.org/arborist, https://github.com/suiji/Arborist",TRUE,https://github.com/suiji/arborist,63848,76,2021-05-06T22:15:47Z,840.1052631578947
rbw,"Residual balancing is a robust method of constructing weights for
  marginal structural models, which can be used to estimate marginal effects of
  time-varying treatments and controlled direct/mediator effects in causal mediation
  analysis (Zhou and Wodtke 2020 <doi:10.1017/pan.2020.2>). This
  package provides two main functions, rbwPanel() and rbwMed(), that produce
  residual balancing weights for analyzing time-varying treatments and
  causal mediation respectively.",2020-04-15,Xiang Zhou,http://github.com/xiangzhou09/rbw,TRUE,https://github.com/xiangzhou09/rbw,6043,6,2021-07-03T15:51:24Z,1007.1666666666666
RCarb,"Translation of the 'MATLAB' program 'Carb' (Nathan and Mauz 2008 <DOI:10.1016/j.radmeas.2007.12.012>; Mauz and Hoffmann 2014) for dose rate modelling for carbonate-rich samples in the context of trapped charged dating (e.g., luminescence dating) applications. ",2020-09-15,Sebastian Kreutzer,https://r-lum.github.io/RCarb/,TRUE,https://github.com/r-lum/rcarb,21468,0,2021-01-03T12:21:04Z,NA
rcarbon,"Enables the calibration and analysis of radiocarbon dates, often but not exclusively for the purposes of archaeological research. It includes functions not only for basic calibration, uncalibration, and plotting of one or more dates, but also a statistical framework for building demographic and related longitudinal inferences from aggregate radiocarbon date lists, including: Monte-Carlo simulation test (Timpson et al 2014 <doi:10.1016/j.jas.2014.08.011>), random mark permutation test (Crema et al 2016 <doi:10.1371/journal.pone.0154809>) and spatial permutation tests (Crema, Bevan, and Shennan 2017 <doi:10.1016/j.jas.2017.09.007>).  ",2021-03-15,Andrew Bevan,https://github.com/ahb108/rcarbon/,TRUE,https://github.com/ahb108/rcarbon,23431,28,2021-08-31T08:57:00Z,836.8214285714286
Rcatch22,"Calculate 22 summary statistics coded in C on time-series vectors to enable 
    pattern detection, classification, and regression applications in the 
    feature space as proposed by Lubba et al. (2019) <doi:10.1007/s10618-019-00647-x>.",2021-05-31,Trent Henderson,NA,TRUE,https://github.com/hendersontrent/rcatch22,1594,8,2021-06-01T08:05:08Z,199.25
rchallenge,"A simple data science challenge system using R Markdown and 'Dropbox' <https://www.dropbox.com/>.
    It requires no network configuration, does not depend on external platforms
    like e.g. 'Kaggle' <https://www.kaggle.com/> and can be easily installed on a personal computer.",2021-03-09,Adrien Todeschini,"https://adrien.tspace.fr/rchallenge/,
https://github.com/adrtod/rchallenge",TRUE,https://github.com/adrtod/rchallenge,17083,6,2021-03-19T08:56:36Z,2847.1666666666665
rcheology,Provides a dataset of functions in all base packages of R versions 1.0.1 onwards.,2021-08-12,David Hugh-Jones,https://github.com/hughjonesd/rcheology,TRUE,https://github.com/hughjonesd/rcheology,22550,29,2021-08-12T10:30:20Z,777.5862068965517
RChest,"Provides algorithms to locate multiple
    distributional change-points in piecewise stationary time series. The
    algorithms are provably consistent, even in the presence of long-range
    dependencies. Knowledge of the number of change-points is not
    required. The code is written in Go and interfaced with R.",2021-02-13,Lukas Zierahn,https://github.com/azalk/GoChest,TRUE,https://github.com/azalk/gochest,2379,1,2021-08-04T14:18:16Z,2379
rcites,A programmatic interface to the Species+ <https://speciesplus.net/> database via the Species+/CITES Checklist API <https://api.speciesplus.net/>.,2020-07-01,Jonas Geschke,"https://docs.ropensci.org/rcites/,
https://github.com/ropensci/rcites",TRUE,https://github.com/ropensci/rcites,13245,11,2020-12-04T16:04:25Z,1204.090909090909
rcitoid,"Client for 'Citoid' (<https://www.mediawiki.org/wiki/Citoid>),
    an API for getting citations for various scholarly work identifiers
    found on 'Wikipedia'.",2019-02-12,Scott Chamberlain,https://github.com/ropenscilabs/rcitoid,TRUE,https://github.com/ropenscilabs/rcitoid,10278,2,2020-11-26T05:40:40Z,5139
RClickhouse,"'Yandex Clickhouse' (<https://clickhouse.yandex/>) is a high-performance relational column-store database to enable
    big data exploration and 'analytics' scaling to petabytes of data. Methods are
    provided that enable working with 'Yandex Clickhouse' databases via
    'DBI' methods and using 'dplyr'/'dbplyr' idioms.",2020-03-06,Christian Hotz-Behofsits,https://github.com/IMSMWU/RClickhouse,TRUE,https://github.com/imsmwu/rclickhouse,19748,64,2020-11-08T17:37:12Z,308.5625
RClimacell,"'Climacell' is a weather platform that provides hyper-local forecasts and weather 
    data. This package enables the user to query the core layers of the 
    time line interface of the 'Climacell' v4 API <https://www.climacell.co/weather-api/>. 
    This package requires a valid API key. See vignettes for instructions on use.",2021-03-23,Nikhil Agarwal,https://nikdata.github.io/RClimacell/,TRUE,https://github.com/nikdata/rclimacell,2779,0,2021-03-24T19:22:06Z,NA
rclipboard,"Leverages the functionality of 'clipboard.js', a JavaScript library
    for HMTL5-based copy to clipboard from web pages (see <https://clipboardjs.com>
    for more information), and provides a reactive copy-to-clipboard UI button 
    component, called 'rclipButton', for 'shiny' R applications.",2021-01-13,Sebastien Bihorel,https://github.com/sbihorel/rclipboard/,TRUE,https://github.com/sbihorel/rclipboard,38015,34,2021-01-13T12:17:20Z,1118.0882352941176
RClone,"R version of 'GenClone' (a computer program to analyse genotypic data, test for clonality and describe spatial clonal organization, Arnaud-Haond & Belkhir 2007, <https://wwz.ifremer.fr/clonix/content/download/68205/903914/file/GenClone2.0.setup.zip>), this package allows clone handling as 'GenClone' does, plus the possibility to work with several populations, MultiLocus Lineages (MLL) custom definition and use, and p-value calculation for psex statistic (probability of originating from distinct sexual events) and psex_Fis statistic (taking account of Hardy-Weinberg equilibrium departure) as 'MLGsim'/'MLGsim2' (a program for detecting clones using a simulation approach, Stenberg et al. 2003).",2021-05-15,Diane Bailleul,https://github.com/dbailleul/RClone,TRUE,https://github.com/dbailleul/rclone,24053,3,2021-05-13T16:36:29Z,8017.666666666667
RcmdrPlugin.temis,"An 'R Commander' plug-in providing an integrated solution to perform
    a series of text mining tasks such as importing and cleaning a corpus, and
    analyses like terms and documents counts, vocabulary tables, terms
    co-occurrences and documents similarity measures, time series analysis,
    correspondence analysis and hierarchical clustering. Corpora can be imported
    from spreadsheet-like files, directories of raw text files, 'Twitter' queries,
    as well as from 'Dow Jones Factiva', 'LexisNexis', 'Europresse' and 'Alceste' files.",2018-06-22,Milan Bouchet-Valat,https://github.com/nalimilan/R.TeMiS,TRUE,https://github.com/nalimilan/r.temis,36892,20,2021-05-12T18:07:40Z,1844.6
rco,"Automatically apply different strategies to optimize R code. 
    'rco' functions take R code as input, and returns R code as output.",2021-04-17,Juan Cruz Rodriguez,https://jcrodriguez1989.github.io/rco/,TRUE,https://github.com/jcrodriguez1989/rco,6416,61,2021-07-14T12:28:16Z,105.18032786885246
rcol,"Client for the Catalogue of Life ('CoL')
    (<https://www.catalogueoflife.org/>); based on the new
    'CoL' service, not the old one. Catalogue of Life is a database of
    taxonomic names. Includes functions for each
    of the API methods, including searching for names, and more.",2021-07-02,Scott Chamberlain,"https://docs.ropensci.org/rcol/ (docs),
https://github.com/ropensci/rcol",TRUE,https://github.com/ropensci/rcol,2787,2,2021-07-02T18:53:05Z,1393.5
Rcompadre,Utility functions for interacting with the 'COMPADRE' and 'COMADRE' databases of matrix population models. Described in Jones et al. (2021) <doi:10.1101/2021.04.26.441330>.,2021-04-30,Roberto Salguero-Gomez,https://github.com/jonesor/Rcompadre,TRUE,https://github.com/jonesor/rcompadre,2185,9,2021-08-18T15:07:27Z,242.77777777777777
rcompendium,"Makes easier the creation of R package or research compendium 
    (i.e. a predefined files/folders structure) so that users can focus on the 
    code/analysis instead of wasting time organizing files. A full 
    ready-to-work structure is set up with some additional features: version 
    control, remote repository creation, CI/CD configuration (check package 
    integrity under several OS, test code with 'testthat', and build and deploy 
    website using 'pkgdown'). This package heavily relies on the R packages 
    'devtools' and 'usethis' and follows recommendations made by Wickham H. 
    (2015) <ISBN:9781491910597> and Marwick B. et al. (2018) 
    <doi:10.7287/peerj.preprints.3192v2>.",2021-03-24,Nicolas Casajus,https://github.com/FRBCesab/rcompendium,TRUE,https://github.com/frbcesab/rcompendium,1778,14,2021-06-17T06:33:10Z,127
rcoreoa,"Client for the CORE API (<https://core.ac.uk/docs/>).
    CORE (<https://core.ac.uk>) aggregates open access research
    outputs from repositories and journals worldwide and make them
    available to the public.",2020-07-07,Scott Chamberlain,"https://docs.ropensci.org/rcoreoa,
https://github.com/ropensci/rcoreoa",TRUE,https://github.com/ropensci/rcoreoa,16142,11,2020-12-10T22:18:33Z,1467.4545454545455
rcosmo,"Handling and Analysing Spherical, 
      HEALPix and Cosmic Microwave Background data on a HEALPix grid.",2020-02-14,Daniel Fryer,https://github.com/frycast/rcosmo,TRUE,https://github.com/frycast/rcosmo,14988,10,2021-07-13T08:02:59Z,1498.8
Rcpp,"The 'Rcpp' package provides R functions as well as C++ classes which
 offer a seamless integration of R and C++. Many R data types and objects can be
 mapped back and forth to C++ equivalents which facilitates both writing of new
 code as well as easier integration of third-party libraries. Documentation
 about 'Rcpp' is provided by several vignettes included in this package, via the
 'Rcpp Gallery' site at <https://gallery.rcpp.org>, the paper by Eddelbuettel and
 Francois (2011, <doi:10.18637/jss.v040.i08>), the book by Eddelbuettel (2013,
 <doi:10.1007/978-1-4614-6868-4>) and the paper by Eddelbuettel and Balamuta (2018,
 <doi:10.1080/00031305.2017.1375990>); see 'citation(""Rcpp"")' for details.",2021-07-07,Dirk Eddelbuettel,"http://www.rcpp.org, https://dirk.eddelbuettel.com/code/rcpp.html,
https://github.com/RcppCore/Rcpp",TRUE,https://github.com/rcppcore/rcpp,33774143,601,2021-08-06T00:25:48Z,56196.57737104825
RcppAlgos,"Provides optimized functions and flexible combinatorial iterators
    implemented in C++ with 'Rcpp' for solving problems in combinatorics and
    computational mathematics. Utilizes parallel programming via 'RcppThread'
    for maximal performance. Also makes use of the RMatrix class from the
    'RcppParallel' library. There are combination/permutation functions with
    constraint parameters that allow for generation of all results of a vector
    meeting specific criteria (e.g. generating integer partitions/compositions
    or finding all combinations such that the sum is between two bounds).
    Capable of generating specific combinations/permutations (e.g. retrieve
    only the nth lexicographical result) which sets up nicely for
    parallelization as well as random sampling. Gmp support permits exploration
    where the total number of results is large (e.g. comboSample(10000, 500,
    n = 4)). Additionally, there are several high performance number theoretic
    functions that are useful for problems common in computational mathematics.
    Some of these functions make use of the fast integer division library
    'libdivide'. The primeSieve function is based on the segmented sieve of
    Eratosthenes implementation by Kim Walisch. It is also efficient for large
    numbers by using the cache friendly improvements originally developed by
    Tomás Oliveira. Finally, there is a prime counting function that implements
    Legendre's formula based on the work of Kim Walisch.",2021-05-30,Joseph Wood,"https://github.com/jwood000/RcppAlgos, https://gmplib.org/,
https://github.com/kimwalisch/primesieve, http://libdivide.com,
https://github.com/kimwalisch/primecount,
http://ridiculousfish.com/,
http://sweet.ua.pt/tos/software/prime_sieve.html",TRUE,https://github.com/jwood000/rcppalgos,43611,30,2021-06-05T04:46:31Z,1453.7
RcppAnnoy,"'Annoy' is a small C++ library for Approximate Nearest Neighbors 
 written for efficient memory usage as well an ability to load from / save to
 disk. This package provides an R interface by relying on the 'Rcpp' package,
 exposing the same interface as the original Python wrapper to 'Annoy'. See
 <https://github.com/spotify/annoy> for more on 'Annoy'. 'Annoy' is released
 under Version 2.0 of the Apache License. Also included is a small Windows
 port of 'mmap' which is released under the MIT license.",2021-07-30,Dirk Eddelbuettel,"https://github.com/eddelbuettel/rcppannoy,
https://dirk.eddelbuettel.com/code/rcpp.annoy.html",TRUE,https://github.com/eddelbuettel/rcppannoy,398686,58,2021-07-30T18:06:29Z,6873.896551724138
RcppAPT,"The 'APT Package Management System' provides Debian and
 Debian-derived Linux systems with a powerful system to resolve package
 dependencies. This package offers access directly from R.  This can
 only work on a system with a suitable 'libapt-pkg-dev' installation
 so functionality is curtailed if such a library is not found.",2021-04-16,Dirk Eddelbuettel,"https://github.com/eddelbuettel/rcppapt,
https://dirk.eddelbuettel.com/code/rcpp.apt.html",TRUE,https://github.com/eddelbuettel/rcppapt,14177,7,2021-08-08T16:07:40Z,2025.2857142857142
RcppArmadillo,"'Armadillo' is a templated C++ linear algebra library (by Conrad
 Sanderson) that aims towards a good balance between speed and ease of
 use. Integer, floating point and complex numbers are supported, as
 well as a subset of trigonometric and statistics functions. Various
 matrix decompositions are provided through optional integration with
 LAPACK and ATLAS libraries.  The 'RcppArmadillo' package includes the
 header files from the templated 'Armadillo' library. Thus users do
 not need to install 'Armadillo' itself in order to use
 'RcppArmadillo'. From release 7.800.0 on, 'Armadillo' is licensed
 under Apache License 2; previous releases were under licensed as MPL
 2.0 from version 3.800.0 onwards and LGPL-3 prior to that;
 'RcppArmadillo' (the 'Rcpp' bindings/bridge to Armadillo) is licensed
 under the GNU GPL version 2 or later, as is the rest of 'Rcpp'.
 Armadillo requires a C++11 compiler.",2021-07-16,Dirk Eddelbuettel,"https://github.com/RcppCore/RcppArmadillo,
https://dirk.eddelbuettel.com/code/rcpp.armadillo.html",TRUE,https://github.com/rcppcore/rcpparmadillo,14925970,135,2021-08-08T16:08:28Z,110562.74074074074
RcppBDT,"Access to Boost Date_Time functionality for dates,
 durations (both for days and date time objects), time zones, and 
 posix time ('ptime') is provided by using 'Rcpp modules'. The 
 posix time implementation can support high-resolution of up to 
 nano-second  precision by using 96 bits (instead of R's 64)
 to present a 'ptime' object (but this needs recompilation with
 a #define set).",2021-08-15,Dirk Eddelbuettel and Romain Francois,"https://github.com/eddelbuettel/rcppbdt,
https://dirk.eddelbuettel.com/code/rcpp.bdt.html",TRUE,https://github.com/eddelbuettel/rcppbdt,37498,17,2021-08-15T21:16:54Z,2205.764705882353
RcppBigIntAlgos,"Features the multiple polynomial quadratic sieve (MPQS) algorithm
    for factoring large integers and a vectorized factoring function that
    returns the complete factorization of an integer. The MPQS is based off of
    the seminal work of Carl Pomerance (1984) <doi:10.1007/3-540-39757-4_17>
    along with the modification of multiple polynomials introduced by Peter
    Montgomery and J. Davis as outlined by Robert D. Silverman (1987)
    <doi:10.1090/S0025-5718-1987-0866119-8>. Utilizes the C library
    GMP (GNU Multiple Precision Arithmetic) and 'RcppThread' for factoring
    integers in parallel. For smaller integers, a simple Elliptic
    Curve algorithm is attempted followed by a constrained version of 
    Pollard's rho algorithm. The Pollard's rho algorithm is the same algorithm
    used by the factorize function in the 'gmp' package.",2021-01-08,Joseph Wood,"https://github.com/jwood000/RcppBigIntAlgos, https://gmplib.org/,
http://mathworld.wolfram.com/QuadraticSieve.html,
http://micsymposium.org/mics_2011_proceedings/mics2011_submission_28.pdf,
https://www.math.colostate.edu/~hulpke/lectures/m400c/quadsievex.pdf,
https://blogs.msdn.microsoft.com/devdev/2006/06/19/factoring-large-numbers-with-quadratic-sieve/",TRUE,https://github.com/jwood000/rcppbigintalgos,7966,7,2021-01-08T12:49:01Z,1138
RcppCCTZ,"'Rcpp' Access to the 'CCTZ' timezone library is provided. 'CCTZ' is
 a C++ library for translating between absolute and civil times using the rules
 of a time zone. The 'CCTZ' source code, released under the Apache 2.0 License,
 is included in this package. See <https://github.com/google/cctz> for more
 details.",2020-08-30,Dirk Eddelbuettel,https://github.com/eddelbuettel/rcppcctz,TRUE,https://github.com/eddelbuettel/rcppcctz,332301,20,2021-08-13T23:13:08Z,16615.05
RcppClassic,"The 'RcppClassic' package provides a deprecated C++ library which
 facilitates the integration of R and C++. New projects should use the new 'Rcpp'
 'API' in the 'Rcpp' package.",2019-12-09,Dirk Eddelbuettel and Romain Francois,NA,TRUE,https://github.com/eddelbuettel/rcppclassic,43176,1,2021-04-21T23:01:30Z,43176
RcppCNPy,"The 'cnpy' library written by Carl Rogers provides read and write
 facilities for files created with (or for) the 'NumPy' extension for 'Python'.
 Vectors and matrices of numeric types can be read or written to and from
 files as well as compressed files. Support for integer files is available if
 the package has been built with -std=c++11 which should be the default on
 all platforms since the release of R 3.3.0.",2018-07-29,Dirk Eddelbuettel and Wush Wu,http://dirk.eddelbuettel.com/code/rcpp.cnpy.html,TRUE,https://github.com/eddelbuettel/rcppcnpy,28032,21,2021-08-14T23:16:20Z,1334.857142857143
RcppCWB,"'Rcpp' Bindings for the C code of the 'Corpus Workbench' ('CWB'), an indexing and query 
  engine to efficiently analyze large corpora (<http://cwb.sourceforge.net>). 'RcppCWB' is licensed
  under the GNU GPL-3, in line with the GPL-3 license of the 'CWB' (<https://www.r-project.org/Licenses/GPL-3>).
  The 'CWB' relies on 'pcre' (BSD license, see <http://www.pcre.org/licence.txt>)
  and 'GLib' (LGPL license, see <https://www.gnu.org/licenses/lgpl-3.0.en.html>).
  See the file LICENSE.note for further information. The package includes modified code of the
  'rcqp' package (GPL-2, see <https://cran.r-project.org/package=rcqp>). The original work of the authors
  of the 'rcqp' package is acknowledged with great respect, and they are listed as authors of this
  package. To achieve cross-platform portability (including Windows), using 'Rcpp' for wrapper code
  is the approach used by 'RcppCWB'.",2021-07-20,Andreas Blaette,https://github.com/PolMine/RcppCWB,TRUE,https://github.com/polmine/rcppcwb,21868,1,2021-07-15T19:52:52Z,21868
RcppDate,"A header-only C++ library is provided with support
 for dates, time zones, ISO weeks, Julian dates, and Islamic dates.
 'date' offers extensive date and time functionality for the C++11,
 C++14 and C++17 standards and was written by Howard Hinnant and released 
 under the MIT license. A slightly modified version has been accepted
 (along with 'tz.h') as part of C++20. This package regroups all
 header files from the upstream repository by Howard Hinnant so that
 other R packages can use them in their C++ code. At present, few of
 the types have explicit 'Rcpp' wrappers though these may be added as
 needed. ",2021-05-19,Dirk Eddelbuettel,"https://github.com/eddelbuettel/rcppdate,
https://dirk.eddelbuettel.com/code/rcpp.date.html",TRUE,https://github.com/eddelbuettel/rcppdate,89286,11,2021-08-14T23:17:25Z,8116.909090909091
RcppDynProg,Dynamic Programming implemented in 'Rcpp'.  Includes example partition and out of sample fitting applications.  Also supplies additional custom coders for the 'vtreat' package.,2020-12-15,John Mount,"https://github.com/WinVector/RcppDynProg/,
https://winvector.github.io/RcppDynProg/",TRUE,https://github.com/winvector/rcppdynprog,14595,13,2020-12-15T17:56:06Z,1122.6923076923076
RcppEigen,"R and 'Eigen' integration using 'Rcpp'.
 'Eigen' is a C++ template library for linear algebra: matrices, vectors,
 numerical solvers and related algorithms.  It supports dense and sparse
 matrices on integer, floating point and complex numbers, decompositions of
 such matrices, and solutions of linear systems. Its performance on many
 algorithms is comparable with some of the best implementations based on
 'Lapack' and level-3 'BLAS'. The 'RcppEigen' package includes the header
 files from the 'Eigen' C++ template library (currently version 3.3.4). Thus
 users do not need to install 'Eigen' itself in order to use 'RcppEigen'.
 Since version 3.1.1, 'Eigen' is licensed under the Mozilla Public License
 (version 2); earlier version were licensed under the GNU LGPL version 3 or
 later. 'RcppEigen' (the 'Rcpp' bindings/bridge to 'Eigen') is licensed under
 the GNU GPL version 2 or later, as is the rest of 'Rcpp'.",2020-12-17,Douglas Bates,http://dirk.eddelbuettel.com/code/rcpp.eigen.html,TRUE,https://github.com/rcppcore/rcppeigen,10457190,68,2021-08-18T22:07:35Z,153782.20588235295
RcppEnsmallen,"'Ensmallen' is a templated C++ mathematical optimization library 
 (by the 'MLPACK' team) that provides a simple set of abstractions for writing an
 objective function to optimize. Provided within are various standard and
 cutting-edge optimizers that include full-batch gradient descent techniques, 
 small-batch techniques, gradient-free optimizers, and constrained optimization.
 The 'RcppEnsmallen' package includes the header files from the 'Ensmallen' library
 and pairs the appropriate header files from 'armadillo' through the 
 'RcppArmadillo' package. Therefore, users do not need to install 'Ensmallen' nor
 'Armadillo' to use 'RcppEnsmallen'. Note that 'Ensmallen' is licensed under 
 3-Clause BSD, 'Armadillo' starting from 7.800.0 is licensed under Apache License 2,
 'RcppArmadillo' (the 'Rcpp' bindings/bridge to 'Armadillo') is licensed under 
 the GNU GPL version 2 or later. Thus, 'RcppEnsmallen' is also licensed under
 similar terms. Note that 'Ensmallen' requires a compiler that supports 
 'C++11' and 'Armadillo' 8.400 or later.",2021-07-06,James Joseph Balamuta,"https://github.com/coatless/rcppensmallen,
https://github.com/mlpack/ensmallen, http://ensmallen.org/",TRUE,https://github.com/coatless/rcppensmallen,23443,28,2021-07-24T17:41:10Z,837.25
RcppExamples,"Examples for Seamless R and C++ integration
 The 'Rcpp' package contains a C++ library that facilitates the integration of
 R and C++ in various ways. This package provides some usage examples.
 Note that the documentation in this package currently does not cover all the
 features in the package. The site <http://gallery.rcpp.org> regroups a large
 number of examples for 'Rcpp'.",2019-08-24,Dirk Eddelbuettel and Romain Francois,http://dirk.eddelbuettel.com/code/rcpp.examples.html,TRUE,https://github.com/eddelbuettel/rcppexamples,19950,37,2021-05-09T21:17:31Z,539.1891891891892
RcppFarmHash,"The Google 'FarmHash' family of hash functions is used by
 the Google 'BigQuery' data warehouse via the 'FARM_FINGERPRINT'
 function. This package permits to calculate these hash digest
 fingerprints directly from R, and uses the included 'FarmHash'
 files written by G. Pike and copyrighted by Google, Inc.",2021-08-02,Dirk Eddelbuettel,"https://github.com/eddelbuettel/rcppfarmhash/,
https://dirk.eddelbuettel.com/code/rcpp.farmhash.html",TRUE,https://github.com/eddelbuettel/rcppfarmhash,643,1,2021-08-02T13:00:32Z,643
RcppFastFloat,"Converting ascii text into (floating-point) numeric values is a
 very common problem. The 'fast_float' header-only C++ library by Daniel Lemire
 does it very well and very fast at up to or over to 1 gigabyte per second as
 described in more detail in <arXiv:2101.11408>. 'fast_float' is licensed under
 the Apache 2.0 license and provided here for use by other R packages via a simple
 'LinkingTo:' statement.",2021-08-21,Dirk Eddelbuettel,"https://github.com/eddelbuettel/rcppfastfloat/,
https://dirk.eddelbuettel.com/code/rcpp.fastfloat.html",TRUE,https://github.com/eddelbuettel/rcppfastfloat,2952,19,2021-08-21T13:15:01Z,155.3684210526316
RcppGetconf,"The 'getconf' command-line tool provided by 'libc' allows
 querying of a large number of system variables. This package provides
 similar functionality.",2018-11-16,Dirk Eddelbuettel,http://dirk.eddelbuettel.com/code/rcpp.getconf.html,TRUE,https://github.com/eddelbuettel/rcppgetconf,9438,1,2021-05-20T21:33:07Z,9438
RcppGSL,"'Rcpp' integration for 'GNU GSL' vectors and matrices
 The 'GNU Scientific Library' (or 'GSL') is a collection of numerical routines for
 scientific computing. It is particularly useful for C and C++ programs as it
 provides a standard C interface to a wide range of mathematical routines. There
 are over 1000 functions in total with an extensive test suite. The 'RcppGSL'
 package provides an easy-to-use interface between 'GSL' data structures and
 R using concepts from 'Rcpp' which is itself a package that eases the
 interfaces between R and C++. This package also serves as a prime example of
 how to build a package that uses 'Rcpp' to connect to another third-party
 library. The 'autoconf' script, 'inline' plugin and example package can all
 be used as a stanza to  write a similar package against another library.",2021-06-23,Dirk Eddelbuettel and Romain Francois,"https://github.com/eddelbuettel/rcppgsl,
https://dirk.eddelbuettel.com/code/rcpp.gsl.html",TRUE,https://github.com/eddelbuettel/rcppgsl,219299,29,2021-06-24T16:43:20Z,7562.0344827586205
RcppHNSW,"'Hnswlib' is a C++ library for Approximate Nearest Neighbors. This 
 package provides a minimal R interface by relying on the 'Rcpp' package. See 
 <https://github.com/nmslib/hnswlib> for more on 'hnswlib'. 'hnswlib' is 
 released under Version 2.0 of the Apache License.",2020-09-06,James Melville,https://github.com/jlmelville/rcpphnsw,TRUE,https://github.com/jlmelville/rcpphnsw,135599,22,2021-03-13T19:07:08Z,6163.590909090909
RcppMeCab,"R package based on 'Rcpp' for 'MeCab': Yet Another Part-of-Speech and Morphological Analyzer. 
	The purpose of this package is providing a seamless developing and analyzing environment for CJK texts.
	This package utilizes parallel programming for providing highly efficient text preprocessing 'posParallel()' function.
	For installation, please refer to README.md file.",2018-07-04,Junhewk Kim,NA,TRUE,https://github.com/junhewk/rcppmecab,11025,23,2021-02-15T07:55:37Z,479.3478260869565
RcppML,"High-performance machine learning algorithms, including  
    matrix factorization and divisive clustering for large sparse and 
    dense matrices.",2021-09-02,Zachary DeBruine,https://github.com/zdebruine/RcppML,TRUE,https://github.com/zdebruine/rcppml,496,4,2021-09-02T17:36:36Z,124
RcppMsgPack,"'MsgPack' header files are provided for use by R packages, along 
 with the ability to access, create and alter 'MsgPack' objects directly from R.
 'MsgPack' is an efficient binary serialization format. It lets you exchange
 data among multiple languages like 'JSON' but it is faster and smaller.
 Small integers are encoded into a single byte, and typical short strings
 require only one extra byte in addition to the strings themselves. This
 package provides headers from the 'msgpack-c' implementation for C and
 C++(11) for use by R, particularly 'Rcpp'. The included 'msgpack-c' headers
 are licensed under the Boost Software License (Version 1.0); the code added
 by this package as well the R integration are licensed under the GPL (>= 2).
 See the files 'COPYRIGHTS' and 'AUTHORS' for a full list of  copyright holders
 and contributors to 'msgpack-c'.  ",2018-11-18,Travers Ching and Dirk Eddelbuettel; the authors and contributors of MsgPack,NA,TRUE,https://github.com/eddelbuettel/rcppmsgpack,17090,15,2021-05-20T21:39:25Z,1139.3333333333333
RcppParallel,"High level functions for parallel programming with 'Rcpp'.
    For example, the 'parallelFor()' function can be used to convert the work of
    a standard serial ""for"" loop into a parallel one and the 'parallelReduce()'
    function can be used for accumulating aggregate or other values.",2021-05-04,Kevin Ushey,"https://rcppcore.github.io/RcppParallel/,
https://github.com/RcppCore/RcppParallel",TRUE,https://github.com/rcppcore/rcppparallel,2937239,146,2021-08-16T17:09:26Z,20118.075342465752
RcppRedis,"Connection to the 'Redis' key/value store using the
 C-language client library 'hiredis' (included as a fallback) with
 'MsgPack' encoding provided via 'RcppMsgPack' headers.",2021-06-26,Dirk Eddelbuettel,"https://github.com/eddelbuettel/rcppredis,
https://dirk.eddelbuettel.com/code/rcpp.redis.html",TRUE,https://github.com/eddelbuettel/rcppredis,24230,44,2021-06-26T21:40:47Z,550.6818181818181
RcppSimdJson,"The 'JSON' format is ubiquitous for data interchange, and the
 'simdjson' library written by Daniel Lemire (and many contributors) provides 
 a high-performance parser for these files which by relying on parallel 'SIMD'
 instruction manages to parse these files as faster than disk speed. See the
 <arXiv:1902.08318> paper for more details about 'simdjson'.  This package 
 parses 'JSON' from string, file, or remote URLs under a variety of settings.",2021-02-24,Dirk Eddelbuettel,https://github.com/eddelbuettel/rcppsimdjson/,TRUE,https://github.com/eddelbuettel/rcppsimdjson,30610,88,2021-08-16T17:07:56Z,347.84090909090907
RcppSMC,"R access to the Sequential Monte Carlo Template Classes
 by Johansen <doi:10.18637/jss.v030.i06> is provided. At present, four
 additional examples have been added, and the first example from the JSS
 paper has been extended. Further integration and extensions are planned.",2021-09-02,Dirk Eddelbuettel,"https://github.com/rcppsmc/rcppsmc,
https://dirk.eddelbuettel.com/code/rcpp.smc.html",TRUE,https://github.com/rcppsmc/rcppsmc,20041,21,2021-09-02T22:00:44Z,954.3333333333334
RcppSpdlog,"The mature and widely-used C++ logging library 'spdlog' by Gabi Melman provides
 many desirable features. This package bundles these header files for easy use by R packages
 via a simple 'LinkingTo:' inclusion.",2021-07-22,Dirk Eddelbuettel,"https://github.com/eddelbuettel/rcppspdlog,
https://dirk.eddelbuettel.com/code/rcpp.spdlog.html",TRUE,https://github.com/eddelbuettel/rcppspdlog,11251,8,2021-08-12T21:46:33Z,1406.375
RcppStreams,"The 'Streamulus' (template, header-only) library by
 Irit Katriel (at <https://github.com/iritkatriel/streamulus>)
 provides a very powerful yet convenient framework for stream
 processing. This package connects 'Streamulus' to R by providing 
 both the header files and all examples.",2019-02-25,Dirk Eddelbuettel,http://dirk.eddelbuettel.com/code/rcpp.streams.html,TRUE,https://github.com/eddelbuettel/rcppstreams,16178,15,2021-01-12T23:35:01Z,1078.5333333333333
RcppThread,"Provides a C++11-style thread class and thread pool that can safely
    be interrupted from R. See Nagler (2021) <doi:10.18637/jss.v097.c01>.",2021-02-02,Thomas Nagler,https://github.com/tnagler/RcppThread,TRUE,https://github.com/tnagler/rcppthread,123991,38,2021-01-14T11:18:24Z,3262.9210526315787
RcppTOML,"The configuration format defined by 'TOML' (which expands to
 ""Tom's Obvious Markup Language"") specifies an excellent format
 (described at <https://toml.io/en/>) suitable for both human editing
 as well as the common uses of a machine-readable format. This package
 uses 'Rcpp' to connect the 'cpptoml' parser written by Chase Geigle
 (in C++11) to R.",2020-12-02,Dirk Eddelbuettel,http://dirk.eddelbuettel.com/code/rcpp.toml.html,TRUE,https://github.com/eddelbuettel/rcpptoml,445016,23,2021-07-21T16:17:17Z,19348.521739130436
rcrossref,"Client for various 'CrossRef' 'APIs', including 'metadata' search
    with their old and newer search 'APIs', get 'citations' in various formats
    (including 'bibtex', 'citeproc-json', 'rdf-xml', etc.), convert 'DOIs'
    to 'PMIDs', and 'vice versa', get citations for 'DOIs', and get links to
    full text of articles when available.",2020-10-02,Scott Chamberlain,"https://docs.ropensci.org/rcrossref/,
https://github.com/ropensci/rcrossref",TRUE,https://github.com/ropensci/rcrossref,50095,140,2021-08-19T07:03:04Z,357.82142857142856
RCSL,"A novel clustering algorithm and toolkit to accurately identify various cell types using single cell RNA sequencing data from a complex tissue. This algorithm considers both local similarity and global similarity among the cells to discern the subtle differences among cells of the same type as well as larger differences among cells of different types. This algorithm uses Spearman’s rank correlations of a cell’s expression vector with those of other cells to measure its global similarity, and learns neighbour representation of a cell as its local similarity. The overall similarity of a cell to other cells is a linear combination of its global similarity and local similarity. See Mei et. al. (2021) <DOI:10.1101/2021.04.12.439254> for more details.",2021-04-19,Qinglin Mei,https://github.com/QinglinMei/RCSL,TRUE,https://github.com/qinglinmei/rcsl,1151,0,2021-04-06T03:43:06Z,NA
Rcssplot,"Provides a means to style plots through cascading style sheets.
    This separates the aesthetics from the data crunching in plots and charts.",2019-12-13,Tomasz Konopka,https://github.com/tkonopka/Rcssplot,TRUE,https://github.com/tkonopka/rcssplot,14275,9,2021-01-11T19:43:04Z,1586.111111111111
RCzechia,Administrative regions and other spatial objects of the Czech Republic.,2021-09-02,Jindra Lacko,https://github.com/jlacko/RCzechia,TRUE,https://github.com/jlacko/rczechia,24267,15,2021-09-02T06:13:28Z,1617.8
Rd2roxygen,"Functions to convert Rd to 'roxygen' documentation. It can parse an
    Rd file to a list, create the 'roxygen' documentation and update the original
    R script (e.g. the one containing the definition of the function)
    accordingly. This package also provides utilities that can help developers
    build packages using 'roxygen' more easily. The 'formatR' package can be used
    to reformat the R code in the examples sections so that the code will be
    more readable.",2020-06-24,Yihui Xie,https://github.com/yihui/Rd2roxygen,TRUE,https://github.com/yihui/rd2roxygen,47103,22,2021-03-08T03:36:06Z,2141.0454545454545
rdacca.hp,"This function conducts variation partitioning and hierarchical partitioning to calculate the unique, shared (referred as to ""common"") and individual contributions of each predictor (or matrix) towards explained variation (R-square and adjusted R-square) on canonical analysis (RDA,CCA and db-RDA), applying the algorithm of Chevan, A. and Sutherland, M. 1991 Hierarchical Partitioning.The American Statistician, 90-96 <doi:10.1080/00031305.1991.10475776>. ",2021-09-03,Jiangshan Lai,https://github.com/laijiangshan/rdacca.hp,TRUE,https://github.com/laijiangshan/rdacca.hp,5278,9,2021-09-01T23:48:06Z,586.4444444444445
rdatacite,"Client for the web service methods provided
    by 'DataCite' (<https://www.datacite.org/>), including functions to interface with
    their 'RESTful' search API. The API is backed by 'Elasticsearch', allowing
    expressive queries, including faceting.",2020-03-04,Scott Chamberlain,"https://docs.ropensci.org/rdatacite,
https://github.com/ropensci/rdatacite",TRUE,https://github.com/ropensci/rdatacite,17920,20,2021-01-08T19:34:28Z,896
rdataretriever,"Provides an R interface to the Data Retriever
    <https://retriever.readthedocs.io/en/latest/> via the Data Retriever's
    command line interface. The Data Retriever automates the
    tasks of finding, downloading, and cleaning public datasets,
    and then stores them in a local database.",2020-09-17,Daniel McGlinn,"https://docs.ropensci.org/rdataretriever/ (website),
https://github.com/ropensci/rdataretriever/",TRUE,https://github.com/ropensci/rdataretriever,16857,37,2021-02-11T11:16:07Z,455.5945945945946
rddtools,"Set of functions for Regression Discontinuity Design ('RDD'), for
    data visualisation, estimation and testing.",2021-05-16,Matthieu Stigler,https://qua.st/rddtools/,TRUE,https://github.com/bquast/rddtools,23660,7,2021-05-09T13:46:09Z,3380
rdhs,"Provides a client for (1) querying the DHS API for survey indicators
  and metadata (<https://api.dhsprogram.com/#/index.html>), (2) identifying surveys
  and datasets for analysis, (3) downloading survey datasets from the DHS website,
  (4) loading datasets and associate metadata into R, and (5) extracting variables
  and combining datasets for pooled analysis.",2021-07-19,OJ Watson,https://docs.ropensci.org/rdhs/,TRUE,https://github.com/ropensci/rdhs,16381,26,2021-06-01T21:53:57Z,630.0384615384615
RDieHarder,"The 'RDieHarder' package provides an R interface to 
 the 'DieHarder' suite of random number generators and tests that 
 was developed by Robert G. Brown and David Bauer, extending 
 earlier work by George Marsaglia and others. The 'DieHarder'
 library is included, but if a version is already installed
 it will be used instead.",2019-12-07,Dirk Eddelbuettel,https://github.com/eddelbuettel/rdieharder,TRUE,https://github.com/eddelbuettel/rdieharder,16032,5,2021-05-27T00:30:22Z,3206.4
Rdimtools,"We provide linear and nonlinear dimension reduction techniques.
	Intrinsic dimension estimation methods for exploratory analysis are also provided.
	For more details on the package, see the paper by You (2020) <arXiv:2005.11107>.",2021-06-11,Kisung You,https://kisungyou.com/Rdimtools/,TRUE,https://github.com/kisungyou/rdimtools,40522,25,2021-06-28T19:46:52Z,1620.88
rdiversity,"Provides a framework for the measurement and partitioning of
    the (similarity-sensitive) biodiversity of a metacommunity and its
    constituent subcommunities. Richard Reeve, et al. (2016) 
    <arXiv:1404.6520v3>.",2020-05-20,Sonia Mitchell,https://github.com/boydorr/rdiversity,TRUE,https://github.com/boydorr/rdiversity,16039,4,2021-03-04T16:26:25Z,4009.75
RDML,"Imports real-time thermo cycler (qPCR) data from Real-time PCR
    Data Markup Language (RDML) and transforms to the appropriate formats of
    the 'qpcR' and 'chipPCR' packages. Contains a dendrogram visualization 
    for the structure of RDML object and GUI for RDML editing.",2019-06-25,Konstantin A. Blagodatskikh,https://github.com/kablag/RDML,TRUE,https://github.com/kablag/rdml,19472,16,2020-09-17T10:11:27Z,1217
rdnb,"A wrapper for the 'Deutsche Nationalbibliothek (German National
    Library) API', available at <https://www.dnb.de>. The German National Library is
    the German central archival library, collecting, archiving, bibliographically
    classifying all German and German-language publications, foreign
    publications about Germany, translations of German works, and the works of
    German-speaking emigrants published abroad between 1933 and 1945.",2021-04-18,Christian Graul,https://github.com/chgrl/rdnb,TRUE,https://github.com/chgrl/rdnb,15550,1,2021-04-26T19:31:59Z,15550
RDP,"Pretty fast implementation of the Ramer-Douglas-Peucker algorithm for reducing the number of points on a 2D curve.
    Urs Ramer (1972), ""An iterative procedure for the polygonal approximation of plane curves"" <doi:10.1016/S0146-664X(72)80017-0>.
    David H. Douglas and Thomas K. Peucker (1973), ""Algorithms for the Reduction of the Number of Points Required to Represent a Digitized Line or its Caricature"" <doi:10.3138/FM57-6770-U75U-7727>.",2021-08-16,Robert Dahl Jacobsen,https://github.com/robertdj/RDP,TRUE,https://github.com/robertdj/rdp,1086,1,2021-08-16T18:44:37Z,1086
Rdpack,"Functions for manipulation of R documentation objects,
    including functions reprompt() and ereprompt() for updating 'Rd'
    documentation for functions, methods and classes; 'Rd' macros for
    citations and import of references from 'bibtex' files for use in
    'Rd' files and 'roxygen2' comments; 'Rd' macros for evaluating and
    inserting snippets of 'R' code and the results of its evaluation or
    creating graphics on the fly; and many functions for manipulation of
    references and Rd files.",2021-06-01,Georgi N. Boshnakov,"https://geobosh.github.io/Rdpack/ (website),
https://github.com/GeoBosh/Rdpack (devel)",TRUE,https://github.com/geobosh/rdpack,1485719,18,2021-08-29T10:06:51Z,82539.94444444444
rdryad,"Interface to the Dryad ""Solr"" API, their ""OAI-PMH"" service, and
    fetch datasets. Dryad (<https://datadryad.org/>) is a curated host of
    data underlying scientific publications.",2020-06-25,Karthik Ram,"https://docs.ropensci.org/rdryad,
https://github.com/ropensci/rdryad",TRUE,https://github.com/ropensci/rdryad,21693,25,2020-12-15T20:21:58Z,867.72
rdwd,"Handle climate data from the 'DWD' ('Deutscher Wetterdienst', see 
             <https://www.dwd.de/EN/climate_environment/cdc/cdc_node_en.html> for more information).
             Choose observational time series from meteorological stations with 'selectDWD()'.
             Find raster data from radar and interpolation according to <https://bookdown.org/brry/rdwd/raster-data.html>.
             Download (multiple) data sets with progress bars and no re-downloads through 'dataDWD()'.
             Read both tabular observational data and binary gridded datasets with 'readDWD()'.",2021-04-08,Berry Boessenkool,https://github.com/brry/rdwd,TRUE,https://github.com/brry/rdwd,28491,39,2021-08-20T20:04:36Z,730.5384615384615
re2,"Pattern matching, extraction, replacement and other string
  processing operations using Google's RE2 <https://github.com/google/re2>
  regular-expression engine. Consistent interface (similar to 'stringr').
  RE2 uses finite-automata based techniques, and offers a
  fast and safe alternative to backtracking regular-expression engines
  like those used in 'stringr', 'stringi' and other PCRE implementations.",2021-05-12,Girish Palya,https://github.com/girishji/re2,TRUE,https://github.com/girishji/re2,1409,12,2021-05-11T19:46:21Z,117.41666666666667
reactable,"Interactive data tables for R, based on the 'React Table'
    JavaScript library. Provides an HTML widget that can be used in 'R Markdown'
    documents and 'Shiny' applications, or viewed from an R console.",2020-10-04,Greg Lin,"https://glin.github.io/reactable/,
https://github.com/glin/reactable",TRUE,https://github.com/glin/reactable,135782,406,2021-08-29T22:43:21Z,334.4384236453202
reactablefmtr,"Enhance the styling of interactive reactable tables with easy-to-use
    and highly-customizable functions. Apply conditional formatting to cells with
    data bars, color scales, and icon sets. Utilize custom table themes inspired by
    popular websites and bootstrap themes. Increase the portability and reproducibility
    of reactable tables by embedding images from the web directly into cells.
    Save the final table output as a static image or interactive file
    (note this feature requires the 'webshot2' package which can 
    be downloaded from <https://github.com/rstudio/webshot2>).",2021-06-16,Kyle Cuilla,"https://kcuilla.github.io/reactablefmtr/,
https://github.com/kcuilla/reactablefmtr",TRUE,https://github.com/kcuilla/reactablefmtr,3568,83,2021-08-27T18:00:57Z,42.98795180722892
reactlog,"Building interactive web applications with R is incredibly easy
  with 'shiny'. Behind the scenes, 'shiny' builds a reactive graph that can
  quickly become intertwined and difficult to debug. 'reactlog'
  (Schloerke 2019) <doi:10.5281/zenodo.2591517> provides a visual insight into
  that black box of 'shiny' reactivity by constructing a directed dependency
  graph of the application's reactive state at any time point in a reactive
  recording.",2020-09-12,Barret Schloerke,"https://rstudio.github.io/reactlog/,
https://github.com/rstudio/reactlog,
https://community.rstudio.com/tags/reactlog",TRUE,https://github.com/rstudio/reactlog,114360,94,2020-09-14T18:17:38Z,1216.595744680851
reactR,"Make it easy to use 'React' in R with 'htmlwidget' scaffolds,
              helper dependency functions, an embedded 'Babel' 'transpiler',
              and examples.",2021-02-22,Facebook Inc,https://github.com/react-R/reactR,TRUE,https://github.com/react-r/reactr,144415,250,2021-02-22T17:39:10Z,577.66
readabs,"Downloads, imports, and tidies time series data from the 
    Australian Bureau of Statistics <https://www.abs.gov.au/>.",2021-05-24,Matt Cowgill,https://github.com/mattcowgill/readabs,TRUE,https://github.com/mattcowgill/readabs,27332,58,2021-08-29T05:12:49Z,471.2413793103448
readJDX,"Import data written in the JCAMP-DX format. This is an instrument-independent format used in the field of spectroscopy. Examples include IR, NMR, and Raman spectroscopy. See the vignette for background and supported formats.  The official JCAMP-DX site is <http://www.jcamp-dx.org/>.",2021-08-08,Bryan A. Hanson,https://github.com/bryanhanson/readJDX,TRUE,https://github.com/bryanhanson/readjdx,23290,5,2021-08-08T15:23:38Z,4658
readobj,"Wraps 'tiny_obj_loader' C++ library for reading the 'Wavefront' OBJ
    3D file format including both mesh objects and materials files. The
    resultant R objects are either structured to match the 'tiny_obj_loader'
    internal data representation or in a form directly compatible with the 'rgl'
    package.",2021-07-03,Gregory Jefferis,https://github.com/jefferis/readobj,TRUE,https://github.com/jefferis/readobj,18064,8,2021-07-02T20:01:36Z,2258
readr,"The goal of 'readr' is to provide a fast and
    friendly way to read rectangular data (like 'csv', 'tsv', and 'fwf').
    It is designed to flexibly parse many types of data found in the wild,
    while still cleanly failing when data unexpectedly changes.",2021-08-10,Jim Hester,"https://readr.tidyverse.org, https://github.com/tidyverse/readr",TRUE,https://github.com/tidyverse/readr,17206806,862,2021-08-27T13:36:24Z,19961.49187935035
readrba,"Download up-to-date data from the Reserve Bank of Australia 
    in a tidy data frame. Package includes functions to download current and 
    historical statistical tables 
    (<https://www.rba.gov.au/statistics/tables/>) and forecasts 
    (<https://www.rba.gov.au/publications/smp/forecasts-archive.html>). Data
    includes a broad range of Australian macroeconomic and financial time
    series.",2021-04-06,Matt Cowgill,https://mattcowgill.github.io/readrba/index.html,TRUE,https://github.com/mattcowgill/readrba,3201,14,2021-09-03T04:56:49Z,228.64285714285714
readsdr,"The goal of 'readsdr' is to bridge the design capabilities from
    specialised System Dynamics software with the powerful numerical tools 
    offered by 'R' libraries. The package accomplishes this goal by parsing 
    'XMILE' files ('Vensim' and 'Stella') models into 'R' objects to construct 
    networks (graph theory); 'ODE' functions for 'Stan'; and inputs to simulate
    via 'deSolve' as described in Duggan (2016) <doi:10.1007/978-3-319-34043-2>.",2021-01-08,Jair Andrade,NA,TRUE,https://github.com/jandraor/readsdr,5076,7,2021-04-29T17:40:03Z,725.1428571428571
readsparse,"Read and write labelled sparse matrices in text format as used by
    software such as 'SVMLight', 'LibSVM', 'ThunderSVM', 'LibFM', 'xLearn', 'XGBoost', 'LightGBM',
    and others. Supports labelled data for regression, classification (binary, multi-class, multi-label),
    and ranking (with 'qid' field), and can handle header metadata and comments in files.",2021-06-22,David Cortes,https://github.com/david-cortes/readsparse,TRUE,https://github.com/david-cortes/readsparse,2180,5,2021-09-02T01:05:25Z,436
readstata13,Function to read and write the 'Stata' file format.,2021-05-25,Sebastian Jeworutzki,https://github.com/sjewo/readstata13,TRUE,https://github.com/sjewo/readstata13,838440,35,2021-05-25T13:30:20Z,23955.428571428572
readtext,"Functions for importing and handling text files and formatted text
    files with additional meta-data, such including '.csv', '.tab', '.json', '.xml',
    '.html', '.pdf', '.doc', '.docx', '.rtf', '.xls', '.xlsx', and others.",2021-07-14,Kenneth Benoit,https://github.com/quanteda/readtext,TRUE,https://github.com/quanteda/readtext,199531,98,2021-07-13T16:53:18Z,2036.030612244898
readwritesqlite,"Reads and writes data frames to 'SQLite'
    databases while preserving time zones (for POSIXct columns),
    projections (for 'sfc' columns), units (for 'units' columns), levels
    (for factors and ordered factors) and classes for logical, Date and
    'hms' columns.  It also logs changes to tables and provides more
    informative error messages.",2020-07-13,Joe Thorley,https://github.com/poissonconsulting/readwritesqlite,TRUE,https://github.com/poissonconsulting/readwritesqlite,13615,34,2021-03-31T21:44:52Z,400.44117647058823
readxl,"Import excel files into R. Supports '.xls' via the
    embedded 'libxls' C library <https://github.com/libxls/libxls> and
    '.xlsx' via the embedded 'RapidXML' C++ library
    <http://rapidxml.sourceforge.net>.  Works on Windows, Mac and Linux
    without external dependencies.",2019-03-13,Hadley Wickham,"https://readxl.tidyverse.org, https://github.com/tidyverse/readxl",TRUE,https://github.com/tidyverse/readxl,15766262,610,2021-08-17T22:28:58Z,25846.331147540983
readxlsb,Import data from 'Excel' binary (.xlsb) workbooks into R.,2020-09-29,Michael Allen,https://github.com/velofrog/readxlsb,TRUE,https://github.com/velofrog/readxlsb,70883,13,2020-09-29T09:27:28Z,5452.538461538462
realtest,"
    A framework for unit testing for realistic minimalists,
    where we distinguish between expected, acceptable, current, fallback,
    ideal, or regressive behaviour. It can also be used for monitoring
    third-party software projects for changes.",2021-06-17,Marek Gagolewski,https://realtest.gagolewski.com,TRUE,https://github.com/gagolews/realtest,1326,6,2021-08-29T01:27:21Z,221
rearrr,"Arrange data by a set of methods. Use rearrangers to reorder
    data points and mutators to change their values. From basic utilities,
    to centering the greatest value, to swirling in 3-dimensional space,
    'rearrr' enables creativity when plotting and experimenting with data.",2020-11-12,Ludvig Renbo Olsen,https://github.com/ludvigolsen/rearrr,TRUE,https://github.com/ludvigolsen/rearrr,9132,9,2021-02-15T13:24:03Z,1014.6666666666666
reasonabletools,"Functions for cleaning and summarising water quality data for use in National Pollutant Discharge Elimination Service (NPDES) permit reasonable potential analyses and water quality-based effluent limitation calculations. Procedures are based on those contained in the ""Technical Support Document for Water Quality-based Toxics Control"", United States Environmental Protection Agency (1991). ",2020-11-05,Matthew Reusswig,https://github.com/mattreusswig/reasonabletools,TRUE,https://github.com/mattreusswig/reasonabletools,3003,0,2020-11-11T02:17:24Z,NA
rebird,"A programmatic client for the eBird database 
    (<https://ebird.org/home>), including functions for searching for bird 
    observations by geographic location (latitude, longitude), eBird 
    hotspots, location identifiers, by notable sightings, by region, and by 
    taxonomic name.",2021-01-27,Sebastian Pardo,"https://docs.ropensci.org/rebird/,
https://github.com/ropensci/rebird",TRUE,https://github.com/ropensci/rebird,54834,54,2021-01-30T21:04:19Z,1015.4444444444445
RECA,"Relevant Component Analysis (RCA) tries to find a linear
    transformation of the feature space such that the effect of irrelevant
    variability is reduced in the transformed space.",2019-05-17,Nan Xiao,"https://nanx.me/RECA/, https://github.com/nanxstats/RECA",TRUE,https://github.com/nanxstats/reca,16441,7,2021-03-04T02:31:59Z,2348.714285714286
recipes,"An extensible framework to create and preprocess 
    design matrices. Recipes consist of one or more data manipulation 
    and analysis ""steps"". Statistical parameters for the steps can 
    be estimated from an initial data set and then applied to 
    other data sets. The resulting design matrices can then be used 
    as inputs into statistical or machine learning models. ",2021-04-16,Max Kuhn,"https://github.com/tidymodels/recipes,
https://recipes.tidymodels.org",TRUE,https://github.com/tidymodels/recipes,4701743,401,2021-08-27T14:08:16Z,11725.044887780548
recmap,"Provides an interface and a C++ implementation of the RecMap MP2
  construction heuristic (Panse C. (2018) <doi:10.18637/jss.v086.c01>).
  This algorithm draws maps according to a given statistical value
  (e.g., election results, population or epidemiological data).
  The basic idea of the RecMap algorithm is that each map region
  (e.g., different countries) is represented by a rectangle.
  The area of each rectangle represents the statistical value given
  as input (maintain zero cartographic error). Documentation about the usage
  of the recmap algorithm is provided by a vignette included in this package.",2021-05-10,Christian Panse,NA,TRUE,https://github.com/cpanse/recmap,24592,17,2021-05-27T12:31:10Z,1446.5882352941176
Recocrop,"The ecocrop model estimates environmental suitability for plants using a limiting factor approach for plant growth following Hackett (1991) <doi:10.1007/BF00045728>. The implementation in this package is fast and flexible: it allows for the use of any (environmental) predictor variable. Predictors can be either static (for example, soil pH) or dynamic (for example, monthly precipitation).",2021-05-04,Robert J. Hijmans,https://github.com/cropmodels/Recocrop/,TRUE,https://github.com/cropmodels/recocrop,1386,4,2021-05-05T01:24:38Z,346.5
recodeflow,"Contains functions to interface with variables and variable details sheets, including recoding variables and converting them to PMML.",2021-06-09,Rostyslav Vyuha,https://github.com/Big-Life-Lab/recodeflow,TRUE,https://github.com/big-life-lab/recodeflow,1011,1,2021-08-09T14:44:23Z,1011
recogito,"Annotate text with entities and the relations between them. Annotate areas of interest in images with your labels. 
    Providing 'htmlwidgets' bindings to the 'recogito' <https://github.com/recogito/recogito-js> and 'annotorious' <https://github.com/recogito/annotorious> libraries.",2021-06-17,Jan Wijffels,https://github.com/DIGI-VUB/recogito,TRUE,https://github.com/digi-vub/recogito,946,2,2021-06-16T07:58:33Z,473
recometrics,"Calculates evaluation metrics for implicit-feedback recommender systems
  that are based on low-rank matrix factorization models, given the fitted model
  matrices and data, thus allowing to compare models from a variety of libraries.
  Metrics include P@K (precision-at-k, for top-K recommendations), R@K (recall at k),
  AP@K (average precision at k), NDCG@K (normalized discounted cumulative gain at k),
  Hit@K (from which the 'Hit Rate' is calculated), RR@K (reciprocal rank at k, from
  which the 'MRR' or 'mean reciprocal rank' is calculated), ROC-AUC (area under the
  receiver-operating characteristic curve), and PR-AUC (area under the
  precision-recall curve).
  These are calculated on a per-user basis according to the ranking of items induced
  by the model, using efficient multi-threaded routines. Also provides functions
  for creating train-test splits for model fitting and evaluation.",2021-09-02,David Cortes,https://github.com/david-cortes/recometrics,TRUE,https://github.com/david-cortes/recometrics,726,10,2021-09-02T01:06:32Z,72.6
recommenderlab,"Provides a research infrastructure to test and develop
    recommender algorithms including UBCF, IBCF, FunkSVD and association
    rule-based algorithms.",2021-02-26,Michael Hahsler,https://github.com/mhahsler/recommenderlab,TRUE,https://github.com/mhahsler/recommenderlab,139000,180,2021-08-31T20:16:22Z,772.2222222222222
RecordTest,"Statistical tools based on the probabilistic properties of the 
    record occurrence in a sequence of independent and identically distributed 
    continuous random variables. In particular, tools to prepare a time series 
    as well as distribution-free trend and change-point tests and graphical 
    tools to study the record occurrence.",2021-08-08,Jorge Castillo-Mateo,https://github.com/JorgeCastilloMateo/RecordTest,TRUE,https://github.com/jorgecastillomateo/recordtest,9614,1,2021-08-21T13:10:43Z,9614
recosystem,"R wrapper of the 'libmf' library
    <https://www.csie.ntu.edu.tw/~cjlin/libmf/> for recommender
    system using matrix factorization. It is typically used to
    approximate an incomplete matrix using the product of two
    matrices in a latent space. Other common names for this task
    include ""collaborative filtering"", ""matrix completion"",
    ""matrix recovery"", etc. High performance multi-core parallel
    computing is supported in this package.",2021-05-14,Yixuan Qiu,https://github.com/yixuan/recosystem,TRUE,https://github.com/yixuan/recosystem,95228,77,2021-05-24T03:03:33Z,1236.7272727272727
reda,"Contains implementations of recurrent event data analysis routines
    including (1) survival and recurrent event data simulation from
    stochastic process point of view by the thinning method
    proposed by Lewis and Shedler (1979) <doi:10.1002/nav.3800260304>
    and the inversion method introduced in Cinlar (1975, ISBN:978-0486497976),
    (2) the mean cumulative function (MCF) estimation by the
    Nelson-Aalen estimator of the cumulative hazard rate function,
    (3) two-sample recurrent event responses comparison with the pseudo-score
    tests proposed by Lawless and Nadeau (1995) <doi:10.2307/1269617>,
    (4) gamma frailty model with spline rate function following
    Fu, et al. (2016) <doi:10.1080/10543406.2014.992524>.",2021-04-02,Wenjie Wang,https://github.com/wenjie2wang/reda,TRUE,https://github.com/wenjie2wang/reda,24424,9,2021-04-04T05:32:41Z,2713.777777777778
ReDaMoR,"The aim of this package is to manipulate relational data models in R.
   It provides functions to create, modify and export data models in json format.
   It also allows importing models created with 'MySQL Workbench' (<https://www.mysql.com/products/workbench/>).
   These functions are accessible through a graphical user interface made with 'shiny'.
   Constraints such as types, keys, uniqueness and mandatory fields are automatically checked and corrected when editing a model.
   Finally, real data can be confronted to a model to check their compatibility.",2020-12-14,Patrice Godard,https://github.com/patzaw/ReDaMoR,TRUE,https://github.com/patzaw/redamor,6173,11,2021-07-29T04:47:13Z,561.1818181818181
REDCapExporter,"Export all data, including metadata, from a REDCap (Research
    Electronic Data Capture) Project via the REDCap API
    <https://projectredcap.org/wp-content/resources/REDCapTechnicalOverview.pdf>.
    The exported (meta)data will be processed and formatted into a stand alone R
    data package which can be installed and shared between researchers.  Several
    default reports are generated as vignettes in the resulting package.",2021-02-02,Peter DeWitt,https://github.com/dewittpe/REDCapExporter,TRUE,https://github.com/dewittpe/redcapexporter,9515,1,2021-03-06T01:29:21Z,9515
REDCapR,"Encapsulates functions to streamline calls from R to the REDCap
    API.  REDCap (Research Electronic Data CAPture) is a web application for
    building and managing online surveys and databases developed at Vanderbilt
    University.  The Application Programming Interface (API) offers an avenue
    to access and modify data programmatically, improving the capacity for
    literate and reproducible programming.",2021-07-22,Will Beasley,"https://ouhscbbmc.github.io/REDCapR/,
https://github.com/OuhscBbmc/REDCapR,
https://www.ouhsc.edu/bbmc/, https://project-redcap.org",TRUE,https://github.com/ouhscbbmc/redcapr,33275,69,2021-08-25T03:39:33Z,482.2463768115942
REddyProc,"Standard and extensible Eddy-Covariance data post-processing 
  (Wutzler et al. (2018) <doi:10.5194/bg-15-5015-2018>)
  includes  
  uStar-filtering, gap-filling, and flux-partitioning.
  The Eddy-Covariance (EC)  micrometeorological technique quantifies continuous 
  exchange fluxes of gases, energy, and momentum between an ecosystem and the atmosphere.
  It is important for understanding ecosystem dynamics and upscaling exchange fluxes.
  (Aubinet et al. (2012) <doi:10.1007/978-94-007-2351-1>).
  This package inputs pre-processed (half-)hourly data and supports further processing. 
  First, a quality-check and filtering is performed based on the relationship between 
  measured flux and friction
  velocity (uStar) to discard biased data 
  (Papale et al. (2006) <doi:10.5194/bg-3-571-2006>).
  Second, gaps in the data are filled based on information from environmental conditions
  (Reichstein et al. (2005) <doi:10.1111/j.1365-2486.2005.001002.x>).
  Third, the net flux of carbon dioxide is partitioned
  into its gross fluxes in and out of the ecosystem by night-time 
  based and day-time based approaches
  (Lasslop et al. (2010) <doi:10.1111/j.1365-2486.2009.02041.x>).",2020-03-18,Thomas Wutzler,"https://www.bgc-jena.mpg.de/bgi/index.php/Services/REddyProcWeb,
https://github.com/bgctw/REddyProc",TRUE,https://github.com/bgctw/reddyproc,23287,34,2021-06-25T10:47:49Z,684.9117647058823
redist,"Enables researchers to sample redistricting plans from a pre-specified
    target distribution using Sequential Monte Carlo and Markov Chain Monte Carlo
    algorithms.  The package allows for the implementation of various constraints in
    the redistricting process such as geographic compactness and population parity
    requirements. Tools for analysis such as computation of various summary statistics
    and plotting functionality are also included. The package implements methods
    described in Fifield, Higgins, Imai and Tarr (2020) <doi:10.1080/10618600.2020.1739532>,
    Fifield, Imai, Kawahara, and Kenny (2020) <doi:10.1080/2330443X.2020.1791773>,
    and McCartan and Imai (2020) <arXiv:2008.06131>.",2021-05-15,Christopher T. Kenny,https://alarm-redist.github.io/redist/,TRUE,https://github.com/alarm-redist/redist,19800,31,2021-09-03T15:44:45Z,638.7096774193549
redlistr,"A toolbox created by members of the International Union for
    Conservation of Nature (IUCN) Red List of Ecosystems Committee for
    Scientific Standards. Primarily, it is a set of tools suitable
    for calculating the metrics required for making assessments of species and
    ecosystems against the IUCN Red List of Threatened Species and the IUCN Red
    List of Ecosystems categories and criteria. See the IUCN website for
    detailed guidelines, the criteria, publications and other information.",2019-07-11,Calvin Lee,https://github.com/red-list-ecosystem/redlistr,TRUE,https://github.com/red-list-ecosystem/redlistr,14214,21,2021-02-22T04:57:16Z,676.8571428571429
redoc,"A collection of 'HTML', 'JavaScript', 'CSS' and fonts
  assets that generate 'Redoc' documentation from an 'OpenAPI' Specification:
   <https://redoc.ly/redoc/>.",2021-02-05,Bruno Tremblay,https://github.com/meztez/redoc,TRUE,https://github.com/meztez/redoc,2954,7,2021-04-13T14:25:07Z,422
redux,"A 'hiredis' wrapper that includes support for
    transactions, pipelining, blocking subscription, serialisation of
    all keys and values, 'Redis' error handling with R errors.
    Includes an automatically generated 'R6' interface to the full
    'hiredis' 'API'.  Generated functions are faithful to the
    'hiredis' documentation while attempting to match R's argument
    semantics.  Serialisation must be explicitly done by the user, but
    both binary and text-mode serialisation is supported.",2018-05-31,Rich FitzJohn,https://github.com/richfitz/redux,TRUE,https://github.com/richfitz/redux,155301,63,2021-06-24T09:04:08Z,2465.095238095238
RefManageR,"Provides tools for importing and working with bibliographic
    references. It greatly enhances the 'bibentry' class by providing a class
    'BibEntry' which stores 'BibTeX' and 'BibLaTeX' references, supports 'UTF-8'
    encoding, and can be easily searched by any field, by date ranges, and by
    various formats for name lists (author by last names, translator by full names,
    etc.). Entries can be updated, combined, sorted, printed in a number of styles,
    and exported. 'BibTeX' and 'BibLaTeX' '.bib' files can be read into 'R' and
    converted to 'BibEntry' objects. Interfaces to 'NCBI Entrez', 'CrossRef', and
    'Zotero' are provided for importing references and references can be created
    from locally stored 'PDF' files using 'Poppler'. Includes functions for citing
    and generating a bibliography with hyperlinks for documents prepared with
    'RMarkdown' or 'RHTML'.",2020-11-13,Mathew W. McLean,https://github.com/ropensci/RefManageR/,TRUE,https://github.com/ropensci/refmanager,177311,95,2021-04-27T01:04:13Z,1866.4315789473685
refuge,"Access the 'Refuge' API, a web-application for locating trans and 
    intersex-friendly restrooms, including unisex and accessible restrooms. 
    Includes data on the location of restrooms, along with directions, 
    comments, user ratings and amenities. Coverage is global, but data is 
    most comprehensive in the United States.
    See <https://www.refugerestrooms.org/api/docs/> for full API documentation.",2021-01-11,Evan Odell,https://docs.evanodell.com/refuge,TRUE,https://github.com/evanodell/refuge,13954,1,2021-01-11T18:19:10Z,13954
regexTestR,"A 'Shiny' app to aid in writing, explaining, and debugging regular expressions in an R environment.",2020-06-29,Adam Spannbauer,https://github.com/AdamSpannbauer/r_regex_tester_app,TRUE,https://github.com/adamspannbauer/r_regex_tester_app,4193,46,2021-05-09T20:40:03Z,91.15217391304348
regioncode,"A fast tool to conquer the difficulties to convert various region names and administration division codes of Chinese regions. The current version enables seamlessly converting Chinese regions' formal names, common-used names, and codes between each other at the city level from 1986 to 2019.",2021-08-02,Yue Hu,NA,TRUE,https://github.com/sammo3182/regioncode,2162,7,2021-08-05T11:49:51Z,308.85714285714283
regions,"Validating sub-national statistical typologies, re-coding across 
    standard typologies of sub-national statistics, and making valid aggregate
    level imputation, re-aggregation, re-weighting and projection down to 
    lower hierarchical levels to create meaningful data panels and time series.",2021-06-21,Daniel Antal,https://regions.dataobservatory.eu/,TRUE,https://github.com/ropengov/regions,5768,3,2021-06-27T11:23:12Z,1922.6666666666667
regmedint,"'R' implementation of the regression-based causal mediation analysis with a treatment-mediator interaction term, as originally implemented in the 'SAS' macro by Valeri and VanderWeele (2013) <doi:10.1037/a0031034> and Valeri and VanderWeele (2015) <doi:10.1097/EDE.0000000000000253>. Linear and logistic models are supported for the mediator model. Linear, logistic, loglinear, Poisson, negative binomial, Cox, and accelerated failure time (exponential and Weibull) models are supported for the outcome model.",2021-05-12,Kazuki Yoshida,https://kaz-yos.github.io/regmedint/,TRUE,https://github.com/kaz-yos/regmedint,6745,8,2021-07-05T11:57:31Z,843.125
regnet,"Network-based regularization has achieved success in variable selection for 
    high-dimensional biological data due to its ability to incorporate correlations among 
    genomic features. This package provides procedures of network-based variable selection 
    for generalized linear models (Ren et al. (2017) <doi:10.1186/s12863-017-0495-5> and 
    Ren et al. (2019) <doi:10.1002/gepi.22194>). Two recent additions are the robust network 
    regularization for the survival response and the network regularization for continuous 
    response. Functions for other regularization methods will be included in the forthcoming 
    upgraded versions. ",2019-06-08,Jie Ren,https://github.com/jrhub/regnet,TRUE,https://github.com/jrhub/regnet,15313,4,2021-07-27T21:55:45Z,3828.25
regress,"Functions to fit Gaussian linear model by maximising the
        residual log likelihood where the covariance structure can be
        written as a linear combination of known matrices.  Can be used
        for multivariate models and random effects models.  Easy
        straight forward manner to specify random effects models,
        including random interactions. Code now optimised to use
        Sherman Morrison Woodbury identities for matrix inversion in
        random effects models. We've added the ability to fit models
        using any kernel as well as a function to return the mean and
        covariance of random effects conditional on the data (best
        linear unbiased predictors, BLUPs).
        Clifford and McCullagh (2006)
        <https://www.r-project.org/doc/Rnews/Rnews_2006-2.pdf>.",2020-06-18,David Clifford,https://github.com/kbroman/regress,TRUE,https://github.com/kbroman/regress,35211,1,2020-12-07T06:19:27Z,35211
regrrr,"Compiling regression results into a publishable format, conducting post-hoc hypothesis testing, and plotting moderating effects (the effect of X on Y becomes stronger/weaker as Z increases).",2021-08-13,Rui K. Yang,NA,TRUE,https://github.com/rkzyang/regrrr,11085,1,2021-08-13T14:00:30Z,11085
RegSDC,"Implementation of the methods described in the paper with the above title: Langsrud, Ø. (2019) <doi:10.1007/s11222-018-9848-9>. The package can be used to generate synthetic or hybrid continuous microdata, and the relationship to the original data can be controlled in several ways. A function for replacing suppressed tabular cell frequencies with decimal numbers is included.",2021-05-14,Øyvind Langsrud,https://github.com/olangsrud/RegSDC,TRUE,https://github.com/olangsrud/regsdc,11954,0,2021-05-14T11:26:10Z,NA
regtools,"Tools for linear, nonlinear and nonparametric regression
             and classification.  Novel graphical methods for assessment 
             of parametric models using nonparametric methods. One 
             vs. All and All vs. All multiclass classification, optional
             class probabilities adjustment.  Nonparametric regression 
             (k-NN) for general dimension, local-linear option.  Nonlinear 
             regression with Eickert-White method for dealing with 
             heteroscedasticity.  Utilities for converting time series
             to rectangular form.  Utilities for conversion between
             factors and indicator variables.  Some code related to
             ""Statistical Regression and Classification: from Linear
             Models to Machine Learning"", N. Matloff, 2017, CRC,
             ISBN 9781498710916.",2019-08-25,Norm Matloff,https://github.com/matloff/regtools,TRUE,https://github.com/matloff/regtools,20675,110,2021-09-02T04:53:10Z,187.95454545454547
reliabilitydiag,"Checking the reliability of predictions via the CORP approach,
    which generates provably statistically 'C'onsistent, 'O'ptimally binned, and
    'R'eproducible reliability diagrams using the 'P'ool-adjacent-violators
    algorithm. See Dimitriadis, Gneiting, Jordan (2020) <arXiv:2008.03033>.",2020-11-20,Alexander I. Jordan,https://github.com/aijordan/reliabilitydiag/,TRUE,https://github.com/aijordan/reliabilitydiag,3645,2,2021-04-28T16:52:23Z,1822.5
remap,"Automatically creates separate regression models for different spatial 
    regions. The prediction surface is smoothed using a regional border smoothing 
    method. If regional models are continuous, the resulting prediction surface is 
    continuous across the spatial dimensions, even at region borders. Methodology 
    is described in Wagstaff (2021) <https://digitalcommons.usu.edu/etd/8065/>.",2021-04-16,Jadon Wagstaff,https://github.com/jadonwagstaff/remap,TRUE,https://github.com/jadonwagstaff/remap,3748,0,2021-04-15T20:16:06Z,NA
remoter,"A set of utilities for client/server computing with R, controlling
    a remote R session (the server) from a local one (the client).  Simply set
    up a server (see package vignette for more details) and connect to it from
    your local R session ('RStudio', terminal, etc).  The client/server
    framework is a custom 'REPL' and runs entirely in your R session without the
    need for installing a custom environment on your system.  Network
    communication is handled by the 'ZeroMQ' library by way of the 'pbdZMQ'
    package.",2018-01-05,Drew Schmidt,https://github.com/RBigData/remoter,TRUE,https://github.com/rbigdata/remoter,19079,69,2021-02-28T16:04:12Z,276.5072463768116
REndo,"Fits linear models with endogenous regressor using latent instrumental variable approaches. 
    The methods included in the package are Lewbel's (1997) <doi:10.2307/2171884> higher moments approach as well as 
    Lewbel's (2012) <doi:10.1080/07350015.2012.643126> heteroscedasticity approach, Park and Gupta's (2012) <doi:10.1287/mksc.1120.0718> joint estimation method 
    that uses Gaussian copula and Kim and Frees's (2007) <doi:10.1007/s11336-007-9008-1> multilevel generalized
    method of moment approach that deals with endogeneity in a multilevel setting.
    These are statistical techniques to address the endogeneity problem where no external instrumental variables are needed.
    Note that with version 2.0.0 sweeping changes were introduced which greatly improve functionality and usability but break backwards compatibility.",2021-02-10,Raluca Gui,https://github.com/mmeierer/REndo,TRUE,https://github.com/mmeierer/rendo,28567,5,2021-02-10T13:25:44Z,5713.4
renv,"A dependency management toolkit for R. Using 'renv', you can create
    and manage project-local R libraries, save the state of these libraries to
    a 'lockfile', and later restore your library as required. Together, these
    tools can help make your projects more isolated, portable, and reproducible.",2021-07-21,Kevin Ushey,https://rstudio.github.io/renv/,TRUE,https://github.com/rstudio/renv,1873032,617,2021-09-02T21:29:12Z,3035.708265802269
repana,"Set of utilities to facilitate the reproduction of analysis in R.
 It allow to make_structure(), clean_structure(), and run and log programs in a
 predefined order to allow secondary files, analysis and reports be constructed in
 an ordered form.",2021-05-18,John J. Aponte,https://github.com/johnaponte/repana,TRUE,https://github.com/johnaponte/repana,1171,2,2021-05-18T10:51:29Z,585.5
repello,"Creates reports from Trello, a collaborative, project organization 
             and list-making application. <https://trello.com/>
             Reports are created by comparing individual Trello board
             cards from two different points in time and documenting any changes made
             to the cards.",2021-01-14,Andrew Guide,https://github.com/thomasgstewart/repello,TRUE,https://github.com/thomasgstewart/repello,3427,0,2021-01-25T18:04:59Z,NA
replicateBE,"Performs comparative bioavailability calculations for Average Bioequivalence with
  Expanding Limits (ABEL). Implemented are 'Method A' and 'Method B' and the detection of outliers.
  If the design allows, assessment of the empiric Type I Error and iteratively adjusting alpha to
  control the consumer risk.
  Average Bioequivalence - optionally with a tighter (narrow therapeutic index drugs) or wider
  acceptance range (Gulf Cooperation Council, South Africa: Cmax) - is implemented as well.",2021-06-21,Helmut Schütz,https://github.com/Helmut01/replicateBE,TRUE,https://github.com/helmut01/replicatebe,15159,5,2021-06-24T21:06:58Z,3031.8
ReplicationSuccess,"Provides utilities for the design and analysis of replication studies.
    Features both traditional methods based on statistical significance and
    more recent methods such as the sceptical p-value; Held L. (2020) <doi:10.1111/rssa.12493>.
    Also provides related methods including the harmonic mean chi-squared test; Held, L. (2020), <doi:10.1111/rssc.12410>,
    and intrinsic credibility; Held, L. (2019) <doi:10.1098/rsos.181534>.
    Contains datasets from four large-scale replication projects.",2021-07-16,Leonhard Held,https://florafauna.github.io/ReplicationSuccess/,TRUE,https://github.com/florafauna/replicationsuccess,650,1,2021-08-12T13:08:19Z,650
report,"The aim of the 'report' package is to bridge the gap between 
    R’s output and the formatted results contained in your manuscript. 
    This package converts statistical models and data frames into textual 
    reports suited for publication, ensuring standardization and quality 
    in results reporting.",2021-06-10,Dominique Makowski  (<https://orcid.org/0000-0001-5375-9967>,https://easystats.github.io/report/,TRUE,https://github.com/easystats/report,8972,436,2021-08-10T11:48:15Z,20.577981651376145
reporter,"Contains functions to create regulatory-style statistical reports.
    Originally designed to create tables, listings, and figures for the 
    pharmaceutical, biotechnology, and medical device industries, these
    reports are generalized enough that they could be used in any industry.
    Generates text, rich-text, and PDF file formats.  The package specializes 
    in printing wide and long tables with automatic page wrapping and splitting.  
    Reports can be produced with a minimum of function calls, and without 
    relying on other table packages.  The package supports titles, footnotes, 
    page header, page footers, spanning headers, page by variables, 
    and automatic page numbering.",2021-08-06,David Bosak,https://reporter.r-sassy.org,TRUE,https://github.com/dbosak01/reporter,3670,6,2021-09-03T12:58:41Z,611.6666666666666
reportfactory,"Provides an infrastructure for handling multiple R Markdown
  reports, including automated curation and time-stamping of outputs,
  parameterisation and provision of helper functions to manage dependencies.",2021-08-09,Tim Taylor,https://github.com/reconverse/reportfactory,TRUE,https://github.com/reconverse/reportfactory,4873,65,2021-08-09T12:10:02Z,74.96923076923076
repr,"String and binary representations of objects for several formats /
    mime types.",2021-01-21,Philipp Angerer,https://github.com/IRkernel/repr/,TRUE,https://github.com/irkernel/repr,1316908,47,2021-01-21T09:19:27Z,28019.31914893617
reproducible,"Collection of high-level, machine- and OS-independent tools
    for making deeply reproducible and reusable content in R.
    The two workhorse functions are Cache and prepInputs; 
    these allow for: nested caching, robust to environments, and objects with
    environments (like functions); and data retrieval and processing 
    in continuous workflow environments. In all cases,
    efforts are made to make the first and subsequent calls of functions have 
    the same result, but vastly faster at subsequent times by way of checksums
    and digesting. Several features are still under active development, including
    cloud storage of cached objects, allowing for sharing between users. Several
    advanced options are available, see ?reproducibleOptions.",2021-05-28,Eliot J B McIntire,"https://reproducible.predictiveecology.org,
https://github.com/PredictiveEcology/reproducible",TRUE,https://github.com/predictiveecology/reproducible,47712,31,2021-05-25T01:57:17Z,1539.0967741935483
reproj,"Transform coordinates from a specified source to a specified 
 target map projection. This uses the 'PROJ' library directly, by wrapping the 
 'PROJ' package (if functional), otherwise the 'proj4' package. The 'reproj()' 
 function is generic, methods may be added to remove the need for an explicit 
 source definition. If 'proj4' is in use 'reproj()' handles the requirement for 
 conversion of angular units where necessary. This is for use primarily to 
 transform generic data formats and direct leverage of the underlying
 'PROJ' library. (There are transformations that aren't possible with 'PROJ' and
 that are provided by the 'GDAL' library, a limitation which users of this 
 package should be aware of.) The 'PROJ' library is available at 
 <https://proj.org/>.  ",2020-04-15,Michael D. Sumner,https://github.com/hypertidy/reproj/,TRUE,https://github.com/hypertidy/reproj,31004,2,2020-09-10T03:41:08Z,15502
Require,"A single key function, 'Require' that wraps 'install.packages',
    'remotes::install_github', 'versions::install.versions', and 'base::require'
    that allows for reproducible workflows. As with other functions in a
    reproducible workflow, this package emphasizes functions that return the 
    same result whether it is the first or subsequent times running the function.
    Maturing.",2021-05-31,Eliot J B McIntire,"https://Require.predictiveecology.org,
https://github.com/PredictiveEcology/Require",TRUE,https://github.com/predictiveecology/require,26543,8,2021-05-28T20:00:14Z,3317.875
requiRements,"Helper function to install packages for R using an external
    'requirements.txt' or a string containing diverse packages from
    several resources like Github or CRAN.",2021-06-18,Jonathan M. Mang,https://github.com/joundso/requirements,TRUE,https://github.com/joundso/requirements,542,1,2021-06-21T08:09:26Z,542
rerddap,"General purpose R client for 'ERDDAP' servers. Includes
    functions to search for 'datasets', get summary information on
    'datasets', and fetch 'datasets', in either 'csv' or 'netCDF' format.
    'ERDDAP' information: 
    <https://upwell.pfeg.noaa.gov/erddap/information.html>.",2021-08-18,Roy Mendelssohn,"https://docs.ropensci.org/rerddap/,
https://github.com/ropensci/rerddap",TRUE,https://github.com/ropensci/rerddap,29042,31,2021-08-19T14:55:53Z,936.8387096774194
rerddapXtracto,"Contains three functions that access
    environmental data from any 'ERDDAP' data web service. The rxtracto() function extracts
    data along a trajectory for a given ""radius"" around the point. The
    rxtracto_3D() function extracts data in a box. The rxtractogon() function
    extracts data in a polygon. All of those three function use the 'rerddap' package
    to extract the data, and should work with any 'ERDDAP' server.
    There are also two functions, plotBBox() and plotTrack() that use the 'plotdap'
    package to simplify the creation of maps of the data.",2021-06-02,Roy Mendelssohn,https://github.com/rmendels/rerddapXtracto,TRUE,https://github.com/rmendels/rerddapxtracto,16723,10,2021-06-08T15:36:03Z,1672.3
reReg,"A comprehensive collection of practical and easy-to-use tools for regression analysis of recurrent events, with or without the presence of a (possibly) informative terminal event. The modeling framework is based on a joint frailty scale-change model, that includes models described in Wang et al. (2001) <doi:10.1198/016214501753209031>, Huang and Wang (2004) <doi:10.1198/016214504000001033>, Xu et al. (2017) <doi:10.1080/01621459.2016.1173557>, and Xu et al. (2019) <doi:10.5705/SS.202018.0224> as special cases. The implemented estimating procedure does not require any parametric assumption on the frailty distribution. The package also allows the users to specify different model forms for both the recurrent event process and the terminal event. ",2021-04-14,Sy Han (Steven) Chiou,https://github.com/stc04003/reReg,TRUE,https://github.com/stc04003/rereg,23614,8,2021-07-16T07:22:16Z,2951.75
resemble,"
    Functions for dissimilarity analysis and memory-based learning 
    (MBL, a.k.a local modeling) in complex spectral data sets. 
    Most of these functions are based the methods presented in 
    Ramirez-Lopez et al. (2013) <doi:10.1016/j.geoderma.2012.12.014>.",2020-11-09,Leonardo Ramirez-Lopez,http://l-ramirez-lopez.github.io/resemble/,TRUE,https://github.com/l-ramirez-lopez/resemble,16043,9,2020-11-15T23:43:01Z,1782.5555555555557
reshape2,"Flexibly restructure and aggregate data using just two
    functions: melt and 'dcast' (or 'acast').",2020-04-09,Hadley Wickham,https://github.com/hadley/reshape,TRUE,https://github.com/hadley/reshape,15460521,197,2021-03-08T20:00:42Z,78479.80203045685
resourcer,"A resource represents some data or a computation unit. It is 
    described by a URL and credentials. This package proposes a Resource model
    with ""resolver"" and ""client"" classes to facilitate the access and the usage of the 
    resources.",2020-11-03,Yannick Marcon,NA,TRUE,https://github.com/obiba/resourcer,12007,2,2021-05-17T08:18:32Z,6003.5
restatapi,"Eurostat is the statistical office of the European Union and provides high quality statistics for Europe.
             Large set of the data is disseminated through the Eurostat database (<https://ec.europa.eu/eurostat/data/database>). 
             The tools are using the REST API with the Statistical Data and Metadata eXchange (SDMX <https://sdmx.org>) Web Services 
             (<https://ec.europa.eu/eurostat/web/sdmx-web-services/about-this-service>) to search and download data from 
             the Eurostat database using the SDMX standard. ",2021-05-24,Mátyás Mészáros,https://github.com/eurostat/restatapi,TRUE,https://github.com/eurostat/restatapi,18624,11,2021-06-24T16:40:59Z,1693.090909090909
restorepoint,"Debugging with restore points instead of break points. A restore
    point stores all local variables when called inside a function. The stored
    values can later be retrieved and evaluated in a modified R console that
    replicates the function's environment. To debug step by step, one can simply
    copy & paste the function body from the R script. Particularly convenient
    in combination with ""RStudio"". See the ""Github"" page inst/vignettes for a
    tutorial.",2019-01-02,Sebastian Kranz,https://github.com/skranz/restorepoint,TRUE,https://github.com/skranz/restorepoint,14884,14,2021-03-16T18:53:41Z,1063.142857142857
RestRserve,"
  Allows to easily create high-performance full featured HTTP APIs from R
  functions. Provides high-level classes such as 'Request', 'Response',
  'Application', 'Middleware' in order to streamline server side
  application development. Out of the box allows to serve requests using
  'Rserve' package, but flexible enough to integrate with other HTTP servers
  such as 'httpuv'.",2021-01-04,Dmitriy Selivanov,"https://restrserve.org, https://github.com/rexyai/RestRserve",TRUE,https://github.com/rexyai/restrserve,12542,214,2020-12-22T13:52:56Z,58.60747663551402
resumer,"Using a CSV, LaTeX and R to easily build attractive resumes.",2021-02-12,Jared Lander,https://github.com/jaredlander/resumer,TRUE,https://github.com/jaredlander/resumer,16868,65,2021-02-12T13:57:50Z,259.5076923076923
reticulate,"Interface to 'Python' modules, classes, and functions. When calling
    into 'Python', R data types are automatically converted to their equivalent 'Python'
    types. When values are returned from 'Python' to R they are converted back to R
    types. Compatible with all versions of 'Python' >= 2.7.",2021-05-03,Kevin Ushey,https://github.com/rstudio/reticulate,TRUE,https://github.com/rstudio/reticulate,4851826,1320,2021-08-24T17:59:03Z,3675.6257575757577
retistruct,"Reconstructs retinae by morphing a flat surface with cuts (a
    dissected flat-mount retina) onto a curvilinear surface (the standard retinal
    shape). It can estimate the position of a point on the intact adult retina
    to within 8 degrees of arc (3.6% of nasotemporal axis). The coordinates in
    reconstructed retinae can be transformed to visuotopic coordinates.",2020-04-04,David C. Sterratt,http://davidcsterratt.github.io/retistruct/,TRUE,https://github.com/davidcsterratt/retistruct,19160,5,2020-09-13T12:44:14Z,3832
retroharmonize,"Assist in reproducible retrospective (ex-post) harmonization of data,
    particularly individual level survey data, by providing tools for organizing metadata,
    standardizing the coding of variables, and variable names and value labels, 
    including missing values, and documenting the data transformations,
    with the help of comprehensive s3 classes.",2021-06-27,Daniel Antal,https://retroharmonize.dataobservatory.eu/,TRUE,https://github.com/ropengov/retroharmonize,4183,2,2021-07-20T22:49:20Z,2091.5
retrosheet,"A collection of tools to import and structure the (currently) single-season
    event, game-log, roster, and schedule data available from <https://www.retrosheet.org>.
    In particular, the event (a.k.a. play-by-play) files can be especially difficult to parse.
    This package does the parsing on those files, returning the requested data in the most
    practical R structure to use for sabermetric or other analyses.",2020-12-14,Colin Douglas,https://github.com/colindouglas/retrosheet,TRUE,https://github.com/colindouglas/retrosheet,11840,1,2020-12-14T00:20:15Z,11840
retry,Provide simple mechanism to repeatedly evaluate an expression until either it succeeds or timeout exceeded. It is useful in situations that random failures could happen.,2020-04-23,Randy Lai,https://github.com/randy3k/retry,TRUE,https://github.com/randy3k/retry,8080,12,2021-06-05T08:23:30Z,673.3333333333334
reutils,"An interface to NCBI databases such as PubMed, GenBank, or GEO
    powered by the Entrez Programming Utilities (EUtils). The nine EUtils
    provide programmatic access to the NCBI Entrez query and database
    system for searching and retrieving biological data.",2016-09-03,Gerhard Schöfl,https://github.com/gschofl/reutils,TRUE,https://github.com/gschofl/reutils,21881,16,2020-10-08T17:07:01Z,1367.5625
reval,"Simplified scenario testing and sensitivity analysis,
    redesigned to use packages 'future' and 'furrr'. Provides
    functions for generating function argument sets using
    one-factor-at-a-time (OFAT) and (sampled) permutations.",2021-06-27,Michael C Koohafkan,https://github.com/mkoohafkan/reval,TRUE,https://github.com/mkoohafkan/reval,15048,1,2021-06-27T04:17:42Z,15048
revdbayes,"Provides functions for the Bayesian analysis of extreme value
    models.  The 'rust' package <https://cran.r-project.org/package=rust> is
    used to simulate a random sample from the required posterior distribution.
    The functionality of 'revdbayes' is similar to the 'evdbayes' package
    <https://cran.r-project.org/package=evdbayes>, which uses Markov Chain
    Monte Carlo ('MCMC') methods for posterior simulation.  Also provided
    are functions for making inferences about the extremal index, using 
    the K-gaps model of Suveges and Davison (2010) <doi:10.1214/09-AOAS292>.
    Also provided are d,p,q,r functions for the Generalised Extreme Value 
    ('GEV') and Generalised Pareto ('GP') distributions that deal 
    appropriately with cases where the shape parameter is very close to zero.",2020-09-12,Paul J. Northrop,"https://paulnorthrop.github.io/revdbayes/,
https://github.com/paulnorthrop/revdbayes",TRUE,https://github.com/paulnorthrop/revdbayes,64227,5,2021-06-27T10:36:57Z,12845.4
revealjs,"R Markdown format for 'reveal.js' presentations, a framework 
  for easily creating beautiful presentations using HTML.",2017-03-13,Hakim El Hattab,https://github.com/rstudio/revealjs,TRUE,https://github.com/rstudio/revealjs,46013,280,2021-07-26T12:50:53Z,164.33214285714286
reveneraR,"Facilitates making a connection to the 
  'Revenera' API and executing various queries. You can use it to
  get event data and metadata. The 'Revenera' documentation 
  is available at 
  <https://docs.revenera.com/ui560/report/Content/helplibrary/APIRoot.htm>. 
  This package is not supported by 'Flexera' (owner of the software). ",2021-05-21,Chris Umphlett,https://github.com/chrisumphlett/reveneraR,TRUE,https://github.com/chrisumphlett/revenerar,1140,0,2021-08-17T19:16:01Z,NA
ReviewR,"A portable Shiny tool to explore patient-level electronic health record data 
    and perform chart review in a single integrated framework. This tool supports 
    browsing clinical data in many different formats including multiple versions 
    of the 'OMOP' common data model as well as the 'MIMIC-III' data model. In 
    addition, chart review information is captured and stored securely via the 
    Shiny interface in a 'REDCap' (Research Electronic Data Capture) project 
    using the 'REDCap' API. See the 'ReviewR' website for additional information, 
    documentation, and examples.",2021-09-02,Laura Wiley,"https://reviewr.thewileylab.org/,
https://github.com/thewileylab/ReviewR/",TRUE,https://github.com/thewileylab/reviewr,1736,10,2021-09-02T15:23:47Z,173.6
revpref,"Tools to (i) check consistency of a finite set of consumer demand observations with a number of revealed preference axioms at a given efficiency level, (ii) compute goodness-of-fit indices when the data do not obey the axioms, and (iii) compute power against uniformly random behavior.",2021-07-07,Khushboo Surana,https://github.com/ksurana21/revpref,TRUE,https://github.com/ksurana21/revpref,692,0,2021-07-07T11:30:08Z,NA
revss,"Implements the estimation techniques described in Rousseeuw &
    Verboven (2002) <doi:10.1016/S0167-9473(02)00078-6> for the location and
    scale of very small samples.",2020-11-10,Avraham Adler,https://github.com/aadler/revss,TRUE,https://github.com/aadler/revss,5065,2,2021-01-12T03:52:33Z,2532.5
revulyticsR,"Facilitates making a connection to the 
  'Revulytics' API and executing various queries. You can use it to
  get event data and metadata. The Revulytics documentation 
  is available at <https://docs.revenera.com/ui560/report/>. This
  package is not supported by 'Flexera' (owner of the software). ",2020-12-04,Chris Umphlett,https://github.com/chrisumphlett/revulyticsR,TRUE,https://github.com/chrisumphlett/revulyticsr,5348,0,2021-08-17T19:16:01Z,NA
rex,A friendly interface for the construction of regular expressions.,2020-04-21,Jim Hester,https://github.com/kevinushey/rex,TRUE,https://github.com/kevinushey/rex,6282498,242,2020-11-19T23:01:01Z,25960.73553719008
rextendr,"Provides functions to compile and load Rust code from R, similar
    to how 'Rcpp' or 'cpp11' allow easy interfacing with C++ code. Also provides
    helper functions to create R packages that use Rust code. Under the hood,
    the Rust crate 'extendr' is used to do all the heavy lifting.",2021-06-15,Claus O. Wilke,https://extendr.github.io/rextendr/,TRUE,https://github.com/extendr/rextendr,851,67,2021-08-30T04:24:59Z,12.701492537313433
rfacebookstat,"Load data by campaigns, ads, ad sets and insights, ad account and business manager 
    from Facebook Marketing API into R. For more details see official documents by Facebook 
    Marketing API <https://developers.facebook.com/docs/marketing-apis/>.",2021-07-27,Alexey Seleznev,"https://selesnow.github.io/rfacebookstat/,
https://www.youtube.com/playlist?list=PLD2LDq8edf4pItOb-vZTG5AXZK2niJ8_R",TRUE,https://github.com/selesnow/rfacebookstat,20806,21,2021-07-27T09:36:16Z,990.7619047619048
rfacts,"The 'rfacts' package is an R interface to the
  Fixed and Adaptive Clinical Trial Simulator ('FACTS')
  on Unix-like systems. It programmatically invokes 'FACTS' to run clinical
  trial simulations, and it aggregates simulation output data
  into tidy data frames. These capabilities provide end-to-end
  automation for large-scale simulation pipelines, and
  they enhance computational reproducibility.
  For more information on 'FACTS' itself,
  please visit <https://www.berryconsultants.com/software/>.",2021-03-27,William Michael Landau,"https://elilillyco.github.io/rfacts/,
https://github.com/EliLillyCo/rfacts",TRUE,https://github.com/elilillyco/rfacts,6549,3,2021-04-23T17:11:07Z,2183
Rfast,"A collection of fast (utility) functions for data analysis. Column- and row- wise means, medians, variances, minimums, maximums, many t, F and G-square tests, many regressions (normal, logistic, Poisson), are some of the many fast functions. References: a) Tsagris M., Papadakis M. (2018). Taking R to its limits: 70+ tips. PeerJ Preprints 6:e26605v1 <doi:10.7287/peerj.preprints.26605v1>. b) Tsagris M. and Papadakis M. (2018). Forward regression in R: from the extreme slow to the extreme fast. Journal of Data Science, 16(4): 771--780. <doi:10.6339/JDS.201810_16(4).00006>. ",2021-05-17,Manos Papadakis,https://github.com/RfastOfficial/Rfast,TRUE,https://github.com/rfastofficial/rfast,181162,58,2021-07-31T14:40:05Z,3123.4827586206898
Rfast2,"A collection of fast statistical and utility functions for data analysis. Functions for regression, maximum likelihood, column-wise statistics and many more have been included. C++ has been utilized to speed up the functions.",2021-03-22,Manos Papadakis,https://github.com/RfastOfficial/Rfast2,TRUE,https://github.com/rfastofficial/rfast2,40767,14,2021-08-04T19:57:45Z,2911.9285714285716
RFCCA,"Random Forest with Canonical Correlation Analysis (RFCCA) is a 
  random forest method for estimating the canonical correlations between two 
  sets of variables depending on the subject-related covariates. The trees are 
  built with a splitting rule specifically designed to partition the data to 
  maximize the canonical correlation heterogeneity between child nodes. The 
  method is described in Alakus et al. (2020) <arXiv:2011.11555>. RFCCA uses 
  'randomForestSRC' package (Ishwaran and Kogalur, 2020) by freezing at the 
  version 2.9.3. The custom splitting rule feature is utilised to apply the 
  proposed splitting rule.",2021-02-03,Cansu Alakus,https://github.com/calakus/RFCCA,TRUE,https://github.com/calakus/rfcca,3475,1,2021-02-03T21:06:32Z,3475
rfishbase,"A programmatic interface to 'FishBase', re-written
    based on an accompanying 'RESTful' API. Access tables describing over 30,000
    species of fish, their biology, ecology, morphology, and more. This package also
    supports experimental access to 'SeaLifeBase' data, which contains
    nearly 200,000 species records for all types of aquatic life not covered by
    'FishBase.'",2021-08-09,Carl Boettiger,"https://docs.ropensci.org/rfishbase/,
https://github.com/ropensci/rfishbase",TRUE,https://github.com/ropensci/rfishbase,36941,76,2021-08-23T16:41:28Z,486.0657894736842
RFishBC,"Helps fisheries scientists collect measurements from calcified
    structures and back-calculate estimated lengths at previous ages using
    standard procedures and models. This is intended to replace much of the
    functionality provided by the now out-dated 'fishBC' software
    (<https://fisheries.org/bookstore/all-titles/software/70317/>).",2019-12-12,Derek Ogle,http://derekogle.com/RFishBC,TRUE,https://github.com/droglenc/rfishbc,13163,6,2020-12-17T22:05:21Z,2193.8333333333335
rfm,"Tools for RFM (recency, frequency and monetary value) analysis. 
    Generate RFM score from both transaction and customer level data. Visualize the
    relationship between recency, frequency and monetary value using heatmap, 
    histograms, bar charts and scatter plots. Includes a 'shiny' app for 
    interactive segmentation. References:
    i. Blattberg R.C., Kim BD., Neslin S.A (2008) <doi:10.1007/978-0-387-72579-6_12>.",2020-07-21,Aravind Hebbali,"https://github.com/rsquaredacademy/rfm,
https://rfm.rsquaredacademy.com/",TRUE,https://github.com/rsquaredacademy/rfm,122564,42,2020-12-08T07:19:20Z,2918.190476190476
rfoaas,R access to the 'FOAAS' (F... Off As A Service) web service is provided.,2020-01-09,Dirk Eddelbuettel,http://dirk.eddelbuettel.com/code/rfoaas.html,TRUE,https://github.com/eddelbuettel/rfoaas,18910,27,2021-05-28T01:21:42Z,700.3703703703703
rfordummies,"Contains all the code examples in the book ""R for Dummies"" (2nd
    edition) by Andrie de Vries and Joris Meys. You can view the table of 
    contents as well as the sample code for each chapter.",2020-09-19,Andrie de Vries,"https://rfordummies.com, http://andrie.github.io/rfordummies/",TRUE,https://github.com/andrie/rfordummies,18432,3,2020-09-19T06:51:58Z,6144
Rforestry,"Provides fast implementations of Honest Random Forests, 
    Gradient Boosting, and Linear Random Forests, with an emphasis on inference 
    and interpretability. Additionally contains methods for variable 
    importance, out-of-bag prediction, regression monotonicity, and
    several methods for missing data imputation. Soren R. Kunzel, 
    Theo F. Saarinen, Edward W. Liu, Jasjeet S. Sekhon (2019) <arXiv:1906.06463>.",2021-09-02,Theo Saarinen,https://github.com/forestry-labs/Rforestry,TRUE,https://github.com/forestry-labs/rforestry,2942,6,2021-08-15T19:54:54Z,490.3333333333333
rfPermute,"Estimate significance of importance metrics
    for a Random Forest model by permuting the response
    variable. Produces null distribution of importance
    metrics for each predictor variable and p-value of
    observed. Provides summary and visualization functions for 'randomForest' 
    results.",2021-05-22,Eric Archer,https://github.com/EricArcher/rfPermute,TRUE,https://github.com/ericarcher/rfpermute,29493,17,2021-06-24T20:03:47Z,1734.8823529411766
RFpredInterval,"Implements various prediction interval methods with random forests and boosted forests.
    The package has two main functions: pibf() produces prediction intervals with boosted forests
    (PIBF) as described in Alakus et al. (2021) <arXiv:2106.08217> and rfpi() builds 15 distinct
    variations of prediction intervals with random forests (RFPI) proposed by Roy and Larocque (2020)
    <doi:10.1177/0962280219829885>.",2021-06-28,Cansu Alakus,https://github.com/calakus/RFpredInterval,TRUE,https://github.com/calakus/rfpredinterval,1043,3,2021-06-28T18:46:45Z,347.6666666666667
Rfssa,"Methods and tools for implementing functional singular spectrum analysis for functional time series
    as described in Haghbin H., Najibi, S.M., Mahmoudvand R., Trinka J., Maadooliat M. (2019). Functional singular spectrum Analysis. Manuscript submitted for publication. ",2019-09-12,Hossein Haghbin,https://github.com/haghbinh/Rfssa.git,TRUE,https://github.com/haghbinh/rfssa,11226,4,2020-12-21T22:33:06Z,2806.5
rfUtilities,"Utilities for Random Forest model selection, class balance
    correction, significance test, cross validation and partial dependency
    plots.",2019-10-03,Jeffrey S. Evans,https://github.com/jeffreyevans/rfUtilities,TRUE,https://github.com/jeffreyevans/rfutilities,53970,6,2021-05-28T19:22:02Z,8995
Rgb,"Classes and methods to efficiently handle (slice, annotate, draw ...) genomic features (such as genes or transcripts), and an interactive interface to browse them.",2018-03-18,Sylvain Mareschal,http://bioinformatics.ovsa.fr/Rgb,TRUE,https://github.com/maressyl/r.rgb,19540,0,2020-11-24T11:24:40Z,NA
rgbif,"A programmatic interface to the Web Service methods
    provided by the Global Biodiversity Information Facility ('GBIF';
    <https://www.gbif.org/developer/summary>). 'GBIF' is a database
    of species occurrence records from sources all over the globe.
    'rgbif' includes functions for searching for taxonomic names,
    retrieving information on data providers, getting species occurrence
    records, getting counts of occurrence records, and using the 'GBIF'
    tile map service to make 'rasters' summarizing huge amounts of data.",2021-06-02,Scott Chamberlain,"https://github.com/ropensci/rgbif (devel),
https://docs.ropensci.org/rgbif/ (documentation)",TRUE,https://github.com/ropensci/rgbif,142013,116,2021-08-09T12:44:00Z,1224.25
rgdax,"Allow access to both public and private end points to Coinbase Pro (erstwhile GDAX) 
    cryptocurrency exchange. For authenticated flow, users must have valid api, secret and passphrase to be able to connect.",2021-08-03,Dheeraj Agarwal,https://github.com/DheerajAgarwal/rgdax/,TRUE,https://github.com/dheerajagarwal/rgdax,18520,28,2021-08-08T07:15:19Z,661.4285714285714
rGEDI,"Set of tools for downloading, reading, visualizing and processing GEDI Level1B, Level2A and Level2B data.",2021-01-20,Carlos Alberto Silva,https://github.com/carlos-alberto-silva/rGEDI,TRUE,https://github.com/carlos-alberto-silva/rgedi,9426,85,2021-08-20T16:31:12Z,110.89411764705882
rgee,"Earth Engine <https://earthengine.google.com/> client library for R. All
  of the 'Earth Engine' API classes, modules, and functions are made available. Additional
  functions implemented include importing (exporting) of Earth Engine spatial objects, 
  extraction of time series, interactive map display, assets management interface, 
  and metadata display. See <https://r-spatial.github.io/rgee/> for further details.",2021-08-10,Cesar Aybar,"https://github.com/r-spatial/rgee/,
https://r-spatial.github.io/rgee/,
https://github.com/google/earthengine-api/",TRUE,https://github.com/r-spatial/rgee,9372,337,2021-09-01T23:18:11Z,27.810089020771514
RGENERATEPREC,"The method 'generate()' is extended for spatial multi-site
    stochastic generation of daily precipitation. It generates precipitation
    occurrence in several sites using logit regression (Generalized Linear
    Models) and the approach by D.S. Wilks (1998) <doi:10.1016/S0022-1694(98)00186-3> . ",2021-01-19,Emanuele Cordano,https://github.com/ecor/RGENERATEPREC,TRUE,https://github.com/ecor/rgenerateprec,16027,2,2021-01-13T11:19:44Z,8013.5
rgeoda,"Provides spatial data analysis functionalities including Exploratory Spatial Data Analysis, 
    Spatial Cluster Detection and Clustering Analysis, Regionalization, etc. based on the C++ source code 
    of 'GeoDa', which is an open-source software tool that serves as an introduction to spatial data analysis.
    The 'GeoDa' software and its documentation are available at <https://geodacenter.github.io>.",2021-08-05,Xun Li,"https://github.com/geodacenter/rgeoda/,
https://geodacenter.github.io/rgeoda/",TRUE,https://github.com/geodacenter/rgeoda,2923,44,2021-08-04T21:35:14Z,66.43181818181819
rgeolocate,"Connectors to online and offline sources for taking IP addresses
    and geolocating them to country, city, timezone and other geographic ranges. For
    individual connectors, see the package index.",2020-05-17,Os Keyes,NA,TRUE,https://github.com/ironholds/rgeolocate,38207,66,2021-08-20T19:41:31Z,578.8939393939394
rgexf,"Create, read and write 'GEXF' (Graph Exchange 'XML' Format) graph
    files (used in 'Gephi' and others). Using the 'XML' package, it allows the user to
    easily build/read graph files including attributes, 'GEXF' visual attributes (such
    as color, size, and position), network dynamics (for both edges and nodes) and
    edge weighting. Users can build/handle graphs element-by-element or massively
    through data-frames, visualize the graph on a web browser through 'gexf-js' (a
    'javascript' library) and interact with the 'igraph' package.",2021-08-12,George Vega Yon,https://gvegayon.github.io/rgexf/,TRUE,https://github.com/gvegayon/rgexf,356753,16,2021-08-12T21:09:56Z,22297.0625
rgho,"Access WHO Global Health Observatory
    (<https://www.who.int/data/gho/>)
    data from R via the Athena web service
    (<https://apps.who.int/gho/data/node.resources.api>),
    an application program interface providing
    a simple query interface to the World
    Health Organization's data and statistics content.",2020-11-30,Antoine Filipovic-Pierucci,https://github.com/kzarca/rgho,TRUE,https://github.com/kzarca/rgho,17123,3,2020-11-30T14:09:48Z,5707.666666666667
rgl,"Provides medium to high level functions for 3D interactive graphics, including
    functions modelled on base graphics (plot3d(), etc.) as well as functions for
    constructing representations of geometric objects (cube3d(), etc.).  Output
    may be on screen using OpenGL, or to various standard 3D file formats including
    WebGL, PLY, OBJ, STL as well as 2D image formats, including PNG, Postscript, SVG, PGF.",2021-08-21,Duncan Murdoch,"https://github.com/dmurdoch/rgl, https://dmurdoch.github.io/rgl/",TRUE,https://github.com/dmurdoch/rgl,6135382,38,2021-08-21T16:37:17Z,161457.42105263157
rglobi,"A programmatic interface to the web service methods
    provided by Global Biotic Interactions (GloBI)
    (<https://www.globalbioticinteractions.org/>). GloBI provides 
    access to spatial-temporal species interaction records from 
    sources all over the world. rglobi provides methods to search 
    species interactions by location, interaction type, and 
    taxonomic name. In addition, it supports Cypher, a graph query
    language, to allow for executing custom queries on the GloBI 
    aggregate species interaction data set.",2021-08-12,Jorrit Poelen,"https://docs.ropensci.org/rglobi/,
https://github.com/ropensci/rglobi",TRUE,https://github.com/ropensci/rglobi,22818,12,2021-08-12T16:24:53Z,1901.5
rgnparser,"Parse scientific names using 'gnparser'
    (<https://github.com/gnames/gnparser>), written in Go. 'gnparser'
    parses scientific names into their component parts; it utilizes
    a Parsing Expression Grammar specifically for scientific names.",2021-01-25,Scott Chamberlain,"https://ropensci.github.io/rgnparser/,
https://github.com/ropensci/rgnparser",TRUE,https://github.com/ropensci/rgnparser,3819,3,2021-05-03T22:40:06Z,1273
rgoogleads,"Interface for loading data from 'Google Ads API', 
    see <https://developers.google.com/google-ads/api/docs/start>. 
    Package provide function for authorization and loading reports.",2021-09-03,Alexey Seleznev,"https://selesnow.github.io/rgoogleads/,
https://selesnow.github.io/rgoogleads/docs/,
https://github.com/selesnow/rgoogleads",TRUE,https://github.com/selesnow/rgoogleads,976,1,2021-09-03T08:08:47Z,976
rgovcan,"Allows to search for existing resources, including datasets, on 
    the Canadian Open Government portal (<https://open.canada.ca/en>). It is 
    also designed to allow users to easily download a range of files directly 
    from the portal in a reproducible manner.",2021-06-18,Valentin Lucet,https://vlucet.github.io/rgovcan/,TRUE,https://github.com/vlucet/rgovcan,1021,15,2021-07-03T18:48:15Z,68.06666666666666
rgplates,"Query functions to the GPlates <https://www.gplates.org/> desktop application and the GPlates Web Service <https://gws.gplates.org/> allow users to reconstruct coordinates, static plates, and Spatial objects without leaving the R running environment. This R extension was supported by the FAU GeoZentrum Nordbayern and is developed under the umbrella of the DFG (Deutsche Forschungsgemeinschaft) Research Unit TERSANE2 (For 2332, TEmperature Related Stressors in ANcient Extinctions).",2021-05-18,Adam T. Kocsis,NA,TRUE,https://github.com/adamkocsis/rgplates,1097,0,2021-05-17T09:21:56Z,NA
rgrass7,"Interpreted interface between 'GRASS' 7 geographical 
    information system and R, based on starting R from within the 'GRASS' 'GIS'
    environment, or running free-standing R in a temporary 'GRASS' location;
    the package provides facilities for using all 'GRASS' commands from the 
    R command line. This package may not be used for 'GRASS' 6, for which
    'spgrass6' should be used.",2021-01-29,Roger Bivand,"https://grass.osgeo.org/, https://github.com/rsbivand/rgrass7,
https://rsbivand.github.io/rgrass7/",TRUE,https://github.com/rsbivand/rgrass7,50298,13,2021-04-09T11:48:33Z,3869.076923076923
rgrassdoc,"A tool for easy viewing of the documentation of 'GRASS GIS' (see 
    <https://grass.osgeo.org/>). Pages of the 'GRASS GIS' manuals found at 
    <https://grass.osgeo.org/grass78/manuals/full_index.html> and at  
    <https://grass.osgeo.org/grass78/manuals/addons/> can be viewed within the 
    Viewer pane of 'RStudio', or be opened in the user's default browser. ",2021-03-16,Valentin Lucet,https://vlucet.github.io/rgrassdoc/,TRUE,https://github.com/vlucet/rgrassdoc,1765,7,2021-03-16T12:53:24Z,252.14285714285714
rgugik,"Automatic open data acquisition from resources of Polish Head Office
    of Geodesy and Cartography ('Główny Urząd Geodezji i Kartografii')
    (<www.gugik.gov.pl>).
    Available datasets include various types of numeric, raster and vector data,
    such as orthophotomaps, digital elevation models (digital terrain models,
    digital surface model, point clouds), state register of borders, spatial
    databases, geometries of cadastral parcels, 3D models of buildings, and more.
    It is also possible to geocode addresses or objects using the geocodePL_get()
    function.",2021-05-04,Krzysztof Dyba,"https://kadyb.github.io/rgugik/, https://github.com/kadyb/rgugik",TRUE,https://github.com/kadyb/rgugik,4593,27,2021-05-04T17:12:45Z,170.11111111111111
rhandsontable,"An R interface to the 'Handsontable' JavaScript library, which is a
    minimalist Excel-like data grid editor.  See <https://handsontable.com/> for details.",2021-05-27,Jonathan Owen,http://jrowen.github.io/rhandsontable/,TRUE,https://github.com/jrowen/rhandsontable,378658,336,2021-05-27T11:28:04Z,1126.9583333333333
rhierbaps,"Implements the hierarchical Bayesian analysis of populations structure (hierBAPS) 
  algorithm of Cheng et al. (2013) <doi:10.1093/molbev/mst028> for clustering DNA sequences 
  from multiple sequence alignments in FASTA format. 
  The implementation includes improved defaults and plotting capabilities 
  and unlike the original 'MATLAB' version removes singleton SNPs by default.",2019-12-11,Gerry Tonkin-Hill,https://github.com/gtonkinhill/rhierbaps,TRUE,https://github.com/gtonkinhill/rhierbaps,13917,18,2021-05-02T06:15:41Z,773.1666666666666
rhosa,"Higher-order spectra or polyspectra of time series, such as bispectrum and bicoherence, have been investigated in abundant literature and applied to problems of signal detection in a wide range of fields. This package aims to provide a simple API to estimate and analyze them. The current implementation is based on Brillinger and Irizarry (1998) <doi:10.1016/S0165-1684(97)00217-X> for estimating bispectrum, with Lii and Helland (1981) <doi:10.1145/355958.355961> for cross-bispectrum.",2020-08-10,Takeshi Abe,https://github.com/tabe/rhosa,TRUE,https://github.com/tabe/rhosa,5782,0,2021-01-05T03:52:13Z,NA
RHRT,Methods to scan RR interval data for Premature Ventricular Complexes (PVCs) and parameterise and plot the resulting Heart Rate Turbulence (HRT). The methodology of HRT analysis is based on the original publication by Schmidt et al. <doi:10.1016/S0140-6736(98)08428-1> and extended with suggestions from <doi:10.1088/1361-6579/ab98b3>.,2021-06-29,Valeria Blesius,NA,TRUE,https://github.com/vblesius/rhrt,930,3,2021-08-16T06:22:50Z,310
rhub,"Run 'R CMD check' on any of the 'R-hub' (<https://builder.r-hub.io/>)
    architectures, from the command line. The current architectures include
    'Windows', 'macOS', 'Solaris' and various 'Linux' distributions.",2019-04-08,Gábor Csárdi,"https://github.com/r-hub/rhub, https://r-hub.github.io/rhub/",TRUE,https://github.com/r-hub/rhub,568992,306,2021-04-29T21:16:17Z,1859.450980392157
ribd,"Recursive algorithms for computing various relatedness coefficients, 
    including pairwise kinship, kappa and identity coefficients. Both autosomal 
    and X-linked coefficients are computed. Founders are allowed to be inbred, 
    enabling construction of any given kappa coefficients (Vigeland (2020) 
    <doi:10.1007/s00285-020-01505-x>). In addition to the standard pairwise 
    coefficients, ribd also computes a range of lesser-known coefficients, including
    generalised kinship coefficients (Karigl (1981) 
    <doi:10.1111/j.1469-1809.1981.tb00341.x>; Weeks and Lange (1988) 
    <https:www.ncbi.nlm.nih.gov/pmc/articles/PMC1715269>), two-locus coefficients
    (Thompson (1988) <doi:10.1093/imammb/5.4.261>) and multi-person coefficients. 
    This package is part of the ped suite, a collection of packages for pedigree 
    analysis in R.",2021-05-28,Magnus Dehli Vigeland,https://github.com/magnusdv/ribd,TRUE,https://github.com/magnusdv/ribd,10646,2,2021-08-02T18:10:02Z,5323
riceware,"The Diceware method can be used to generate strong passphrases.
  In short, you roll a 6-faced dice 5 times in a row, the number obtained is
  matched against a dictionary of easily remembered words. By combining together 7
  words thus generated, you obtain a password that is relatively easy to remember,
  but would take several millions years (on average) for a powerful computer to guess.",2015-05-21,Francois Michonneau,https://github.com/fmichonneau/riceware,TRUE,https://github.com/fmichonneau/riceware,14541,3,2021-02-04T16:22:10Z,4847
ricu,"Focused on (but not exclusive to) data sets hosted on PhysioNet
    (<https://physionet.org>), 'ricu' provides utilities for download, setup
    and access of intensive care unit (ICU) data sets. In addition to
    functions for running arbitrary queries against available data sets, a
    system for defining clinical concepts and encoding their representations
    in tabular ICU data is presented.",2021-08-18,Nicolas Bennett,"https://github.com/eth-mds/ricu, https://physionet.org",TRUE,https://github.com/eth-mds/ricu,3685,0,2021-09-03T13:58:35Z,NA
ridge,"Linear and logistic ridge regression functions. Additionally includes special functions for 
            genome-wide single-nucleotide polymorphism (SNP) data. More details can be found in
            <doi: 10.1002/gepi.21750> and <doi: 10.1186/1471-2105-12-372>.",2021-02-04,Steffen Moritz,https://github.com/SteffenMoritz/ridge,TRUE,https://github.com/steffenmoritz/ridge,79587,13,2021-01-28T02:55:26Z,6122.076923076923
riem,"Allows to get weather data from Automated Surface Observing System (ASOS) stations (airports) in the
    whole world thanks to the Iowa Environment Mesonet website.",2016-09-10,Maëlle Salmon,http://github.com/ropenscilabs/riem,TRUE,https://github.com/ropenscilabs/riem,22425,39,2021-04-20T08:31:01Z,575
Riemann,"We provide a variety of algorithms for manifold-valued data, including Fréchet summaries, hypothesis testing, clustering, visualization, and other learning tasks. See Bhattacharya and Bhattacharya (2012) <doi:10.1017/CBO9781139094764> for general exposition to statistics on manifolds.",2021-06-20,Kisung You,https://kisungyou.com/Riemann/,TRUE,https://github.com/kisungyou/riemann,4497,4,2021-06-08T19:49:04Z,1124.25
RiemBase,"We provide a number of algorithms to estimate fundamental statistics including Fréchet mean and geometric median for manifold-valued data. Also, C++ header files are contained that implement elementary operations on manifolds such as Sphere, Grassmann, and others. See Bhattacharya and Bhattacharya (2012) <doi:10.1017/CBO9781139094764> if you are interested in statistics on manifolds, and Absil et al (2007, ISBN:9780691132983) on computational aspects of optimization on matrix manifolds.",2021-08-21,Kisung You,https://github.com/kisungyou/RiemBase,TRUE,https://github.com/kisungyou/riembase,13665,2,2021-08-21T04:20:08Z,6832.5
Riex,"Retrieves efficiently and reliably Investors Exchange ('IEX') stock and market data using 'IEX Cloud API'. The platform is offered by Investors Exchange Group (IEX Group).
    Main goal is to leverage 'R' capabilities including existing packages to effectively provide financial and statistical analysis as well as visualization in support of fact-based decisions.
    In addition, continuously improve and enhance 'Riex' by applying best practices and being in tune with users' feedback and requirements.
    Please, make sure to review and acknowledge Investors Exchange Group (IEX Group) terms and conditions before using 'Riex' (<https://iexcloud.io/terms/>).",2021-04-24,Myriam Ibrahim,https://github.com/TheEliteAnalyst/Riex,TRUE,https://github.com/theeliteanalyst/riex,10902,8,2021-04-25T01:54:45Z,1362.75
riingo,"Functionality to download stock prices,
    cryptocurrency data, and more from the 'Tiingo' API
    <https://api.tiingo.com/>.",2020-09-12,Davis Vaughan,https://github.com/business-science/riingo,TRUE,https://github.com/business-science/riingo,336735,46,2020-09-14T12:18:50Z,7320.326086956522
rijkspalette,"Create colour palettes based on famous paintings. Using the 
    function rijksPalette(), you can search for any painting in the collection
    of the Dutch Rijksmuseum and generate a colour palette from it. This package 
    was developed using the fantastic Rijksmuseum API 
    <https://www.rijksmuseum.nl/api>.",2021-05-10,Erik-Jan van Kesteren,https://vankesteren.github.io/rijkspalette,TRUE,https://github.com/vankesteren/rijkspalette,12330,12,2021-05-04T07:47:22Z,1027.5
Rilostat,"Tools to download data from the ilostat database
    <https://ilostat.ilo.org> together with search and
    manipulation utilities.",2021-07-29,David Bescond,https://ilostat.github.io/Rilostat/,TRUE,https://github.com/ilostat/rilostat,21471,16,2021-07-29T11:25:10Z,1341.9375
rim,"Provides an interface to the powerful and fairly complete computer algebra system maxima.
    It can be used to start and control maxima from within R by entering 'Maxima' commands. 
    It facilitates outputting results from 'Maxima' in 'LaTeX' and 'MathML'. 2D and 3D plots can be displayed directly. This package also registers a 'knitr'-engine 
    enabling 'Maxima' code chunks to be written in 'RMarkdown' documents.",2021-07-01,Eric Stemmler,https://rcst.github.io/rim/,TRUE,https://github.com/rcst/rim,1010,3,2021-08-29T11:52:34Z,336.6666666666667
RImageJROI,"Provides functions to read 'ImageJ' (<http://imagej.nih.gov/ij/>)
    Region of Interest (ROI) files, to plot the ROIs and to convert them to
    'spatstat' (<http://spatstat.org/>) spatial patterns.",2021-03-23,David C Sterratt,https://github.com/davidcsterratt/RImageJROI,TRUE,https://github.com/davidcsterratt/rimagejroi,19262,5,2021-02-11T11:33:41Z,3852.4
ring,"Circular / ring buffers in R and C.  There are a couple
    of different buffers here with different implementations that
    represent different trade-offs.",2021-06-15,Rich FitzJohn,https://github.com/mrc-ide/ring,TRUE,https://github.com/mrc-ide/ring,27249,11,2021-06-15T08:36:07Z,2477.181818181818
RInside,"C++ classes to embed R in C++ (and C) applications
 A C++ class providing the R interpreter is offered by this package
 making it easier to have ""R inside"" your C++ application. As R itself
 is embedded into your application, a shared library build of R is
 required. This works on Linux, OS X and even on Windows provided you
 use the same tools used to build R itself. Numerous examples are
 provided in the nine subdirectories of the examples/ directory of
 the installed package: standard, 'mpi' (for parallel computing), 'qt'
 (showing how to embed 'RInside' inside a Qt GUI application), 'wt'
 (showing how to build a ""web-application"" using the Wt toolkit),
 'armadillo' (for 'RInside' use with 'RcppArmadillo'), 'eigen' (for
 'RInside' use with 'RcppEigen'), and 'c_interface' for a basic C
 interface and 'Ruby' illustration.  The examples use 'GNUmakefile(s)'
 with GNU extensions, so a GNU make is required (and will use the
 'GNUmakefile' automatically). 'Doxygen'-generated documentation of
 the C++ classes is available at the 'RInside' website as well.",2020-03-12,Dirk Eddelbuettel,http://dirk.eddelbuettel.com/code/rinside.html,TRUE,https://github.com/eddelbuettel/rinside,49803,115,2021-05-28T01:23:25Z,433.0695652173913
rintimg,"Allows the user to view an image in full screen when clicking on it in 'RMarkdown' documents and 'shiny' applications. 
  The package relies on the 'JavaScript' library 'intense-images'. See <https://tholman.com/intense-images/> for more information.",2020-09-30,Mohamed El Fodil Ihaddaden,https://github.com/feddelegrand7/rintimg,TRUE,https://github.com/feddelegrand7/rintimg,3640,3,2020-12-12T22:19:06Z,1213.3333333333333
rintrojs,"A wrapper for the 'Intro.js' library (For more info: <https://introjs.com/>). 
  This package makes it easy to include step-by-step introductions, and clickable hints in a 'Shiny' 
  application. It supports both static introductions in the UI, and programmatic introductions from 
  the server-side. ",2021-06-06,Carl Ganz,https://github.com/carlganz/rintrojs,TRUE,https://github.com/carlganz/rintrojs,77974,101,2021-06-03T15:53:47Z,772.019801980198
rio,"Streamlined data import and export by making assumptions that
    the user is probably willing to make: 'import()' and 'export()' determine
    the data structure from the file extension, reasonable defaults are used for
    data import and export (e.g., 'stringsAsFactors=FALSE'), web-based import is
    natively supported (including from SSL/HTTPS), compressed files can be read
    directly without explicit decompression, and fast import packages are used where
    appropriate. An additional convenience function, 'convert()', provides a simple
    method for converting between file types.",2021-06-21,Thomas J. Leeper,https://github.com/leeper/rio,TRUE,https://github.com/leeper/rio,7010784,489,2021-06-18T09:41:51Z,14336.981595092024
rioja,"Constrained clustering, transfer functions, and other methods for analysing Quaternary science data.",2020-10-28,Steve Juggins,"http://www.staff.ncl.ac.uk/stephen.juggins/,
https://github.com/nsj3/rioja",TRUE,https://github.com/nsj3/rioja,40611,2,2021-03-17T18:50:59Z,20305.5
rIP,"Takes an array of IPs and the keys for the services the user wishes to use (IP Hub, IP Intel, and Proxycheck), 
    and passes these to all respective APIs. Returns a dataframe with the IP addresses (used for merging), 
    country, ISP, labels for non-US IP Addresses, VPS use, and recommendations for blocking. The package 
    also provides optional visualization tools for checking the distributions. Additional
    functions are provided to call each discrete API endpoint. The package and methods are detailed in the recent paper 
    Waggoner, Kennedy, and Clifford (2019) <doi:10.21105/joss.01285>.",2019-05-29,Ryan Kennedy,http://joss.theoj.org/papers/10.21105/joss.01285,TRUE,https://github.com/mahdlab/rip,13372,22,2021-01-10T19:43:29Z,607.8181818181819
ripserr,"Ports the Ripser <arXiv:1908.02518> and Cubical Ripser
        <arXiv:2005.12692> persistent homology calculation engines from
        C++. Can be used as a rapid calculation tool in topological
        data analysis pipelines.",2020-10-20,Raoul Wadhwa,https://rrrlw.github.io/ripserr/,TRUE,https://github.com/rrrlw/ripserr,3844,3,2021-06-01T21:48:54Z,1281.3333333333333
riskParityPortfolio,"Fast design of risk parity portfolios for financial investment.
    The goal of the risk parity portfolio formulation is to equalize or distribute
    the risk contributions of the different assets, which is missing if we simply
    consider the overall volatility of the portfolio as in the mean-variance
    Markowitz portfolio. In addition to the vanilla formulation, where the risk
    contributions are perfectly equalized subject to no shortselling and budget
    constraints, many other formulations are considered that allow for box
    constraints and shortselling, as well as the inclusion of additional
    objectives like the expected return and overall variance. See vignette for
    a detailed documentation and comparison, with several illustrative examples.
    The package is based on the papers:
    Y. Feng, and D. P. Palomar (2015). SCRIP: Successive Convex Optimization Methods
    for Risk Parity Portfolio Design. IEEE Trans. on Signal Processing, vol. 63,
    no. 19, pp. 5285-5300. <doi:10.1109/TSP.2015.2452219>.
    F. Spinu (2013), An Algorithm for Computing Risk Parity Weights.
    <doi:10.2139/ssrn.2297383>.
    T. Griveau-Billion, J. Richard, and T. Roncalli (2013). A fast algorithm for computing
    High-dimensional risk parity portfolios. <arXiv:1311.4057>.",2021-06-01,Daniel P. Palomar,"https://CRAN.R-project.org/package=riskParityPortfolio,
https://github.com/dppalomar/riskParityPortfolio,
https://www.danielppalomar.com,
https://doi.org/10.1109/TSP.2015.2452219",TRUE,https://github.com/dppalomar/riskparityportfolio,21933,67,2021-06-01T05:48:08Z,327.35820895522386
RiskPortfolios,"Collection of functions designed to compute risk-based portfolios as described 
    in Ardia et al. (2017) <doi:10.1007/s10479-017-2474-7> and Ardia et al. (2017) <doi:10.21105/joss.00171>.",2021-05-16,David Ardia,https://github.com/ArdiaD/RiskPortfolios,TRUE,https://github.com/ardiad/riskportfolios,29451,32,2021-05-16T15:40:27Z,920.34375
riskRegression,"Implementation of the following methods for event history analysis.
    Risk regression models for survival endpoints also in the presence of competing
    risks are fitted using binomial regression based on a time sequence of binary
    event status variables. A formula interface for the Fine-Gray regression model
    and an interface for the combination of cause-specific Cox regression models.
    A toolbox for assessing and comparing performance of risk predictions (risk
    markers and risk prediction models). Prediction performance is measured by the
    Brier score and the area under the ROC curve for binary possibly time-dependent
    outcome. Inverse probability of censoring weighting and pseudo values are used
    to deal with right censored data. Lists of risk markers and lists of risk models
    are assessed simultaneously. Cross-validation repeatedly splits the data, trains
    the risk prediction models on one part of each split and then summarizes and
    compares the performance across splits.",2020-12-09,Thomas Alexander Gerds,https://github.com/tagteam/riskRegression,TRUE,https://github.com/tagteam/riskregression,156299,22,2021-08-04T16:11:58Z,7104.5
riskyr,"Risk-related information (like the prevalence of conditions, the sensitivity and specificity of diagnostic tests, or the effectiveness of interventions or treatments) can be expressed in terms of frequencies or probabilities. By providing a toolbox of corresponding metrics and representations, 'riskyr' computes, translates, and visualizes risk-related information in a variety of ways. Adopting multiple complementary perspectives provides insights into the interplay between key parameters and renders teaching and training programs on risk literacy more transparent. ",2021-03-23,Hansjoerg Neth,"https://CRAN.R-project.org/package=riskyr,
https://github.com/hneth/riskyr/",TRUE,https://github.com/hneth/riskyr,12977,10,2021-07-16T11:09:55Z,1297.7
ritis,"An interface to the Integrated Taxonomic Information System ('ITIS')
    (<https://www.itis.gov>). Includes functions to work with the 'ITIS' REST
    API methods (<https://www.itis.gov/ws_description.html>), as well as the
    'Solr' web service (<https://www.itis.gov/solr_documentation.html>).",2021-02-02,Scott Chamberlain,"https://github.com/ropensci/ritis (devel)
https://docs.ropensci.org/ritis/ (docs)",TRUE,https://github.com/ropensci/ritis,102709,11,2021-02-02T02:35:24Z,9337.181818181818
rivr,"A tool for undergraduate and graduate courses in open-channel
    hydraulics. Provides functions for computing normal and critical depths,
    steady-state water surface profiles (e.g. backwater curves) and unsteady flow
    computations (e.g. flood wave routing) as described in
    Koohafkan MC, Younis BA (2015). ""Open-channel computation with R.""
    The R Journal, 7(2), 249–262. <doi: 10.32614/RJ-2015-034>.",2021-01-21,Michael C Koohafkan,https://github.com/mkoohafkan/rivr,TRUE,https://github.com/mkoohafkan/rivr,17512,13,2021-01-21T05:34:13Z,1347.076923076923
rjade,"Jade is a high performance template engine heavily influenced by
    Haml and implemented with JavaScript for node and browsers.",2021-04-16,Jeroen Ooms,https://github.com/jeroen/rjade https://www.npmjs.com/package/jade,TRUE,https://github.com/jeroen/rjade,17245,5,2021-04-16T11:57:20Z,3449
rJava,"Low-level interface to Java VM very much like .C/.Call and friends. Allows creation of objects, calling methods and accessing fields.",2021-04-29,Simon Urbanek,http://www.rforge.net/rJava/,TRUE,https://github.com/s-u/rjava,7050711,210,2021-09-03T00:23:50Z,33574.81428571429
RJDemetra,"Interface around 'JDemetra+' (<https://github.com/jdemetra/jdemetra-app>), the seasonal adjustment software officially
    recommended to the members of the European Statistical System (ESS) and the European System of Central Banks.
    It offers full access to all options and outputs of 'JDemetra+', including the two leading seasonal adjustment methods
    TRAMO/SEATS+ and X-12ARIMA/X-13ARIMA-SEATS.",2021-06-07,Alain Quartier-la-Tente,https://github.com/jdemetra/rjdemetra,TRUE,https://github.com/jdemetra/rjdemetra,23964,33,2021-06-07T11:09:16Z,726.1818181818181
rjdmarkdown,"Functions to have nice 'rmarkdown' outputs of the 
  seasonal and trading day adjustment models made with 'RJDemetra'.",2020-10-01,Alain Quartier-la-Tente,https://github.com/AQLT/rjdmarkdown,TRUE,https://github.com/aqlt/rjdmarkdown,5754,3,2021-07-15T14:28:29Z,1918
rjdqa,"Add-in to the 'RJDemetra' package on seasonal adjustments.
    It allows to produce quality assessments outputs (dashboards, quality report matrix, etc.).",2020-08-06,Alain Quartier-la-Tente,https://github.com/AQLT/rjdqa,TRUE,https://github.com/aqlt/rjdqa,10102,1,2021-07-15T17:50:56Z,10102
rjqpd,"Implementation of the Johnson Quantile-Parameterised Distribution in R. 
  The Johnson Quantile-Parameterised Distribution (J-QPD) is a flexible distribution
  system that is parameterised by a symmetric percentile triplet of quantile values
  (typically the 10th-50th-90th) along with known support bounds for the distribution.
  The J-QPD system was developed by Hadlock and Bickel (2017) <doi:10.1287/deca.2016.0343>.
  This package implements the density, quantile, CDF and random number generator functions.",2020-09-25,Bobby Ingram,https://github.com/bobbyingram/rjqpd,TRUE,https://github.com/bobbyingram/rjqpd,3728,2,2020-09-28T20:17:13Z,1864
RJSDMX,"Provides functions to retrieve data and metadata from providers 
			 that disseminate data by means of SDMX web services. 
			 SDMX (Statistical Data and Metadata eXchange) is a standard that 
			 has been developed with the aim of simplifying the exchange of 
			 statistical information. 
			 More about the SDMX standard and the SDMX Web Services 
			 can be found at: <https://sdmx.org>.",2020-02-27,Attilio Mattiocco,https://github.com/amattioc/SDMX/,TRUE,https://github.com/amattioc/sdmx,22487,70,2021-06-28T13:40:18Z,321.24285714285713
rjwsacruncher,"'JDemetra+' (<https://github.com/jdemetra/jdemetra-app>) is the seasonal adjustment software officially recommended
  to the members of the European Statistical System and the European System of Central Banks. Seasonal adjustment models performed
  with 'JDemetra+' can be stored into workspaces. 'JWSACruncher' (<https://github.com/jdemetra/jwsacruncher/releases>) is a console tool that 
  re-estimates all the multi-processing defined in a workspace and to export the result. 'rjwsacruncher' allows to launch easily the 'JWSACruncher'.",2021-07-21,Alain Quartier-la-Tente,https://github.com/AQLT/rjwsacruncher,TRUE,https://github.com/aqlt/rjwsacruncher,9669,1,2021-07-21T20:48:18Z,9669
rkeops,"The 'KeOps' library lets you compute generic reductions of very 
    large arrays whose entries are given by a mathematical formula with CPU and 
    GPU computing support. It combines a tiled reduction scheme with an 
    automatic differentiation engine. It is perfectly suited to the efficient 
    computation of Kernel dot products and the associated gradients, even when 
    the full kernel matrix does not fit into the GPU memory.",2021-02-17,Benjamin Charlier,"https://www.kernel-operations.io/,
https://github.com/getkeops/keops/",TRUE,https://github.com/getkeops/keops,20255,535,2021-08-10T14:45:09Z,37.85981308411215
rKolada,"Methods for downloading and processing data and metadata from 'Kolada', the official Swedish regions and municipalities database <https://kolada.se/>.",2021-03-22,Love Hansson,"https://lchansson.github.io/rKolada/,
https://github.com/lchansson/rKolada",TRUE,https://github.com/lchansson/rkolada,3955,0,2021-04-22T11:14:27Z,NA
RKorAPClient,"A client package that makes the 'KorAP' web service API accessible from R.
  The corpus analysis platform 'KorAP' has been developed as a scientific tool to make
  potentially large, stratified and multiply annotated corpora, such as the 'German Reference Corpus DeReKo'
  or the 'Corpus of the Contemporary Romanian Language CoRoLa', accessible for linguists to let them verify
  hypotheses and to find interesting patterns in real language use.
  The 'RKorAPClient' package provides access to 'KorAP' and the corpora behind it for user-created R code,
  as a programmatic alternative to the 'KorAP' web user-interface.
  You can learn more about 'KorAP' and use it directly on 'DeReKo' at <https://korap.ids-mannheim.de/>.",2021-03-12,Marc Kupietz,"https://github.com/KorAP/RKorAPClient/,
https://korap.ids-mannheim.de/,
https://www1.ids-mannheim.de/kl/projekte/korap.html",TRUE,https://github.com/korap/rkorapclient,8435,5,2021-08-27T15:31:37Z,1687
rLakeAnalyzer,"Standardized methods for calculating common important derived
    physical features of lakes including water density based based on
    temperature, thermal layers, thermocline depth, lake number, Wedderburn
    number, Schmidt stability and others.",2019-06-09,Luke Winslow,NA,TRUE,https://github.com/gleon/rlakeanalyzer,26461,22,2021-04-29T19:16:08Z,1202.7727272727273
rlang,"A toolbox for working with base types, core R features
  like the condition system, and core 'Tidyverse' features like tidy
  evaluation.",2021-04-30,Lionel Henry,"https://rlang.r-lib.org, https://github.com/r-lib/rlang",TRUE,https://github.com/r-lib/rlang,52549875,340,2021-09-01T09:13:55Z,154558.45588235295
rlas,Read and write 'las' and 'laz' binary file formats. The LAS file format is a public file format for the interchange of 3-dimensional point cloud data between data users. The LAS specifications are approved by the American Society for Photogrammetry and Remote Sensing <https://www.asprs.org/divisions-committees/lidar-division/laser-las-file-format-exchange-activities>. The LAZ file format is an open and lossless compression scheme for binary LAS format versions 1.0 to 1.4 <https://laszip.org/>.,2021-06-02,Jean-Romain Roussel,https://github.com/Jean-Romain/rlas,TRUE,https://github.com/jean-romain/rlas,68214,23,2021-08-04T11:17:07Z,2965.8260869565215
rle,"Common 'base' and 'stats' methods for 'rle' objects, aiming to make it possible to treat them transparently as vectors.",2020-09-25,Pavel N. Krivitsky,NA,TRUE,https://github.com/statnet/rle,182349,1,2021-06-16T14:18:42Z,182349
rliger,"Uses an extension of nonnegative matrix factorization to identify shared and dataset-specific factors. See Welch J, Kozareva V, et al (2019) <doi:10.1016/j.cell.2019.05.006>, and Liu J, Gao C, Sodicoff J, et al (2020) <doi:10.1038/s41596-020-0391-8> for more details.",2021-04-19,Joshua Welch,https://github.com/welch-lab/liger,TRUE,https://github.com/welch-lab/liger,4568,206,2021-09-02T18:32:56Z,22.174757281553397
rlist,"Provides a set of functions for data manipulation with
    list objects, including mapping, filtering, grouping, sorting,
    updating, searching, and other useful functions. Most functions
    are designed to be pipeline friendly so that data processing with
    lists can be chained.",2021-09-03,Kun Ren,"https://renkun-ken.github.io/rlist/,
https://github.com/renkun-ken/rlist,
https://renkun-ken.github.io/rlist-tutorial/",TRUE,https://github.com/renkun-ken/rlist,3645429,157,2021-09-02T23:37:04Z,23219.292993630574
rlog,"A very lightweight package that writes out log messages in an opinionated way.
  Simpler and lighter than other logging packages, 'rlog' provides a compact feature set that
  focuses on getting the job done in a Unix-like way.",2021-02-24,Mark Sellors,https://github.com/sellorm/rlog,TRUE,https://github.com/sellorm/rlog,1994,8,2021-06-17T04:39:52Z,249.25
RLumCarlo,"A collection of functions to simulate luminescence production in 
    dosimetric materials using Monte Carlo methods. Implemented are models for 
    delocalised transitions (e.g., Chen and McKeever (1997) <doi:10.1142/2781>), 
    localised transitions (e.g., Pagonis et al. (2019) <doi:10.1016/j.jlumin.2018.11.024>) 
    and tunnelling transitions (Jain et al. (2012) <doi:10.1088/0953-8984/24/38/385402> 
    and Pagonis et al. (2019) <doi:10.1016/j.jlumin.2018.11.024>). 
    Supported stimulation methods are thermal luminescence (TL), 
    continuous-wave optically stimulated luminescence (CW-OSL), 
    linearly-modulated optically stimulated luminescence (LM-OSL), 
    linearly-modulated infrared stimulated luminescence (LM-IRSL),
    and isothermal luminescence (ITL or ISO-TL).",2020-12-10,Johannes Friedrich,https://CRAN.R-project.org/package=RLumCarlo,TRUE,https://github.com/r-lum/rlumcarlo,5358,1,2021-01-04T17:15:00Z,5358
RLumModel,A collection of functions to simulate luminescence signals in quartz and Al2O3 based on published models.,2021-04-07,Johannes Friedrich,https://CRAN.R-project.org/package=RLumModel,TRUE,https://github.com/r-lum/rlummodel,21091,3,2021-04-06T17:05:43Z,7030.333333333333
RLumShiny,"A collection of 'shiny' applications for the R package
    'Luminescence'. These mainly, but not exclusively, include applications for
    plotting chronometric data from e.g. luminescence or radiocarbon dating. It
    further provides access to bootstraps tooltip and popover functionality and
    contains the 'jscolor.js' library with a custom 'shiny' output binding.",2019-01-11,Christoph Burow,https://tzerk.github.io/RLumShiny/,TRUE,https://github.com/tzerk/rlumshiny,27661,5,2021-03-03T14:04:02Z,5532.2
rmapshaper,"Edit and simplify 'geojson', 'Spatial', and 'sf'
    objects.  This is wrapper around the 'mapshaper' 'JavaScript' library
    by Matthew Bloch <https://github.com/mbloch/mapshaper/> to perform
    topologically-aware polygon simplification, as well as other
    operations such as clipping, erasing, dissolving, and converting
    'multi-part' to 'single-part' geometries.  It relies on the
    'geojsonio' package for working with 'geojson' objects, the 'sf'
    package for working with 'sf' objects, and the 'sp' and 'rgdal'
    packages for working with 'Spatial' objects.",2021-05-26,Andy Teucher,https://github.com/ateucher/rmapshaper,TRUE,https://github.com/ateucher/rmapshaper,224862,143,2021-05-26T21:38:08Z,1572.4615384615386
rmapzen,"Provides an interface to 'Mapzen'-based APIs (including 
    geocode.earth, Nextzen, and NYC GeoSearch) for geographic search 
    and geocoding, isochrone calculation, and vector data to draw map tiles. 
    See <https://www.mapzen.com/documentation/> for more information. The original 
    Mapzen has gone out of business, but 'rmapzen' can be set up to work with 
    any provider who implements the Mapzen API. ",2021-06-18,Tarak Shah,https://tarakc02.github.io/rmapzen/,TRUE,https://github.com/tarakc02/rmapzen,17914,35,2021-06-17T21:20:04Z,511.8285714285714
rmarchingcubes,"A port of the C++ routine for applying the marching cubes algorithm written by 
    Thomas Lewiner et al. (2012) <doi:10.1080/10867651.2003.10487582> into an R package. 
    The package supplies the contour3d() function, which takes a 3-dimensional array of voxel 
    data and calculates the vertices, vertex normals, and faces for a 3d mesh representing 
    the contour(s) at a given level.",2021-06-16,S. H. Wilks,https://github.com/shwilks/rmarchingcubes,TRUE,https://github.com/shwilks/rmarchingcubes,1038,3,2021-06-17T08:25:13Z,346
RMariaDB,"Implements a 'DBI'-compliant interface to 'MariaDB'
    (<https://mariadb.org/>) and 'MySQL' (<https://www.mysql.com/>)
    databases.",2021-04-13,Kirill Müller,"https://rmariadb.r-dbi.org, https://github.com/r-dbi/RMariaDB,
https://downloads.mariadb.org/connector-c/",TRUE,https://github.com/r-dbi/rmariadb,1219395,95,2021-08-30T00:25:02Z,12835.736842105263
rmarkdown,Convert R Markdown documents into a variety of formats.,2021-08-06,Yihui Xie,"https://github.com/rstudio/rmarkdown,
https://pkgs.rstudio.com/rmarkdown/",TRUE,https://github.com/rstudio/rmarkdown,22531214,2213,2021-09-01T16:23:50Z,10181.298689561681
rmatio,"Read and write 'Matlab' MAT files from R. The 'rmatio'
    package supports reading MAT version 4, MAT version 5 and MAT
    compressed version 5. The 'rmatio' package can write version 5 MAT
    files and version 5 files with variable compression.",2021-06-14,Stefan Widgren,https://github.com/stewid/rmatio,TRUE,https://github.com/stewid/rmatio,102091,8,2021-06-15T16:31:14Z,12761.375
RMAWGEN,"S3 and S4 functions are implemented for spatial multi-site
    stochastic generation of daily time series of temperature and
    precipitation. These tools make use of Vector AutoRegressive models (VARs).
    The weather generator model is then saved as an object and is calibrated by
    daily instrumental ""Gaussianized"" time series through the 'vars' package
    tools. Once obtained this model, it can it can be used for weather
    generations and be adapted to work with several climatic monthly time
    series.",2019-12-12,Emanuele Cordano,"https://github.com/ecor/RMAWGEN,
https://docs.google.com/file/d/0B66otCUk3Bv6V3RPbm1mUG4zVHc/edit,
http://presentations.copernicus.org/EGU2012-14026_presentation.pdf,
http://presentations.copernicus.org/EGU2012-5404_presentation.pdf",TRUE,https://github.com/ecor/rmawgen,31890,1,2021-08-18T07:22:32Z,31890
rmdcev,"Estimates and simulates Kuhn-Tucker demand models with individual heterogeneity. The package implements the multiple-discrete continuous extreme value (MDCEV) model and the Kuhn-Tucker specification common in the environmental economics literature on recreation demand. Latent class and random parameters specifications can be implemented and the models are fit using maximum likelihood estimation or Bayesian estimation. All models are implemented in Stan, which is a C++ package for performing full Bayesian inference (see Stan Development Team, 2019) <https://mc-stan.org/>. The package also implements demand forecasting (Pinjari and Bhat (2011) <https://repositories.lib.utexas.edu/handle/2152/23880>) and welfare calculation (Lloyd-Smith (2018) <doi:10.1016/j.jocm.2017.12.002>) for policy simulation.",2020-09-30,Patrick Lloyd-Smith,https://github.com/plloydsmith/rmdcev,TRUE,https://github.com/plloydsmith/rmdcev,11054,8,2021-08-28T21:08:26Z,1381.75
rmdfiltr,"A collection of 'Lua' filters that extend the functionality
  of R Markdown templates (e.g., count words or post-process citations).",2020-11-25,Frederik Aust,https://github.com/crsh/rmdfiltr,TRUE,https://github.com/crsh/rmdfiltr,26601,38,2020-11-25T10:05:54Z,700.0263157894736
rmdformats,"HTML formats and templates for 'rmarkdown' documents, with some extra
    features such as automatic table of contents, lightboxed figures, dynamic
    crosstab helper.",2021-04-19,Julien Barnier,https://github.com/juba/rmdformats,TRUE,https://github.com/juba/rmdformats,106170,558,2021-08-25T12:42:10Z,190.2688172043011
rMEA,"A suite of tools useful to read, visualize and export bivariate motion energy time-series. Lagged synchrony between subjects can be analyzed through windowed cross-correlation. Surrogate data generation allows an estimation of pseudosynchrony that helps to estimate the effect size of the observed synchronization. Ramseyer & Tschacher (2011) <doi:10.1037/a0023419>.",2020-08-04,Johann R. Kleinbub,https://github.com/kleinbub/rMEA https://www.psync.ch,TRUE,https://github.com/kleinbub/rmea,16973,5,2021-06-10T09:16:13Z,3394.6
rmetalog,"Implementation of the metalog distribution in R.
    The metalog distribution is a modern, highly flexible, data-driven distribution. 
    Metalogs are developed by Keelin (2016) <doi:10.1287/deca.2016.0338>.
    This package provides functions to build these distributions from raw data. 
    Resulting metalog objects are then useful for exploratory and probabilistic analysis.",2021-01-25,Isaac Faber,NA,TRUE,https://github.com/isaacfab/rmetalog,14988,19,2021-01-25T18:35:03Z,788.8421052631579
rMIDAS,"A tool for multiply imputing missing data using 'MIDAS', a deep learning method based on denoising autoencoder neural networks. This algorithm offers significant accuracy and efficiency advantages over other multiple imputation strategies, particularly when applied to large datasets with complex features. Alongside interfacing with 'Python' to run the core algorithm, this package contains functions for processing data before and after model training, running imputation model diagnostics, generating multiple completed datasets, and estimating regression models on these datasets.",2021-01-30,Thomas Robinson,https://github.com/MIDASverse/rMIDAS,TRUE,https://github.com/midasverse/rmidas,4302,15,2021-05-16T10:36:57Z,286.8
rminizinc,"Constraint optimization, or constraint programming, is the name given to identifying
  feasible solutions out of a very large set of candidates, where the problem can be modeled in terms 
  of arbitrary constraints. 'MiniZinc' is a free and open-source constraint modeling language. 
  Constraint satisfaction and discrete optimization problems can be formulated in a high-level 
  modeling language. Models are compiled into an intermediate representation that is understood by a
  wide range of solvers. 'MiniZinc' itself provides several solvers, for instance 'GeCode'. R users 
  can use the package to solve constraint programming problems without using 'MiniZinc' directly, 
  modify existing 'MiniZinc' models and also create their own models.",2021-08-04,Akshit Achara,https://github.com/acharaakshit/RMiniZinc,TRUE,https://github.com/acharaakshit/rminizinc,3780,6,2021-08-03T21:58:40Z,630
rmio,"Provides header files of 'mio', a cross-platform C++11 header-only 
    library for memory mapped file IO <https://github.com/mandreyel/mio>.",2021-06-29,Florian Privé,https://github.com/privefl/rmio,TRUE,https://github.com/privefl/rmio,54541,4,2021-06-29T11:22:47Z,13635.25
RMixtComp,"Mixture Composer <https://github.com/modal-inria/MixtComp> is a project to build mixture models with
    heterogeneous data sets and partially missing data management. 
    It includes 8 models for real, categorical, counting, functional and ranking data.",2021-03-29,Vincent Kubicki,https://github.com/modal-inria/MixtComp,TRUE,https://github.com/modal-inria/mixtcomp,10993,7,2021-03-29T18:20:21Z,1570.4285714285713
RMixtCompIO,"Mixture Composer <https://github.com/modal-inria/MixtComp> is a project to build mixture models with
    heterogeneous data sets and partially missing data management. 
    It includes models for real, categorical, counting, functional and ranking data.
    This package contains the minimal R interface of the C++ 'MixtComp' library.",2020-11-20,Vincent Kubicki,"https://github.com/modal-inria/MixtComp,
https://massiccc.lille.inria.fr/",TRUE,https://github.com/modal-inria/mixtcomp,10923,7,2021-03-29T18:20:21Z,1560.4285714285713
RMixtCompUtilities,"Mixture Composer <https://github.com/modal-inria/MixtComp> is a project to build mixture models with
    heterogeneous data sets and partially missing data management. This package contains graphical, getter and some utility 
    functions to facilitate the analysis of 'MixtComp' output.",2020-11-13,Vincent Kubicki,"https://github.com/modal-inria/MixtComp,
https://massiccc.lille.inria.fr/",TRUE,https://github.com/modal-inria/mixtcomp,11161,7,2021-03-29T18:20:21Z,1594.4285714285713
RMLPCA,"R implementation of Maximum Likelihood Principal Component Analysis
    The main idea of this package is to have an alternative way of PCA for 
    subspace modeling that considers measurement errors.
    More details can be found in Peter D. Wentzell (2009) 
    <doi:10.1016/B978-0-444-64165-6.03029-9>.",2020-11-05,Renan Santos Barbosa,https://github.com/renanestatcamp/RMLPCA,TRUE,https://github.com/renanestatcamp/rmlpca,3235,1,2021-05-31T19:22:13Z,3235
rmonad,"
    A monadic solution to pipeline analysis. All operations -- and the errors,
    warnings and messages they emit -- are merged into a directed graph. Infix
    binary operators mediate when values are stored, how exceptions are
    handled, and where pipelines branch and merge. The resulting structure may
    be queried for debugging or report generation. 'rmonad' complements, rather
    than competes with, non-monadic pipeline packages like 'magrittr' or
    'pipeR'. This work is funded by the NSF (award number 1546858).",2020-02-14,Zebulun Arendsee,https://github.com/arendsee/rmonad,TRUE,https://github.com/arendsee/rmonad,15788,59,2020-12-14T21:41:46Z,267.59322033898303
rmoo,"A multiobjective optimization package based on K. Deb's 
    algorithm and inspired in 'GA' package by Luca Scrucca (2017) <DOI:10.32614/RJ-2017-008>. 
    The 'rmoo' package is a framework for multi- and many-objective optimization, 
    allowing to work with representation of real numbers, permutations and 
    binaries, offering a high range of configurations.",2021-02-05,Francisco Benitez,https://github.com/Evolutionary-Optimization-Laboratory/rmoo/,TRUE,https://github.com/evolutionary-optimization-laboratory/rmoo,3784,16,2021-07-17T20:04:02Z,236.5
rms,"Regression modeling, testing, estimation, validation,
	graphics, prediction, and typesetting by storing enhanced model design
	attributes in the fit.  'rms' is a collection of functions that
	assist with and streamline modeling.  It also contains functions for
	binary and ordinal logistic regression models, ordinal models for
  continuous Y with a variety of distribution families, and the Buckley-James
	multiple regression model for right-censored responses, and implements
	penalized maximum likelihood estimation for logistic and ordinary
	linear models.  'rms' works with almost any regression model, but it
	was especially written to work with binary or ordinal regression
	models, Cox regression, accelerated failure time models,
	ordinary linear models,	the Buckley-James model, generalized least
	squares for serially or spatially correlated observations, generalized
	linear models, and quantile regression.",2021-03-18,Frank E Harrell Jr,"https://hbiostat.org/R/rms/, https://github.com/harrelfe/rms",TRUE,https://github.com/harrelfe/rms,1003074,115,2021-08-03T02:21:16Z,8722.382608695652
rmumps,"Some basic features of 'MUMPS' (Multifrontal Massively Parallel
         sparse direct Solver) are wrapped in a class whose methods can be used
         for sequentially solving a sparse linear system (symmetric or not)
         with one or many right hand sides (dense or sparse).
         There is a possibility to do separately symbolic analysis,
         LU (or LDL^t) factorization and system solving.
         Third part ordering libraries are included and can be used: 'PORD', 'METIS', 'SCOTCH'.
         'MUMPS' method was first described in Amestoy et al. (2001) <doi:10.1137/S0895479899358194>
         and Amestoy et al. (2006) <doi:10.1016/j.parco.2005.07.004>.",2021-08-12,Serguei Sokol,"http://mumps.enseeiht.fr/, https://github.com/sgsokol/rmumps/",TRUE,https://github.com/sgsokol/rmumps,23707,8,2021-08-11T10:37:23Z,2963.375
rMVP,"A memory-efficient, visualize-enhanced, parallel-accelerated Genome-Wide Association Study (GWAS) tool. It can
    (1) effectively process large data, 
    (2) rapidly evaluate population structure, 
    (3) efficiently estimate variance components several algorithms, 
    (4) implement parallel-accelerated association tests of markers three methods, 
    (5) globally efficient design on GWAS process computing, 
    (6) enhance visualization of related information. 
    'rMVP' contains three models GLM (Alkes Price (2006) <DOI:10.1038/ng1847>), MLM (Jianming Yu (2006) <DOI:10.1038/ng1702>) 
    and FarmCPU (Xiaolei Liu (2016) <doi:10.1371/journal.pgen.1005767>); variance components estimation methods EMMAX 
    (Hyunmin Kang (2008) <DOI:10.1534/genetics.107.080101>;), FaSTLMM (method: Christoph Lippert (2011) <DOI:10.1038/nmeth.1681>, 
    R implementation from 'GAPIT2': You Tang and Xiaolei Liu (2016) <DOI:10.1371/journal.pone.0107684> and 
    'SUPER': Qishan Wang and Feng Tian (2014) <DOI:10.1371/journal.pone.0107684>), and HE regression 
    (Xiang Zhou (2017) <DOI:10.1214/17-AOAS1052>).",2021-04-18,Xiaolei Liu,https://github.com/xiaolei-lab/rMVP,TRUE,https://github.com/xiaolei-lab/rmvp,13539,154,2021-08-30T12:28:13Z,87.91558441558442
rmweather,"An integrated set of tools to allow data users to conduct 
    meteorological normalisation on air quality data. This meteorological 
    normalisation technique uses predictive random forest models to remove 
    variation of pollutant concentrations so trends and interventions can be 
    explored in a robust way. For examples, see Grange et al. (2018) 
    <doi:10.5194/acp-18-6223-2018> and Grange and Carslaw (2019) 
    <doi:10.1016/j.scitotenv.2018.10.344>.",2020-06-15,Stuart K. Grange,https://github.com/skgrange/rmweather,TRUE,https://github.com/skgrange/rmweather,14478,19,2021-05-21T10:20:10Z,762
RMySQL,"Legacy 'DBI' interface to 'MySQL' / 'MariaDB' based on old code
    ported from S-PLUS. A modern 'MySQL' client based on 'Rcpp' is available 
    from the 'RMariaDB' package.",2021-06-22,Jeroen Ooms,https://downloads.mariadb.org/connector-c/ (upstream),TRUE,https://github.com/r-dbi/rmysql,2623710,194,2021-06-22T15:36:58Z,13524.278350515464
rnbp,"Use the <http://api.nbp.pl/> API through R. Retrieve
    currency exchange rates and gold prices data published by the
    National Bank of Poland in form of convenient R objects.",2021-06-07,Ryszard Szymanski,NA,TRUE,https://github.com/szymanskir/rnbp,9515,0,2021-06-06T13:52:15Z,NA
RNifti,"Provides very fast read and write access to images stored in the
    NIfTI-1, NIfTI-2 and ANALYZE-7.5 formats, with seamless synchronisation
    of in-memory image objects between compiled C and interpreted R code. Also
    provides a simple image viewer, and a C/C++ API that can be used by other
    packages. Not to be confused with 'RNiftyReg', which performs image
    registration and applies spatial transformations.",2021-05-13,Jon Clayden,https://github.com/jonclayden/RNifti,TRUE,https://github.com/jonclayden/rnifti,104085,30,2021-06-30T13:45:34Z,3469.5
RNiftyReg,"Provides an 'R' interface to the 'NiftyReg' image registration tools
    <https://github.com/KCL-BMEIS/niftyreg>. Linear and nonlinear registration
    are supported, in two and three dimensions.",2020-09-12,Jon Clayden,https://github.com/jonclayden/RNiftyReg,TRUE,https://github.com/jonclayden/rniftyreg,34619,29,2020-09-22T10:04:05Z,1193.7586206896551
Rnightly,"Allows the user to implement a dark/light toggle mode in 'shiny' using the 'Nightly' 'JavaScript' library. 
    The default mode is dark/light however the user can also specify other colours.",2020-06-25,Mohamed El Fodil Ihaddaden,https://github.com/feddelegrand7/Rnightly,TRUE,https://github.com/feddelegrand7/rnightly,4077,17,2020-12-12T11:26:21Z,239.8235294117647
Rnmr1D,Perform the complete processing of a set of proton nuclear magnetic resonance spectra from the free induction decay (raw data) and based on a processing sequence (macro-command file). An additional file specifies all the spectra to be considered by associating their sample code as well as the levels of experimental factors to which they belong. More detail can be found in Jacob et al. (2017) <doi:10.1007/s11306-017-1178-y>.,2019-12-06,Daniel Jacob,https://github.com/INRA/Rnmr1D,TRUE,https://github.com/inra/rnmr1d,11716,6,2021-08-12T15:03:36Z,1952.6666666666667
rnn,"Implementation of a Recurrent Neural Network architectures in native R, including Long Short-Term Memory (Hochreiter and Schmidhuber, <doi:10.1162/neco.1997.9.8.1735>), Gated Recurrent Unit (Chung et al., <arXiv:1412.3555>) and vanilla RNN.",2020-07-03,Bastiaan Quast,"http://qua.st/rnn, https://github.com/bquast/rnn",TRUE,https://github.com/bquast/rnn,31753,65,2021-05-08T18:27:36Z,488.5076923076923
rnoaa,"Client for many 'NOAA' data sources including the 'NCDC' climate
    'API' at <https://www.ncdc.noaa.gov/cdo-web/webservices/v2>, with functions for
    each of the 'API' 'endpoints': data, data categories, data sets, data types,
    locations, location categories, and stations. In addition, we have an interface
    for 'NOAA' sea ice data, the 'NOAA' severe weather inventory, 'NOAA' Historical
    Observing 'Metadata' Repository ('HOMR') data, 'NOAA' storm data via 'IBTrACS',
    tornado data via the 'NOAA' storm prediction center, and more.",2021-05-19,Scott Chamberlain,"https://docs.ropensci.org/rnoaa/ (docs),
https://github.com/ropensci/rnoaa (devel)",TRUE,https://github.com/ropensci/rnoaa,96096,283,2021-07-14T15:49:51Z,339.5618374558304
rnrfa,"Utility functions to retrieve data from the UK National River Flow
  Archive (<https://nrfa.ceh.ac.uk/>, terms and conditions:
  <https://nrfa.ceh.ac.uk/costs-terms-and-conditions>).
  The package contains R wrappers to the UK NRFA data temporary-API. There are
  functions to retrieve stations falling in a bounding box, to generate a map
  and extracting time series and general information. The package is fully
  described in Vitolo et al (2016) ""rnrfa: An R package to Retrieve, Filter and
  Visualize Data from the UK National River Flow Archive""
  <https://journal.r-project.org/archive/2016/RJ-2016-036/RJ-2016-036.pdf>.",2021-02-15,Claudia Vitolo,http://cvitolo.github.io/rnrfa/,TRUE,https://github.com/cvitolo/rnrfa,26789,10,2021-02-15T15:51:08Z,2678.9
Rnumerai,"Routines to interact with the Numerai Machine Learning Tournament
  API <https://numer.ai>. The functionality includes the ability to automatically download the
  current tournament data, submit predictions, and to get information for your
  user. General 'GraphQL' queries can also be executed.",2021-08-21,Eric Hare,https://github.com/Omni-Analytics-Group/Rnumerai,TRUE,https://github.com/omni-analytics-group/rnumerai,17528,30,2021-08-20T15:57:26Z,584.2666666666667
Rnvd3,"Creates JavaScript charts with the 'nvd3' library. So far only the multibar chart, the horizontal multibar chart, the line chart and the line chart with focus are available.",2021-09-02,Stéphane Laurent,https://github.com/stla/Rnvd3,TRUE,https://github.com/stla/rnvd3,0,0,2021-09-01T12:37:33Z,NA
roadoi,"This web client interfaces Unpaywall <https://unpaywall.org/products/api>, formerly
    oaDOI, a service finding free full-texts of academic papers by linking DOIs with 
    open access journals and repositories. It provides unified access to various data sources 
    for open access full-text links including Crossref and the Directory of Open Access 
    Journals (DOAJ). API usage is free and no registration is required.",2021-07-29,Najko Jahn,"https://docs.ropensci.org/roadoi/,
https://github.com/ropensci/roadoi/",TRUE,https://github.com/ropensci/roadoi,20483,58,2021-07-29T11:50:22Z,353.1551724137931
roahd,"A collection of methods for the robust analysis of univariate and
    multivariate functional data, possibly in high-dimensional cases, and hence
    with attention to computational efficiency and simplicity of use. See the R 
    Journal publication of Ieva et al. (2019) <doi:10.32614/RJ-2019-032> for an 
    in-depth presentation of the 'roahd' package.",2020-08-24,Nicholas Tarabelloni,"https://astamm.github.io/roahd/, https://github.com/astamm/roahd",TRUE,https://github.com/astamm/roahd,14812,0,2021-01-29T23:13:05Z,NA
robin,"Assesses the robustness of the community structure of a network found by one or more community detection algorithm to give indications about their reliability. It detects if the community structure found by a set of algorithms is statistically significant and compares the different selected detection algorithms on the same network. robin helps to choose among different community detection algorithms the one that better fits the network of interest. Reference in Policastro V., Righelli D., Carissimo A., Cutillo L., De Feis I. (2021) <arXiv:2102.03106>.",2021-02-09,Valeria Policastro,https://github.com/ValeriaPolicastro/robin,TRUE,https://github.com/valeriapolicastro/robin,7248,5,2021-06-24T08:20:27Z,1449.6
RobinHood,"Execute API calls to the RobinHood <https://robinhood.com> investing platform. Functionality includes accessing account data and current holdings, retrieving investment statistics and quotes, placing and canceling orders, getting market trading hours, searching investments by popular tag, and interacting with watch lists.",2021-03-01,Joseph Blubaugh,https://github.com/JestonBlu/RobinHood,TRUE,https://github.com/jestonblu/robinhood,16360,35,2021-06-27T15:12:55Z,467.42857142857144
RoBMA,"A framework for estimating ensembles of meta-analytic models
    (assuming either presence or absence of the effect, heterogeneity, and
    publication bias). The RoBMA framework uses Bayesian model-averaging to 
    combine the competing meta-analytic models into a model ensemble, weights 
    the posterior parameter distributions based on posterior model probabilities 
    and uses Bayes factors to test for the presence or absence of the
    individual components (e.g., effect vs. no effect; Bartoš et al., 2021, 
    <doi:10.31234/osf.io/kvsp7>; Maier, Bartoš & Wagenmakers, in press, 
    <doi:10.31234/osf.io/u4cns>). Users can define a wide range of non-informative 
    or informative prior distributions for the effect size, heterogeneity, 
    and publication bias components (including selection models and PET-PEESE). 
    The package provides convenient functions for summary, visualizations, and 
    fit diagnostics.",2021-07-08,František Bartoš,https://fbartos.github.io/RoBMA/,TRUE,https://github.com/fbartos/robma,6971,1,2021-08-26T05:56:28Z,6971
RobMixReg,"Finite mixture models are a popular technique for modelling unobserved heterogeneity or to approximate general distribution functions in a semi-parametric way. They are used in a lot of different areas such as astronomy, biology, economics, marketing or medicine.
             This package is the implementation of popular robust mixture regression methods based on different algorithms including: fleximix, finite mixture models and latent class regression; CTLERob, component-wise adaptive trimming likelihood estimation; mixbi, bi-square estimation; mixL, Laplacian distribution; mixt, t-distribution; TLE, trimmed likelihood estimation.
             The implemented algorithms includes:  CTLERob stands for Component-wise adaptive Trimming Likelihood Estimation based mixture regression; mixbi stands for mixture regression based on bi-square estimation; mixLstands for mixture regression based on Laplacian distribution; TLE stands for Trimmed Likelihood Estimation based mixture regression. For more detail of the algorithms, please refer to below references. 
             Reference: Chun Yu, Weixin Yao, Kun Chen (2017) <doi:10.1002/cjs.11310>.
             NeyKov N, Filzmoser P, Dimova R et al. (2007) <doi:10.1016/j.csda.2006.12.024>.
             Bai X, Yao W. Boyer JE (2012) <doi:10.1016/j.csda.2012.01.016>.
             Wennan Chang, Xinyu Zhou, Yong Zang, Chi Zhang, Sha Cao (2020) <arXiv:2005.11599>.",2020-08-05,Wennan Chang,https://changwn.github.io/RobMixReg/,TRUE,https://github.com/changwn/robmixreg,7393,0,2021-01-20T19:52:16Z,NA
robotstxt,"Provides functions to download and parse 'robots.txt' files.
        Ultimately the package makes it easy to check if bots
        (spiders, crawler, scrapers, ...) are allowed to access specific
        resources on a domain.",2020-09-03,Peter Meissner,"https://docs.ropensci.org/robotstxt/,
https://github.com/ropensci/robotstxt",TRUE,https://github.com/ropensci/robotstxt,51965,55,2020-09-03T19:06:12Z,944.8181818181819
robregcc,"We implement the algorithm estimating the parameters of the robust regression model with compositional covariates. The model simultaneously treats outliers and provides reliable parameter  estimates. Publication reference: Mishra, A., Mueller, C.,(2019) <arXiv:1909.04990>.  ",2020-07-25,Aditya Mishra,"https://arxiv.org/abs/1909.04990,
https://github.com/amishra-stats/robregcc",TRUE,https://github.com/amishra-stats/robregcc,9039,4,2021-04-30T04:09:54Z,2259.75
robsel,"An implementation of algorithms for estimation of the graphical lasso regularization parameter described in Pedro Cisneros-Velarde, Alexander Petersen and Sang-Yun Oh (2020) <http://proceedings.mlr.press/v108/cisneros20a.html>.",2021-05-25,Chau Tran,NA,TRUE,https://github.com/dddlab/robust-selection,1102,2,2021-05-25T16:51:08Z,551
robservable,"Allows loading and displaying an Observable notebook (online JavaScript  
    notebooks powered by <https://observablehq.com>) as an HTML Widget in an R 
    session, 'shiny' application or 'rmarkdown' document.",2020-09-30,Julien Barnier,https://juba.github.io/robservable/,TRUE,https://github.com/juba/robservable,4123,114,2021-08-31T09:18:18Z,36.166666666666664
robumeta,"Functions for conducting robust variance estimation (RVE) meta-regression using both large and small sample RVE estimators under various weighting schemes. These methods are distribution free and provide valid point estimates, standard errors and hypothesis tests even when the degree and structure of dependence between effect sizes is unknown. Also included are functions for conducting sensitivity analyses under correlated effects weighting and producing RVE-based forest plots. ",2017-05-29,Zachary Fisher,https://github.com/zackfisher/robumeta,TRUE,https://github.com/zackfisher/robumeta,80001,4,2021-02-22T15:37:02Z,20000.25
robustarima,"Functions for fitting a linear regression model with ARIMA
  errors using a filtered tau-estimate.",2021-04-30,Stephen Kaluzny,https://github.com/spkaluzny/robustarima,TRUE,https://github.com/spkaluzny/robustarima,22465,0,2021-04-26T00:01:55Z,NA
robustlmm,"A method to fit linear mixed effects models robustly.
    Robustness is achieved by modification of the scoring equations
    combined with the Design Adaptive Scale approach.",2021-05-24,Manuel Koller,https://github.com/kollerma/robustlmm,TRUE,https://github.com/kollerma/robustlmm,72815,18,2021-05-24T10:35:51Z,4045.277777777778
robvis,"Helps users in quickly visualizing risk-of-bias 
    assessments performed as part of a systematic review. It allows users to 
    create weighted bar-plots of the distribution of risk-of-bias judgments 
    within each bias domain, in addition to traffic-light plots of the 
    specific domain-level judgments for each study. The resulting figures are 
    of publication quality and are formatted according the risk-of-bias 
    assessment tool use to perform the assessments. Currently, the supported 
    tools are ROB2.0 (for randomized controlled trials; Sterne et al (2019)  
    <doi:10.1136/bmj.l4898>), ROBINS-I (for non-randomised studies of 
    interventions; Sterne et al (2016) <doi:10.1136/bmj.i4919>), and QUADAS-2 
    (for diagnostic accuracy studies; Whiting et al (2011) 
    <doi:10.7326/0003-4819-155-8-201110180-00009>).",2019-11-22,Luke McGuinness,https://github.com/mcguinlu/robvis,TRUE,https://github.com/mcguinlu/robvis,14027,33,2021-06-04T15:22:11Z,425.06060606060606
ROCket,"A set of functions for receiver operating characteristic (ROC) curve 
    estimation and area under the curve (AUC) calculation.
    All functions are designed to work with aggregated data; 
    nevertheless, they can also handle raw samples. 
    In 'ROCket', we distinguish two types of ROC curve representations:
    1) parametric curves - the true positive rate (TPR) and the false positive rate (FPR) 
    are functions of a parameter (the score),
    2) functions - TPR is a function of FPR.
    There are several ROC curve estimation methods available. An introduction to 
    the mathematical background of the implemented methods (and much more) can be found in 
    de Zea Bermudez, Gonçalves, Oliveira & Subtil (2014) <https://www.ine.pt/revstat/pdf/rs140101.pdf> 
    and Cai & Pepe (2004) <doi:10.1111/j.0006-341X.2004.00200.x>.",2021-02-17,Daniel Lazar,https://github.com/da-zar/ROCket,TRUE,https://github.com/da-zar/rocket,2382,1,2021-02-24T22:19:12Z,2382
roclang,"Efficient diffusing of content across function documentations. Sections, parameters or dot parameters are extracted from function documentations and turned into valid Rd character strings, which are ready to diffuse into the 'roxygen' comments of another function by inserting inline code. ",2021-08-04,Xiurui Zhu,https://github.com/zhuxr11/roclang,TRUE,https://github.com/zhuxr11/roclang,343,0,2021-08-26T15:25:22Z,NA
ROCR,"ROC graphs, sensitivity/specificity curves, lift charts,
  and precision/recall plots are popular examples of trade-off
  visualizations for specific pairs of performance measures. ROCR is a
  flexible tool for creating cutoff-parameterized 2D performance curves
  by freely combining two from over 25 performance measures (new
  performance measures can be added using a standard interface).
  Curves from different cross-validation or bootstrapping runs can be
  averaged by different methods, and standard deviations, standard
  errors or box plots can be used to visualize the variability across
  the runs. The parameterization can be visualized by printing cutoff
  values at the corresponding curve positions, or by coloring the
  curve according to cutoff. All components of a performance plot can
  be quickly adjusted using a flexible parameter dispatching
  mechanism. Despite its flexibility, ROCR is easy to use, with only
  three commands and reasonable default values for all optional
  parameters.",2020-05-02,Felix G.M. Ernst,http://ipa-tys.github.io/ROCR/,TRUE,https://github.com/ipa-tys/rocr,3007133,26,2020-12-04T17:28:07Z,115658.96153846153
rocTree,"Receiver Operating Characteristic (ROC)-guided survival trees and ensemble algorithms are implemented, providing a unified framework for tree-structured analysis with censored survival outcomes. A time-invariant partition scheme on the survivor population was considered to incorporate time-dependent covariates. Motivated by ideas of randomized tests, generalized time-dependent ROC curves were used to evaluate the performance of survival trees and establish the optimality of the target hazard/survival function. The optimality of the target hazard function motivates us to use a weighted average of the time-dependent area under the curve (AUC) on a set of time points to evaluate the prediction performance of survival trees and to guide splitting and pruning. A detailed description of the implemented methods can be found in Sun et al. (2019) <arXiv:1809.05627>.",2020-08-01,Sy Han Chiou,http://github.com/stc04003/rocTree,TRUE,https://github.com/stc04003/roctree,7059,4,2020-09-21T21:41:04Z,1764.75
Rodam,"'ODAM' (Open Data for Access and Mining) is a framework that implements a simple way to make research data broadly accessible and fully available for reuse, including by a script language such as R. The main purpose is to make a data set accessible online with a minimal effort from the data provider, and to allow any scientists or bioinformaticians to be able to explore the data set and then extract a subpart or the totality of the data according to their needs. The Rodam package has only one class, 'odamws', that provides methods to allow you to retrieve online data using 'ODAM' Web Services. This obviously requires that data are implemented according the 'ODAM' approach , namely that the data subsets were deposited in the suitable data repository in the form of TSV files associated with  their metadata also described  in TSV files. See <https://inrae.github.io/ODAM/>.",2021-06-29,Daniel Jacob,https://github.com/inrae/ODAM,TRUE,https://github.com/inrae/odam,15029,5,2021-07-26T15:33:16Z,3005.8
rodeo,"Provides an R6 class and several utility methods to
    facilitate the implementation of models based on ordinary
    differential equations. The heart of the package is a code generator
    that creates compiled 'Fortran' (or 'R') code which can be passed to
    a numerical solver. There is direct support for solvers contained
    in packages 'deSolve' and 'rootSolve'.",2021-03-27,David Kneis,https://github.com/dkneis/rodeo,TRUE,https://github.com/dkneis/rodeo,19382,5,2021-03-29T09:40:01Z,3876.4
Rogue,"Interface to 'RogueNaRok' (Aberer et al. 2013) 
  <doi:10.1093/sysbio/sys078>.
  Rogue taxa are a class of taxa with uncertain position in a
  phylogenetic tree. For inference methods that yield a tree set (bootstrapping,
  Bayesian tree searches), rogue taxa can assume different positions for each
  tree. The presence of rogue taxa in a tree set can potentially remove all
  information from a consensus tree. The sum of branch support 
  values in a consensus tree can often be increased by removing rogue taxa.",2021-07-01,Martin R. Smith,"https://github.com/ms609/Roguer/,
https://github.com/aberer/RogueNaRok/,
https://github.com/ms609/RogueNaRok/",TRUE,https://github.com/ms609/roguer,755,1,2021-09-01T15:24:25Z,755
ROI.plugin.lpsolve,"Enhances the 'R' Optimization Infrastructure ('ROI') package
             with the 'lp_solve' solver.",2021-06-15,Florian Schwendinger,"http://roi.r-forge.r-project.org/,
https://github.com/roigrp/ROI.plugin.lpsolve",TRUE,https://github.com/roigrp/roi.plugin.lpsolve,26053,0,2021-06-12T17:52:09Z,NA
roistats,"Easily applying same t-tests/basic data description across several sub-groups, with the output as a nice arranged data.frame. Multiple comparison and the significance symbols are also provided.",2021-03-10,Yufei Zhao,https://github.com/Irisfee/roistats,TRUE,https://github.com/irisfee/roistats,1941,0,2021-08-21T20:03:50Z,NA
roll,Fast and efficient computation of rolling and expanding statistics for time-series data.,2020-07-13,Jason Foster,https://github.com/jjf234/roll,TRUE,https://github.com/jjf234/roll,118867,74,2020-12-13T15:04:44Z,1606.3108108108108
rollRegres,"Methods for fast rolling and expanding linear regression models. That is, series of linear regression models estimated on either an expanding window of data or a moving window of data. The methods use rank-one updates and downdates of the upper triangular matrix from a QR decomposition (see Dongarra, Moler, Bunch, and Stewart (1979) <doi:10.1137/1.9781611971811>).",2019-11-25,Benjamin Christoffersen,https://github.com/boennecd/rollRegres,TRUE,https://github.com/boennecd/rollregres,23077,16,2021-03-12T05:58:35Z,1442.3125
ropenblas,"The 'ropenblas' package (<https://prdm0.github.io/ropenblas/>) is useful for users of any 'GNU/Linux' distribution. It will be possible to download, compile and link the 'OpenBLAS' library (<https://www.openblas.net/>) with the R language, always by the same procedure, regardless of the 'GNU/Linux' distribution used. With the 'ropenblas' package it is possible to download, compile and link the latest version of the 'OpenBLAS' library even the repositories of the 'GNU/Linux' distribution used do not include the latest versions of 'OpenBLAS'. If of interest, older versions of the 'OpenBLAS' library may be considered. Linking R with an optimized version of 'BLAS' (<http://www.netlib.org/blas/>) may improve the computational performance of R code. The 'OpenBLAS' library is an optimized implementation of 'BLAS' that can be easily linked to R with the 'ropenblas' package.    ",2021-05-07,Pedro Rafael D. Marinho,"https://prdm0.github.io/ropenblas/,
https://github.com/prdm0/ropenblas",TRUE,https://github.com/prdm0/ropenblas,14649,35,2021-05-08T18:06:29Z,418.54285714285714
ROpenCVLite,"Installs 'OpenCV' for use by other packages. 'OpenCV' <https://opencv.org/> 
    is library of programming functions mainly aimed at real-time computer 
    vision. This 'Lite' version contains the stable base version of 'OpenCV' and 
    does not contain any of its externally contributed modules.",2021-07-05,Simon Garnier,"https://swarm-lab.github.io/ROpenCVLite/,
https://github.com/swarm-lab/ROpenCVLite",TRUE,https://github.com/swarm-lab/ropencvlite,15217,40,2021-07-05T12:41:10Z,380.425
Ropj,"Read the data from Origin(R) project files ('*.opj')
	<https://www.originlab.com/doc/User-Guide/Origin-File-Types>.
	No write support is planned.",2021-08-02,Ivan Krylov,https://github.com/aitap/Ropj,TRUE,https://github.com/aitap/ropj,11429,0,2021-08-02T16:01:02Z,NA
rorcid,"Client for the 'Orcid.org' API (<https://orcid.org/>).
    Functions included for searching for people, searching by 'DOI',
    and searching by 'Orcid' 'ID'.",2021-01-20,Scott Chamberlain,"https://github.com/ropensci/rorcid (devel),
https://docs.ropensci.org/rorcid/ (docs)",TRUE,https://github.com/ropensci/rorcid,25774,105,2021-06-01T23:25:54Z,245.46666666666667
rotasym,"Implementation of the tests for rotational symmetry on the
    hypersphere proposed in García-Portugués, Paindaveine and Verdebout (2020)
    <doi:10.1080/01621459.2019.1665527>. The package also implements the
    proposed distributions on the hypersphere, based on the tangent-normal
    decomposition, and allows for the replication of the data application
    considered in the paper.",2021-08-19,Eduardo García-Portugués,https://github.com/egarpor/rotasym,TRUE,https://github.com/egarpor/rotasym,12525,1,2021-08-26T00:58:20Z,12525
rotations,"Tools for working with rotational data, including
    simulation from the most commonly used distributions on SO(3),
    methods for different Bayes, mean and median type estimators for
    the central orientation of a sample, confidence/credible
    regions for the central orientation based on those estimators and
    a novel visualization technique for rotation data.  Most recently,
    functions to identify potentially discordant (outlying) values
    have been added.  References: Bingham, Melissa A. and Nordman, Dan J. and Vardeman, Steve B. (2009) <doi:10.1198/jasa.2009.ap08741>,
    Bingham, Melissa A and Vardeman, Stephen B and Nordman, Daniel J (2009) <doi:10.1214/09-BA423>,
    Bingham, Melissa A and Nordman, Daniel J and Vardeman, Stephen B (2010) <doi:10.1016/j.csda.2009.11.020>,
    Leon, C.A. and Masse, J.C. and Rivest, L.P. (2006) <doi:10.1016/j.jmva.2005.03.009>,
    Hartley, R and Aftab, K and Trumpf, J. (2011) <doi:10.1109/CVPR.2011.5995745>,
    Stanfill, Bryan and Genschel, Ulrike and Hofmann, Heike (2013) <doi:10.1080/00401706.2013.826145>,
    Maonton, Jonathan (2004) <doi:10.1109/ICARCV.2004.1469774>, 
    Mardia, KV and Jupp, PE (2000, ISBN:9780471953333), 
    Rancourt, D. and Rivest, L.P. and Asselin, J. (2000) <doi:10.1111/1467-9876.00180>,
    Chang, Ted and Rivest, Louis-Paul (2001) <doi:10.1214/aos/1009210690>, 
    Fisher, Nicholas I. (1996, ISBN:0521568900).",2021-03-12,Bryan Stanfill,https://github.com/stanfill/rotationsC,TRUE,https://github.com/stanfill/rotationsc,16447,0,2021-03-11T22:58:24Z,NA
rotl,"An interface to the 'Open Tree of Life' API to retrieve
    phylogenetic trees, information about studies used to assemble the
    synthetic tree, and utilities to match taxonomic names to 'Open Tree
    identifiers'. The 'Open Tree of Life' aims at assembling a
    comprehensive phylogenetic tree for all named species.",2020-10-22,Francois Michonneau,"https://docs.ropensci.org/rotl/, https://github.com/ropensci/rotl",TRUE,https://github.com/ropensci/rotl,109793,29,2020-10-22T11:14:53Z,3785.9655172413795
rotor,"Conditionally rotate or back-up files based on
    their size or the date of the last backup; inspired by the 'Linux'
    utility 'logrotate'.",2020-12-13,Stefan Fleck,https://s-fleck.github.io/rotor/,TRUE,https://github.com/s-fleck/rotor,17678,9,2021-08-16T08:32:11Z,1964.2222222222222
Routliers,"Detecting outliers using robust methods, 
 	i.e. the Median Absolute Deviation (MAD) for univariate outliers; Leys, Ley, Klein, Bernard, & Licata (2013) <doi:10.1016/j.jesp.2013.03.013>
        and the Mahalanobis-Minimum Covariance Determinant (MMCD) for multivariate outliers; Leys, C., Klein, O., Dominicy, Y. & Ley, C. (2018) <doi:10.1016/j.jesp.2017.09.011>.
        There is also the more known but less robust Mahalanobis distance method, only for comparison purposes.",2019-05-23,Marie Delacre,NA,TRUE,https://github.com/mdelacre/routliers,11515,7,2020-10-09T10:12:18Z,1645
roxut,"Much as 'roxygen2' allows one to document functions in the same file as the function itself, 'roxut'  allows one to write the unit tests in the same file as the function.  Once processed, the unit tests are moved to the appropriate directory.  Currently supports 'testthat' and 'tinytest' frameworks. The 'roxygen2' package provides much of the infrastructure.",2021-08-22,Bryan A. Hanson,https://github.com/bryanhanson/roxut,TRUE,https://github.com/bryanhanson/roxut,14115,0,2021-08-25T16:49:55Z,NA
roxygen2,"Generate your Rd documentation, 'NAMESPACE' file,
    and collation field using specially formatted comments. Writing
    documentation in-line with code makes it easier to keep your
    documentation up-to-date as your requirements change. 'Roxygen2' is
    inspired by the 'Doxygen' system for C++.",2020-06-27,Hadley Wickham,"https://roxygen2.r-lib.org/, https://github.com/r-lib/roxygen2",TRUE,https://github.com/r-lib/roxygen2,7273435,458,2021-05-13T13:57:16Z,15880.862445414847
roxygen2md,"Converts elements of 'roxygen' documentation to
    'markdown'.",2019-06-17,Kirill Müller,"https://roxygen2md.r-lib.org, https://github.com/r-lib/roxygen2md",TRUE,https://github.com/r-lib/roxygen2md,11438,67,2021-07-29T04:14:05Z,170.71641791044777
roxytest,"Various tests as 'roxygen2' roclets: e.g. 'testthat' and 'tinytest' tests. 
  Also other static analysis tools as checking parameter documentation consistency and others.",2020-06-03,Mikkel Meyer Andersen,NA,TRUE,https://github.com/mikldk/roxytest,8453,85,2021-05-05T20:00:36Z,99.44705882352942
rPACI,"Analysis of corneal data obtained from a Placido disk corneal topographer with calculation of irregularity indices. This package performs analyses of corneal data obtained from a Placido disk corneal topographer, with the calculation of the Placido irregularity indices and the posterior analysis. The package is intended to be easy to use by a practitioner, providing a simple interface and yielding easily interpretable results. A corneal topographer is an ophthalmic clinical device that obtains measurements in the cornea (the anterior part of the eye). A Placido disk corneal topographer makes use of the Placido disk [Rowsey et al. (1981)]<doi:10.1001/archopht.1981.03930011093022>, which produce a circular pattern of measurement nodes. The raw information measured by such a topographer is used by practitioners to analyze curvatures, to study optical aberrations, or to diagnose specific conditions of the eye (e.g. keratoconus, an important corneal disease). The rPACI package allows the calculation of the corneal irregularity indices described in [Castro-Luna et al. (2020)]<doi:10.1016%2Fj.clae.2019.12.006>, [Ramos-Lopez et al. (2013)]<doi:10.1097%2FOPX.0b013e3182843f2a>, and [Ramos-Lopez et al. (2011)]<doi:10.1097/opx.0b013e3182279ff8>. It provides a simple interface to read corneal topography data files as exported by a typical Placido disk topographer, to compute the irregularity indices mentioned before, and to display summary plots that are easy to interpret for a clinician.",2021-08-13,Darío Ramos-López,"https://cran.r-project.org/package=rPACI,
https://github.com/dariorlual/rPACI/",TRUE,https://github.com/dariorlual/rpaci,11852,1,2021-08-12T21:35:46Z,11852
RPANDA,"Implements macroevolutionary analyses on phylogenetic trees. See
    Morlon et al. (2010) <DOI:10.1371/journal.pbio.1000493>, Morlon et al. (2011)
    <DOI:10.1073/pnas.1102543108>, Condamine et al. (2013) <DOI:10.1111/ele.12062>, 
    Morlon et al. (2014) <DOI:10.1111/ele.12251>, Manceau et al. (2015) <DOI:10.1111/ele.12415>,
    Lewitus & Morlon (2016) <DOI:10.1093/sysbio/syv116>, Drury et al. (2016) <DOI:10.1093/sysbio/syw020>,
    Manceau et al. (2016) <DOI:10.1093/sysbio/syw115>, Morlon et al. (2016) <DOI:10.1111/2041-210X.12526>, Clavel & Morlon (2017) <DOI:10.1073/pnas.1606868114>, 
    Drury et al. (2017) <DOI:10.1093/sysbio/syx079>, Lewitus & Morlon (2017) <DOI:10.1093/sysbio/syx095>, 
    Drury et al. (2018) <DOI:10.1371/journal.pbio.2003563>, Clavel et al. (2019) <DOI:10.1093/sysbio/syy045>, Maliet et al. (2019) <DOI:10.1038/s41559-019-0908-0>,
    Billaud et al. (2019) <DOI:10.1093/sysbio/syz057>, Lewitus et al. (2019) <DOI:10.1093/sysbio/syz061>,
    Aristide & Morlon (2019) <DOI:10.1111/ele.13385>, and Maliet et al. (2020) <DOI:10.1111/ele.13592>.",2020-09-15,Hélène Morlon,https://github.com/hmorlon/PANDA,TRUE,https://github.com/hmorlon/panda,25505,15,2021-04-22T07:48:01Z,1700.3333333333333
rPanglaoDB,Download and merge labeled single-cell RNA-seq data from the PanglaoDB <https://panglaodb.se/> into a Seurat object.,2021-05-12,Daniel Osorio,https://github.com/dosorio/rPanglaoDB/,TRUE,https://github.com/dosorio/rpanglaodb,1705,10,2021-05-24T20:16:34Z,170.5
rpart,"Recursive partitioning for classification, 
  regression and survival trees.  An implementation of most of the 
  functionality of the 1984 book by Breiman, Friedman, Olshen and Stone.",2019-04-12,Beth Atkinson,"https://github.com/bethatkinson/rpart,
https://cran.r-project.org/package=rpart",TRUE,https://github.com/bethatkinson/rpart,2186586,20,2021-08-11T17:46:48Z,109329.3
rpdo,"Monthly Pacific Decadal Oscillation (PDO) index
    values from January 1900 to September 2018. 
    Superseded by 'rsoi' package which includes the historical and 
    most recent monthly PDO index values together with related climate indices.",2020-07-09,Joe Thorley,https://github.com/poissonconsulting/rpdo,TRUE,https://github.com/poissonconsulting/rpdo,20382,1,2021-02-09T22:36:46Z,20382
rpf,"The purpose of this package is to factor out logic
    and math common to Item Factor Analysis fitting, diagnostics, and
    analysis. It is envisioned as core support code suitable for more
    specialized IRT packages to build upon. Complete access to optimized C
    functions are made available with R_RegisterCCallable().
    This software is described in Pritikin & Falk (2020) <doi:10.1177/0146621620929431>.",2021-08-11,Joshua Pritikin,https://github.com/jpritikin/rpf,TRUE,https://github.com/jpritikin/rpf,428036,0,2021-08-11T20:18:05Z,NA
rplos,"A programmatic interface to the 'SOLR' based
    search API (<http://api.plos.org/>) provided by the Public
    Library of Science journals to search their articles.
    Functions are included for searching for articles, retrieving
    articles, making plots, doing 'faceted' searches,
    'highlight' searches, and viewing results of 'highlighted'
    searches in a browser.",2021-02-23,Scott Chamberlain,https://docs.ropensci.org/rplos/ https://github.com/ropensci/rplos,TRUE,https://github.com/ropensci/rplos,35258,294,2021-02-23T19:49:14Z,119.92517006802721
rpmodel,"Implements the P-model 
  (Stocker et al., 2020 <doi:10.5194/gmd-13-1545-2020>),
  predicting acclimated parameters of the enzyme kinetics of C3 photosynthesis,
  assimilation, and dark respiration rates as a function of the environment
  (temperature, CO2, vapour pressure deficit, light, atmospheric pressure).",2021-06-09,Benjamin Stocker,https://github.com/stineb/rpmodel,TRUE,https://github.com/stineb/rpmodel,8952,18,2021-06-09T12:07:04Z,497.3333333333333
RPostgres,"Fully 'DBI'-compliant 'Rcpp'-backed interface to
    'PostgreSQL' <https://www.postgresql.org/>, an open-source relational
    database.",2021-07-05,Kirill Müller,"https://rpostgres.r-dbi.org, https://github.com/r-dbi/RPostgres",TRUE,https://github.com/r-dbi/rpostgres,845161,263,2021-08-24T02:43:30Z,3213.5399239543726
RPostgreSQL,"Database interface and 'PostgreSQL' driver for 'R'.
 This package provides a Database Interface 'DBI' compliant 
 driver for 'R' to access 'PostgreSQL' database systems.  
 In order to build and install this package from source, 'PostgreSQL' 
 itself must be present your system to provide 'PostgreSQL' functionality 
 via its libraries and header files. These files are provided as
 'postgresql-devel' package under some Linux distributions.
 On 'macOS' and 'Microsoft Windows' system the attached 'libpq' library source will be used.",2021-07-27,Joe Conway,"https://github.com/tomoakin/RPostgreSQL,
https://cran.r-project.org/package=DBI,
https://www.postgresql.org",TRUE,https://github.com/tomoakin/rpostgresql,1843640,50,2021-08-02T03:45:12Z,36872.8
RPPASPACE,"Provides tools for the analysis of reverse-phase protein arrays (RPPAs), which are also known as ""tissue lysate arrays"" or simply ""lysate arrays"". The package's primary purpose is to input a set of quantification files representing dilution series of samples and control points taken from scanned RPPA slides and determine a relative log concentration value for each valid dilution series present in each slide and provide graphical visualization of the input and output data and their relationships. Other optional features include generation of quality control scores for judging the quality of the input data, spatial adjustment of sample points based on controls added to the slides, and various types of normalization of calculated values across a set of slides. The package was derived from a previous package named SuperCurve. For a detailed description of data inputs and outputs, usage  information, and a list of related papers describing methods used in the package please review the vignette ""Guide_to_RPPASPACE"". Hu (2007) <doi:10.1093/bioinformatics/btm283>.",2020-12-10,James M. Melott,https://github.com/MD-Anderson-Bioinformatics/rppaspace,TRUE,https://github.com/md-anderson-bioinformatics/rppaspace,2827,0,2021-08-26T21:32:04Z,NA
rPraat,"Read, write and manipulate 'Praat' TextGrid, PitchTier, Pitch, IntensityTier, Formant, Sound, and Collection files <https://www.fon.hum.uva.nl/praat/>.",2021-02-27,Tomas Boril,https://github.com/bbTomas/rPraat/,TRUE,https://github.com/bbtomas/rpraat,17552,17,2021-02-27T22:06:49Z,1032.4705882352941
RPresto,"Implements a 'DBI' compliant interface to Presto. Presto is
    an open source distributed SQL query engine for running interactive
    analytic queries against data sources of all sizes ranging from
    gigabytes to petabytes: <https://prestodb.io/>.",2021-05-31,Onur Ismail Filiz,https://github.com/prestodb/RPresto,TRUE,https://github.com/prestodb/rpresto,64415,118,2021-08-18T12:26:30Z,545.8898305084746
rprev,"Estimates disease prevalence for a given index date using existing
    registry data extended with Monte Carlo simulations following the method of Crouch et al (2014) <doi: 10.1016/j.canep.2014.02.005>.",2021-05-04,Stuart Lacy,https://github.com/stulacy/rprev-dev,TRUE,https://github.com/stulacy/rprev-dev,18059,0,2021-05-05T14:20:24Z,NA
rprime,"'Eprime' is a set of programs for administering
    psychological experiments by computer. This package provides functions
    for loading, parsing, filtering and exporting data in the text files
    produced by 'Eprime' experiments.",2020-09-24,Tristan Mahr,https://github.com/tjmahr/rprime,TRUE,https://github.com/tjmahr/rprime,16557,20,2020-09-24T11:01:36Z,827.85
rprojroot,"Robust, reliable and flexible paths to files below
    a project root. The 'root' of a project is defined as a directory that
    matches a certain criterion, e.g., it contains a certain regular file.",2020-11-15,Kirill Müller,"https://rprojroot.r-lib.org/, https://github.com/r-lib/rprojroot",TRUE,https://github.com/r-lib/rprojroot,14868804,130,2021-07-29T04:14:10Z,114375.41538461538
RProtoBuf,"Protocol Buffers are a way of encoding structured data in an
 efficient yet extensible format. Google uses Protocol Buffers for almost all
 of its internal 'RPC' protocols and file formats.  Additional documentation
 is available in two included vignettes one of which corresponds to our 'JSS'
 paper (2016, <doi:10.18637/jss.v071.i02>. Either version 2 or 3 of the
 'Protocol Buffers' 'API' is supported.",2020-03-28,Romain Francois,https://github.com/eddelbuettel/rprotobuf,TRUE,https://github.com/eddelbuettel/rprotobuf,53639,57,2021-07-26T12:35:09Z,941.0350877192982
rpubs,Extract code only or with the output from an Rpubs <https://rpubs.com/> article and write the code-block to R script file.,2020-02-05,Aep Hidayatuloh,https://github.com/aephidayatuloh/rpubs,TRUE,https://github.com/aephidayatuloh/rpubs,8345,2,2020-10-16T15:37:34Z,4172.5
rqdatatable,"Implements the 'rquery' piped Codd-style query algebra using 'data.table'.  This allows
   for a high-speed in memory implementation of Codd-style data manipulation tools.",2021-06-12,John Mount,"https://github.com/WinVector/rqdatatable/,
https://winvector.github.io/rqdatatable/",TRUE,https://github.com/winvector/rqdatatable,56890,33,2021-06-12T01:22:11Z,1723.939393939394
RQuantLib,"The 'RQuantLib' package makes parts of 'QuantLib' accessible from R
 The 'QuantLib' project aims to provide a comprehensive software framework
 for quantitative finance. The goal is to provide a standard open source library
 for quantitative analysis, modeling, trading, and risk management of financial
 assets.",2021-09-03,Dirk Eddelbuettel,"https://github.com/eddelbuettel/rquantlib,
https://dirk.eddelbuettel.com/code/rquantlib.html",TRUE,https://github.com/eddelbuettel/rquantlib,88567,90,2021-09-03T01:11:30Z,984.0777777777778
Rquefts,"An implementation of the QUEFTS (Quantitative Evaluation of the Native Fertility of Tropical Soils) model. The model (1) estimates native nutrient (N, P, K) supply of soils from a few soil chemical properties; and (2) computes crop yield given that supply, crop parameters, fertilizer application, and crop attainable yield. See Janssen et al. (1990) <doi:10.1016/0016-7061(90)90021-Z> for the technical details and Sattari et al. (2014) <doi:10.1016/j.fcr.2013.12.005> for a recent evaluation and improvements.",2021-04-30,Robert J. Hijmans,https://CRAN.R-project.org/package=Rquefts,TRUE,https://github.com/cropmodels/rquefts,10121,0,2021-04-30T16:26:30Z,NA
rquery,"A piped query generator based on Edgar F. Codd's relational
    algebra, and on production experience using 'SQL' and 'dplyr' at big data
    scale.  The design represents an attempt to make 'SQL' more teachable by
    denoting composition by a sequential pipeline notation instead of nested
    queries or functions.   The implementation delivers reliable high 
    performance data processing on large data systems such as 'Spark',
    databases, and 'data.table'. Package features include: data processing trees
    or pipelines as observable objects (able to report both columns
    produced and columns used), optimized 'SQL' generation as an explicit
    user visible table modeling step, plus explicit query reasoning and checking.",2021-06-10,John Mount,"https://github.com/WinVector/rquery/,
https://winvector.github.io/rquery/",TRUE,https://github.com/winvector/rquery,129774,102,2021-08-30T16:32:09Z,1272.2941176470588
rrapply,"The minimal 'rrapply'-package contains a single function rrapply(), providing an extended implementation of 'R'-base rapply() by allowing to recursively apply a function to elements of a nested list based on a general condition function and including the possibility to prune or aggregate nested list elements from the result. In addition, special arguments can be supplied to access the name, location, parents and siblings in the nested list of the element under evaluation. The rrapply() function builds upon rapply()'s native 'C' implementation and requires no other package dependencies.",2021-02-08,Joris Chau,"https://jorischau.github.io/rrapply/,
https://github.com/JorisChau/rrapply",TRUE,https://github.com/jorischau/rrapply,9679,16,2021-02-08T10:23:24Z,604.9375
Rraven,A tool to exchange data between R and 'Raven' sound analysis software (Cornell Lab of Ornithology). Functions work on data formats compatible with the R package 'warbleR'.,2021-04-21,Marcelo Araya-Salas,https://github.com/maRce10/Rraven,TRUE,https://github.com/marce10/rraven,18111,4,2021-04-21T04:32:56Z,4527.75
rrd,"Makes it easy to import the data from a 'RRD' database 
   (<https://oss.oetiker.ch/rrdtool/>) directly into R data structures. The 
   resulting objects are 'tibble' objects or a list of 'tibble' objects, making
   it easy to manipulate the data.  The package uses `librrd` to import the 
   numerical data in a `RRD` database directly into R data structures without 
   using intermediate formats.",2019-07-05,Andrie de Vries,"https://github.com/andrie/rrd/, https://andrie.github.io/rrd/",TRUE,https://github.com/andrie/rrd,4038,5,2020-09-13T20:09:53Z,807.6
rredlist,"'IUCN' Red List (<http://apiv3.iucnredlist.org/api/v3/docs>) client.
    The 'IUCN' Red List is a global list of threatened and endangered species.
    Functions cover all of the Red List 'API' routes. An 'API' key is required.",2020-10-29,Scott Chamberlain,"https://github.com/ropensci/rredlist (devel)
https://docs.ropensci.org/rredlist/ (docs)",TRUE,https://github.com/ropensci/rredlist,94149,27,2020-11-10T19:58:46Z,3487
rrefine,"'OpenRefine' (formerly 'Google Refine') is a popular, open source data cleaning software. This package enables users to programmatically trigger data transfer between R and 'OpenRefine'. Available functionality includes project import, export and deletion.",2021-09-03,VP Nagraj,https://github.com/vpnagraj/rrefine,TRUE,https://github.com/vpnagraj/rrefine,16796,15,2021-09-03T02:07:28Z,1119.7333333333333
rRofex,"Execute API calls to the 'Matba Rofex' <https://apihub.primary.com.ar> trading platform. Functionality includes accessing account data and current holdings, retrieving investment quotes, placing and canceling orders, and getting reference data for instruments.",2021-08-02,Augusto Hassel,"https://matbarofex.github.io/rRofex/,
https://github.com/matbarofex/rRofex/",TRUE,https://github.com/matbarofex/rrofex,8758,19,2021-08-02T00:38:49Z,460.94736842105266
RRPP,"Linear model calculations are made for many random versions of data.  
    Using residual randomization in a permutation procedure, sums of squares are 
    calculated over many permutations to generate empirical probability distributions 
    for evaluating model effects.  This packaged is described by 
    Collyer & Adams (2018) <doi:10.1111/2041-210X.13029>.  Additionally, coefficients, statistics, fitted values, and residuals generated over many 
    permutations can be used for various procedures including pairwise tests, prediction, classification, and
    model comparison.  This package should provide most tools one could need for the analysis of
    high-dimensional data, especially in ecology and evolutionary biology, but certainly other fields, as well.",2021-03-30,Michael Collyer,https://github.com/mlcollyer/RRPP,TRUE,https://github.com/mlcollyer/rrpp,60298,3,2021-09-02T10:20:41Z,20099.333333333332
rSAFE,Provides a model agnostic tool for white-box model trained on features extracted from a black-box model. For more information see: Gosiewska et al. (2020) <arXiv:2002.04267>.,2021-03-31,Alicja Gosiewska,https://github.com/ModelOriented/rSAFE,TRUE,https://github.com/modeloriented/rsafe,1677,20,2021-06-30T22:46:10Z,83.85
rsample,"Classes and functions to create and summarize different types of resampling objects (e.g. bootstrap, cross-validation). ",2021-05-08,Julia Silge,"https://rsample.tidymodels.org,
https://github.com/tidymodels/rsample",TRUE,https://github.com/tidymodels/rsample,1247530,253,2021-08-27T14:04:39Z,4930.948616600791
rscala,"'Scala' <http://www.scala-lang.org/> is embedded in 'R' and callbacks from 'Scala' to 'R' are available. Support is provided to write 'R' packages that access 'Scala'. After installation, please run 'rscala::scalaConfig()'.  The vignette provides an update of the original paper <doi:10.18637/jss.v092.i04>.",2020-04-05,David B. Dahl,https://github.com/dbdahl/rscala,TRUE,https://github.com/dbdahl/rscala,49280,92,2021-03-30T23:53:16Z,535.6521739130435
RSCAT,"As an advanced approach to computerized adaptive testing (CAT), 
  shadow testing (van der Linden(2005) <doi:10.1007/0-387-29054-0>) dynamically 
  assembles entire shadow tests as a part of 
  selecting items throughout the testing process.
  Selecting items from shadow tests guarantees the compliance of all content 
  constraints defined by the blueprint. 'RSCAT' is an R package for the 
  shadow-test approach to CAT. The objective of 
  'RSCAT' is twofold: 1) Enhancing the effectiveness of shadow-test CAT simulation;
  2) Contributing to the academic and scientific community for CAT research.",2021-05-16,Bingnan Jiang,NA,TRUE,https://github.com/act-org/rscat,10647,4,2021-05-16T17:23:37Z,2661.75
rsconnect,"Programmatic deployment interface for 'RPubs', 'shinyapps.io', and
    'RStudio Connect'. Supported content types include R Markdown documents,
    Shiny applications, Plumber APIs, plots, and static web content.",2021-08-05,Jonathan McPherson,https://github.com/rstudio/rsconnect,TRUE,https://github.com/rstudio/rsconnect,50216318,88,2021-08-10T17:08:46Z,570639.9772727273
rscontract,"Provides a generic implementation of the 'RStudio' connection contract to 
    make it easier for database connections, and other type of connections, opened 
    via R packages integrate with the connections pane inside the 'RStudio' interactive
    development environment (IDE).",2020-12-15,Nathan Stephens,https://github.com/rstudio/rscontract,TRUE,https://github.com/rstudio/rscontract,11930,16,2020-12-15T19:17:56Z,745.625
rscopus,"Uses Elsevier 'Scopus' API
    <https://dev.elsevier.com/sc_apis.html> to download 
    information about authors and their citations.",2019-09-17,John Muschelli,"https://dev.elsevier.com/sc_apis.html,
https://github.com/muschellij2/rscopus",TRUE,https://github.com/muschellij2/rscopus,88156,51,2021-03-23T20:25:55Z,1728.549019607843
rscorecard,"A method to download Department of Education College
     Scorecard data using the public API
     <https://collegescorecard.ed.gov/data/documentation/>. It is based on
     the 'dplyr' model of piped commands to select and filter data in a
     single chained function call.  An API key from the U.S. Department of
     Education is required.",2021-07-28,Benjamin Skinner,https://github.com/btskinner/rscorecard,TRUE,https://github.com/btskinner/rscorecard,26196,20,2021-07-28T16:09:00Z,1309.8
rsdmx,"Set of classes and methods to read data and metadata documents
  exchanged through the Statistical Data and Metadata Exchange (SDMX) framework,
  currently focusing on the SDMX XML standard format (SDMX-ML).",2021-02-06,Emmanuel Blondel,"https://github.com/opensdmx/rsdmx, https://sdmx.org",TRUE,https://github.com/opensdmx/rsdmx,89820,86,2021-04-21T21:00:15Z,1044.4186046511627
rsetse,"An R implementation for the Strain Elevation and
    Tension embedding algorithm from Bourne (2020)
    <doi:10.1007/s41109-020-00329-4>. The package embeds graphs and
    networks using the Strain Elevation and Tension embedding (SETSe)
    algorithm. SETSe represents the network as a physical system, where
    edges are elastic, and nodes exert a force either up or down based on
    node features. SETSe positions the nodes vertically such that the
    tension in the edges of a node is equal and opposite to the force it
    exerts for all nodes in the network. The resultant structure can then
    be analysed by looking at the node elevation and the edge strain and
    tension. This algorithm works on weighted and unweighted networks as
    well as networks with or without explicit node features.  Edge
    elasticity can be created from existing edge weights or kept as a
    constant.",2021-06-11,Jonathan Bourne,https://github.com/JonnoB/rSETSe,TRUE,https://github.com/jonnob/rsetse,3549,5,2021-06-11T08:57:01Z,709.8
Rsfar,"This is a collection of functions designed for simulating, estimating and forecasting seasonal functional autoregressive time series of order one. These methods are addressed in the manuscript: <https://www.monash.edu/business/ebs/research/publications/ebs/wp16-2019.pdf>.",2021-05-10,Hossein Haghbin,https://github.com/haghbinh/Rsfar,TRUE,https://github.com/haghbinh/rsfar,1569,1,2021-05-16T12:25:54Z,1569
rsimsum,"Summarise results from simulation studies and compute Monte Carlo
  standard errors of commonly used summary statistics. This package is modelled 
  on the 'simsum' user-written command in 'Stata' (White I.R., 2010 
  <https://www.stata-journal.com/article.html?article=st0200>), further extending
  it with additional functionality.",2021-07-05,Alessandro Gasparini,https://ellessenne.github.io/rsimsum/,TRUE,https://github.com/ellessenne/rsimsum,19651,15,2021-07-05T14:35:49Z,1310.0666666666666
rsinaica,"Easy-to-use functions for downloading air quality data from the 
    Mexican National Air Quality Information System (SINAICA).  Allows you to 
    query pollution and meteorological parameters from more than a hundred
    monitoring stations located throughout Mexico. See <https://sinaica.inecc.gob.mx> 
    for more information.",2019-02-04,Diego Valle-Jones,"https://hoyodesmog.diegovalle.net/rsinaica/,
https://github.com/diegovalle/rsinaica",TRUE,https://github.com/diegovalle/rsinaica,12807,6,2021-08-14T21:16:40Z,2134.5
rskey,"Create custom keyboard shortcuts to examine code selected in the 'Rstudio' editor.
             F3 can for example yield 'str(selection)' and F7 open the source
             code of CRAN and base package functions on 'github'.",2020-06-05,Berry Boessenkool,NA,TRUE,https://github.com/brry/rskey,9461,2,2021-07-19T22:04:30Z,4730.5
rslurm,"Functions that simplify submitting R scripts to a 'Slurm' 
    workload manager, in part by automating the division of embarrassingly
    parallel calculations across cluster nodes.",2021-04-22,Quentin Read,http://cyberhelp.sesync.org/rslurm/,TRUE,https://github.com/sesync-ci/rslurm,21505,36,2021-05-30T19:58:47Z,597.3611111111111
rsmatrix,"A small package for calculating the matrices in Shiller (1991, <doi:10.1016/S1051-1377(05)80028-2>) that serve as the foundation for many repeat-sales price indexes.",2021-01-23,Steve Martin,https://github.com/marberts/rsmatrix,TRUE,https://github.com/marberts/rsmatrix,4195,0,2021-01-23T19:06:43Z,NA
RSNNS,"The Stuttgart Neural Network Simulator (SNNS) is a library
    containing many standard implementations of neural networks. This
    package wraps the SNNS functionality to make it available from
    within R. Using the 'RSNNS' low-level interface, all of the
    algorithmic functionality and flexibility of SNNS can be accessed.
    Furthermore, the package contains a convenient high-level
    interface, so that the most common neural network topologies and
    learning algorithms integrate seamlessly into R.",2021-08-13,Christoph Bergmeir,https://github.com/cbergmeir/RSNNS,TRUE,https://github.com/cbergmeir/rsnns,205271,21,2021-08-09T02:30:15Z,9774.809523809523
rsnps,"A programmatic interface to various 'SNP' 'datasets'
    on the web: 'OpenSNP' (<https://opensnp.org>), and 'NBCIs' 'dbSNP' database
    (<https://www.ncbi.nlm.nih.gov/projects/SNP/>). Functions
    are included for searching for 'NCBI'. For 'OpenSNP', functions are included 
    for getting 'SNPs', and data for 'genotypes', 'phenotypes', annotations, 
    and bulk downloads of data by user.",2020-08-28,Julia Gustavsen,"https://docs.ropensci.org/rsnps/,
https://github.com/ropensci/rsnps/",TRUE,https://github.com/ropensci/rsnps,23155,44,2021-05-17T18:47:42Z,526.25
rspa,"Minimally adjust the values of numerical records in a data.frame, such
    that each record satisfies a predefined set of equality and/or inequality
    constraints. The constraints can be defined using the 'validate' package. 
    The core algorithms have recently been moved to the 'lintools' package,
    refer to 'lintools' for a more basic interface and access to a version
    of the algorithm that works with sparse matrices.",2019-06-19,Mark van der Loo,https://github.com/markvanderloo/rspa,TRUE,https://github.com/markvanderloo/rspa,20993,2,2020-12-01T12:43:03Z,10496.5
rsparse,"Implements many algorithms for statistical learning on 
  sparse matrices - matrix factorizations, matrix completion, 
  elastic net regressions, factorization machines. 
  Also 'rsparse' enhances 'Matrix' package by providing methods for 
  multithreaded <sparse, dense> matrix products and native slicing of 
  the sparse matrices in Compressed Sparse Row (CSR) format.
  List of the algorithms for regression problems:
  1) Elastic Net regression via Follow The Proximally-Regularized Leader (FTRL) 
  Stochastic Gradient Descent (SGD), as per McMahan et al(, <doi:10.1145/2487575.2488200>)
  2) Factorization Machines via SGD, as per Rendle (2010, <doi:10.1109/ICDM.2010.127>)
  List of algorithms for matrix factorization and matrix completion:
  1) Weighted Regularized Matrix Factorization (WRMF) via Alternating Least 
  Squares (ALS) - paper by Hu, Koren, Volinsky (2008, <doi:10.1109/ICDM.2008.22>)
  2) Maximum-Margin Matrix Factorization via ALS, paper by Rennie, Srebro 
  (2005, <doi:10.1145/1102351.1102441>)
  3) Fast Truncated Singular Value Decomposition (SVD), Soft-Thresholded SVD, 
  Soft-Impute matrix completion via ALS - paper by Hastie, Mazumder 
  et al. (2014, <arXiv:1410.2596>)
  4) Linear-Flow matrix factorization, from 'Practical linear models for 
  large-scale one-class collaborative filtering' by Sedhain, Bui, Kawale et al 
  (2016, ISBN:978-1-57735-770-4)
  5) GlobalVectors (GloVe) matrix factorization via SGD, paper by Pennington, 
  Socher, Manning (2014, <https://www.aclweb.org/anthology/D14-1162>)
  Package is reasonably fast and memory efficient - it allows to work with large
  datasets - millions of rows and millions of columns. This is particularly useful 
  for practitioners working on recommender systems.",2020-04-01,Dmitriy Selivanov,https://github.com/rexyai/rsparse,TRUE,https://github.com/rexyai/rsparse,148919,155,2021-05-26T06:06:13Z,960.7677419354839
rSPDE,"Functions that compute rational approximations of fractional elliptic stochastic partial differential equations. The package also contains functions for common statistical usage of these approximations. The main reference for the methods is Bolin and Kirchner (2020) <doi:10.1080/10618600.2019.1665537>, which can be generated by the citation function in R.",2021-02-23,David Bolin,https://github.com/davidbolin/rSPDE,TRUE,https://github.com/davidbolin/rspde,9525,0,2021-07-05T07:24:19Z,NA
RSQL,"Allows the user to generate and execute select, insert, update and delete 'SQL' queries the underlying database without having to explicitly write 'SQL' code. ",2020-07-05,Alejandro Baranek,https://github.com/rOpenStats/RSQL,TRUE,https://github.com/ropenstats/rsql,10695,2,2020-12-02T00:21:36Z,5347.5
RSQLite,"Embeds the 'SQLite' database engine in R and
    provides an interface compliant with the 'DBI' package. The source for
    the 'SQLite' engine is included.",2021-08-21,Kirill Müller,"https://rsqlite.r-dbi.org, https://github.com/r-dbi/RSQLite",TRUE,https://github.com/r-dbi/rsqlite,4138091,250,2021-08-21T19:34:27Z,16552.364
rsqliteadmin,"A comprehensive tool written in R Shiny to explore, manage and update SQLite Databases.",2021-07-04,Vijay Barve,https://github.com/rsqliteadmin/rsqliteadmin,TRUE,https://github.com/rsqliteadmin/rsqliteadmin,1796,1,2021-06-17T22:48:13Z,1796
Rssa,"Methods and tools for Singular Spectrum Analysis including decomposition,
             forecasting and gap-filling for univariate and multivariate time series.
             General description of the methods with many examples can be found in the book
             Golyandina (2018, <doi:10.1007/978-3-662-57380-8>).
             See 'citation(""Rssa"")' for details.",2020-11-19,Anton Korobeynikov,https://github.com/asl/rssa,TRUE,https://github.com/asl/rssa,35962,43,2020-11-18T21:49:42Z,836.3255813953489
RSSL,"A collection of implementations of semi-supervised classifiers
    and methods to evaluate their performance. The package includes implementations
    of, among others, Implicitly Constrained Learning, Moment Constrained Learning,
    the Transductive SVM, Manifold regularization, Maximum Contrastive Pessimistic
    Likelihood estimation, S4VM and WellSVM.",2020-11-13,Jesse Krijthe,https://github.com/jkrijthe/RSSL,TRUE,https://github.com/jkrijthe/rssl,71888,48,2020-11-19T23:15:55Z,1497.6666666666667
rstac,"Provides functions to access, search and download spacetime earth
    observation data via SpatioTemporal Asset Catalog (STAC). This package
    supports the version 1.0.0 of the STAC specification
    (<http://stacspec.org>).",2021-07-10,Brazil Data Cube Team,https://github.com/brazil-data-cube/rstac,TRUE,https://github.com/brazil-data-cube/rstac,3822,22,2021-06-25T17:46:19Z,173.72727272727272
rstackdeque,"Provides fast, persistent (side-effect-free) stack, queue and
    deque (double-ended-queue) data structures. While deques include a superset
    of functionality provided by queues, in these implementations queues are
    more efficient in some specialized situations. See the documentation for
    rstack, rdeque, and rpqueue for details.",2015-04-13,Shawn T. ONeil,https://github.com/oneilsh/rstackdeque,TRUE,https://github.com/oneilsh/rstackdeque,20126,28,2021-03-10T05:56:02Z,718.7857142857143
rstan,"User-facing R functions are provided to parse, compile, test,
    estimate, and analyze Stan models by accessing the header-only Stan library
    provided by the 'StanHeaders' package. The Stan project develops a probabilistic
    programming language that implements full Bayesian statistical inference
    via Markov Chain Monte Carlo, rough Bayesian inference via 'variational'
    approximation, and (optionally penalized) maximum likelihood estimation via
    optimization. In all three cases, automatic differentiation is used to quickly
    and accurately evaluate gradients without burdening the user with the need to
    derive the partial derivatives.",2020-07-27,Ben Goodrich,"https://mc-stan.org/rstan, https://discourse.mc-stan.org",TRUE,https://github.com/stan-dev/rstan,2493925,803,2021-02-08T17:39:07Z,3105.7596513075964
rstanarm,"Estimates previously compiled regression models using the 'rstan'
    package, which provides the R interface to the Stan C++ library for Bayesian
    estimation. Users specify models via the customary R syntax with a formula and
    data.frame plus some additional arguments for priors.",2020-07-20,Simon Wood [cph,"https://mc-stan.org/rstanarm/, https://discourse.mc-stan.org",TRUE,https://github.com/stan-dev/rstanarm,531413,301,2021-05-12T00:15:54Z,1765.4916943521594
rstanemax,"Perform sigmoidal Emax model fit using 'Stan' in a formula notation, without writing 'Stan' model code.",2020-11-24,Kenta Yoshida,https://github.com/yoshidk6/rstanemax,TRUE,https://github.com/yoshidk6/rstanemax,11576,2,2020-11-26T07:07:56Z,5788
rstantools,"Provides various tools for developers of R packages interfacing
    with 'Stan' <https://mc-stan.org>, including functions to set up the required 
    package structure, S3 generics and default methods to unify function naming 
    across 'Stan'-based R packages, and vignettes with recommendations for 
    developers.",2020-07-06,Jonah Gabry,"https://mc-stan.org/rstantools/, https://discourse.mc-stan.org/",TRUE,https://github.com/stan-dev/rstantools,831358,28,2021-08-27T20:33:45Z,29691.35714285714
RStata,"A simple R -> Stata interface allowing the user to
    execute Stata commands (both inline and from a .do file)
    from R.",2016-10-27,Luca Braglia,http://github.com/lbraglia/RStata,TRUE,https://github.com/lbraglia/rstata,28381,68,2021-06-25T07:49:13Z,417.36764705882354
rstatix,"Provides a simple and intuitive pipe-friendly framework, coherent with the 'tidyverse' design philosophy, 
    for performing basic statistical tests, including t-test, Wilcoxon test, ANOVA, Kruskal-Wallis and correlation analyses. 
    The output of each test is automatically transformed into a tidy data frame to facilitate visualization. 
    Additional functions are available for reshaping, reordering, manipulating and visualizing correlation matrix.  
    Functions are also included to facilitate the analysis of factorial experiments, including purely 'within-Ss' designs 
    (repeated measures), purely 'between-Ss' designs, and mixed 'within-and-between-Ss' designs. 
    It's also possible to compute several effect size metrics, including ""eta squared"" for ANOVA, ""Cohen's d"" for t-test and 
    'Cramer V' for the association between categorical variables. 
    The package contains helper functions for identifying univariate and multivariate outliers, assessing normality and homogeneity of variances.",2021-02-13,Alboukadel Kassambara,https://rpkgs.datanovia.com/rstatix/,TRUE,https://github.com/kassambara/rstatix,3423859,249,2021-02-22T18:28:08Z,13750.437751004016
rstoat,"A wrapper for the 'Spatiotemporal Observation Annotation Tool' ('STOAT', <https://www.mol.org/stoat>) which allows users to run annotation jobs and retrieve results in the R environment.",2021-03-10,John Wilshire,https://www.mol.org/stoat,TRUE,https://github.com/mapoflife/rstoat,1903,0,2021-03-08T16:02:22Z,NA
RStoolbox,"Toolbox for remote sensing image processing and analysis such as
    calculating spectral indices, principal component transformation, unsupervised
    and supervised classification or fractional cover analyses.",2019-07-23,Benjamin Leutner,"http://bleutner.github.io/RStoolbox,
https://github.com/bleutner/RStoolbox",TRUE,https://github.com/bleutner/rstoolbox,79703,186,2021-03-01T15:10:48Z,428.51075268817203
RstoxData,"Set of tools to read and manipulate various data formats for fisheries. Mainly
  catered towards scientific trawl survey sampling ('biotic') data, acoustic
  trawl data, and commercial fishing catch ('landings') data. Among the
  supported data formats are the data products from the Norwegian Institute
  Marine Research ('IMR') and the International Council for the Exploration of
  the Sea (ICES).",2021-07-17,Ibrahim Umar,https://github.com/StoXProject/RstoxData,TRUE,https://github.com/stoxproject/rstoxdata,4759,0,2021-08-18T21:12:04Z,NA
rstpm2,"R implementation of generalized survival models (GSMs), smooth accelerated failure time (AFT) models and Markov multi-state models. For the GSMs, g(S(t|x))=eta(t,x) for a link function g, survival S at time t with covariates x and a linear predictor eta(t,x). The main assumption is that the time effect(s) are smooth <doi:10.1177/0962280216664760>. For fully parametric models with natural splines, this re-implements Stata's 'stpm2' function, which are flexible parametric survival models developed by Royston and colleagues. We have extended the parametric models to include any smooth parametric smoothers for time. We have also extended the model to include any smooth penalized smoothers from the 'mgcv' package, using penalized likelihood. These models include left truncation, right censoring, interval censoring, gamma frailties and normal random effects <doi:10.1002/sim.7451>, and copulas. For the smooth AFTs, S(t|x) = S_0(t*eta(t,x)), where the baseline survival function S_0(t)=exp(-exp(eta_0(t))) is modelled for natural splines for eta_0, and the time-dependent cumulative acceleration factor eta(t,x)=\int_0^t exp(eta_1(u,x)) du for log acceleration factor eta_1(u,x). The Markov multi-state models allow for a range of models with smooth transitions to predict transition probabilities, length of stay, utilities and costs, with differences, ratios and standardisation.",2021-03-03,Mark Clements,https://github.com/mclements/rstpm2,TRUE,https://github.com/mclements/rstpm2,48175,16,2021-05-07T13:12:08Z,3010.9375
rstudio.prefs,"As of 'RStudio' v1.3, the preferences in the Global Options
    dialog (and a number of other preferences that aren’t) are now saved
    in simple, plain-text JSON files. This package provides an interface
    for working with these 'RStudio' JSON preference files to easily make
    modifications without using the point-and-click option menus. This is
    particularly helpful when working on teams to ensure a unified
    experience across machines and utilizing settings for best practices.",2021-07-22,Daniel D. Sjoberg,"https://github.com/ddsjoberg/rstudio.prefs,
http://www.danieldsjoberg.com/rstudio.prefs/index.html",TRUE,https://github.com/ddsjoberg/rstudio.prefs,1246,6,2021-08-19T19:09:31Z,207.66666666666666
rstudioapi,"Access the RStudio API (if available) and provide informative error
    messages when it's not.",2020-11-12,Kevin Ushey,https://github.com/rstudio/rstudioapi,TRUE,https://github.com/rstudio/rstudioapi,18892307,138,2021-07-02T23:13:29Z,136900.77536231885
rsvd,"Low-rank matrix decompositions are fundamental tools and widely used for data
  analysis, dimension reduction, and data compression. Classically, highly accurate 
  deterministic matrix algorithms are used for this task. However, the emergence of 
  large-scale data has severely challenged our computational ability to analyze big data. 
  The concept of randomness has been demonstrated as an effective strategy to quickly produce
  approximate answers to familiar problems such as the singular value decomposition (SVD). 
  The rsvd package provides several randomized matrix algorithms such as the randomized 
  singular value decomposition (rsvd), randomized principal component analysis (rpca), 
  randomized robust principal component analysis (rrpca), randomized interpolative 
  decomposition (rid), and the randomized CUR decomposition (rcur). In addition several plot 
  functions are provided.",2021-04-16,N. Benjamin Erichson,https://github.com/erichson/rSVD,TRUE,https://github.com/erichson/rsvd,358696,80,2021-04-13T21:20:34Z,4483.7
rsyncrosim,"'SyncroSim' is a generalized framework for managing scenario-based 
    datasets (<https://syncrosim.com/>). 'rsyncrosim' provides an interface to 
    'SyncroSim'. Simulation models can be added to 'SyncroSim' in order to 
    transform these datasets, taking advantage of general features such as 
    defining scenarios of model inputs, running Monte Carlo simulations, and 
    summarizing model outputs. 'rsyncrosim' requires 'SyncroSim' 2.2.13 or higher 
    (API documentation: <https://docs.syncrosim.com/>).",2021-02-16,Colin Daniel,https://github.com/syncrosim/rsyncrosim/,TRUE,https://github.com/syncrosim/rsyncrosim,5559,3,2021-08-10T23:19:16Z,1853
rsyslog,"Functions to write messages to the 'syslog' system logger API,
  available on all 'POSIX'-compatible operating systems. Features include
  tagging messages with a priority level and application type, as well as
  masking (hiding) messages below a given priority level.",2021-06-04,Aaron Jacobs,https://github.com/atheriel/rsyslog,TRUE,https://github.com/atheriel/rsyslog,12914,23,2021-06-04T13:09:58Z,561.4782608695652
rt,"Provides a programmatic interface to the 'Request Tracker' (RT)
  HTTP API <https://rt-wiki.bestpractical.com/wiki/REST>. 'RT' is a popular
  ticket tracking system.",2021-05-15,Bryce Mecum,https://github.com/nceas/rt,TRUE,https://github.com/nceas/rt,7531,5,2021-06-04T19:48:38Z,1506.2
rtables,"Reporting tables often have structure that goes beyond simple rectangular
  data. The 'rtables' package provides a framework for declaring complex multi-level tabulations
  and then applying them to data. This framework models both tabulation and the resulting
  tables as hierarchical, tree-like objects which support sibling sub-tables, arbitrary splitting
  or grouping of data in row and column dimensions, cells containing multiple values,
  and the concept of contextual summary computations. A convenient pipe-able interface is provided
  for declaring table layouts and the corresponding computations, and then applying them to data.",2021-07-13,Gabriel Becker,"https://github.com/roche/rtables, https://roche.github.io/rtables/",TRUE,https://github.com/roche/rtables,3285,90,2021-08-27T20:34:02Z,36.5
RTD,"
  Upload R data.frame to Arm Treasure Data, see <https://www.treasuredata.com/>. 
  You can execute database or table handling for resources on Arm Treasure Data.",2020-07-26,Aki Ariga,https://github.com/treasure-data/RTD,TRUE,https://github.com/treasure-data/rtd,11755,3,2020-09-08T12:34:51Z,3918.3333333333335
RtD3,"Create interactive visualisations of Rt estimates using 'D3.js' 
  (Gibbs et al. (2020) <doi:10.5281/zenodo.4011842>). Developed primarily targeting Rt estimates 
  generated by the 'EpiNow2' package, 'RtD3' aims to make simple, beautiful 
  visualisations that help researchers explore their results and share them with others.",2020-11-06,Hamish Gibbs,"https:/epiforecasts.io/RtD3, https://github.com/epiforecasts/RtD3",TRUE,https://github.com/epiforecasts/rtd3,3313,5,2020-11-08T17:16:33Z,662.6
rtdists,"Provides response time distributions (density/PDF,
       distribution function/CDF, quantile function, and random
       generation): (a) Ratcliff diffusion model (Ratcliff &
       McKoon, 2008, <doi:10.1162/neco.2008.12-06-420>) based on C
       code by Andreas and Jochen Voss and (b) linear ballistic
       accumulator (LBA; Brown & Heathcote, 2008,
       <doi:10.1016/j.cogpsych.2007.12.002>) with different
       distributions underlying the drift rate.",2020-03-06,Henrik Singmann,https://github.com/rtdists/rtdists/,TRUE,https://github.com/rtdists/rtdists,100369,32,2021-05-12T12:30:14Z,3136.53125
rtemps,"A collection of R Markdown templates for nicely structured, reproducible 
    data analyses in R. The templates have embedded examples on how to write 
    citations, footnotes, equations and use colored message/info boxes, how to 
    cross-reference different parts/sections in the report, provide a nice 
    table of contents (toc) with a References section and proper R session 
    information as well as examples using DT tables and ggplot2 graphs. 
    The bookdown Lite template theme supports code folding.",2020-11-04,John Zobolas,https://github.com/bblodfon/rtemps,TRUE,https://github.com/bblodfon/rtemps,9446,19,2021-02-27T02:20:10Z,497.1578947368421
rTensor,"A set of tools for creation, manipulation, and modeling
    of tensors with arbitrary number of modes. A tensor in the context of data
    analysis is a multidimensional array. rTensor does this by providing a S4
    class 'Tensor' that wraps around the base 'array' class. rTensor
    provides common tensor operations as methods, including matrix unfolding,
    summing/averaging across modes, calculating the Frobenius norm, and taking
    the inner product between two tensors. Familiar array operations are
    overloaded, such as index subsetting via '[' and element-wise operations.
    rTensor also implements various tensor decomposition, including CP, GLRAM,
    MPCA, PVD, and Tucker. For tensors with 3 modes, rTensor also implements
    transpose, t-product, and t-SVD, as defined in Kilmer et al. (2013). Some
    auxiliary functions include the Khatri-Rao product, Kronecker product, and
    the Hadamard product for a list of matrices.",2021-05-15,James Li and Jacob Bien and Martin Wells,https://github.com/rikenbit/rTensor,TRUE,https://github.com/rikenbit/rtensor,32539,2,2021-05-15T08:14:43Z,16269.5
rtern,"A small language extension for succinct conditional assignment using `?` and `:`, emulating the conditional ternary operator syntax using in C, Java, JavaScript and other languages.",2021-04-28,Gethin Davies,https://github.com/grddavies/rtern,TRUE,https://github.com/grddavies/rtern,1482,3,2021-05-13T09:06:56Z,494
rtext,"For natural language processing and analysis of qualitative text
    coding structures which provide a way to bind together text and text data
    are fundamental. The package provides such a structure and accompanying
    methods in form of R6 objects. The 'rtext' class allows for text handling
    and text coding (character or regex based) including data updates on
    text transformations as well as aggregation on various levels.
    Furthermore, the usage of R6 enables inheritance and passing by reference
    which should enable 'rtext' instances to be used as back-end for R based
    graphical text editors or text coding GUIs.",2021-01-28,Peter Meissner,https://github.com/petermeissner/rtext,TRUE,https://github.com/petermeissner/rtext,16526,1,2021-01-27T06:06:40Z,16526
rTG,"Methods for comparing different regression algorithms for 
    describing the temporal dynamics of secondary tree growth (xylem and 
    phloem). Users can compare the accuracy of the most common fitting methods 
    usually used to analyse xylem and phloem data, i.e., Gompertz function and 
    General Additive Models (GAMs); and an algorithm newly introduced to the 
    field, i.e., Bayesian Regularised Neural Networks (brnn). The core function
    of the package is XPSgrowth(), while the results can be interpreted using 
    implemented generic S3 methods, such as plot() and summary().",2021-07-21,Jernej Jevsenak,https://github.com/jernejjevsenak/rTG,TRUE,https://github.com/jernejjevsenak/rtg,592,0,2021-07-20T10:55:39Z,NA
Rthingsboard,"The goal of 'Rthingsboard' is to provide interaction with the API of 'ThingsBoard' (<https://thingsboard.io/>), an open-source IoT platform for device management, data collection, processing and visualization.",2021-01-21,David Dorchies,https://github.com/DDorch/Rthingsboard,TRUE,https://github.com/ddorch/rthingsboard,2611,3,2021-01-20T17:09:05Z,870.3333333333334
rticles,"A suite of custom R Markdown formats and templates for
  authoring journal articles and conference submissions.",2021-06-23,Yihui Xie,https://github.com/rstudio/rticles,TRUE,https://github.com/rstudio/rticles,336855,997,2021-09-02T13:27:07Z,337.86860581745236
rticulate,"It provides functions for processing Articulate
    Assistant Advanced™ (AAA) export files and plot tongue contour data from any
    system.",2021-01-11,Stefano Coretta,https://github.com/stefanocoretta/rticulate,TRUE,https://github.com/stefanocoretta/rticulate,2796,4,2021-04-26T07:30:22Z,699
rtide,"Calculates tide heights based on tide station harmonics.  It
    includes the harmonics data for 637 US stations.  The harmonics data
    was converted from
    <https://github.com/poissonconsulting/rtide/blob/master/data-raw/harmonics-dwf-20151227-free.tar.bz2>,
    NOAA web site data processed by David Flater for 'XTide'.  The code to
    calculate tide heights from the harmonics is based on 'XTide'.",2021-05-29,Joe Thorley,https://github.com/poissonconsulting/rtide,TRUE,https://github.com/poissonconsulting/rtide,16598,13,2021-05-29T21:47:54Z,1276.7692307692307
RTL,"Collection of functions and metadata to complement core packages
    in Finance and Commodities, including futures expiry tables and <https://www.morningstar.com/products/commodities-and-energy>
    API functions. See <https://github.com/risktoollib/RTL>.",2021-06-12,Philippe Cote,https://github.com/risktoollib/RTL,TRUE,https://github.com/risktoollib/rtl,7941,7,2021-08-24T00:14:20Z,1134.4285714285713
rtodoist,"Allows you to interact with the API of the ""Todoist"" platform.
    'Todoist' <https://todoist.com/> provides an online task manager service for teams.",2020-05-14,Cervan Girard,https://github.com/ThinkR-open/rtodoist,TRUE,https://github.com/thinkr-open/rtodoist,4814,7,2021-04-16T05:14:07Z,687.7142857142857
rTorch,"'R' implementation and interface of the Machine Learning platform 
    'PyTorch' <https://pytorch.org/> developed in 'Python'. It requires a 'conda'
    environment with 'torch' and 'torchvision' Python packages to provide 
    'PyTorch' functions, methods and classes. The key object in 'PyTorch' is the 
    tensor which is in essence a multidimensional array. These tensors are fairly 
    flexible in performing calculations in CPUs as well as 'GPUs' to accelerate 
    tensor operations.",2020-10-12,Alfonso R. Reyes,https://github.com/f0nzie/rTorch,TRUE,https://github.com/f0nzie/rtorch,11506,77,2020-10-15T16:52:49Z,149.42857142857142
Rtrack,"A toolkit for the analysis of paths from spatial tracking experiments (such as the Morris water maze) and calculation of goal-finding strategies. 
    This package is centered on an approach using machine learning for path classification.",2020-05-04,Rupert Overall,"https://rupertoverall.net/Rtrack/,
https://github.com/rupertoverall/Rtrack",TRUE,https://github.com/rupertoverall/rtrack,9356,2,2021-04-17T07:07:10Z,4678
RTransferEntropy,Measuring information flow between time series with Shannon and Rényi transfer entropy. See also Dimpfl and Peter (2013) <doi:10.1515/snde-2012-0044> and Dimpfl and Peter (2014) <doi:10.1016/j.intfin.2014.03.004> for theory and applications to financial time series. Additional references can be found in the theory part of the vignette.,2021-04-02,David Zimmermann,https://github.com/BZPaper/RTransferEntropy,TRUE,https://github.com/bzpaper/rtransferentropy,19916,14,2021-04-02T09:16:41Z,1422.5714285714287
rtrek,"Provides datasets related to the Star Trek fictional universe and functions for working with the data.
    The package also provides access to real world datasets based on the televised series and other related licensed media productions.
    It interfaces with the Star Trek API (STAPI) (<http://stapi.co/>), 
    Memory Alpha (<https://memory-alpha.fandom.com/wiki/Portal:Main>), and Memory Beta (<https://memory-beta.fandom.com/wiki/Main_Page>) 
    to retrieve data, metadata and other information relating to Star Trek.
    It also contains several local datasets covering a variety of topics. 
    The package also provides functions for working with data from other Star Trek-related 
    R data packages containing larger datasets not stored in 'rtrek'.",2021-06-01,Matthew Leonawicz,https://github.com/leonawicz/rtrek,TRUE,https://github.com/leonawicz/rtrek,14256,41,2021-05-29T18:55:52Z,347.7073170731707
rtrend,"The traditional linear regression trend, Modified Mann-Kendall (MK)
    non-parameter trend and bootstrap trend are included in this package. Linear 
    regression trend is rewritten by '.lm.fit'. MK trend is rewritten by 'Rcpp'.
    Finally, those functions are about 10 times faster than previous version 
    in R.
    Reference:
    Hamed, K. H., & Rao, A. R. (1998). A modified Mann-Kendall trend test for 
    autocorrelated data. Journal of hydrology, 204(1-4), 182-196. 
    <doi:10.1016/S0022-1694(97)00125-X>.",2021-09-02,Dongdong Kong,https://github.com/rpkgs/rtrend,TRUE,https://github.com/rpkgs/rtrend,1333,3,2021-09-02T05:04:10Z,444.3333333333333
rtsVis,"A lightweight 'R' package to visualize large raster time series, building on a fast temporal interpolation core.",2021-05-26,Johannes Mast,NA,TRUE,https://github.com/johmast/rtsvis,2645,9,2021-06-20T12:26:14Z,293.8888888888889
Rttf2pt1,"Contains the program 'ttf2pt1', for use with the
    'extrafont' package. This product includes software developed by the 'TTF2PT1'
    Project and its contributors.",2021-07-22,Winston Chang,https://github.com/wch/Rttf2pt1,TRUE,https://github.com/wch/rttf2pt1,1199012,9,2021-07-20T20:25:45Z,133223.55555555556
rtweet,"An implementation of calls designed to collect and organize Twitter data via Twitter's REST and stream Application Program Interfaces (API), which can be found at the following URL: <https://developer.twitter.com/en/docs>.
 This package has been peer-reviewed by rOpenSci (v. 0.6.9).",2020-01-08,Michael W. Kearney,https://CRAN.R-project.org/package=rtweet,TRUE,https://github.com/ropensci/rtweet,504996,664,2021-07-20T07:45:33Z,760.5361445783133
rucrdtw,"R bindings for functions from the UCR Suite by Rakthanmanon et al. (2012) <DOI:10.1145/2339530.2339576>, which enables ultrafast subsequence
      search for a best match under Dynamic Time Warping and Euclidean Distance.",2020-03-04,Philipp Boersch-Supan,https://github.com/pboesu/rucrdtw,TRUE,https://github.com/pboesu/rucrdtw,60351,13,2021-06-03T11:08:02Z,4642.384615384615
ruimtehol,"Wraps the 'StarSpace' library <https://github.com/facebookresearch/StarSpace> 
    allowing users to calculate word, sentence, article, document, webpage, link and entity 'embeddings'. 
    By using the 'embeddings', you can perform text based multi-label classification, 
    find similarities between texts and categories, do collaborative-filtering based recommendation 
    as well as content-based recommendation, find out relations between entities, calculate 
    graph 'embeddings' as well as perform semi-supervised learning and multi-task learning on plain text. 
    The techniques are explained in detail in the paper: 'StarSpace: Embed All The Things!' by Wu et al. (2017), available at <arXiv:1709.03856>.",2020-11-29,Jan Wijffels,https://github.com/bnosac/ruimtehol,TRUE,https://github.com/bnosac/ruimtehol,16268,93,2020-11-27T14:04:27Z,174.9247311827957
ruler,"Tools for creating data validation pipelines and
    tidy reports. This package offers a framework for exploring and
    validating data frame like objects using 'dplyr' grammar of data
    manipulation.",2020-11-25,Evgeni Chasnovski,"https://echasnovski.github.io/ruler/,
https://github.com/echasnovski/ruler",TRUE,https://github.com/echasnovski/ruler,17601,32,2020-11-29T14:50:31Z,550.03125
rules,"Bindings for additional models for use with the 'parsnip'
    package.  Models include prediction rule ensembles (Friedman and
    Popescu, 2008) <doi:10.1214/07-AOAS148>, C5.0 rules (Quinlan, 1992
    ISBN: 1558602380), and Cubist (Kuhn and Johnson, 2013)
    <doi:10.1007/978-1-4614-6849-3>.",2021-08-07,Max Kuhn,"https://github.com/tidymodels/rules, https://rules.tidymodels.org",TRUE,https://github.com/tidymodels/rules,9876,27,2021-08-17T16:27:40Z,365.77777777777777
rUM,"This holds some r markdown templates and a template to create a
    research project in RStudio.",2021-07-26,Raymond Balise,"https://raymondbalise.github.io/rUM/,
https://github.com/RaymondBalise/rUM",TRUE,https://github.com/raymondbalise/rum,558,1,2021-07-25T00:47:41Z,558
runes,Convert a string of text characters to Elder Futhark Runes <https://en.wikipedia.org/wiki/Elder_Futhark>.,2020-05-29,Bryan Jenks,NA,TRUE,https://github.com/tallguyjenks/runes,7207,7,2020-12-13T06:43:55Z,1029.5714285714287
runjags,"User-friendly interface utilities for MCMC models via
    Just Another Gibbs Sampler (JAGS), facilitating the use of parallel
    (or distributed) processors for multiple chains, automated control
    of convergence and sample length diagnostics, and evaluation of the
    performance of a model using drop-k validation or against simulated
    data. Template model specifications can be generated using a standard
    lme4-style formula interface to assist users less familiar with the
    BUGS syntax.  A JAGS extension module provides additional distributions
    including the Pareto family of distributions, the DuMouchel prior and
    the half-Cauchy prior.",2021-03-02,Matthew Denwood,https://github.com/ku-awdc/runjags,TRUE,https://github.com/ku-awdc/runjags,124376,1,2021-07-13T14:39:29Z,124376
runner,"Lightweight library for rolling windows operations. Package enables
  full control over the window length, window lag and a time indices. With a runner 
  one can apply any R function on a rolling windows. The package eases work with 
  equally and unequally spaced time series.",2021-04-22,Dawid Kałędkowski,NA,TRUE,https://github.com/gogonzo/runner,35703,31,2021-04-22T06:52:28Z,1151.7096774193549
runstats,"Provides methods for fast computation of running sample 
    statistics for time series. These include: (1) mean, (2) 
    standard deviation, and (3) variance over a fixed-length window 
    of time-series, (4) correlation, (5) covariance, and (6) 
    Euclidean distance (L2 norm) between short-time pattern and 
    time-series. Implemented methods utilize Convolution Theorem to 
    compute convolutions via Fast Fourier Transform (FFT).",2019-11-14,Marta Karas,https://github.com/martakarass/runstats,TRUE,https://github.com/martakarass/runstats,13677,1,2021-01-09T05:30:25Z,13677
rust,"Uses the generalized ratio-of-uniforms (RU) method to simulate
    from univariate and (low-dimensional) multivariate continuous distributions.
    The user specifies the log-density, up to an additive constant. The RU
    algorithm is applied after relocation of mode of the density to zero, and
    the user can choose a tuning parameter r. For details see Wakefield, Gelfand
    and Smith (1991) <DOI:10.1007/BF01889987>, Efficient generation of random
    variates via the ratio-of-uniforms method, Statistics and Computing (1991)
    1, 129-133.  A Box-Cox variable transformation can be used to make the input
    density suitable for the RU method and to improve efficiency.  In the
    multivariate case rotation of axes can also be used to improve efficiency.
    From version 1.2.0 the 'Rcpp' package 
    <https://cran.r-project.org/package=Rcpp> can be used to improve efficiency.",2021-06-03,Paul J. Northrop,"https://paulnorthrop.github.io/rust/,
https://github.com/paulnorthrop/rust",TRUE,https://github.com/paulnorthrop/rust,40973,0,2021-06-17T17:01:35Z,NA
rv,"Implements a simulation-based random variable class and a suite of
  methods for extracting parts of random vectors, calculating extremes of random
  vectors, and generating random vectors under a variety of distributions 
  following Kerman and Gelman (2007) <doi:10.1007/s11222-007-9020-4>. ",2020-02-03,Jouni Kerman,https://github.com/jsta/rv,TRUE,https://github.com/jsta/rv,16811,2,2021-07-22T15:06:47Z,8405.5
RVA,Automate downstream visualization & pathway analysis in RNAseq analysis. 'RVA' is a collection of functions that efficiently visualize RNAseq differential expression analysis result from summary statistics tables. It also utilize the Fisher's exact test to evaluate gene set or pathway enrichment in a convenient and efficient manner.,2021-02-13,Xingpeng Li,https://github.com/THERMOSTATS/RVA,TRUE,https://github.com/thermostats/rva,1947,5,2021-02-12T19:30:23Z,389.4
Rvcg,"Operations on triangular meshes based on 'VCGLIB'. This package
    integrates nicely with the R-package 'rgl' to render the meshes processed by
    'Rvcg'. The Visualization and Computer Graphics Library (VCG for short) is
    an open source portable C++ templated library for manipulation, processing
    and displaying with OpenGL of triangle and tetrahedral meshes. The library,
    composed by more than 100k lines of code, is released under the GPL license,
    and it is the base of most of the software tools of the Visual Computing Lab of
    the Italian National Research Council Institute ISTI <http://vcg.isti.cnr.it>,
    like 'metro' and 'MeshLab'. The 'VCGLIB' source is pulled from trunk
    <https://github.com/cnr-isti-vclab/vcglib> and patched to work with options
    determined by the configure script as well as to work with the header files
    included by 'RcppEigen'.",2021-09-02,Stefan Schlager,"https://github.com/zarquon42b/Rvcg,
https://github.com/cnr-isti-vclab/vcglib",TRUE,https://github.com/zarquon42b/rvcg,47307,21,2021-09-03T14:11:33Z,2252.714285714286
rversions,"Query the main 'R' 'SVN' repository to find the
    versions 'r-release' and 'r-oldrel' refer to, and also all previous
    'R' versions and their release dates.",2021-05-31,Gábor Csárdi,"https://github.com/r-hub/rversions,
https://r-hub.github.io/rversions/",TRUE,https://github.com/r-hub/rversions,5291018,27,2021-05-31T09:06:34Z,195963.62962962964
rvertnet,"Retrieve, map and summarize data from the 'VertNet.org' 
    archives (<http://vertnet.org/>).  Functions allow searching by many 
    parameters, including 'taxonomic' names, places, and dates. In addition, 
    there is an interface for conducting spatially delimited searches, and 
    another for requesting large 'datasets' via email.",2021-05-13,Scott Chamberlain,"https://github.com/ropensci/rvertnet (devel)
https://docs.ropensci.org/rvertnet/ (documentation)",TRUE,https://github.com/ropensci/rvertnet,55264,4,2021-05-13T18:36:50Z,13816
rvest,"Wrappers around the 'xml2' and 'httr' packages to
    make it easy to download, then manipulate, HTML and XML.",2021-07-26,Hadley Wickham,"https://rvest.tidyverse.org/, https://github.com/tidyverse/rvest",TRUE,https://github.com/tidyverse/rvest,15853098,1278,2021-07-30T15:04:11Z,12404.615023474178
rvinecopulib,"Provides an interface to 'vinecopulib', a C++ library for vine 
 copula modeling. The 'rvinecopulib' package implements the core features of the
 popular 'VineCopula' package, in particular inference algorithms for both vine 
 copula and bivariate copula models. Advantages over 'VineCopula' are a sleeker 
 and more modern API, improved performances, especially in high dimensions, 
 nonparametric and multi-parameter families, and the ability to model discrete 
 variables. The 'rvinecopulib' package includes 'vinecopulib' as header-only 
 C++ library (currently version 0.5.5). Thus users do not need to install 
 'vinecopulib' itself in order to use 'rvinecopulib'. Since their initial 
 releases, 'vinecopulib' is licensed under the MIT License, and 'rvinecopulib' 
 is licensed under the GNU GPL version 3.",2021-01-06,Thomas Nagler,NA,TRUE,https://github.com/vinecopulib/rvinecopulib,24446,14,2021-07-13T23:30:50Z,1746.142857142857
rvkstat,"Load data from vk.com api about your communiti users and views,
    ads performance, post on user wall and etc.	For more information 
    see API Documentation <https://vk.com/dev/first_guide>.",2021-04-07,Alexey Seleznev,https://selesnow.github.io/rvkstat/,TRUE,https://github.com/selesnow/rvkstat,10750,14,2021-07-16T10:28:27Z,767.8571428571429
RVowpalWabbit,"The 'Vowpal Wabbit' project is a fast out-of-core learning
 system sponsored by Microsoft Research (having started at Yahoo! Research)
 and written by John Langford along with a number of contributors. This R
 package does not include the distributed computing implementation of the
 cluster/ directory of the upstream sources. Use of the software as a network
 service is also not directly supported as the aim is a simpler direct call
 from R for validation and comparison. Note that this package contains an
 embedded older version of 'Vowpal Wabbit'. The package 'rvw' at the GitHub
 repo <https://github.com/eddelbuettel/rvw> can provide an alternative using 
 an external 'Vowpal Wabbit' library installation.",2020-08-07,Dirk Eddelbuettel,https://vowpalwabbit.org/,TRUE,https://github.com/eddelbuettel/rvowpalwabbit,7619,22,2021-06-01T22:24:11Z,346.3181818181818
rwa,"Perform a Relative Weights Analysis (RWA) (a.k.a. Key Drivers Analysis) as per the method described 
    in Tonidandel & LeBreton (2015) <DOI:10.1007/s10869-014-9351-z>, with its original roots in Johnson (2000) <DOI:10.1207/S15327906MBR3501_1>. In essence, RWA decomposes
    the total variance predicted in a regression model into weights that accurately reflect the proportional 
    contribution of the predictor variables, which addresses the issue of multi-collinearity. In typical scenarios,
    RWA returns similar results to Shapley regression, but with a significant advantage on computational performance.",2020-11-24,Martin Chan,https://github.com/martinctc/rwa,TRUE,https://github.com/martinctc/rwa,3797,5,2021-03-06T23:17:35Z,759.4
rwalkr,"Provides API to Melbourne pedestrian and weather data
    <https://data.melbourne.vic.gov.au> in tidy data form.",2021-08-16,Earo Wang,https://pkg.earo.me/rwalkr/,TRUE,https://github.com/earowang/rwalkr,21392,10,2021-08-17T09:45:59Z,2139.2
rwavelet,"Perform wavelet analysis (orthogonal,translation invariant, tensorial, 1-2-3d transforms, thresholding, block thresholding, linear,...) with applications to data compression or denoising/regression. The core of the code is a port of 'MATLAB' Wavelab toolbox written by D. Donoho, A. Maleki and M. Shahram (<https://statweb.stanford.edu/~wavelab/>).",2020-12-12,F. Navarro and C. Chesneau,https://github.com/fabnavarro/rwavelet,TRUE,https://github.com/fabnavarro/rwavelet,12794,4,2020-12-12T09:28:31Z,3198.5
rWBclimate,"This package will download model predictions from 15 different global circulation models in 20 year intervals from the world bank.  Users can also access historical data, and create maps at 2 different spatial scales.",2014-04-19,Edmund Hart,http://github.com/ropensci/rWBclimate,TRUE,https://github.com/ropensci/rwbclimate,16872,57,2020-12-09T15:12:55Z,296
rwhatsapp,"A straightforward, easy-to-use and robust parsing package which aims to
    digest history files from the popular messenger service 'WhatsApp' in all locales
    and from all devices.",2020-09-19,Johannes Gruber,https://github.com/JBGruber/rwhatsapp,TRUE,https://github.com/jbgruber/rwhatsapp,17078,70,2020-12-19T20:23:24Z,243.97142857142856
rWind,Tools for download and manage surface wind and sea currents data from the Global Forecasting System <https://www.ncdc.noaa.gov/data-access/model-data/model-datasets/global-forcast-system-gfs> and to compute connectivity between locations.,2020-02-17,Javier Fernández-López,http://allthiswasfield.blogspot.com.es/,TRUE,https://github.com/jabiologo/rwind,22298,18,2021-04-27T21:50:33Z,1238.7777777777778
RWmisc,"Contains convenience functions for working with spatial data across
    multiple UTM zones, raster-vector operations common in the analysis of 
    conflict data, and converting degrees, minutes, and seconds latitude and
    longitude coordinates to decimal degrees.",2021-06-20,Rob Williams,https://github.com/jayrobwilliams/RWmisc,TRUE,https://github.com/jayrobwilliams/rwmisc,2517,0,2021-06-20T12:52:32Z,NA
Rwofost,"An implementation of the WOFOST (""World Food Studies"") crop growth model. WOFOST is a dynamic simulation model that uses daily weather data, and crop, soil and management parameters to simulate crop growth and development. See De Wit et al. (2019) <doi:10.1016/j.agsy.2018.06.018> for a recent review of the history and use of the model.",2021-05-27,Robert J. Hijmans,https://CRAN.R-project.org/package=Rwofost,TRUE,https://github.com/cropmodels/rwofost,7463,12,2021-05-21T03:17:02Z,621.9166666666666
rworldmap,Enables mapping of country level and gridded user datasets.,2016-02-03,Andy South,"https://github.com/AndySouth/rworldmap/,
https://groups.google.com/forum/#!forum/rworldmap,
http://andysouth.co.uk/",TRUE,https://github.com/andysouth/rworldmap,254948,26,2020-11-29T20:35:07Z,9805.692307692309
Rwtss,"Allows remote access to satellite image time 
    series provided by the web time series service (WTSS) available 
    at servers such as <https://brazildatacube.dpi.inpe.br/wtss/>. 
    The functions include listing the data sets available in WTSS servers, 
    describing the contents of a data set, and retrieving a time series 
    based on spatial location and temporal filters.",2021-06-28,Gilberto Queiroz,https://github.com/e-sensing/Rwtss/,TRUE,https://github.com/e-sensing/rwtss,2717,10,2021-08-30T21:15:01Z,271.7
Rxnat,"Allows communication with Extensible Neuroimaging Archive Toolkit <https://www.xnat.org>. 
  'Rxnat' is using the 'XNAT' REST API to perform data queries and download images.",2021-08-19,Adi Gherman,NA,TRUE,https://github.com/adigherman/rxnat,13548,3,2021-08-12T18:30:09Z,4516
RxODE,"Facilities for running simulations from ordinary
    differential equation ('ODE') models, such as pharmacometrics and other
    compartmental models.  A compilation manager translates the ODE model
    into C, compiles it, and dynamically loads the object code into R for
    improved computational efficiency.  An event table object facilitates
    the specification of complex dosing regimens (optional) and sampling
    schedules.  NB: The use of this package requires both C and
    Fortran compilers, for details on their use with R please see
    Section 6.3, Appendix A, and Appendix D in the ""R Administration and
    Installation"" manual. Also the code is mostly released under GPL.  The
    'VODE' and 'LSODA' are in the public domain.  The information is available
    in the inst/COPYRIGHTS. ",2021-09-03,Wenping Wang,"https://nlmixrdevelopment.github.io/RxODE/,
https://github.com/nlmixrdevelopment/RxODE/",TRUE,https://github.com/nlmixrdevelopment/rxode,38011,45,2021-09-01T01:27:56Z,844.6888888888889
rxylib,"Provides access to the 'xylib' C library for to import xy 
  data from powder diffraction, spectroscopy and other experimental methods.",2020-09-14,Sebastian Kreutzer,https://github.com/R-Lum/rxylib,TRUE,https://github.com/r-lum/rxylib,17049,7,2021-03-05T23:36:33Z,2435.5714285714284
Ryacas,Interface to the 'yacas' computer algebra system (<http://www.yacas.org/>).,2020-11-15,Mikkel Meyer Andersen,"https://github.com/r-cas/ryacas, http://www.yacas.org",TRUE,https://github.com/r-cas/ryacas,46360,20,2021-03-02T13:36:07Z,2318
Ryacas0,"A legacy version of 'Ryacas', an interface to the 'yacas' computer algebra system (<http://www.yacas.org/>).",2020-09-29,Mikkel Meyer Andersen,"https://github.com/r-cas/ryacas0, http://www.yacas.org",TRUE,https://github.com/r-cas/ryacas0,10674,1,2020-10-01T18:55:02Z,10674
ryandexdirect,"Load data from 'Yandex Direct' API V5 
    <https://tech.yandex.ru/direct/doc/dg/concepts/about-docpage/> into R.
	Provide function for load lists of campaings, ads, keywords and other 
	objects from 'Yandex Direct' account. Also you can load statistic from
	API 'Reports Service' <https://tech.yandex.ru/direct/doc/reports/reports-docpage/>.
	And allows keyword bids management.",2020-06-09,Alexey Seleznev,"https://selesnow.github.io/ryandexdirect,
https://t.me/R4marketing,
https://www.youtube.com/playlist?list=PLD2LDq8edf4oUo0L9Kw77ZXf0KcV1hu67",TRUE,https://github.com/selesnow/ryandexdirect,15301,50,2021-05-25T14:05:15Z,306.02
rym,"Allows work with 'Management API' for load counters, segments, filters,
	user permissions and goals list from Yandex Metrica, 'Reporting API' allows you to get 
	information about the statistics of site visits and other data without
	using the web interface, 'Logs API' allows to receive non-aggregated data and 
	'Compatible with Google Analytics Core Reporting API v3' allows 
	receive information about site traffic and other data using field names 
	from Google Analytics Core API.	For more information see official 
	documents <https://yandex.ru/dev/metrika/doc/api2/concept/about-docpage>.",2021-09-03,Alexey Seleznev,https://selesnow.github.io/rym/,TRUE,https://github.com/selesnow/rym,16809,9,2021-09-03T07:02:12Z,1867.6666666666667
rzmq,Interface to the 'ZeroMQ' lightweight messaging kernel (see <https://zeromq.org/> for more information).,2021-05-04,Jeroen Ooms,https://docs.ropensci.org/rzmq/,TRUE,https://github.com/ropensci/rzmq,57276,82,2021-05-04T15:26:13Z,698.4878048780488
s2,"Provides R bindings for Google's s2 library for geometric calculations on
    the sphere. High-performance constructors and exporters provide high compatibility
    with existing spatial packages, transformers construct new geometries from existing
    geometries, predicates provide a means to select geometries based on spatial 
    relationships, and accessors extract information about geometries.",2021-06-17,Dewey Dunnington,"https://r-spatial.github.io/s2/, https://github.com/r-spatial/s2,
https://s2geometry.io/",TRUE,https://github.com/r-spatial/s2,416039,40,2021-06-17T12:23:56Z,10400.975
sabre,"Calculates a degree of spatial association between regionalizations 
    or categorical maps using the information-theoretical V-measure 
    (Nowosad and Stepinski (2018) <doi:10.1080/13658816.2018.1511794>). It also
    offers an R implementation of the MapCurve method 
    (Hargrove et al. (2006) <doi:10.1007/s10109-006-0025-x>).",2021-06-03,Jakub Nowosad,https://nowosad.github.io/sabre/,TRUE,https://github.com/nowosad/sabre,14677,31,2021-06-06T10:42:05Z,473.4516129032258
sadists,"Provides the density, distribution, quantile and generation functions of some obscure probability 
    distributions, including the doubly non-central t, F, Beta, and Eta distributions; 
    the lambda-prime and K-prime; the upsilon distribution; the (weighted) sum of 
    non-central chi-squares to a power; the (weighted) sum of log non-central chi-squares;
    the product of non-central chi-squares to powers; the product of doubly non-central
    F variables; the product of independent normals.",2020-06-26,Steven E. Pav,https://github.com/shabbychef/sadists,TRUE,https://github.com/shabbychef/sadists,22921,6,2021-04-03T22:35:36Z,3820.1666666666665
saekernel,"Propose an area-level, non-parametric regression estimator based on Nadaraya-Watson kernel on small area mean. Adopt a two-stage estimation approach proposed by Prasad and Rao (1990). Mean Squared Error (MSE) estimators are not readily available, so resampling method that called bootstrap is applied. This package are based on the model proposed in Two stage non-parametric approach for small area estimation by Pushpal Mukhopadhyay and Tapabrata Maiti(2004) <http://www.asasrms.org/Proceedings/y2004/files/Jsm2004-000737.pdf>.",2021-06-04,Wicak Surya Hasani,https://github.com/wicaksh/saekernel,TRUE,https://github.com/wicaksh/saekernel,1010,0,2021-06-04T00:23:26Z,NA
SAFARI,"Provides functionality for image processing and shape analysis in 
    the context of reconstructed medical images generated by deep learning-based
    methods or standard image processing algorithms and produced from different
    medical imaging types, such as X-ray, Computational Tomography (CT),
    Magnetic Resonance Imaging (MRI), and pathology imaging. Specifically,
    offers tools to segment regions of interest and to extract quantitative
    shape descriptors for applications in signal processing,
    statistical analysis and modeling, and machine learning.",2021-02-25,Esteban Fernandez Morales,https://github.com/estfernandez/SAFARI,TRUE,https://github.com/estfernandez/safari,1939,0,2021-03-02T05:18:32Z,NA
safedata,"The SAFE Project (<https://www.safeproject.net/>) is a large 
	scale ecological experiment in Malaysian Borneo that explores the impact 
	of habitat fragmentation and conversion on ecosystem function and services. 
	Data collected at the SAFE Project is made available under a common format 
	through the Zenodo data repository and this package makes it easy to 
	discover and load that data into R.",2020-11-18,David Orme,https://imperialcollegelondon.github.io/safedata/index.html,TRUE,https://github.com/imperialcollegelondon/safedata,6396,2,2020-11-18T18:22:57Z,3198
safejoin,The goal of 'safejoin' is to guarantee that when performing joins extra rows are not added to your data. 'safejoin' provides a wrapper around 'dplyr::left_join' that will raise an error when extra rows are unexpectedly added to your data. This can be useful when working with data where you expect there to be a many to one relationship but you are not certain the relationship holds.,2021-04-26,Sam Edwardes,https://github.com/SamEdwardes/safejoin,TRUE,https://github.com/samedwardes/safejoin,1535,0,2021-05-05T15:35:42Z,NA
safetyGraphics,A framework for evaluation of clinical trial safety. Users can interactively explore their data using the 'Shiny' application or create standalone 'htmlwidget' charts. Interactive charts are built using 'd3.js' and 'webcharts.js' 'JavaScript' libraries.,2020-01-15,Jeremy Wildfire,https://github.com/SafetyGraphics/safetyGraphics,TRUE,https://github.com/safetygraphics/safetygraphics,11671,48,2021-09-03T01:32:05Z,243.14583333333334
sageR,"Datasets and functions for the book ""Statistiques pour l’économie et la gestion"", ""Théorie et applications en entreprise"", F. Bertrand, Ch. Derquenne, G. Dufrénot, F. Jawadi and M. Maumy, C. Borsenberger editor, (2021, ISBN:9782807319448, De Boeck Supérieur, Louvain-la-Neuve). 
    The first chapter of the book is dedicated to an introduction to statistics and their world. 
    The second chapter deals with univariate exploratory statistics and graphics. 
    The third chapter deals with bivariate and multivariate exploratory statistics and graphics. 
    The fourth chapter is dedicated to data exploration with Principal Component Analysis. 
    The fifth chapter is dedicated to data exploration with Correspondance Analysis.
    The sixth chapter is dedicated to data exploration with Multiple Correspondance Analysis. 
    The seventh chapter is dedicated to data exploration with automatic clustering. 
    The eighth chapter is dedicated to an introduction to probability theory and classical probability distributions.
    The ninth chapter is dedicated to an estimation theory, one-sample and two-sample tests.
    The tenth chapter is dedicated to an Gaussian linear model.
    The eleventh chapter is dedicated to an introduction to time series.
    The twelfth chapter is dedicated to an introduction to probit and logit models.
    Various example datasets are shipped with the package as well as some new functions.",2021-07-20,Frederic Bertrand,"https://fbertran.github.io/homepage/,
https://fbertran.github.io/sageR/,
https://github.com/fbertran/sageR/",TRUE,https://github.com/fbertran/sager,650,1,2021-08-07T16:13:36Z,650
sail,"Sparse additive interaction learning with the strong heredity property, i.e., 
    an interaction is selected only if its corresponding main effects are also included. 
    Fits a linear model with non-linear interactions via penalized maximum likelihood. 
    Interactions are limited to a single exposure or environment variable. For more information, 
    see the website below and the accompanying paper: Bhatnagar et al., ""A sparse additive model for 
    high-dimensional interactions with an exposure variable"", 2019, <DOI:10.1101/445304>.",2019-12-13,Sahir Bhatnagar,https://sahirbhatnagar.com/sail,TRUE,https://github.com/sahirbhatnagar/sail,7727,4,2021-07-17T00:14:56Z,1931.75
salesforcer,"Functions connecting to the 'Salesforce' Platform APIs (REST, SOAP, 
    Bulk 1.0, Bulk 2.0, Metadata, Reports and Dashboards) 
    <https://trailhead.salesforce.com/en/content/learn/modules/api_basics/api_basics_overview>. 
    ""API"" is an acronym for ""application programming interface"". Most all calls 
    from these APIs are supported as they use CSV, XML or JSON data that can be 
    parsed into R data structures. For more details please see the 'Salesforce' 
    API documentation and this package's website 
    <https://stevenmmortimer.github.io/salesforcer/> for more information, 
    documentation, and examples.",2021-07-06,Steven M. Mortimer,https://github.com/StevenMMortimer/salesforcer,TRUE,https://github.com/stevenmmortimer/salesforcer,33724,58,2021-07-07T03:32:39Z,581.448275862069
samc,"Implements functions for working with absorbing Markov chains. The
    implementation is based on the framework described in ""Toward a unified
    framework for connectivity that disentangles movement and mortality in space
    and time"" by Fletcher et al. (2019) <doi:10.1111/ele.13333>, which applies
    them to spatial ecology. This framework incorporates both resistance and 
    absorption with spatial absorbing Markov chains (SAMC) to provide several
    short-term and long-term predictions for metrics related to connectivity in 
    landscapes. Despite the ecological context of the framework, this package
    can be used in any application of absorbing Markov chains.",2021-09-02,Andrew Marx,https://andrewmarx.github.io/samc/,TRUE,https://github.com/andrewmarx/samc,11296,3,2021-06-29T12:31:59Z,3765.3333333333335
SAMGEP,A novel semi-supervised machine learning algorithm to predict phenotype event times using Electronic Health Record (EHR) data.,2021-01-06,Yuri Ahuja,https://github.com/celehs/SAMGEP,TRUE,https://github.com/celehs/samgep,2544,1,2021-02-23T07:10:18Z,2544
sampler,"Determine sample sizes, draw samples, and conduct data analysis using data frames. It specifically enables you to determine simple random sample sizes, stratified sample sizes, and complex stratified sample sizes using a secondary variable such as population; draw simple random samples and stratified random samples from sampling data frames; determine which observations are missing from a random sample, missing by strata, duplicated within a dataset; and perform data analysis, including proportions, margins of error and upper and lower bounds for simple, stratified and cluster sample designs. ",2019-09-15,Michael Baldassaro,https://github.com/mbaldassaro/sampler,TRUE,https://github.com/mbaldassaro/sampler,17342,8,2021-05-26T20:28:07Z,2167.75
samplingbook,Sampling procedures from the book 'Stichproben - Methoden und praktische Umsetzung mit R' by Goeran Kauermann and Helmut Kuechenhoff (2010).,2021-04-02,Juliane Manitz,https://www.samplingbook.manitz.org,TRUE,https://github.com/jmanitz/samplingbook,54962,1,2021-04-02T21:13:07Z,54962
SamplingStrata,"In the field of stratified sampling design, this package offers an approach for the determination of the best stratification of a sampling frame, the one that ensures the minimum sample cost under the condition to satisfy precision constraints in a multivariate and multidomain case. This approach is based on the use of the genetic algorithm: each solution (i.e. a particular partition in strata of the sampling frame) is considered as an individual in a population; the fitness of all individuals is evaluated applying the Bethel-Chromy algorithm to calculate the sampling size satisfying precision constraints on the target estimates. Functions in the package allows to: (a) analyse the obtained results of the optimisation step; (b) assign the new strata labels to the sampling frame; (c) select a sample from the new frame accordingly to the best allocation. Functions for the execution of the genetic algorithm are a modified version of the functions in the 'genalg' package.  ",2020-03-02,Giulio Barcaroli,https://barcaroli.github.io/SamplingStrata,TRUE,https://github.com/barcaroli/samplingstrata,26015,7,2021-08-05T15:08:19Z,3716.4285714285716
sampsizeval,"Estimation of the required sample size to validate a risk model for binary outcomes, based on the sample size equations proposed by Pavlou et al. (2021) <doi:10.1177/09622802211007522>. For precision-based sample size calculations, the user is required to enter the anticipated values of the C-statistic and outcome prevalence, which can be obtained from a previous study. The user also needs to specify the required precision (standard error) for the C-statistic, the calibration slope and the calibration in the large. The calculations are valid under the assumption of marginal normality for the distribution of the linear predictor. ",2021-05-28,Menelaos Pavlou,https://github.com/mpavlou/sampsizeval,TRUE,https://github.com/mpavlou/sampsizeval,1078,1,2021-05-28T09:43:21Z,1078
SAMtool,"Simulation tools for closed-loop simulation are provided for the 'MSEtool' operating model to inform data-rich fisheries. 
  'SAMtool' provides a conditioning model, assessment models of varying complexity with standardized reporting, 
  model-based management procedures, and diagnostic tools for evaluating assessments inside closed-loop simulation.",2021-08-12,Quang Huynh,https://github.com/Blue-Matter/SAMtool,TRUE,https://github.com/blue-matter/samtool,3947,1,2021-08-12T02:06:47Z,3947
sanic,"Routines for solving large systems of linear equations in R.
  Direct and iterative solvers from the Eigen C++ library are made available.
  Solvers include Cholesky, LU, QR, and Krylov subspace methods (Conjugate
  Gradient, BiCGSTAB). Both dense and sparse problems are supported.",2020-09-22,Nikolas Kuschnig,https://github.com/nk027/sanic,TRUE,https://github.com/nk027/sanic,3870,4,2021-06-07T12:50:40Z,967.5
sanityTracker,"During the preparation of data set(s) one usually performs
    some sanity checks. The idea is that irrespective of where the
    checks are performed, they are centralized by this package in order
    to list all at once with examples if a check failed.",2020-04-22,Marsel Scheer,https://github.com/MarselScheer/sanityTracker,TRUE,https://github.com/marselscheer/sanitytracker,4757,0,2021-09-01T21:04:50Z,NA
santoku,"A tool for cutting data into intervals. Allows singleton intervals.
  Always includes the whole range of data by default. Flexible labelling. 
  Convenience functions for cutting by quantiles etc. Handles dates and times.",2020-08-27,David Hugh-Jones,"https://github.com/hughjonesd/santoku,
https://hughjonesd.github.io/santoku/",TRUE,https://github.com/hughjonesd/santoku,11425,120,2021-04-17T15:36:32Z,95.20833333333333
saqgetr,"A collection of tools to access prepared air quality monitoring
    data files from web servers with ease and speed. Air quality data are 
    sourced from open and publicly accessible repositories and can be found in 
    these locations: 
    <https://www.eea.europa.eu/data-and-maps/data/airbase-the-european-air-quality-database-8> 
    and <https://discomap.eea.europa.eu/map/fme/AirQualityExport.htm>. The web 
    server space has been provided by Ricardo Energy & Environment.",2021-01-12,Stuart K. Grange,https://github.com/skgrange/saqgetr,TRUE,https://github.com/skgrange/saqgetr,11424,6,2021-01-11T13:27:31Z,1904
SAR,"'Smart Adaptive Recommendations' (SAR) is the name of a fast, scalable, adaptive algorithm for personalized recommendations based on user transactions and item descriptions. It produces easily explainable/interpretable recommendations and handles ""cold item"" and ""semi-cold user"" scenarios. This package provides two implementations of 'SAR': a standalone implementation, and an interface to a web service in Microsoft's 'Azure' cloud: <https://github.com/Microsoft/Product-Recommendations/blob/master/doc/sar.md>. The former allows fast and easy experimentation, and the latter provides robust scalability and extra features for production use.",2020-10-07,Hong Ooi,https://github.com/hongooi73/SAR,TRUE,https://github.com/hongooi73/sar,11883,17,2020-10-06T21:59:03Z,699
sars,"Implements the basic elements of the multi-model
    inference paradigm for up to twenty species-area relationship models (SAR), using simple
    R list-objects and functions, as in Triantis et al. 2012 <DOI:10.1111/j.1365-2699.2011.02652.x>.
    The package is scalable and users can easily create their own model and data objects. Additional
    SAR related functions are provided.",2021-08-05,Thomas J. Matthews,"https://github.com/txm676/sars, https://txm676.github.io/sars/",TRUE,https://github.com/txm676/sars,17673,6,2021-08-05T09:23:02Z,2945.5
sarsop,"A toolkit for Partially Observed Markov Decision Processes (POMDP). Provides
    bindings to C++ libraries implementing the algorithm SARSOP (Successive Approximations
    of the Reachable Space under Optimal Policies) and described in Kurniawati et al (2008),
    <doi:10.15607/RSS.2008.IV.009>.  This package also provides a high-level interface
    for generating, solving and simulating POMDP problems and their solutions.",2021-08-05,Carl Boettiger,https://github.com/boettiger-lab/sarsop,TRUE,https://github.com/boettiger-lab/sarsop,5788,5,2021-08-04T20:36:23Z,1157.6
sasfunclust,Implements the sparse and smooth functional clustering (SaS-Funclust) method (Centofanti et al. (2021) <arXiv:2103.15224>) that aims to classify a sample of curves into homogeneous groups while jointly detecting the most informative portions of domain.,2021-04-02,Fabio Centofanti,https://github.com/unina-sfere/sasfunclust,TRUE,https://github.com/unina-sfere/sasfunclust,1319,0,2021-04-23T19:51:14Z,NA
sasMap,"A static code analysis tool for 'SAS' scripts. It is designed to load, count, extract, remove, and summarise components of 'SAS' code.",2017-08-18,Nic Crane,https://github.com/MangoTheCat/sasMap,TRUE,https://github.com/mangothecat/sasmap,13538,6,2021-08-31T14:16:52Z,2256.3333333333335
SASmarkdown,Settings and functions to extend the 'knitr' 'SAS' engine.,2017-11-30,Doug Hemken  (SSCC,http://www.ssc.wisc.edu/~hemken/SASworkshops/sas.html#writing-sas-documentation,TRUE,https://github.com/hemken/sasmarkdown,23702,6,2021-05-21T23:14:12Z,3950.3333333333335
sass,"An 'SCSS' compiler, powered by the 'LibSass' library. With this,
    R developers can use variables, inheritance, and functions to generate
    dynamic style sheets. The package uses the 'Sass CSS' extension language,
    which is stable, powerful, and CSS compatible.",2021-05-12,Richard Iannone,https://github.com/rstudio/sass,TRUE,https://github.com/rstudio/sass,2291777,91,2021-05-27T18:48:56Z,25184.362637362636
sassy,"A meta-package that aims to make 'R' easier for 'SAS®' programmers.
    This set of packages brings many familiar concepts to 'R', including
    data libraries, data dictionaries, formats 
    and format catalogs, a data step, and a traceable log.  The 'flagship'
    package is a reporting package that can output in text, rich text, and 
    'PDF' file formats.",2021-08-07,David J. Bosak,https://r-sassy.org,TRUE,https://github.com/dbosak01/sassy,2187,0,2021-08-10T14:17:43Z,NA
satin,"With 'satin' functions, visualisation, data extraction and further analysis like producing climatologies from several images, and anomalies of satellite derived ocean data can be easily done.  Reading functions can import a user defined geographical extent of data stored in netCDF files.  Currently supported ocean data sources include NASA's Oceancolor web page <https://oceancolor.gsfc.nasa.gov/>, sensors VIIRS-SNPP; MODIS-Terra; MODIS-Aqua; and SeaWiFS.  Available variables from this source includes chlorophyll concentration, sea surface temperature (SST), and several others.  Data sources specific for SST that can be imported too includes Pathfinder AVHRR <https://www.ncei.noaa.gov/products/avhrr-pathfinder-sst> and GHRSST <https://www.ghrsst.org/>.  In addition, ocean productivity data produced by Oregon State University <http://sites.science.oregonstate.edu/ocean.productivity/> can also be handled previous conversion from HDF4 to HDF5 format.  Many other ocean variables can be processed by importing netCDF data files from two European Union's Copernicus Marine Service databases <https://marine.copernicus.eu/>, namely Global Ocean Physical Reanalysis and Global Ocean Biogeochemistry Hindcast.",2020-10-07,Héctor Villalobos and Eduardo González-Rodríguez,https://github.com/hvillalo/satin,TRUE,https://github.com/hvillalo/satin,4317,2,2021-07-19T18:36:26Z,2158.5
SAVER,"An implementation of a regularized regression prediction and 
    empirical Bayes method to recover the true gene expression profile in 
    noisy and sparse single-cell RNA-seq data. See Huang M, et al (2018) 
    <doi:10.1038/s41592-018-0033-z> for more details.",2019-11-13,Mo Huang,https://github.com/mohuangx/SAVER,TRUE,https://github.com/mohuangx/saver,15983,80,2021-01-08T01:17:26Z,199.7875
savonliquide,Provides a toolbox that allows the user to implement accessibility related concepts. ,2021-02-22,Mohamed El Fodil Ihaddaden,https://github.com/feddelegrand7/savonliquide,TRUE,https://github.com/feddelegrand7/savonliquide,3056,13,2021-02-26T21:46:45Z,235.07692307692307
sazedR,"Spectral and Average Autocorrelation Zero Distance Density
    ('sazed') is a method for estimating the season length of a 
    seasonal time series. 'sazed' is aimed at practitioners, as it employs only 
    domain-agnostic preprocessing and does not depend on parameter tuning or 
    empirical constants. The computation of 'sazed' relies on the efficient 
    autocorrelation computation methods suggested by Thibauld Nion (2012, URL: 
    <https://etudes.tibonihoo.net/literate_musing/autocorrelations.html>) and by 
    Bob Carpenter (2012, URL: 
    <https://lingpipe-blog.com/2012/06/08/autocorrelation-fft-kiss-eigen/>).",2020-09-29,Tiago Santos,https://github.com/mtoller/autocorr_season_length_detection/,TRUE,https://github.com/mtoller/autocorr_season_length_detection,15717,5,2020-09-30T05:18:38Z,3143.4
sbm,"A collection of tools and functions to adjust a variety of stochastic blockmodels (SBM). 
  Supports at the moment Simple, Bipartite, 'Multipartite' and Multiplex SBM (undirected or directed with Bernoulli,
  Poisson or Gaussian emission laws on the edges, and possibly covariate for Simple and Bipartite SBM).
  See Léger (2016) <arxiv:1602.07587>, 'Barbillon et al.' (2020) <doi:10.1111/rssa.12193> and 
  'Bar-Hen et al.' (2020) <arxiv:1807.10138>.",2021-06-09,Julien Chiquet,https://grosssbm.github.io/sbm/,TRUE,https://github.com/grosssbm/sbm,6965,4,2021-06-09T17:56:39Z,1741.25
sbo,"Utilities for training and evaluating text predictors based on Stupid Back-Off N-gram models (Brants et al., 2007, <https://www.aclweb.org/anthology/D07-1090/>).",2020-12-05,Valerio Gherardi,"https://vgherard.github.io/sbo/, https://github.com/vgherard/sbo",TRUE,https://github.com/vgherard/sbo,3757,7,2021-07-07T13:49:55Z,536.7142857142857
sbtools,"Tools for interacting with U.S. Geological Survey ScienceBase
    <https://www.sciencebase.gov> interfaces. ScienceBase is a data cataloging and
    collaborative data management platform. Functions included for querying
    ScienceBase, and creating and fetching datasets.",2021-07-20,David Blodgett,https://github.com/USGS-R/sbtools,TRUE,https://github.com/usgs-r/sbtools,19397,18,2021-06-07T15:28:57Z,1077.611111111111
scaffolder,"Comprehensive set of tools for scaffolding R
  interfaces to modules, classes, functions, and documentations
  written in other programming languages, such as 'Python'.",2020-03-20,Yuan Tang,https://github.com/terrytangyuan/scaffolder,TRUE,https://github.com/terrytangyuan/scaffolder,6483,27,2020-10-22T19:07:45Z,240.11111111111111
scales,"Graphical scales map data to aesthetics, and
    provide methods for automatically determining breaks and labels for
    axes and legends.",2020-05-11,Hadley Wickham,"https://scales.r-lib.org, https://github.com/r-lib/scales",TRUE,https://github.com/r-lib/scales,20726930,280,2021-03-26T16:42:16Z,74024.75
scatterD3,"Creates 'D3' 'JavaScript' scatterplots from 'R' with interactive
    features : panning, zooming, tooltips, etc.",2021-06-24,Julien Barnier,https://juba.github.io/scatterD3/,TRUE,https://github.com/juba/scatterd3,58361,143,2021-08-27T13:27:46Z,408.1188811188811
scattermore,"C-based conversion of large scatterplot data to rasters. Speeds up
             plotting of data with millions of points.",2020-11-24,Mirek Kratochvil,https://github.com/exaexa/scattermore,TRUE,https://github.com/exaexa/scattermore,101044,80,2021-01-30T09:56:03Z,1263.05
sccore,"Core utilities for single-cell RNA-seq data analysis. Contained within are utility functions for working with differential expression (DE) matrices and count matrices, a collection of functions for manipulating and plotting data via 'ggplot2', and functions to work with cell graphs and cell embeddings. Graph-based methods include embedding kNN cell graphs into a UMAP <doi:10.21105/joss.00861>, collapsing vertices of each cluster in the graph, and propagating graph labels.",2021-05-05,Evan Biederstedt,https://github.com/kharchenkolab/sccore,TRUE,https://github.com/kharchenkolab/sccore,8413,7,2021-05-05T05:43:20Z,1201.857142857143
scDHA,"Provides a fast and accurate pipeline for single-cell analyses. 
    The 'scDHA' software package can perform clustering, dimension reduction and visualization, classification, and time-trajectory inference on single-cell data (Tran et.al. (2021) <DOI:10.1038/s41467-021-21312-2>).",2021-06-10,Duc Tran,https://github.com/duct317/scDHA,TRUE,https://github.com/duct317/scdha,1109,18,2021-06-12T23:12:13Z,61.611111111111114
scdhlm,"Provides a set of tools for estimating hierarchical linear
    models and effect sizes based on data from single-case designs. 
    Functions are provided for calculating standardized mean difference effect sizes that 
    are directly comparable to standardized mean differences estimated from between-subjects randomized experiments,
    as described in Hedges, Pustejovsky, and Shadish (2012) <DOI:10.1002/jrsm.1052>; 
    Hedges, Pustejovsky, and Shadish (2013) <DOI:10.1002/jrsm.1086>; and 
    Pustejovsky, Hedges, and Shadish (2014) <DOI:10.3102/1076998614547577>. 
    Includes an interactive web interface.",2021-01-07,James Pustejovsky,https://jepusto.github.io/scdhlm/,TRUE,https://github.com/jepusto/scdhlm,20016,2,2021-04-29T21:00:01Z,10008
SCEM,"We introduce improved methods for statistically assessing birth seasonality and intra-annual variation. The first method we propose is a new idea that uses a nonparametric clustering procedure to group individuals with similar time series data and estimate birth seasonality based on the clusters. One can use the function SCEM() to implement this method. The second method estimates input parameters for use with a previously-developed parametric approach (Tornero et al., 2013). The relevant code for this approach is makeFits_OLS(), while makeFits_initial() is the code to implement the same method but with given initial conditions for two parameters. The latter can be used to show the disadvantage of the existing approach. One can use the function makeFits() to generate parametric birth seasonality estimates using either initialization. Detailed description can be found here: Chazin Hannah, Soudeep Deb, Joshua Falk, and Arun Srinivasan. (2019) ""New Statistical Approaches to Intra-Individual Isotopic Analysis and Modeling Birth Seasonality in Studies of Herd Animals."" <doi:10.1111/arcm.12432>.",2021-09-02,Kyung Serk Cho,https://github.com/kserkcho/SCEM,TRUE,https://github.com/kserkcho/scem,345,0,2021-08-28T14:59:16Z,NA
scenario,"Uses the neural gas algorithm to construct
    a scenario tree for use in multi-stage stochastic programming.
    The primary input is a set of initial scenarios or realizations
    of a disturbance. The scenario tree nodal structure must be
    predefined using a scenario tree nodal partition matrix.",2016-02-17,Sean Turner,https://github.com/swd-turner/scenario,TRUE,https://github.com/swd-turner/scenario,13815,1,2021-06-06T20:07:00Z,13815
SCGLR,"
    An extension of the Fisher Scoring Algorithm to combine PLS regression with GLM 
    estimation in the multivariate context. Covariates can also be grouped in themes.",2018-09-28,Guillaume Cornu,"https://scnext.github.io/SCGLR, https://github.com/SCnext/SCGLR,
https://cran.r-project.org/package=SCGLR",TRUE,https://github.com/scnext/scglr,17331,0,2021-07-08T15:51:28Z,NA
scholar,"Provides functions to extract citation data from Google
    Scholar.  Convenience functions are also provided for comparing
    multiple scholars and predicting future h-index values.",2021-07-13,Guangchuang Yu,https://github.com/YuLab-SMU/scholar,TRUE,https://github.com/yulab-smu/scholar,42150,2,2021-07-14T03:03:54Z,21075
schrute,"The complete scripts from the American version of
    the Office television show in tibble format. Use this package to
    analyze and have fun with text from the best series of all time.",2020-06-30,Brad Lindblad,https://github.com/bradlindblad/schrute,TRUE,https://github.com/bradlindblad/schrute,12726,15,2021-08-15T21:36:36Z,848.4
scipub,"Create and format tables and APA statistics for
    scientific publication. This includes making a 'Table 1'
    to summarize demographics across groups, correlation tables
    with significance indicated by stars, and extracting formatted
    statistical summarizes from simple tests for in-text notation.
    The package also includes functions for Winsorizing data based
    on a Z-statistic cutoff.  ",2021-03-18,David Pagliaccio,"https://github.com/dpagliaccio/scipub,
https://dpagliaccio.github.io/scipub/",TRUE,https://github.com/dpagliaccio/scipub,6479,1,2020-09-28T22:42:48Z,6479
SciViews,"Functions to install SciViews additions to R, and more
        tools.",2019-11-16,Philippe Grosjean,"https://github.com/SciViews/SciViews,
http://www.sciviews.org/SciViews-R",TRUE,https://github.com/sciviews/sciviews,38120,5,2021-06-01T07:29:16Z,7624
scopr,"Handling of behavioural data from the Ethoscope platform 
    (Geissmann, Garcia Rodriguez, Beckwith, French, Jamasb and Gilestro (2017) <DOI:10.1371/journal.pbio.2003026>).
    Ethoscopes (<http://gilestrolab.github.io/ethoscope/>) are an open source/open hardware framework made of 
    interconnected raspberry pis (<https://www.raspberrypi.org>) designed to quantify the behaviour of multiple 
    small animals in a distributed and real-time fashion. The default tracking algorithm records primary variables
    such as xy coordinates, dimensions and speed.
    This package is part of the rethomics framework <http://rethomics.github.io/>.",2019-02-15,Quentin Geissmann,https://github.com/rethomics/scopr,TRUE,https://github.com/rethomics/scopr,12790,3,2021-06-11T08:53:10Z,4263.333333333333
scorepeak,"Provides peak functions, which enable us to detect peaks in time series. The methods implemented in this package are based on Girish Keshav Palshikar (2009) <https://www.researchgate.net/publication/228853276_Simple_Algorithms_for_Peak_Detection_in_Time-Series>.",2019-08-21,Shota Ochi,https://github.com/ShotaOchi/scorepeak,TRUE,https://github.com/shotaochi/scorepeak,8469,0,2021-03-06T03:46:36Z,NA
scoringRules,"Dictionary-like reference for computing scoring rules in a wide
    range of situations. Covers both parametric forecast distributions (such as
    mixtures of Gaussians) and distributions generated via simulation.",2020-10-05,Alexander Jordan,https://github.com/FK83/scoringRules,TRUE,https://github.com/fk83/scoringrules,62835,33,2021-08-30T07:51:32Z,1904.090909090909
scoringTools,"Grouping essential tools for credit scoring. These statistical tools may be useful for other use-cases as well but were primarily designed for it. First, there are Reject Inference methods (Ehrhardt et al. (2017) <arXiv:1903.10855>). Second, we build upon the already CRAN-available package 'discretization' to automate discretization of continuous features.",2021-01-10,Adrien Ehrhardt,https://adimajo.github.io/scoringTools/,TRUE,https://github.com/adimajo/scoringtools,3986,3,2021-01-10T14:42:41Z,1328.6666666666667
scoringutils,"
    Combines a collection of metrics and proper scoring rules 
    (Tilmann Gneiting & Adrian E Raftery (2007) 
    <doi:10.1198/016214506000001437>) with an easy to
    use wrapper that can be used to automatically evaluate predictions. 
    Apart from proper scoring rules functions are provided to assess bias, 
    sharpness and calibration (Sebastian Funk, Anton Camacho, Adam J. Kucharski, 
    Rachel Lowe, Rosalind M. Eggo, W. John Edmunds (2019) 
    <doi:10.1371/journal.pcbi.1006785>) of forecasts. 
    Several types of predictions can be evaluated: 
    probabilistic forecasts (generally 
    predictive samples generated by Markov Chain Monte Carlo procedures), 
    quantile forecasts or point forecasts. Observed values and predictions
    can be either continuous, integer, or binary. Users can either choose 
    to apply these rules separately in a vector / matrix format that can 
    be flexibly used within other packages, or they can choose to do an 
    automatic evaluation of their forecasts. This is implemented with
    'data.table' and provides a consistent and very efficient framework for 
    evaluating various types of predictions. ",2021-07-21,Nikos Bosse,https://github.com/epiforecasts/scoringutils,TRUE,https://github.com/epiforecasts/scoringutils,8537,8,2021-08-24T19:35:59Z,1067.125
SCORNET,"A consistent, semi-supervised, non-parametric survival curve estimator optimized for efficient use of Electronic Health Record (EHR) data with a limited number of current status labels. See van der Laan and Robins (1997) <doi:10.2307/2670119>.",2021-01-04,Yuri Ahuja,https://github.com/celehs/SCORNET,TRUE,https://github.com/celehs/scornet,3312,1,2021-02-23T07:05:24Z,3312
SCORPIUS,"An accurate and easy tool for performing linear trajectory inference on
  single cells using single-cell RNA sequencing data. In addition, SCORPIUS
  provides functions for discovering the most important genes with respect to
  the reconstructed trajectory, as well as nice visualisation tools.
  Cannoodt et al. (2016) <doi:10.1101/079509>.",2021-06-09,Robrecht Cannoodt  (<https://orcid.org/0000-0003-3641-729X>,https://github.com/rcannood/SCORPIUS,TRUE,https://github.com/rcannood/scorpius,17218,35,2021-06-09T14:02:36Z,491.9428571428571
ScottKnott,"Perform the balanced (Scott and Knott, 1974) and unbalanced <doi:10.1590/1984-70332017v17n1a1> Scott & Knott algorithm.",2021-07-21,Enio Jelihovschi,"https://github.com/jcfaria/ScottKnott,
http://nbcgib.uesc.br/lec/software/pac-r/scottknott",TRUE,https://github.com/jcfaria/scottknott,29916,1,2020-10-15T23:55:15Z,29916
ScottKnottESD,"The Scott-Knott Effect Size Difference (ESD) test is a mean comparison approach that leverages a hierarchical clustering to partition the set of treatment means (e.g., means of variable importance scores, means of model performance) into statistically distinct groups with non-negligible difference [Tantithamthavorn et al., (2018) <doi:10.1109/TSE.2018.2794977>].",2018-05-08,Chakkrit Tantithamthavorn,https://github.com/klainfo/ScottKnottESD,TRUE,https://github.com/klainfo/scottknottesd,15830,21,2021-02-09T00:16:13Z,753.8095238095239
scrappy,"A group of functions to scrape data from different websites, for 
    academic purposes.",2021-01-09,Roberto Villegas-Diaz,"https://github.com/villegar/scrappy/,
https://villegar.github.io/scrappy/",TRUE,https://github.com/villegar/scrappy,2598,2,2021-08-10T21:04:49Z,1299
scriptexec,"Run complex native scripts with a single command, similar to system commands.",2019-04-12,Sagie Gur-Ari,https://github.com/sagiegurari/scriptexec,TRUE,https://github.com/sagiegurari/scriptexec,13967,3,2020-10-03T08:24:43Z,4655.666666666667
ScriptMapR,Displays the content of a R script into the 'Cytoscape' network-visualization app <https://cytoscape.org/>.,2020-09-24,Raphael Bonnet,NA,TRUE,https://github.com/peyronlab/scriptmapr,4005,4,2020-09-29T13:46:44Z,1001.25
scrobbler,"'Last.fm'<https://www.last.fm> is a music platform focussed on building a 
    detailed profile of a users listening habits. It does this by 'scrobbling' (recording) 
    every track you listen to on other platforms ('spotify', 'youtube', 'soundcloud' etc)
    and transferring them to your 'Last.fm' database. This allows 'Last.fm' to act as a 
    complete record of your entire listening history. 'scrobbler' provides helper functions
    to download and analyse your listening history in R.",2021-08-15,Conor Neilson,https://github.com/condwanaland/scrobbler,TRUE,https://github.com/condwanaland/scrobbler,12316,1,2021-08-29T04:04:58Z,12316
scrollrevealR,"Allows the user to animate 'shiny' elements when scrolling to view them.
    The animations are activated using the 'scrollrevealjs' library. See <https://scrollrevealjs.org/> for more information.",2020-10-14,Mohamed El Fodil Ihaddaden,https://github.com/feddelegrand7/scrollrevealR,TRUE,https://github.com/feddelegrand7/scrollrevealr,4981,17,2020-12-09T21:52:46Z,293
scrubr,"Clean biological occurrence records. Includes functionality
    for cleaning based on various aspects of spatial coordinates,
    unlikely values due to political 'centroids', coordinates based on
    where collections of specimens are held, and more.",2021-06-12,Scott Chamberlain,"https://github.com/ropensci/scrubr (devel)
https://docs.ropensci.org/scrubr/ (docs)",TRUE,https://github.com/ropensci/scrubr,24820,30,2021-06-11T22:43:44Z,827.3333333333334
scTenifoldKnk,"A workflow based on 'scTenifoldNet' to perform in-silico knockout experiments using single-cell RNA sequencing (scRNA-seq) data from wild-type (WT) control samples as input.  First, the package constructs a single-cell gene regulatory network (scGRN) and knocks out a target gene from the adjacency matrix of the WT scGRN by setting the gene’s outdegree edges to zero. Then, it compares the knocked out scGRN with the WT scGRN to identify differentially regulated genes, called virtual-knockout perturbed genes, which are used to assess the impact of the gene knockout and reveal the gene’s function in the analyzed cells.",2021-01-22,Daniel Osorio,https://github.com/cailab-tamu/scTenifoldKnk,TRUE,https://github.com/cailab-tamu/sctenifoldknk,3155,12,2021-08-18T12:19:08Z,262.9166666666667
scTenifoldNet,"A workflow based on machine learning methods to construct and compare single-cell gene regulatory networks (scGRN) using single-cell RNA-seq (scRNA-seq) data collected from different conditions. Uses principal component regression, tensor decomposition, and manifold alignment, to accurately identify even subtly shifted gene expression programs. See <doi:10.1016/j.patter.2020.100139> for more details.",2020-11-17,Daniel Osorio,https://github.com/cailab-tamu/scTenifoldNet,TRUE,https://github.com/cailab-tamu/sctenifoldnet,11484,8,2021-08-21T11:49:02Z,1435.5
sctransform,"A normalization method for single-cell UMI count data using a 
  variance stabilizing transformation. The transformation is based on a 
  negative binomial regression model with regularized parameters. As part of the
  same regression framework, this package also provides functions for
  batch correction, and data correction. See Hafemeister and Satija 2019 
  <doi:10.1186/s13059-019-1874-1> for more details.",2020-12-16,Christoph Hafemeister,https://github.com/ChristophH/sctransform,TRUE,https://github.com/christophh/sctransform,311214,119,2021-06-28T08:20:26Z,2615.2436974789916
scutr,"Imbalanced training datasets impede many popular classifiers. To balance training data, a combination of oversampling minority classes and undersampling majority classes is useful. This package implements the SCUT (SMOTE and Cluster-based Undersampling Technique) algorithm as described in Agrawal et. al. (2015) <doi:10.5220/0005595502260234>. Their paper uses model-based clustering and synthetic oversampling to balance multiclass training datasets, although other resampling methods are provided in this package.",2021-06-24,Keenan Ganz,https://github.com/s-kganz/scutr,TRUE,https://github.com/s-kganz/scutr,839,2,2021-06-24T01:27:46Z,419.5
sdcHierarchies,"Provides functionality to generate, (interactively) modify (by adding, removing and renaming nodes) and convert nested hierarchies between different formats.
  These tree like structures can be used to define for example complex hierarchical tables used for statistical disclosure control.",2021-08-05,Bernhard Meindl,https://github.com/bernhard-da/sdcHierarchies,TRUE,https://github.com/bernhard-da/sdchierarchies,17316,0,2021-08-05T10:38:57Z,NA
sdcLog,"Tools for researchers to explicitly show that their results
    comply to rules for statistical disclosure control imposed by research
    data centers. These tools help in checking descriptive statistics and
    models and in calculating extreme values that are not individual data.
    Also included is a simple function to create log files. The methods
    used here are described in the ""Guidelines for the checking of output
    based on microdata research"" by Bond, Brandt, and de Wolf (2015)
    <https://ec.europa.eu/eurostat/cros/system/files/dwb_standalone-document_output-checking-guidelines.pdf>.",2021-07-22,Matthias Gomolka,https://github.com/matthiasgomolka/sdcLog,TRUE,https://github.com/matthiasgomolka/sdclog,3469,3,2021-08-18T19:17:28Z,1156.3333333333333
sdcMicro,"Data from statistical agencies and other institutions are mostly
    confidential. This package (see also Templ, Kowarik and Meindl (2017) <doi:10.18637/jss.v067.i04>) can be used for the generation of anonymized
    (micro)data, i.e. for the creation of public- and scientific-use files.
    The theoretical basis for the methods implemented can be found in Templ (2017) <doi:10.1007/978-3-319-50272-4>.
    Various risk estimation and anonymisation methods are included. Note that the package
    includes a graphical user interface (Meindl and Templ, 2019 <doi:10.3390/a12090191>) that allows to use various methods of this
    package.",2021-07-26,Matthias Templ,https://github.com/sdcTools/sdcMicro,TRUE,https://github.com/sdctools/sdcmicro,100739,48,2021-07-26T12:54:27Z,2098.7291666666665
sdcSpatial,"Privacy protected raster maps 
  can be created from spatial point data. Protection
  methods include smoothing of dichotomous variables by de Jonge and de Wolf (2016) 
  <doi:10.1007/978-3-319-45381-1_9>, continuous variables by de Wolf and 
  de Jonge (2018) <doi:10.1007/978-3-319-99771-1_23>, suppressing 
  revealing values and a generalization of the quad tree method by 
  Suñé, Rovira, Ibáñez and Farré (2017) <doi:10.2901/EUROSTAT.C2017.001>.",2019-07-19,Edwin de Jonge,https://github.com/edwindj/sdcSpatial,TRUE,https://github.com/edwindj/sdcspatial,9693,5,2021-01-21T15:18:42Z,1938.6
sdcTable,"Methods for statistical disclosure control in
    tabular data such as primary and secondary cell suppression as described for example
    in Hundepol et al. (2012) <doi:10.1002/9781118348239> are covered in this package.",2021-08-06,Bernhard Meindl,https://github.com/sdcTools/sdcTable,TRUE,https://github.com/sdctools/sdctable,35204,3,2021-08-06T13:07:15Z,11734.666666666666
sdetorus,"Implementation of statistical methods for the estimation of
    toroidal diffusions. Several diffusive models are provided, most of them
    belonging to the Langevin family of diffusions on the torus. Specifically,
    the wrapped normal and von Mises processes are included, which can be seen
    as toroidal analogues of the Ornstein-Uhlenbeck diffusion. A collection of
    methods for approximate maximum likelihood estimation, organized in four
    blocks, is given: (i) based on the exact transition probability density,
    obtained as the numerical solution to the Fokker-Plank equation; (ii) based
    on wrapped pseudo-likelihoods; (iii) based on specific analytic
    approximations by wrapped processes; (iv) based on maximum likelihood of
    the stationary densities. The package allows the reproducibility of the
    results in García-Portugués et al. (2019) <doi:10.1007/s11222-017-9790-2>.",2021-08-19,Eduardo García-Portugués,https://github.com/egarpor/sdetorus,TRUE,https://github.com/egarpor/sdetorus,4573,2,2021-08-26T00:58:25Z,2286.5
SDLfilter,"Functions to filter GPS/Argos locations, as well as assessing the sample size for the analysis of animal distributions. The filters remove temporal and spatial duplicates, fixes located at a given height from estimated high tide line, and locations with high error as described in Shimada et al. (2012) <doi:10.3354/meps09747> and Shimada et al. (2016) <doi:10.1007/s00227-015-2771-0>. Sample size for the analysis of animal distributions can be assessed by the conventional area-based approach or the alternative probability-based approach as described in Shimada et al. (2021) <doi:10.1111/2041-210X.13506>. ",2021-07-20,Takahiro Shimada,https://github.com/TakahiroShimada/SDLfilter,TRUE,https://github.com/takahiroshimada/sdlfilter,17492,3,2021-07-20T08:21:36Z,5830.666666666667
sdmApp,"A 'shiny' 
    application that allows non-expert 'R' users to easily model
    species distribution. It offers a reproducible work flow for
    species distribution modeling into a single and user friendly environment.
    'sdmApp' takes 'raster' data (in format supported by the 'raster package') 
    and species occurrence data (several format supported) as input argument.
    It provides an interactive graphical user interface (GUI).",2021-07-07,Aboubacar HEMA,https://github.com/Abson-dev/sdmApp,TRUE,https://github.com/abson-dev/sdmapp,3015,2,2021-07-07T08:50:50Z,1507.5
sdmpredictors,"Terrestrial and marine predictors for species distribution modelling
    from multiple sources, including WorldClim <https://www.worldclim.org/>,,
    ENVIREM <https://envirem.github.io/>, Bio-ORACLE <https://bio-oracle.org/>
    and MARSPEC <http://www.marspec.org/>.",2020-10-13,Samuel Bosch,https://www.samuelbosch.com/p/sdmpredictors.html,TRUE,https://github.com/lifewatch/sdmpredictors,26832,19,2021-08-20T07:20:45Z,1412.2105263157894
SDMtune,"User-friendly framework that enables the training and the
    evaluation of species distribution models (SDMs). The package implements
    functions for data driven variable selection and model tuning and includes
    numerous utilities to display the results. All the functions used to select
    variables or to tune model hyperparameters have an interactive real-time
    chart displayed in the 'RStudio' viewer pane during their execution.",2021-07-17,Sergio Vignali,"https://consbiol-unibern.github.io/SDMtune/,
https://github.com/ConsBiol-unibern/SDMtune",TRUE,https://github.com/consbiol-unibern/sdmtune,15199,9,2021-07-16T15:00:04Z,1688.7777777777778
SEAGLE,"
    The explosion of biobank data offers immediate opportunities for 
    gene-environment (GxE) interaction studies of complex diseases because of the 
    large sample sizes and rich collection in genetic and non-genetic information. 
    However, the extremely large sample size also introduces new computational 
    challenges in GxE assessment, especially for set-based GxE variance component (VC) 
    tests, a widely used strategy to boost overall GxE signals and to evaluate the 
    joint GxE effect of multiple variants from a biologically meaningful unit 
    (e.g., gene). 
    We present 'SEAGLE', a Scalable Exact AlGorithm for Large-scale Set-based 
    GxE tests, to permit GxE VC test scalable to biobank data. 'SEAGLE' employs modern 
    matrix computations to achieve the same “exact” results as the original GxE VC 
    tests, and does not impose additional assumptions nor relies on approximations. 
    'SEAGLE' can easily accommodate sample sizes in the order of 10^5, is implementable 
    on standard laptops, and does not require specialized equipment. 
    The accompanying manuscript for this package can be found at Chi, Ipsen, Hsiao, 
    Lin, Wang, Lee, Lu, and Tzeng. (2021+) <arXiv:2105.03228>.",2021-06-07,Jocelyn Chi,https://github.com/jocelynchi/SEAGLE,TRUE,https://github.com/jocelynchi/seagle,982,0,2021-08-23T18:10:24Z,NA
searchConsoleR,"Provides an interface with the Google Search Console,
    formally called Google Webmaster Tools.",2019-09-06,Mark Edmondson,http://code.markedmondson.me/searchConsoleR/,TRUE,https://github.com/markedmondson1234/searchconsoler,33773,101,2021-04-20T13:55:03Z,334.38613861386136
searcher,"Provides a search interface to look up terms
    on 'Google', 'Bing', 'DuckDuckGo', 'Startpage', 'Ecosia', 'rseek',
    'Twitter', 'StackOverflow', 'RStudio Community', 'GitHub', and 'BitBucket'.
    Upon searching, a browser window will open with the aforementioned search
    results.",2021-07-24,James Balamuta,https://github.com/r-assist/searcher,TRUE,https://github.com/r-assist/searcher,18789,60,2021-07-24T19:14:53Z,313.15
seasonal,"Easy-to-use interface to X-13-ARIMA-SEATS, the seasonal adjustment
    software by the US Census Bureau. It offers full access to almost all
    options and outputs of X-13, including X-11 and SEATS, automatic ARIMA model
    search, outlier detection and support for user defined holiday variables,
    such as Chinese New Year or Indian Diwali. A graphical user interface can be
    used through the 'seasonalview' package. Uses the X-13-binaries from the
    'x13binary' package.",2021-06-15,Christoph Sax,http://www.seasonal.website,TRUE,https://github.com/christophsax/seasonal,333735,101,2021-08-29T11:50:05Z,3304.3069306930693
seasonalclumped,"Compiles a set of functions and dummy data that simplify reconstructions of seasonal temperature
	 variability in the geological past from stable isotope and clumped isotope records in sub–annually resolved
	 carbonate archives (e.g. mollusk shells, corals and speleothems). For more information, see
	 de Winter et al., 2020 (Climate of the Past Discussions, <doi:10.5194/cp-2020-118>).",2021-01-14,Niels de Winter,https://github.com/nielsjdewinter/seasonalclumped,TRUE,https://github.com/nielsjdewinter/seasonalclumped,2487,0,2021-01-11T17:19:09Z,NA
secuTrialR,"Seamless and standardized interaction with data
    exported from the clinical data management system (CDMS)
    'secuTrial'<https://www.secutrial.com>.
    The primary data export the package works with is a standard non-rectangular export.",2021-03-11,Alan G. Haynes,https://github.com/SwissClinicalTrialOrganisation/secuTrialR,TRUE,https://github.com/swissclinicaltrialorganisation/secutrialr,6210,6,2021-03-10T16:18:45Z,1035
see,"Provides plotting utilities supporting easystats-packages
    (<https://github.com/easystats/easystats>) and some extra themes,
    geoms, and scales for 'ggplot2'. Color scales are based on
    <https://materialui.co/colors>.",2021-08-23,Daniel Lüdecke  (<https://orcid.org/0000-0002-8895-3206>,https://easystats.github.io/see/,TRUE,https://github.com/easystats/see,120717,454,2021-08-23T07:57:38Z,265.8964757709251
seecolor,"Output colors used in literal vectors, palettes and plot objects (ggplot).",2020-12-07,Shangchen Song,https://github.com/lovestat/seecolor,TRUE,https://github.com/lovestat/seecolor,3409,1,2020-12-09T01:20:42Z,3409
seededlda,"Implements the seeded-LDA model (Lu, Ott, Cardie & Tsou 2010) <doi:10.1109/ICDMW.2011.125> using the quanteda package and the GibbsLDA++ library for semisupervised topic modeling. 
    Seeded-LDA allows users to pre-define topics with keywords to perform theory-driven analysis of textual data in social sciences and humanities (Watanabe & Zhou 2020) <doi:10.1177/0894439320907027>.",2021-04-08,Kohei Watanabe,https://github.com/koheiw/seededlda,TRUE,https://github.com/koheiw/seededlda,8185,21,2021-06-13T23:06:09Z,389.76190476190476
seedr,"Analysis of seed germination data
    using the physiological time modelling approach. Includes functions
    to fit hydrotime and thermal-time models with the traditional approaches
    of Bradford (1990) <doi:10.1104/pp.94.2.840>
    and Garcia-Huidobro (1982) <doi:10.1093/jxb/33.2.288>. 
    Allows to fit models to grouped datasets,
    i.e. datasets containing multiple species, seedlots or experiments.",2020-11-03,Fernández-Pascual Eduardo,https://github.com/efernandezpascual/seedr,TRUE,https://github.com/efernandezpascual/seedr,2328,1,2021-05-16T11:19:35Z,2328
seeds,"Algorithms to calculate the hidden inputs of systems of differential equations. 
  These hidden inputs can be interpreted as a control that tries to minimize the
  discrepancies between a given model and taken measurements. The idea is 
  also called the Dynamic Elastic Net, as proposed in the paper ""Learning (from) the errors of a systems biology model"" 
  (Engelhardt, Froelich, Kschischo 2016) <doi:10.1038/srep20772>.
  To use the experimental SBML import function, the 'rsbml' package is required. For installation I refer to the official 'rsbml' page: <https://bioconductor.org/packages/release/bioc/html/rsbml.html>.",2020-07-14,Tobias Newmiwaka,https://github.com/Newmi1988/seeds,TRUE,https://github.com/newmi1988/seeds,8096,0,2021-07-30T10:53:35Z,NA
segmenTier,"A dynamic programming solution to segmentation based on
        maximization of arbitrary similarity measures within segments.
	The general idea, theory and this implementation are described in
	Machne, Murray & Stadler (2017) <doi:10.1038/s41598-017-12401-8>.
	In addition to the core algorithm, the package provides time-series
	processing and clustering functions as described in the publication.
	These are generally applicable where a `k-means` clustering yields
	meaningful results, and have been specifically developed for
	clustering of the Discrete Fourier Transform of periodic gene
	expression data (`circadian' or `yeast metabolic oscillations').
	This clustering approach is outlined in the supplemental material of
	Machne & Murray (2012) <doi:10.1371/journal.pone.0037906>), and here
	is used as a basis of segment similarity measures. Notably, the
	time-series processing and clustering functions can also be used as
	stand-alone tools, independent of segmentation, e.g., for 
        transcriptome data already mapped to genes.",2019-02-18,Rainer Machne,https://github.com/raim/segmenTier,TRUE,https://github.com/raim/segmentier,10043,2,2020-12-15T09:05:27Z,5021.5
segRDA,"Tools for modeling non-continuous linear responses of ecological communities to environmental data. The package is straightforward through three steps: (1) data ordering (function OrdData()), (2) split-moving-window analysis (function SMW()) and (3) piecewise redundancy analysis (function pwRDA()). Relevant references include Cornelius and Reynolds (1991) <doi:10.2307/1941559> and Legendre and Legendre (2012, ISBN: 9780444538697).",2019-07-31,Danilo C Vieira,https://github.com/DaniloCVieira/segRDA,TRUE,https://github.com/danilocvieira/segrda,8856,0,2020-11-08T17:57:48Z,NA
segregation,"Computes segregation indices, including the Index of Dissimilarity,
    as well as the information-theoretic indices developed by
    Theil (1971) <isbn:978-0471858454>, namely
    the Mutual Information Index (M) and Theil's Information Index (H).
    The M, further described by Mora and Ruiz-Castillo (2011) <doi:10.1111/j.1467-9531.2011.01237.x>
    and Frankel and Volij (2011) <doi:10.1016/j.jet.2010.10.008>,
    is a measure of segregation that is highly decomposable. The package provides
    tools to decompose the index by units and groups (local segregation),
    and by within and between terms. The package also provides a method to decompose
    differences in segregation as described by Elbers (2021) <doi:10.1177/0049124121986204>.
    The package includes standard error estimation by bootstrapping, which also corrects for
    small sample bias.",2021-09-02,Benjamin Elbers,https://elbersb.github.io/segregation/,TRUE,https://github.com/elbersb/segregation,14324,26,2021-09-03T07:11:33Z,550.9230769230769
segregatr,"An implementation of the full-likelihood Bayes factor (FLB) 
    for evaluating segregation evidence in clinical medical genetics. The method 
    was introduced by Thompson et al. (2003) <doi:10.1086/378100>, and further 
    popularised by Bayrak-Toydemir et al. (2008) <doi:10.1016/j.yexmp.2008.03.006>.
    This implementation allows custom penetrance values and liability classes, and 
    includes specialised pedigree visualisations.",2021-04-15,Magnus Dehli Vigeland,https://github.com/magnusdv/segregatr,TRUE,https://github.com/magnusdv/segregatr,1557,1,2021-04-16T19:01:09Z,1557
SEIRfansy,"Extended Susceptible-Exposed-Infected-Recovery Model for 
    handling high false negative rate and symptom based administration of 
    diagnostic tests. <doi:10.1101/2020.09.24.20200238>.",2020-11-03,Michael Kleinsasser,https://github.com/umich-biostatistics/SEIRfansy,TRUE,https://github.com/umich-biostatistics/seirfansy,3341,2,2021-04-20T18:21:38Z,1670.5
SelectBoost,"An implementation of the selectboost algorithm (Bertrand et al. 2020, 'Bioinformatics', <doi:10.1093/bioinformatics/btaa855>), which is a general algorithm that improves the precision of any existing variable selection method. This algorithm is based on highly intensive simulations and takes into account the correlation structure of the data. It can either produce a confidence index for variable selection or it can be used in an experimental design planning perspective.",2021-03-20,Frederic Bertrand,"https://github.com/fbertran/SelectBoost,
http://www-irma.u-strasbg.fr/~fbertran/",TRUE,https://github.com/fbertran/selectboost,10716,6,2021-07-15T00:06:21Z,1786
selection.index,"The aim of most plant breeding programmes is simultaneous improvement of several characters. An objective method involving simultaneous selection for several attributes then becomes necessary. It has been recognised that most rapid improvements in the economic value is expected from selection applied simultaneously to all the characters which determine the economic value of a plant, and appropriate assigned weights to each character according to their economic importance, heritability and correlations between characters. So the selection for economic value is a complex matter. If the component characters are combined together into an index in such a way that when selection is applied to the index, as if index is the character to be improved, most rapid improvement of economic value is expected. Such an index was first proposed by Smith (1937 <doi:10.1111/j.1469-1809.1936.tb02143.x>) based on the Fisher's (1936 <doi:10.1111/j.1469-1809.1936.tb02137.x>) ""discriminant function"" Dabholkar (1999 <https://books.google.co.in/books?id=mlFtumAXQ0oC&lpg=PA4&ots=Xgxp1qLuxS&dq=elements%20of%20biometrical%20genetics&lr&pg=PP1#v=onepage&q&f=false>). In this package selection index is calculated based on the Smith (1937) selection index method.",2021-07-26,Zankrut Goyani,https://github.com/zankrut20/selection.index,TRUE,https://github.com/zankrut20/selection.index,2091,0,2021-07-26T12:08:56Z,NA
selectr,"Translates a CSS3 selector into an equivalent XPath
  expression. This allows us to use CSS selectors when working with
  the XML package as it can only evaluate XPath expressions. Also
  provided are convenience functions useful for using CSS selectors on
  XML nodes. This package is a port of the Python package 'cssselect'
  (<https://cssselect.readthedocs.io/>).",2019-11-20,Simon Potter,https://sjp.co.nz/projects/selectr,TRUE,https://github.com/sjp/selectr,13087509,44,2020-09-03T09:58:48Z,297443.38636363635
semantic.dashboard,It offers functions for creating dashboard with Semantic elements. ,2021-01-15,Dominik Krzeminski,NA,TRUE,https://github.com/appsilon/semantic.dashboard,28851,187,2021-06-10T13:14:27Z,154.28342245989305
semEff,"Provides functionality to automatically calculate direct, indirect, 
    and total effects for piecewise structural equation models, comprising lists
    of fitted models representing structured equations (Lefcheck 2016 
    <doi:10/f8s8rb>). Confidence intervals are provided via bootstrapping.",2021-04-09,Mark Murphy,https://github.com/murphymv/semEff,TRUE,https://github.com/murphymv/semeff,15220,3,2021-04-16T15:15:57Z,5073.333333333333
SEMgraph,"Estimate networks and causal relationships in complex systems through
  Structural Equation Modeling. This package also includes functions to import,
  weight, manipulate, and fit biological network models within the
  Structural Equation Modeling framework;
  Palluzzi and Grassi (2021) <arXiv:2103.08332>.",2021-07-08,Fernando Palluzzi,https://github.com/fernandoPalluzzi/SEMgraph,TRUE,https://github.com/fernandopalluzzi/semgraph,642,2,2021-09-03T09:24:54Z,321
SEMID,"Provides routines to check identifiability or non-identifiability
    of linear structural equation models as described in Drton, Foygel, and
    Sullivant (2011) <DOI:10.1214/10-AOS859>, Foygel, Draisma, and Drton (2012) 
    <DOI:10.1214/12-AOS1012>, and other works. The routines are based on the graphical 
    representation of structural equation models by a path diagram/mixed graph.",2019-05-21,Luca Weihs,https://github.com/Lucaweihs/SEMID,TRUE,https://github.com/lucaweihs/semid,17646,3,2021-06-23T14:33:53Z,5882
SemNeT,"Implements several functions for the analysis of semantic networks including different network estimation algorithms, partial node bootstrapping (Kenett, Anaki, & Faust, 2014 <doi:10.3389/fnhum.2014.00407>), random walk simulation (Kenett & Austerweil, 2016 <http://alab.psych.wisc.edu/papers/files/Kenett16CreativityRW.pdf>), and a function to compute global network measures. Significance tests and plotting features are also implemented. ",2021-06-22,Alexander P. Christensen,https://github.com/AlexChristensen/SemNeT,TRUE,https://github.com/alexchristensen/semnet,15946,11,2021-08-27T18:05:31Z,1449.6363636363637
SemNetCleaner,"Implements several functions that automates the cleaning and spell-checking of text data. Also converges, finalizes, removes plurals and continuous strings, and puts text data in binary format for semantic network analysis. Uses the 'SemNetDictionaries' package to make the cleaning process more accurate, efficient, and reproducible.",2021-08-16,Alexander P. Christensen,https://github.com/AlexChristensen/SemNetCleaner,TRUE,https://github.com/alexchristensen/semnetcleaner,19009,5,2021-08-16T15:57:41Z,3801.8
SemNetDictionaries,Implements dictionaries that can be used in the 'SemNetCleaner' package. Also includes several functions aimed at facilitating the text cleaning analysis in the 'SemNetCleaner' package. This package is designed to integrate and update word lists and dictionaries based on each user's individual needs by allowing users to store and save their own dictionaries. Dictionaries can be added to the 'SemNetDictionaries' package by submitting user-defined dictionaries to <https://github.com/AlexChristensen/SemNetDictionaries>.,2021-08-16,Alexander P. Christensen,https://github.com/AlexChristensen/SemNetDictionaries,TRUE,https://github.com/alexchristensen/semnetdictionaries,14960,2,2021-08-16T15:37:59Z,7480
sen2r,"Functions to download Sentinel-2 optical images
 and perform preliminary processing operations.
 'sen2r' provides the instruments required to easily perform
 (and eventually automate) the steps necessary to build a complete
 Sentinel-2 processing chain.
 A Graphical User Interface to facilitate data processing is also provided.
 For additional documentation refer to the following article: 
 Ranghetti et al. (2020) <doi:10.1016/j.cageo.2020.104473>.",2021-06-27,Luigi Ranghetti,https://sen2r.ranghetti.info,TRUE,https://github.com/ranghetti/sen2r,25352,127,2021-06-28T05:55:06Z,199.6220472440945
sendgridr,Send email using 'Sendgrid' <https://sendgrid.com/> mail API(v3) <https://sendgrid.com/docs/api-reference/>.,2021-05-04,Chanyub Park,https://github.com/mrchypark/sendgridr,TRUE,https://github.com/mrchypark/sendgridr,1553,13,2021-05-04T12:32:56Z,119.46153846153847
sensemakr,"Implements a suite of sensitivity analysis tools 
  that extends the traditional omitted variable bias framework and makes it easier 
  to understand the impact of omitted variables in regression models, as discussed in Cinelli, C. and Hazlett, C. (2020), ""Making Sense of Sensitivity: Extending Omitted Variable Bias."" Journal of the Royal Statistical Society, Series B (Statistical Methodology) <doi:10.1111/rssb.12348>.",2020-04-28,Carlos Cinelli,https://github.com/chadhazlett/sensemakr,TRUE,https://github.com/chadhazlett/sensemakr,10940,41,2021-07-26T18:18:16Z,266.8292682926829
sensobol,"It allows to rapidly compute, bootstrap and plot up to third-order Sobol'-based sensitivity indices using several state-of-the-art first and total-order estimators. Sobol' indices can be computed either for models that yield a scalar as a model output or for systems of differential equations. The package also provides a suit of benchmark tests functions and several options to obtain publication-ready figures of the model output uncertainty and sensitivity-related analysis. An overview of Sobol'-based sensitivity indices can be found in Saltelli et al. (2008, ISBN:9780470059975) and in Puy, Lo Piano, Saltelli, and Levin (2021) <arXiv:2101.10103>.",2021-07-24,Arnald Puy,https://github.com/arnaldpuy/sensobol,TRUE,https://github.com/arnaldpuy/sensobol,14177,6,2021-07-24T13:17:37Z,2362.8333333333335
sentencepiece,"Unsupervised text tokenizer allowing to perform byte pair encoding and unigram modelling. 
    Wraps the 'sentencepiece' library <https://github.com/google/sentencepiece> which provides a language independent tokenizer to split text in words and smaller subword units. 
    The techniques are explained in the paper ""SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"" by Taku Kudo and John Richardson (2018) <doi:10.18653/v1/D18-2012>.
    Provides as well straightforward access to pretrained byte pair encoding models and subword embeddings trained on Wikipedia using 'word2vec', 
    as described in ""BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages"" by Benjamin Heinzerling and Michael Strube (2018) <http://www.lrec-conf.org/proceedings/lrec2018/pdf/1049.pdf>.",2020-06-08,Jan Wijffels,https://github.com/bnosac/sentencepiece,TRUE,https://github.com/bnosac/sentencepiece,4572,18,2021-01-08T15:54:28Z,254
SentimentAnalysis,"Performs a sentiment analysis of textual contents in R. This implementation
    utilizes various existing dictionaries, such as Harvard IV, or finance-specific 
    dictionaries. Furthermore, it can also create customized dictionaries. The latter 
    uses LASSO regularization as a statistical approach to select relevant terms based on 
    an exogenous response variable. ",2021-02-18,Nicolas Proellochs,https://github.com/sfeuerriegel/SentimentAnalysis,TRUE,https://github.com/sfeuerriegel/sentimentanalysis,80330,128,2021-02-17T23:01:36Z,627.578125
sentimentr,"Calculate text polarity sentiment at the sentence level and
         optionally aggregate by rows or grouping variable(s).",2019-03-22,Tyler Rinker,http://github.com/trinker/sentimentr,TRUE,https://github.com/trinker/sentimentr,157915,352,2020-09-04T19:05:26Z,448.62215909090907
sentometrics,"Optimized prediction based on textual sentiment, accounting for the intrinsic challenge that sentiment can be computed and pooled across texts and time in various ways. See Ardia et al. (2021) <doi:10.18637/jss.v099.i02>.",2021-08-18,Samuel Borms,https://sentometrics-research.com/sentometrics/,TRUE,https://github.com/sentometricsresearch/sentometrics,20226,70,2021-08-18T09:15:38Z,288.9428571428571
sentryR,"Unofficial client for 'Sentry' <https://sentry.io>,
  a self-hosted or cloud-based error-monitoring service. It will inform about
  errors in real-time, and includes integration with the 'Plumber' package.",2020-03-19,Joao Santiago,https://github.com/ozean12/sentryR,TRUE,https://github.com/ozean12/sentryr,7255,30,2020-12-30T19:31:22Z,241.83333333333334
seplyr,"The 'seplyr' (standard evaluation plying) package supplies improved
    standard evaluation adapter methods for important common 'dplyr' data manipulation tasks.
    In addition the 'seplyr' package supplies several new ""key operations
    bound together"" methods.  These include 'group_summarize()' (which
    combines grouping, arranging and calculation in an atomic unit),
    'add_group_summaries()' (which joins grouped summaries into a 'data.frame'
    in a well documented manner), 'add_group_indices()' (which adds
    per-group identifiers to a 'data.frame' without depending on row-order),
    'partition_mutate_qt()' (which optimizes mutate sequences), and 'if_else_device()'
    (which simulates per-row if-else blocks in expression sequences).",2021-09-02,John Mount,"https://github.com/WinVector/seplyr/,
https://winvector.github.io/seplyr/",TRUE,https://github.com/winvector/seplyr,39585,48,2021-09-01T16:27:26Z,824.6875
seqDesign,"A modification of the preventive vaccine efficacy trial design of Gilbert, Grove et al. (2011, Statistical Communications in Infectious Diseases) is implemented, with application generally to individual-randomized clinical trials with multiple active treatment groups and a shared control group, and a study endpoint that is a time-to-event endpoint subject to right-censoring. The design accounts for the issues that the efficacy of the treatment/vaccine groups may take time to accrue while the multiple treatment administrations/vaccinations are given; there is interest in assessing the durability of treatment efficacy over time; and group sequential monitoring of each treatment group for potential harm, non-efficacy/efficacy futility, and high efficacy is warranted. The design divides the trial into two stages of time periods, where each treatment is first evaluated for efficacy in the first stage of follow-up, and, if and only if it shows significant treatment efficacy in stage one, it is evaluated for longer-term durability of efficacy in stage two. The package produces plots and tables describing operating characteristics of a specified design including an unconditional power for intention-to-treat and per-protocol/as-treated analyses; trial duration; probabilities of the different possible trial monitoring outcomes (e.g., stopping early for non-efficacy); unconditional power for comparing treatment efficacies; and distributions of numbers of endpoint events occurring after the treatments/vaccinations are given, useful as input parameters for the design of studies of the association of biomarkers with a clinical outcome (surrogate endpoint problem). The code can be used for a single active treatment versus control design and for a single-stage design.",2019-05-22,Michal Juraska,https://github.com/mjuraska/seqDesign,TRUE,https://github.com/mjuraska/seqdesign,16898,1,2021-02-17T23:47:28Z,16898
SeqExpMatch,"Generates the following sequential two-arm experimental designs:
    (1) completely randomized (Bernoulli)
    (2) balanced completely randomized
    (3) Efron's (1971) Biased Coin
    (4) Atkinson's (1982) Covariate-Adjusted Biased Coin
    (5) Kapelner and Krieger's (2014) Covariate-Adjusted Matching on the Fly
    (6) Kapelner and Krieger's (2021) CARA Matching on the Fly with Differential Covariate Weights (Naive)
    (7) Kapelner and Krieger's (2021) CARA Matching on the Fly with Differential Covariate Weights (Stepwise)
    and also provides the following types of inference:
    (1) estimation (with both Z-style estimators and OLS estimators), 
    (2) frequentist testing (via asymptotic distribution results and via employing the nonparametric randomization test) and
    (3) frequentist confidence intervals (only under the superpopulation sampling assumption currently). Details can be found
    in our publication: Kapelner and Krieger ""A Matching Procedure for Sequential Experiments that Iteratively Learns which 
    Covariates Improve Power"" (2020) <arXiv:2010.05980>.",2021-06-01,Adam Kapelner,https://github.com/kapelner/matching_on_the_fly_designs_R_package_and_paper_repr,TRUE,https://github.com/kapelner/matching_on_the_fly_designs_r_package_and_paper_repr,873,0,2021-05-31T14:34:56Z,NA
seqHMM,"Designed for fitting hidden (latent) Markov models and mixture
    hidden Markov models for social sequence data and other categorical time series.
    Also some more restricted versions of these type of models are available: Markov
    models, mixture Markov models, and latent class models. The package supports
    models for one or multiple subjects with one or multiple parallel sequences
    (channels). External covariates can be added to explain cluster membership in
    mixture models. The package provides functions for evaluating and comparing
    models, as well as functions for visualizing of multichannel sequence data and
    hidden Markov models. Models are estimated using maximum likelihood via the EM
    algorithm and/or direct numerical maximization with analytical gradients. All
    main algorithms are written in C++ with support for parallel computation. 
    Documentation is available via several vignettes in this page, and the  
    paper by Helske and Helske (2019, <doi:10.18637/jss.v088.i03>).",2021-08-13,Jouni Helske,NA,TRUE,https://github.com/helske/seqhmm,26858,64,2021-08-13T06:52:26Z,419.65625
seqmagick,"Supports reading and writing sequences for different formats (currently interleaved and sequential formats for 'FASTA' and 'PHYLIP'), file conversion, and manipulation (e.g. filter sequences that contain specify pattern, export consensus sequence from an alignment).",2021-04-25,Guangchuang Yu,https://github.com/YuLab-SMU/seqmagick,TRUE,https://github.com/yulab-smu/seqmagick,12250,4,2021-04-25T06:27:18Z,3062.5
seriation,"Infrastructure for ordering objects with an implementation of several
    seriation/sequencing/ordination techniques to reorder matrices, dissimilarity
    matrices, and dendrograms. Also provides (optimally) reordered heatmaps,
    color images and clustering visualizations like dissimilarity plots, and
    visual assessment of cluster tendency plots (VAT and iVAT). Hahsler et al (2008) <doi:10.18637/jss.v025.i03>.",2021-06-30,Michael Hahsler,https://github.com/mhahsler/seriation,TRUE,https://github.com/mhahsler/seriation,920602,54,2021-06-30T16:48:13Z,17048.185185185186
serp,"A regularization method for the cumulative link
    models.  The 'smooth-effect-on-response penalty' ('SERP') provides
    flexible modelling of the ordinal model by enabling the smooth
    transition from the general cumulative link model to a coarser form of
    the same model. In other words, as the tuning parameter goes from zero
    to infinity, the subject-specific effects associated with each
    variable in the model tend to a unique global effect. The parameter
    estimates of the general cumulative model are mostly unidentifiable or
    at least only identifiable within a range of the entire parameter
    space. Thus, by maximizing a penalized rather than the usual
    non-penalized log-likelihood, this and other numerical problems common
    with the general model are to a large extent eliminated. Fitting is
    via a modified Newton's method. Several standard model performance and
    descriptive methods are also available. For more details on the penalty
    implemented here, see, 'Ugba et al. (2021)' <doi:10.3390/stats4030037> and 
    Tutz and Gertheiss (2016) <doi:10.1177/1471082X16642560>.",2021-09-01,Ejike R. Ugba,https://github.com/ejikeugba/serp,TRUE,https://github.com/ejikeugba/serp,2539,1,2021-09-02T09:22:20Z,2539
serrsBayes,"Sequential Monte Carlo (SMC) algorithms for fitting a generalised additive
    mixed model (GAMM) to surface-enhanced resonance Raman spectroscopy (SERRS),
    using the method of Moores et al. (2016) <arXiv:1604.07299>. Multivariate
    observations of SERRS are highly collinear and lend themselves to a reduced-rank
    representation. The GAMM separates the SERRS signal into three components: a
    sequence of Lorentzian, Gaussian, or pseudo-Voigt peaks; a smoothly-varying baseline;
    and additive white noise. The parameters of each component of the model are estimated
    iteratively using SMC. The posterior distributions of the parameters given the observed
    spectra are represented as a population of weighted particles.",2021-06-28,Matt Moores,"https://github.com/mooresm/serrsBayes,
https://mooresm.github.io/serrsBayes/",TRUE,https://github.com/mooresm/serrsbayes,13806,4,2021-06-28T10:17:46Z,3451.5
servosphereR,"Functions that facilitate and speed up the analysis of data
    produced by a Syntech servosphere <http://www.ockenfels-syntech.com/products/locomotion-compensation/>,
    which is equipment for studying the movement behavior of arthropods.
    This package is designed to make working with data produced from a 
    servosphere easy for someone new to or unfamiliar with R. The functions
    provided in this package fall into three broad-use categories: functions for
    cleaning raw data produced by the servosphere software, functions for
    deriving movement variables based on position data, and functions for 
    summarizing movement variables for easier analysis. These functions are
    built with functions from the tidyverse package to work efficiently, as a
    single servosphere file may consist of hundreds of thousands of rows of data
    and a user may wish to analyze hundreds of files at a time. Many of the 
    movement variables derivable through this package are described in the 
    following papers:
    Otálora-Luna, Fernando; Dickens, Joseph C. (2011) <doi:10.1371/journal.pone.0020990>
    Party, Virginie; Hanot, Christophe; Busser, Daniela Schmidt; Rochat, Didier; Renou, Michel (2013) <doi:10.1371/journal.pone.0052897>
    Bell, William J.; Kramer, Ernest (1980) <doi:10.1007/BF01402908>
    Becher, Paul G; Guerin, Patrick M. (2009) <doi:10.1016/j.jinsphys.2009.01.006>.",2019-05-14,Jacob T. Wittman,http://github.com/wittja01/servosphereR,TRUE,https://github.com/wittja01/servospherer,10338,0,2020-09-11T14:47:19Z,NA
servr,"Start an HTTP server in R to serve static files, or dynamic
    documents that can be converted to HTML files (e.g., R Markdown) under a
    given directory.",2021-08-11,Yihui Xie,https://github.com/yihui/servr,TRUE,https://github.com/yihui/servr,558076,239,2021-08-18T13:55:01Z,2335.0460251046024
set6,"An object-oriented package for mathematical sets, upgrading the current gold-standard {sets}. Many forms of mathematical sets are implemented, including (countably finite) sets, tuples, intervals (countably infinite or uncountable), and fuzzy variants. Wrappers extend functionality by allowing symbolic representations of complex operations on sets, including unions, (cartesian) products, exponentiation, and differences (asymmetric and symmetric).",2021-07-25,Raphael Sonabend,"https://xoopR.github.io/set6/, https://github.com/xoopR/set6",TRUE,https://github.com/xoopr/set6,99241,12,2021-07-25T17:44:54Z,8270.083333333334
settings,"Provides option settings management that goes
    beyond R's default 'options' function. With this package, users can define
    their own option settings manager holding option names, default values and 
    (if so desired) ranges or sets of allowed option values that will be 
    automatically checked. Settings can then be retrieved, altered and reset 
    to defaults with ease. For R programmers and package developers it offers 
    cloning and merging functionality which allows for conveniently defining 
    global and local options, possibly in a multilevel options hierarchy. See 
    the package vignette for some examples concerning functions, S4 classes, 
    and reference classes. There are convenience functions to reset par() 
    and options() to their 'factory defaults'.",2021-05-07,Mark van der Loo,https://github.com/markvanderloo/settings,TRUE,https://github.com/markvanderloo/settings,79990,7,2021-04-29T11:21:31Z,11427.142857142857
Seurat,"A toolkit for quality control, analysis, and exploration of single cell RNA sequencing data. 'Seurat' aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. See Satija R, Farrell J, Gennert D, et al (2015) <doi:10.1038/nbt.3192>, Macosko E, Basu A, Satija R, et al (2015) <doi:10.1016/j.cell.2015.05.002>, Stuart T, Butler A, et al (2019) <doi:10.1016/j.cell.2019.05.031>, and Hao, Hao, et al (2020) <doi:10.1101/2020.10.12.335331> for more details.",2021-08-20,Christoph Hafemeister [ctb,"https://satijalab.org/seurat, https://github.com/satijalab/seurat",TRUE,https://github.com/satijalab/seurat,654186,1180,2021-08-27T13:33:47Z,554.3949152542373
SeuratObject,"Defines S4 classes for single-cell genomic data and associated
  information, such as dimensionality reduction embeddings, nearest-neighbor
  graphs, and spatially-resolved coordinates. Provides data access methods and
  R-native hooks to ensure the Seurat object is familiar to other R users. See
  Satija R, Farrell J, Gennert D, et al (2015) <doi:10.1038/nbt.3192>,
  Macosko E, Basu A, Satija R, et al (2015) <doi:10.1016/j.cell.2015.05.002>,
  and Stuart T, Butler A, et al (2019) <doi:10.1016/j.cell.2019.05.031> for
  more details.",2021-06-09,Rahul Satija,"https://satijalab.org/seurat,
https://github.com/mojaveazure/seurat-object",TRUE,https://github.com/mojaveazure/seurat-object,112479,1,2021-06-09T13:17:03Z,112479
sever,Customise 'Shiny' disconnected screens as well as sanitize error messages to make them clearer and friendlier to the user.,2021-07-14,John Coene,https://sever.john-coene.com/,TRUE,https://github.com/johncoene/sever,7930,64,2021-07-14T19:07:29Z,123.90625
sf,"Support for simple features, a standardized way to
    encode spatial vector data. Binds to 'GDAL' for reading and writing
    data, to 'GEOS' for geometrical operations, and to 'PROJ' for
    projection conversions and datum transformations. Uses by default the 's2'
    package for spherical geometry operations on ellipsoidal (long/lat) coordinates.",2021-07-26,Edzer Pebesma,"https://r-spatial.github.io/sf/, https://github.com/r-spatial/sf/",TRUE,https://github.com/r-spatial/sf,16231320,907,2021-09-01T19:12:30Z,17895.61190738699
sfarrow,"Support for reading/writing simple feature ('sf') spatial objects from/to 'Parquet' files. 'Parquet' files are an open-source, column-oriented data storage format from Apache (<https://parquet.apache.org/>), now popular across programming languages. This implementation converts simple feature list geometries into well-known binary format for use by 'arrow', and coordinate reference system information is maintained in a standard metadata format.",2021-06-21,Chris Jochem,"https://github.com/wcjochem/sfarrow,
https://wcjochem.github.io/sfarrow/",TRUE,https://github.com/wcjochem/sfarrow,876,37,2021-06-25T15:21:36Z,23.675675675675677
sfcr,"Routines to write, simulate, and validate stock-flow consistent (SFC) models. The accounting structure of SFC models are described in Godley and Lavoie (2007, ISBN:978-1-137-08599-3). The algorithms implemented to solve the models (Gauss-Seidel and Broyden) are described in Kinsella and O'Shea (2010) <doi:10.2139/ssrn.1729205> and Peressini and Sullivan (1988, ISBN:0-387-96614-5).",2021-01-08,Joao Macalos,https://github.com/joaomacalos/sfcr,TRUE,https://github.com/joaomacalos/sfcr,2786,8,2021-01-11T07:58:47Z,348.25
sfdct,"Build a constrained Delaunay triangulation from simple features
    objects, applying constraints based on input line segments, and triangle
    properties including maximum area, minimum internal angle. The triangulation code
    in 'RTriangle' uses the method of Cheng, Dey and Shewchuk (2012, ISBN:9781584887300). 
    For a low-dependency alternative with low-quality path-based constrained 
    triangulation see <https://CRAN.R-project.org/package=decido> and for high-quality configurable
    triangulation see <https://CRAN.R-project.org/package=anglr>. ",2021-01-06,Michael D. Sumner,https://github.com/hypertidy/sfdct,TRUE,https://github.com/hypertidy/sfdct,15476,2,2020-12-22T00:59:12Z,7738
sfheaders,"Converts between R and Simple Feature 'sf' objects, without depending
  on the Simple Feature library. Conversion functions are available at both the R level, 
  and through 'Rcpp'.",2020-12-01,David Cooley,https://dcooley.github.io/sfheaders/,TRUE,https://github.com/dcooley/sfheaders,300452,51,2021-05-10T03:02:52Z,5891.21568627451
sfnetworks,"Provides a tidy approach to spatial network
    analysis, in the form of classes and functions that enable a seamless
    interaction between the network analysis package 'tidygraph' and the
    spatial analysis package 'sf'.",2021-05-13,Lucas van der Meer,"https://luukvdmeer.github.io/sfnetworks/,
https://github.com/luukvdmeer/sfnetworks",TRUE,https://github.com/luukvdmeer/sfnetworks,3267,200,2021-06-11T09:47:44Z,16.335
sfsmisc,"Useful utilities ['goodies'] from Seminar fuer Statistik ETH Zurich,
   some of which were ported from S-plus in the 1990s.
 For graphics, have pretty (Log-scale) axes, an enhanced Tukey-Anscombe
   plot, combining histogram and boxplot, 2d-residual plots, a 'tachoPlot()',
   pretty arrows, etc.
 For robustness, have a robust F test and robust range().
 For system support, notably on Linux, provides 'Sys.*()' functions with
   more access to system and CPU information.
 Finally, miscellaneous utilities such as simple efficient prime numbers,
   integer codes, Duplicated(), toLatex.numeric() and is.whole().",2021-04-12,Martin Maechler,https://github.com/mmaechler/sfsmisc,TRUE,https://github.com/mmaechler/sfsmisc,784204,7,2021-04-08T19:04:20Z,112029.14285714286
SFtools,"Contains space filling based tools for
    machine learning and data mining. Some functions offer
    several computational techniques and deal with the out of
    memory for large big data by using the ff package.",2017-06-28,Mohamed Laib and Mikhail Kanevski,https://sites.google.com/site/mohamedlaibwebpage/,TRUE,https://github.com/mlaib/sftools,13560,0,2021-02-01T08:27:02Z,NA
sftrack,"Modern classes for tracking and movement data, building
    on 'sf' spatial infrastructure, and early theoretical work from
    Turchin (1998, ISBN: 9780878938476), and Calenge et al. (2009)
    <doi:10.1016/j.ecoinf.2008.10.002>. Tracking data are series of
    locations with at least 2-dimensional spatial coordinates (x,y), a
    time index (t), and individual identification (id) of the object
    being monitored; movement data are made of trajectories, i.e. the
    line representation of the path, composed by steps (the
    straight-line segments connecting successive locations). 'sftrack'
    is designed to handle movement of both living organisms and
    inanimate objects.",2021-07-01,Matthew Boone,"https://mablab.org/sftrack/, https://github.com/mablab/sftrack",TRUE,https://github.com/mablab/sftrack,3786,39,2021-07-01T13:27:26Z,97.07692307692308
sgat,"Once you've identified a real life place, such as a shop, a restaurant, a bar, etc. use this package to simulate a Google search and retrieve its ""Popular Times"" and geographic location information and save them in Comma-Separated Values files. This package also downloads a list of restaurants and bars of Ushuaia city, Argentina.",2021-02-26,Matias Poullain,https://github.com/matiaspoullain/sgat,TRUE,https://github.com/matiaspoullain/sgat,2029,0,2021-03-24T16:25:39Z,NA
sgmcmc,"Provides functions that performs popular stochastic gradient Markov chain Monte Carlo (SGMCMC) methods on user specified models. The required gradients are automatically calculated using 'TensorFlow' <https://www.tensorflow.org/>, an efficient library for numerical computation. This means only the log likelihood and log prior functions need to be specified. The methods implemented include stochastic gradient Langevin dynamics (SGLD), stochastic gradient Hamiltonian Monte Carlo (SGHMC), stochastic gradient Nose-Hoover thermostat (SGNHT) and their respective control variate versions for increased efficiency. References: M. Welling, Y. W. Teh (2011) <http://www.icml-2011.org/papers/398_icmlpaper.pdf>; T. Chen, E. B. Fox, C. E. Guestrin (2014) <arXiv:1402.4102>; N. Ding, Y. Fang, R. Babbush, C. Chen, R. D. Skeel, H. Neven (2014) <https://papers.nips.cc/paper/5592-bayesian-sampling-using-stochastic-gradient-thermostats>; J. Baker, P. Fearnhead, E. B. Fox, C. Nemeth (2017) <arXiv:1706.05439>. For more details see <doi:10.18637/jss.v091.i03>.",2019-10-24,Jack Baker,https://github.com/STOR-i/sgmcmc,TRUE,https://github.com/stor-i/sgmcmc,15862,27,2020-11-05T11:02:22Z,587.4814814814815
SGP,"An analytic framework for the calculation of norm- and criterion-referenced academic growth estimates using large scale, longitudinal education assessment data as developed in Betebenner (2009) <doi:10.1111/j.1745-3992.2009.00161.x>.",2020-01-30,Damian W. Betebenner,"https://sgp.io, https://github.com/CenterForAssessment/SGP,
https://CRAN.R-project.org/package=SGP",TRUE,https://github.com/centerforassessment/sgp,30860,18,2021-08-23T03:49:38Z,1714.4444444444443
SGPdata,Data sets utilized by the 'SGP' package as exemplars for users to conduct their own student growth percentiles (SGP) analyses.,2021-04-18,Damian W. Betebenner,"https://CenterForAssessment.github.io/SGPdata,
https://github.com/CenterForAssessment/SGPdata,
https://cran.r-project.org/package=SGPdata",TRUE,https://github.com/centerforassessment/sgpdata,26217,2,2021-05-26T12:53:31Z,13108.5
sgstar,"A set of function that implements for seasonal multivariate time series analysis based on Seasonal Generalized Space
            Time Autoregressive with Seemingly Unrelated Regression (S-GSTAR-SUR) Model by Setiawan(2016)<https://www.researchgate.net/publication/316517889_S-GSTAR-SUR_model_for_seasonal_spatio_temporal_data_forecasting>.",2021-05-23,M. Yoga Satria Utama,https://github.com/yogasatria30/sgstar,TRUE,https://github.com/yogasatria30/sgstar,2953,0,2021-01-28T07:55:51Z,NA
shadow,"Functions for calculating: (1) shadow height, (2) logical shadow flag, (3) shadow footprint, (4) Sky View Factor and (5) radiation load. Basic required inputs include a polygonal layer of obstacle outlines along with their heights (i.e. ""extruded polygons""), sun azimuth and sun elevation. The package also provides functions for related preliminary calculations: breaking polygons into line segments, determining azimuth of line segments, shifting segments by azimuth and distance, constructing the footprint of a line-of-sight between an observer and the sun, and creating a 3D grid covering the surface area of extruded polygons.",2021-03-14,Michael Dorman,"https://michaeldorman.github.io/shadow/,
https://github.com/michaeldorman/shadow/",TRUE,https://github.com/michaeldorman/shadow,24216,25,2020-12-16T09:11:05Z,968.64
shadowtext,"Implement shadowtextGrob() for 'grid' and geom_shadowtext() layer for 'ggplot2'.
             These functions create/draw text grob with background shadow.",2021-04-23,Guangchuang Yu,https://github.com/GuangchuangYu/shadowtext/,TRUE,https://github.com/guangchuangyu/shadowtext,93174,28,2021-04-19T07:39:22Z,3327.6428571428573
shallot,"Implementations are provided for the models described in the paper D. B. Dahl, R. Day, J. Tsai (2017) <DOI:10.1080/01621459.2016.1165103>. The Ewens, Ewens-Pitman, Ewens attraction, Ewens-Pitman attraction, and ddCRP distributions are available for prior and posterior simulation. Posterior simulation is based on a user-supplied likelihood. Supporting functions for partition estimation and plotting are also provided.",2020-11-09,David B. Dahl,https://github.com/dbdahl/shallot,TRUE,https://github.com/dbdahl/shallot,20244,2,2020-11-09T15:52:02Z,10122
ShapeRotator,"Here we describe a simple geometric rigid rotation approach that removes the effect of random translation and rotation, enabling the morphological analysis of 3D articulated structures. Our method is based on Cartesian coordinates in 3D space so it can be applied to any morphometric problem that also uses 3D coordinates. See Vidal-García, M., Bandara, L., Keogh, J.S. (2018) <doi:10.1002/ece3.4018>.",2020-05-06,Marta Vidal-Garcia,https://github.com/marta-vidalgarcia/ShapeRotator,TRUE,https://github.com/marta-vidalgarcia/shaperotator,4525,7,2021-02-05T03:41:39Z,646.4285714285714
SHAPforxgboost,"Aid in visual data investigations
 using SHAP (SHapley Additive exPlanation) visualization plots for 'XGBoost' and 'LightGBM'. 
 It provides summary plot, dependence plot, interaction plot, and force plot and relies on
 the SHAP implementation provided by 'XGBoost' and 'LightGBM'.
 Please refer to 'slundberg/shap' for the original implementation of SHAP in 'Python'. ",2021-03-28,Yang Liu,https://github.com/liuyanguu/SHAPforxgboost,TRUE,https://github.com/liuyanguu/shapforxgboost,32390,53,2021-05-19T19:16:27Z,611.1320754716982
shapper,"Provides SHAP explanations of machine learning models. In applied machine learning, there is a strong belief that we need to strike a balance between interpretability and accuracy. However, in field of the Interpretable Machine Learning, there are more and more new ideas for explaining black-box models. One of the best known method for local explanations is SHapley Additive exPlanations (SHAP) introduced by Lundberg, S., et al., (2016) <arXiv:1705.07874> The SHAP method is used to calculate influences of variables on the particular observation. This method is based on Shapley values, a technique used in game theory. The R package 'shapper' is a port of the Python library 'shap'. ",2020-08-28,Szymon Maksymiuk,https://github.com/ModelOriented/shapper,TRUE,https://github.com/modeloriented/shapper,20678,51,2021-06-30T22:41:47Z,405.45098039215685
shapr,"Complex machine learning models are often hard to interpret. However, in 
  many situations it is crucial to understand and explain why a model made a specific 
  prediction. Shapley values is the only method for such prediction explanation framework 
  with a solid theoretical foundation. Previously known methods for estimating the Shapley 
  values do, however, assume feature independence. This package implements the method 
  described in Aas, Jullum and Løland (2019) <arXiv:1903.10464>, which accounts for any feature 
  dependence, and thereby produces more accurate estimates of the true Shapley values.",2021-01-28,Nikolai Sellereite,"https://norskregnesentral.github.io/shapr/,
https://github.com/NorskRegnesentral/shapr",TRUE,https://github.com/norskregnesentral/shapr,6686,88,2021-08-27T09:45:07Z,75.97727272727273
shar,"
  Analyse species-habitat associations in R. Therefore, information about the 
  location of the species is needed and about the environmental conditions. To test 
  for significance habitat associations, one of the two components is randomized. 
  Methods are mainly based on Plotkin et al. (2000) <doi:10.1006/jtbi.2000.2158> and 
  Harms et al. (2001) <doi:10.1111/j.1365-2745.2001.00615.x>.",2021-05-04,Maximillian H.K. Hesselbarth,https://r-spatialecology.github.io/shar/,TRUE,https://github.com/r-spatialecology/shar,14020,4,2021-05-04T13:29:42Z,3505
SharpeR,"A collection of tools for analyzing significance of assets,
    funds, and trading strategies, based on the Sharpe ratio and overfit 
    of the same. Provides density, distribution, quantile and random generation 
    of the Sharpe ratio distribution based on normal returns, as well
    as the optimal Sharpe ratio over multiple assets. Computes confidence intervals
    on the Sharpe and provides a test of equality of Sharpe ratios based on 
    the Delta method. The statistical foundations of the Sharpe can be found in
    the author's Short Sharpe Course  <doi:10.2139/ssrn.3036276>.",2021-08-18,Steven E. Pav,https://github.com/shabbychef/SharpeR,TRUE,https://github.com/shabbychef/sharper,24439,17,2021-08-18T17:50:04Z,1437.5882352941176
sharpshootR,"Miscellaneous soil data management, summary, visualization, and conversion utilities to support soil survey.",2021-05-04,Dylan Beaudette,https://github.com/ncss-tech/aqp,TRUE,https://github.com/ncss-tech/aqp,23587,34,2021-09-03T08:05:52Z,693.7352941176471
SHELF,"Implements various methods for eliciting a probability distribution
    for a single parameter from an expert or a group of experts. The expert
    provides a small number of probability judgements, corresponding
    to points on his or her cumulative distribution function. A range of parametric
    distributions can then be fitted and displayed, with feedback provided in the
    form of fitted probabilities and percentiles. For multiple experts, a weighted
    linear pool can be calculated. Also includes functions for eliciting beliefs
    about population distributions, eliciting multivariate distributions using a
    Gaussian copula, eliciting a Dirichlet distribution, and eliciting distributions 
    for variance parameters in a random effects meta-analysis model. R Shiny apps  
    for most of the methods are included. ",2021-06-18,Jeremy Oakley,https://github.com/OakleyJ/SHELF,TRUE,https://github.com/oakleyj/shelf,19508,4,2021-06-18T09:37:38Z,4877
ShellChron,"Takes as input a stable oxygen isotope (d18O) profile measured in growth direction (D)
	through a shell + uncertainties in both variables (d18O_err & D_err). It then models the seasonality
	in the d18O record by fitting a combination of a growth and temperature sine wave to year-length chunks of
	the data (see Judd et al., (2018) <doi:10.1016/j.palaeo.2017.09.034>). This modeling is carried out along a sliding window through the data and yields estimates of
	the day of the year (Julian Day) and local growth rate for each data point. Uncertainties in both modeling
	routine and the data itself are propagated and pooled to obtain a confidence envelope around the age of
	each data point in the shell. The end result is a shell chronology consisting of estimated ages of shell
	formation relative to the annual cycle with their uncertainties. All formulae in the package serve this
	purpose, but the user can customize the model (e.g. number of days in a year and the mineralogy of the
	shell carbonate) through input parameters.",2021-07-05,Niels de Winter,https://github.com/nielsjdewinter/ShellChron,TRUE,https://github.com/nielsjdewinter/shellchron,3328,0,2021-07-05T11:37:52Z,NA
ShiftShareSE,"Provides confidence intervals in least-squares regressions when the
  variable of interest has a shift-share structure, and in instrumental
  variables regressions when the instrument has a shift-share structure. The
  confidence intervals implement the AKM and AKM0 methods developed in Adão,
  Kolesár, and Morales (2019) <doi:10.1093/qje/qjz025>.",2020-01-07,Michal Kolesár,https://github.com/kolesarm/ShiftShareSE,TRUE,https://github.com/kolesarm/shiftsharese,8710,11,2021-03-24T14:46:22Z,791.8181818181819
shinipsum,"Prototype your 'shiny' apps quickly with these
    Lorem-Ipsum helper functions. Generate random elements
    for 'shiny' outputs that can be used as placeholder in your application.",2020-04-30,Colin Fay,https://github.com/Thinkr-open/shinipsum,TRUE,https://github.com/thinkr-open/shinipsum,7883,91,2020-10-21T21:29:02Z,86.62637362637362
shiny,"Makes it incredibly easy to build interactive web
    applications with R. Automatic ""reactive"" binding between inputs and
    outputs and extensive prebuilt widgets make it possible to build
    beautiful, responsive, and powerful applications with minimal effort.",2021-01-25,Winston Chang,https://shiny.rstudio.com/,TRUE,https://github.com/rstudio/shiny,13234461,4364,2021-08-24T18:59:52Z,3032.6445921173236
shiny.i18n,"It provides easy internationalization of Shiny
    applications. It can be used as standalone translation package
    to translate reports, interactive visualizations or
    graphical elements as well.",2020-10-02,Dominik Krzemiński,https://github.com/Appsilon/shiny.i18n,TRUE,https://github.com/appsilon/shiny.i18n,23738,112,2021-06-10T12:59:41Z,211.94642857142858
shiny.info,Displays simple diagnostic information of the 'shiny' project in the user interface of the app.,2020-03-23,Jakub Nowicki,NA,TRUE,https://github.com/appsilon/shiny.info,5988,42,2021-04-02T09:58:20Z,142.57142857142858
shiny.pwa,"Adds Progressive Web App support for Shiny apps, including
  desktop and mobile installations.",2021-06-19,Pedro Silva,https://github.com/pedrocoutinhosilva/shiny.pwa,TRUE,https://github.com/pedrocoutinhosilva/shiny.pwa,4443,23,2021-06-21T18:48:07Z,193.17391304347825
shiny.semantic,"Creating a great user interface for your Shiny apps
    can be a hassle, especially if you want to work purely in R
    and don't want to use, for instance HTML templates. This
    package adds support for a powerful UI library Semantic UI -
    <https://fomantic-ui.com/>. It also supports
    universal UI input binding that works with various DOM elements.",2021-01-10,Dominik Krzeminski,NA,TRUE,https://github.com/appsilon/shiny.semantic,37039,388,2021-08-31T19:50:37Z,95.46134020618557
shinyAce,"Ace editor bindings to enable a rich text editing environment
    within Shiny.",2019-09-24,Vincent Nijs,NA,TRUE,https://github.com/trestletech/shinyace,282923,190,2020-11-23T04:41:52Z,1489.0684210526315
shinyauthr,"Add in-app user authentication to 'shiny', 
    allowing you to secure publicly hosted apps and 
    build dynamic user interfaces from user information.",2021-07-20,Paul Campbell,https://github.com/paulc91/shinyauthr,TRUE,https://github.com/paulc91/shinyauthr,1263,302,2021-08-30T10:05:11Z,4.182119205298013
shinybrms,"A graphical user interface (GUI) for fitting Bayesian
    regression models using the package 'brms' which in turn relies on
    'Stan' (<https://mc-stan.org/>). The 'shinybrms' GUI is a 'shiny'
    app.",2021-03-06,Frank Weber,"https://fweber144.github.io/shinybrms/,
https://github.com/fweber144/shinybrms",TRUE,https://github.com/fweber144/shinybrms,9078,5,2021-03-06T18:57:07Z,1815.6
shinybusy,"Add indicators (spinner, progress bar, gif) in your 'shiny' applications to show the user that the server is busy.",2020-09-27,Victor Perrier,https://github.com/dreamRs/shinybusy,TRUE,https://github.com/dreamrs/shinybusy,54391,96,2021-04-16T08:06:48Z,566.5729166666666
shinyChakraUI,"Makes the 'React' library 'Chakra UI' usable in 'Shiny' apps. 'Chakra UI' components include alert dialogs, drawers (sliding panels), menus, modals, popovers, sliders, and more. ",2021-07-15,Stéphane Laurent,https://github.com/stla/shinyChakraUI,TRUE,https://github.com/stla/shinychakraui,668,6,2021-07-14T13:46:11Z,111.33333333333333
shinycssloaders,"When a 'Shiny' output (such as a plot, table, map, etc.) is recalculating, it remains 
    visible but gets greyed out. Using 'shinycssloaders', you can add a loading animation (""spinner"")
    to outputs instead. By wrapping a 'Shiny' output in 'withSpinner()', a spinner will automatically
    appear while the output is recalculating. See the demo online at
    <https://daattali.com/shiny/shinycssloaders-demo/>.",2020-07-28,Andras Sali,https://github.com/daattali/shinycssloaders,TRUE,https://github.com/daattali/shinycssloaders,460270,281,2021-01-09T02:30:58Z,1637.9715302491104
shinyCyJS,"Create Interactive Graph (Network) Visualizations. 
  'shinyCyJS' can be used in 'Shiny' apps or viewed from 'Rstudio' Viewer.
  'shinyCyJS' includes API to build Graph model like node or edge with customized attributes for R. 
  'shinyCyJS' is built with 'cytoscape.js' and 'htmlwidgets' R package.",2020-03-26,Jinhwan Kim,https://github.com/jhk0530/shinyCyJS,TRUE,https://github.com/jhk0530/shinycyjs,5455,2,2020-12-10T07:25:17Z,2727.5
shinydashboardPlus,"Extend 'shinydashboard' with 'AdminLTE2' components. 
             'AdminLTE2' is a free 'Bootstrap 3' dashboard template available
             at <https://adminlte.io>. Customize boxes, add timelines and a lot more. ",2021-07-16,David Granjon,"https://github.com/RinteRface/shinydashboardPlus,
https://rinterface.com/shiny/shinydashboardPlus/",TRUE,https://github.com/rinterface/shinydashboardplus,228441,347,2021-07-16T10:38:55Z,658.3314121037464
shinydisconnect,"A 'Shiny' app can disconnect for a variety of reasons: an unrecoverable error occurred in
    the app, the server went down, the user lost internet connection, or any other reason
    that might cause the 'Shiny' app to lose connection to its server. With 'shinydisconnect', you can
    call disonnectMessage() anywhere in a Shiny app's UI to add a nice message when this happens. 
    Works locally (running Shiny apps within 'RStudio') and on Shiny servers (such as shinyapps.io,
    'RStudio Connect', 'Shiny Server Open Source', 'Shiny Server Pro'). See demo online at 
    <https://daattali.com/shiny/shinydisconnect-demo/>.",2020-07-23,Dean Attali,https://github.com/daattali/shinydisconnect,TRUE,https://github.com/daattali/shinydisconnect,37639,37,2021-01-17T09:14:30Z,1017.2702702702703
shinyEffects,"Add fancy CSS effects to your 'shinydashboards' or 'shiny' apps.
             100% compatible with 'shinydashboardPlus' and 'bs4Dash'.",2021-05-14,David Granjon,https://github.com/RinteRface/shinyEffects,TRUE,https://github.com/rinterface/shinyeffects,62346,41,2021-05-13T21:55:03Z,1520.6341463414635
shinyFeedback,Easily display user feedback in Shiny apps.,2020-09-22,Andy Merlino,https://github.com/merlinoa/shinyFeedback,TRUE,https://github.com/merlinoa/shinyfeedback,46428,154,2021-05-25T14:52:19Z,301.4805194805195
shinyFiles,"Provides functionality for client-side navigation of
    the server side file system in shiny apps. In case the app is running
    locally this gives the user direct access to the file system without the
    need to ""download"" files to a temporary location. Both file and folder
    selection as well as file saving is available.",2020-11-09,Thomas Lin Pedersen,https://github.com/thomasp85/shinyFiles,TRUE,https://github.com/thomasp85/shinyfiles,280967,139,2021-08-09T00:58:03Z,2021.3453237410072
shinyfilter,"Allows to connect 'selectizeInputs' widgets as filters to a 'reactable' table. 
  As known from spreadsheet applications, column filters are interdependent, so each 
  filter only shows the values that are really available at the moment based on 
  the current selection in other filters. Filter values currently not available 
  (and also those being available) can be shown via popovers or tooltips.",2021-05-10,Joachim Zuckarelli,https://github.com/jsugarelli/shinyfilter/,TRUE,https://github.com/jsugarelli/shinyfilter,3475,9,2021-05-10T20:21:01Z,386.1111111111111
shinyfullscreen,"In 'Shiny' apps, it is sometimes useful to see a plot or a
    table in full screen. Using 'Shinyfullscreen', you can easily designate
    the 'HTML' elements that can be displayed on fullscreen and use buttons to 
    trigger the fullscreen view.",2021-01-11,Etienne Bacher,https://github.com/etiennebacher/shinyfullscreen,TRUE,https://github.com/etiennebacher/shinyfullscreen,3316,19,2021-01-28T10:07:45Z,174.52631578947367
shinyglide,"Insert Glide JavaScript component into Shiny applications for
    carousel or assistant-like user interfaces.",2021-06-11,Julien Barnier,"https://juba.github.io/shinyglide/,
https://github.com/juba/shinyglide",TRUE,https://github.com/juba/shinyglide,11463,60,2021-08-04T17:30:51Z,191.05
ShinyItemAnalysis,"Package including functions and interactive shiny
    application for the psychometric analysis of educational tests,
    psychological assessments, health-related and other types of
    multi-item measurements, or ratings from multiple raters.",2021-05-31,Patricia Martinkova,"http://www.ShinyItemAnalysis.org,
https://CRAN.R-project.org/package=ShinyItemAnalysis",TRUE,https://github.com/patriciamar/shinyitemanalysis,33485,24,2021-08-27T15:48:07Z,1395.2083333333333
shinyjqui,"An extension to shiny that brings interactions and animation effects from
    'jQuery UI' library.",2021-02-23,Yang Tang,"https://github.com/yang-tang/shinyjqui,
https://yang-tang.github.io/shinyjqui/",TRUE,https://github.com/yang-tang/shinyjqui,160347,236,2021-07-05T06:26:11Z,679.4364406779661
shinyjs,"Perform common useful JavaScript operations in Shiny apps that will
    greatly improve your apps without having to know any JavaScript. Examples
    include: hiding an element, disabling an input, resetting an input back to
    its original value, delaying code execution by a few seconds, and many more
    useful functions for both the end user and the developer. 'shinyjs' can also
    be used to easily call your own custom JavaScript functions from R.",2020-09-09,Dean Attali,https://deanattali.com/shinyjs/,TRUE,https://github.com/daattali/shinyjs,1998687,590,2021-01-23T02:31:54Z,3387.6050847457627
shinyloadtest,"Assesses the number of concurrent users 'shiny'
  applications are capable of supporting, and for directing application changes
  in order to support a higher number of users. Provides facilities for recording
  'shiny' application sessions, playing recorded sessions against a target
  server at load, and analyzing the resulting metrics.",2021-02-11,Barret Schloerke,"https://rstudio.github.io/shinyloadtest/,
https://github.com/rstudio/shinyloadtest",TRUE,https://github.com/rstudio/shinyloadtest,10469,89,2021-02-11T17:21:23Z,117.62921348314607
shinylogs,"Track and record the use of applications and the user's interactions with 'Shiny' inputs.
  Allow to save inputs clicked, output generated and eventually errors.",2019-08-21,Victor Perrier,https://github.com/dreamRs/shinylogs,TRUE,https://github.com/dreamrs/shinylogs,14801,66,2020-09-08T08:50:01Z,224.25757575757575
shinymanager,"Simple and secure authentification mechanism for single 'Shiny' applications.
    Credentials are stored in an encrypted 'SQLite' database. Source code of main application
    is protected until authentication is successful.",2021-06-16,Benoit Thieurmel,https://github.com/datastorm-open/shinymanager,TRUE,https://github.com/datastorm-open/shinymanager,33820,258,2021-07-13T15:42:54Z,131.08527131782947
shinyMergely,"A 'Shiny' app allowing to compare and merge two files, with syntax highlighting for several coding languages.",2020-09-14,Stéphane Laurent,https://github.com/stla/shinyMergely,TRUE,https://github.com/stla/shinymergely,3885,4,2020-09-04T15:37:45Z,971.25
shinymeta,"Provides tools for capturing logic in a Shiny app and exposing it as code that can be run outside of Shiny (e.g., from an R console). It also provides tools for bundling both the code and results to the end user.",2021-06-15,Carson Sievert,"https://rstudio.github.io/shinymeta/,
https://github.com/rstudio/shinymeta",TRUE,https://github.com/rstudio/shinymeta,1381,187,2021-06-16T18:27:10Z,7.385026737967914
shinyML,"Implementation of a shiny app to easily compare supervised machine learning model performances. 
    You provide the data and configure each model parameter directly on the shiny app.  
    Different supervised learning algorithms can be tested either on Spark or H2O frameworks to suit your regression and classification tasks.
    Implementation of available machine learning models on R has been done by Lantz (2013, ISBN:9781782162148).",2021-02-24,Jean Bertin,https://jeanbertinr.github.io/shinyMLpackage/,TRUE,https://github.com/jeanbertinr/shinyml,12009,17,2021-02-24T14:54:50Z,706.4117647058823
shinyMobile,"Develop outstanding 'shiny' apps for 'iOS', 'Android', desktop as well as beautiful 'shiny' gadgets.
    'shinyMobile' is built on top of the latest 'Framework7' template <https://framework7.io>.
    Discover 14 new input widgets (sliders, vertical sliders, stepper, 
    grouped action buttons, toggles, picker, smart select, ...), 2 themes (light and dark), 
    12 new widgets (expandable cards, badges, chips, timelines, gauges, progress bars, ...) 
    combined with the power of server-side notifications such as alerts, modals, toasts,
    action sheets, sheets (and more) as well as 3 layouts (single, tabs and split).",2021-07-22,David Granjon,"https://github.com/RinteRface/shinyMobile,
https://rinterface.github.io/shinyMobile/",TRUE,https://github.com/rinterface/shinymobile,14699,277,2021-07-22T14:23:58Z,53.064981949458485
shinyMonacoEditor,"A 'Shiny' app including the 'Monaco' editor. The 'Monaco' editor is the code editor which powers 'VS Code'. It is particularly well developed for 'JavaScript'. In addition to the 'Monaco' editor features, the app provides prettifiers and minifiers for multiple languages, 'SCSS' and 'TypeScript' compilers, code checking for 'C' and 'C++' (requires 'cppcheck').",2020-10-14,Stéphane Laurent,https://github.com/stla/shinyMonacoEditor,TRUE,https://github.com/stla/shinymonacoeditor,4560,9,2020-10-21T17:58:20Z,506.6666666666667
shinyobjects,"Troubleshooting reactive data in 'shiny' can be difficult. These functions will convert reactive data frames into functions and load all assigned objects into your local environment. If you create a dummy input object, as the function will suggest, you will be able to test your server and ui functions interactively.",2020-07-29,Jake Riley,NA,TRUE,https://github.com/rjake/shinyobjects,6440,20,2020-12-13T04:11:36Z,322
shinypanels,"Create 'Shiny Apps' with collapsible vertical panels. 
    This package provides a new visual arrangement for elements on top of 'Shiny'. 
    Use the expand and collapse capabilities to leverage web applications with
    many elements to focus the user attention on the panel of interest.",2020-01-26,Juan Pablo Marin Diaz,http://github.com/datasketch/shinypanels,TRUE,https://github.com/datasketch/shinypanels,7578,57,2021-08-10T19:15:17Z,132.94736842105263
shinystan,"A graphical user interface for interactive Markov chain Monte
    Carlo (MCMC) diagnostics and plots and tables helpful for analyzing a
    posterior sample. The interface is powered by the 'Shiny' web
    application framework from 'RStudio' and works with the output of MCMC 
    programs written in any programming language (and has extended 
    functionality for 'Stan' models fit using the 'rstan' and 'rstanarm' 
    packages).",2018-05-01,Jonah Gabry,"http://mc-stan.org/, http://discourse.mc-stan.org",TRUE,https://github.com/stan-dev/shinystan,918491,175,2021-04-13T22:32:18Z,5248.52
shinytest,"For automated testing of Shiny applications, using
    a headless browser, driven through 'WebDriver'.",2021-01-13,Winston Chang,https://github.com/rstudio/shinytest,TRUE,https://github.com/rstudio/shinytest,250548,212,2021-07-19T19:53:36Z,1181.8301886792453
shinytitle,Enables the ability to change or flash the title of the browser window during a 'shiny' session.,2021-06-16,Ashley Baldry,"https://github.com/ashbaldry/shinytitle,
https://ashbaldry.github.io/shinytitle/",TRUE,https://github.com/ashbaldry/shinytitle,924,0,2021-06-15T17:54:10Z,NA
shinyToastify,"This is a wrapper of the 'React' library 'React-Toastify'. It allows to show some notifications (toasts) in 'Shiny' applications. There are options for the style, the position, the transition effect, and more.",2021-07-31,Stéphane Laurent,https://github.com/stla/shinyToastify,TRUE,https://github.com/stla/shinytoastify,769,6,2021-07-31T10:41:33Z,128.16666666666666
shinyTree,"Exposes bindings to jsTree -- a JavaScript library
    that supports interactive trees -- to enable a rich, editable trees in
    Shiny.",2019-05-27,Mike Schaffer,NA,TRUE,https://github.com/trestletech/shinytree,66926,115,2021-05-07T19:50:26Z,581.9652173913043
shinyvalidate,"Improves the user experience of Shiny apps by helping to
    provide feedback when required inputs are missing, or input values
    are not valid.",2021-06-29,Richard Iannone,"https://rstudio.github.io/shinyvalidate/,
https://github.com/rstudio/shinyvalidate",TRUE,https://github.com/rstudio/shinyvalidate,3118,77,2021-07-08T17:19:56Z,40.493506493506494
shinyWidgets,"Collection of custom input controls and user interface components for 'Shiny' applications. 
  Give your applications a unique and colorful style !",2021-03-15,Victor Perrier,https://github.com/dreamRs/shinyWidgets,TRUE,https://github.com/dreamrs/shinywidgets,1109428,602,2021-08-31T13:32:42Z,1842.9036544850499
shinyypr,A user interface to the 'ypr' R package. 'Ypr' implements equilibrium-based yield per recruit methods for estimating the optimal yield for a fish population.,2020-03-24,Sebastian Dalgarno,https://github.com/poissonconsulting/shinyypr,TRUE,https://github.com/poissonconsulting/shinyypr,7359,0,2021-02-26T16:59:25Z,NA
ShortForm,"Performs automatic creation of short forms of scales with an 
    ant colony optimization algorithm and a Tabu search. As implemented in the 
    package, the ant colony algorithm randomly selects items to build a model of 
    a specified length, then updates the probability of item selection according 
    to the fit of the best model within each set of searches. The algorithm 
    continues until the same items are selected by multiple ants a given number 
    of times in a row. On the other hand, the Tabu search changes one parameter at
    a time to be either free, constrained, or fixed while keeping track of the
    changes made and putting changes that result in worse fit in a ""tabu"" list
    so that the algorithm does not revisit them for some number of searches. 
    See Leite, Huang, & Marcoulides (2008) <doi:10.1080/00273170802285743> for
    an applied example of the ant colony algorithm, and Marcoulides & Falk (2018)
    <doi:10.1080/10705511.2017.1409074> for an applied example of the Tabu search.",2020-03-13,Anthony Raborn,https://github.com/AnthonyRaborn/ShortForm,TRUE,https://github.com/anthonyraborn/shortform,18577,3,2021-05-11T20:25:37Z,6192.333333333333
shorts,"Create short sprint (<6sec) profiles using the split times or the radar gun data.
    Mono-exponential equation is used to estimate maximal sprinting speed (MSS), relative acceleration (TAU),
    and other parameters such us maximal acceleration (MAC) and maximal relative power (PMAX). These parameters 
    can be used to predict kinematic and kinetics variables and to compare individuals. The modeling method utilized
    in this package is based on the works of Chelly SM, Denis C. (2001) <doi: 10.1097/00005768-200102000-00024>,
    Clark KP, Rieger RH, Bruno RF, Stearne DJ. (2017) <doi: 10.1519/JSC.0000000000002081>, 
    Furusawa K, Hill AV, Parkinson JL (1927) <doi: 10.1098/rspb.1927.0035>, 
    Greene PR. (1986) <doi: 10.1016/0025-5564(86)90063-5>, and 
    Samozino P. (2018) <doi: 10.1007/978-3-319-05633-3_11>.",2021-07-19,Mladen Jovanović,https://mladenjovanovic.github.io/shorts/,TRUE,https://github.com/mladenjovanovic/shorts,9099,6,2021-07-19T13:18:58Z,1516.5
showtext,"Making it easy to use various types of fonts ('TrueType',
    'OpenType', Type 1, web fonts, etc.) in R graphs, and supporting most output
    formats of R graphics including PNG, PDF and SVG. Text glyphs will be converted
    into polygons or raster images, hence after the plot has been created, it no
    longer relies on the font files. No external software such as 'Ghostscript' is
    needed to use this package.",2021-08-14,"Yixuan Qiu and authors/contributors of the
    included software. See file AUTHORS for details.",https://github.com/yixuan/showtext,TRUE,https://github.com/yixuan/showtext,728924,372,2021-08-14T05:21:24Z,1959.47311827957
ShrinkCovMat,"Provides nonparametric Steinian shrinkage estimators of the covariance matrix that are suitable in high dimensional settings, that is when the number of variables is larger than the sample size.",2019-07-30,Anestis Touloumis,http://github.com/AnestisTouloumis/ShrinkCovMat,TRUE,https://github.com/anestistouloumis/shrinkcovmat,18217,5,2021-05-09T23:51:23Z,3643.4
SHT,"We provide a collection of statistical hypothesis testing procedures ranging from classical to modern methods for non-trivial settings such as high-dimensional scenario. For the general treatment of statistical hypothesis testing, see the book by Lehmann and Romano (2005) <doi:10.1007/0-387-27605-X>.",2021-07-04,Kisung You,https://kisungyou.com/SHT/,TRUE,https://github.com/kisungyou/sht,13810,5,2021-07-11T03:29:00Z,2762
sicegar,"
    Aims to quantify time intensity data by using sigmoidal and
    double sigmoidal curves. It fits straight lines, sigmoidal, 
    and double sigmoidal curves on to time vs intensity data. 
    Then all the fits are used to make decision on which model 
    best describes the data. This method was first developed 
    in the context of single-cell viral growth analysis (for
    details, see Caglar et al. (2018) <doi:10.7717/peerj.4251>),
    and the package name stands for ""SIngle CEll Growth Analysis in R"".",2021-05-08,Claus O. Wilke,https://github.com/wilkelab/sicegar,TRUE,https://github.com/wilkelab/sicegar,16629,5,2021-05-08T00:57:47Z,3325.8
siconfir,"Access tax and accounting data of Brazilian states 
    and municipalities provided by the Brazilian Public Sector 
    Accounting and Tax Information System.",2021-04-20,Pedro Castro,https://github.com/pedrocastroo/siconfir,TRUE,https://github.com/pedrocastroo/siconfir,1440,5,2021-04-20T13:35:08Z,288
siconvr,"Fetch data on targeted public investments from Plataforma +Brasil (SICONV) <http://plataformamaisbrasil.gov.br/>, 
    the responsible system for requests, execution, and monitoring of federal discretionary transfers in Brazil.",2021-05-18,Fernando Meireles,"https://github.com/meirelesff/siconvr,
https://fmeireles.com/siconvr/",TRUE,https://github.com/meirelesff/siconvr,1132,6,2021-06-24T17:05:17Z,188.66666666666666
sidrar,"Allows the user to connect with IBGE's (Instituto Brasileiro de 
    Geografia e Estatistica, see <https://www.ibge.gov.br/> for more information)
    SIDRA API in a flexible way. SIDRA is the acronym to ""Sistema IBGE de 
    Recuperacao Automatica"" and is the system where IBGE turns available 
    aggregate data from their researches.",2021-08-02,Renato Prado Siqueira,https://github.com/rpradosiqueira/sidrar/,TRUE,https://github.com/rpradosiqueira/sidrar,46899,39,2021-07-30T22:31:20Z,1202.5384615384614
sievePH,"Implements semiparametric estimation and testing procedures for a continuous, possibly multivariate, mark-specific hazard ratio (treatment/placebo) of an event of interest in a randomized treatment efficacy trial with a time-to-event endpoint, as described in Juraska M and Gilbert PB (2013), Mark-specific hazard ratio model with multivariate continuous marks: an application to vaccine efficacy. Biometrics 69(2):328 337 <doi:10.1111/biom.12016>, and in Juraska M and Gilbert PB (2015), Mark-specific hazard ratio model with missing multivariate marks. Lifetime Data Analysis 22(4): 606-25 <doi:10.1007/s10985-015-9353-9>. The former considers continuous multivariate marks fully observed in all subjects who experience the event of interest, whereas the latter extends the previous work to allow multivariate marks that are subject to missingness-at-random. For models with missing marks, two estimators are implemented based on (i) inverse probability weighting (IPW) of complete cases, and (ii) augmentation of the IPW estimating functions by leveraging correlations between the mark and auxiliary data to 'impute' the expected profile score vectors for subjects with missing marks. The augmented IPW estimator is doubly robust and recommended for use with incomplete mark data. The methods make two key assumptions: (i) the time-to-event is assumed to be conditionally independent of the mark given treatment, and (ii) the weight function in the semiparametric density ratio/biased sampling model is assumed to be exponential. Diagnostic testing procedures for evaluating validity of both assumptions are implemented. Summary and plotting functions are provided for estimation and inferential results.",2019-12-06,Michal Juraska,https://github.com/mjuraska/sievePH,TRUE,https://github.com/mjuraska/sieveph,10489,0,2020-12-07T23:54:35Z,NA
SightabilityModel,"Uses logistic regression to model the probability of detection as a function of covariates. 
             This model is then used with observational survey data to estimate population size, while
             accounting for uncertain detection.  See Steinhorst and Samuel (1989).",2020-09-08,Schwarz Carl James,https://github.com/jfieberg/SightabilityModel,TRUE,https://github.com/jfieberg/sightabilitymodel,19157,0,2020-09-08T01:54:27Z,NA
sigmajs,"Interface to 'sigma.js' graph visualization library including animations, plugins and shiny proxies.",2020-06-18,John Coene,http://sigmajs.john-coene.com/,TRUE,https://github.com/johncoene/sigmajs,15482,58,2021-01-29T09:28:34Z,266.9310344827586
sigminer,"Genomic alterations including single nucleotide
    substitution, copy number alteration, etc. are the major force for
    cancer initialization and development. Due to the specificity of
    molecular lesions caused by genomic alterations, we can generate
    characteristic alteration spectra, called 'signature' (Wang, Shixiang,
    et al.  (2020) <DOI:10.1371/journal.pgen.1009557> & Alexandrov, Ludmil
    B., et al. (2020) <DOI:10.1038/s41586-020-1943-3>).  This package helps
    users to extract, analyze and visualize signatures from genomic
    alteration records, thus providing new insight into cancer study.",2021-09-03,Shixiang Wang,https://github.com/ShixiangWang/sigminer,TRUE,https://github.com/shixiangwang/sigminer,12270,86,2021-09-03T05:57:29Z,142.67441860465115
Signac,"A framework for the analysis and exploration of single-cell chromatin data.
    The 'Signac' package contains functions for quantifying single-cell chromatin data,
    computing per-cell quality control metrics, dimension reduction
    and normalization, visualization, and DNA sequence motif analysis.
    Reference: Stuart et al. (2020) <doi:10.1101/2020.11.09.373613>.",2021-07-12,Tim Stuart,"https://github.com/timoast/signac, https://satijalab.org/signac",TRUE,https://github.com/timoast/signac,71126,114,2021-07-12T13:10:52Z,623.9122807017544
SignacX,An implementation of neural networks trained with flow-sorted gene expression data to classify cellular phenotypes in single cell RNA-sequencing data. See Chamberlain M et al. (2021) <doi:10.1101/2021.02.01.429207> for more details.,2021-07-22,Mathew Chamberlain,https://github.com/mathewchamberlain/SignacX,TRUE,https://github.com/mathewchamberlain/signacx,2792,13,2021-07-22T20:19:06Z,214.76923076923077
signnet,"Methods for the analysis of signed networks. This includes several measures for structural balance as introduced by Cartwright and Harary (1956) <doi:10.1037/h0046049>, blockmodeling algorithms from Doreian (2008) <doi:10.1016/j.socnet.2008.03.005>, various centrality indices, and projections of signed two-mode networks introduced by Schoch (2020) <doi:10.1080/0022250X.2019.1711376>.",2021-04-28,David Schoch,https://github.com/schochastics/signnet,TRUE,https://github.com/schochastics/signnet,10733,16,2021-04-28T13:20:33Z,670.8125
sigr,"Succinctly and correctly format statistical summaries of
    various models and tests (F-test, Chi-Sq-test, Fisher-test, T-test, and rank-significance). 
    This package also includes empirical tests, such as Monte Carlo and bootstrap distribution estimates.",2021-06-12,John Mount,"https://github.com/WinVector/sigr/,
https://winvector.github.io/sigr/",TRUE,https://github.com/winvector/sigr,53218,27,2021-06-12T02:32:46Z,1971.037037037037
SIHR,"Inference procedures in the high-dimensional setting for (1) linear functionals and quadratic functionals in linear regression ('Cai et al.' (2019) <arXiv:1904.12891>, 'Guo et al.' (2019) <arXiv:1909.01503>), (2) linear functional in logistic regression ('Guo et al.' <arXiv:2012.07133>), (3) individual treatment effects in linear and logistic regression.",2021-06-14,Prabrisha Rakshit,https://github.com/prabrishar1/SIHR,TRUE,https://github.com/prabrishar1/sihr,948,0,2021-06-11T17:58:40Z,NA
silicate,"Generate common data forms for complex data suitable for conversions and
 transmission by decomposition as paths or primitives. Paths are sequentially-linked records, 
 primitives are basic atomic elements and both can model many forms and be grouped into hierarchical 
 structures.  The universal models 'SC0' (structural) and 'SC' (labelled, relational) are composed of 
 edges and can represent any hierarchical form. Specialist models 'PATH', 'ARC' and 'TRI' provide the 
 most common intermediate forms used for converting from one form to another. The methods are
 inspired by the simplicial complex <https://en.wikipedia.org/wiki/Simplicial_complex> and 
 provide intermediate forms that relate spatial data structures to this mathematical construct. ",2020-11-13,Michael D. Sumner,https://github.com/hypertidy/silicate,TRUE,https://github.com/hypertidy/silicate,15331,45,2020-11-05T13:17:42Z,340.68888888888887
Sim.DiffProc,"It provides users with a wide range of tools to simulate, estimate, analyze, and visualize the dynamics of stochastic differential systems in both forms Ito and Stratonovich. Statistical analysis with parallel Monte Carlo and moment equations methods of SDEs <doi:10.18637/jss.v096.i02>. Enabled many searchers in different domains to use these equations to modeling practical problems in financial and actuarial modeling and other areas of application, e.g., modeling and simulate of first passage time problem in shallow water using the attractive center (Boukhetala K, 1996) ISBN:1-56252-342-2. ",2020-11-08,Arsalane Chouaib Guidoum,https://github.com/acguidoum/Sim.DiffProc,TRUE,https://github.com/acguidoum/sim.diffproc,52427,9,2020-12-03T14:20:50Z,5825.222222222223
sim2Dpredictr,"Provides tools for simulating spatially dependent predictors (continuous or binary),
    which are used to generate scalar outcomes in a (generalized) linear model framework. Continuous
    predictors are generated using traditional multivariate normal distributions or Gauss Markov random
    fields with several correlation function approaches (e.g., see Rue (2001) <doi:10.1111/1467-9868.00288>
    and Furrer and Sain (2010) <doi:10.18637/jss.v036.i10>), while binary predictors are generated using
    a Boolean model (see Cressie and Wikle (2011, ISBN: 978-0-471-69274-4)). Parameter vectors 
	exhibiting spatial clustering can also be easily specified by the user.  ",2020-03-14,Justin Leach,http://github.com/jmleach-bst/sim2Dpredictr,TRUE,https://github.com/jmleach-bst/sim2dpredictr,6610,0,2021-06-11T17:58:15Z,NA
SimBIID,"Provides some code to run simulations of state-space models, and then
             use these in the Approximate Bayesian Computation Sequential Monte Carlo (ABC-SMC) 
             algorithm of Toni et al. (2009) <doi:10.1098/rsif.2008.0172> and a bootstrap particle
             filter based particle Markov chain Monte Carlo (PMCMC) algorithm 
             (Andrieu et al., 2010 <doi:10.1111/j.1467-9868.2009.00736.x>). 
             Also provides functions to plot and summarise the outputs.",2021-02-04,Trevelyan J. McKinley,https://github.com/tjmckinley/SimBIID,TRUE,https://github.com/tjmckinley/simbiid,11066,1,2021-02-03T23:22:43Z,11066
SimCorMultRes,Simulates correlated multinomial responses conditional on a marginal model specification.,2021-06-10,Anestis Touloumis,https://github.com/AnestisTouloumis/SimCorMultRes,TRUE,https://github.com/anestistouloumis/simcormultres,19884,7,2021-06-10T12:42:21Z,2840.5714285714284
simcross,"Simulate and plot general experimental crosses. The focus is on simulating genotypes with an aim towards flexibility rather than speed. Meiosis is simulated following the Stahl model, in which chiasma locations are the superposition of two processes: a proportion p coming from a process exhibiting no interference, and the remainder coming from a process following the chi-square model.",2020-09-24,Karl W Broman,"https://kbroman.org/simcross/, https://github.com/kbroman/simcross",TRUE,https://github.com/kbroman/simcross,3841,6,2021-07-31T14:05:57Z,640.1666666666666
SimDesign,"Provides tools to safely and efficiently organize and execute 
    Monte Carlo simulation experiments in R.
    The package controls the structure and back-end of Monte Carlo simulation experiments
    by utilizing a generate-analyse-summarise workflow. The workflow safeguards against 
    common simulation coding issues, such as automatically re-simulating non-convergent results, 
    prevents inadvertently overwriting simulation files, catches error and warning messages
    during execution, and implicitly supports parallel processing.
    For a pedagogical introduction to the package see
    Sigal and Chalmers (2016) <doi:10.1080/10691898.2016.1246953>. For a more in-depth overview of 
    the package and its design philosophy see Chalmers and Adkins (2020) <doi:10.20982/tqmp.16.4.p248>.",2021-09-02,Phil Chalmers,"https://github.com/philchalmers/SimDesign,
https://github.com/philchalmers/SimDesign/wiki",TRUE,https://github.com/philchalmers/simdesign,82491,43,2021-08-31T01:55:05Z,1918.3953488372092
simfam,"The focus is on simulating and modeling families with founders drawn from a structured population (for example, with different ancestries or other potentially non-family relatedness), in contrast to traditional pedigree analysis that treats all founders as equally unrelated.  Main function simulates a random pedigree for many generations, avoiding close relatives, pairing closest individuals according to a 1D geography and their randomly-drawn sex, and with variable children sizes to result in a target population size per generation.  Auxiliary functions calculate kinship matrices, admixture matrices, and draw random genotypes across arbitrary pedigree structures starting from the corresponding founder values.  The code is built around the plink FAM table format for pedigrees.  Partially described in Yao and Ochoa (2019) <doi:10.1101/858399>.",2021-09-02,Alejandro Ochoa,https://github.com/OchoaLab/simfam,TRUE,https://github.com/ochoalab/simfam,0,0,2021-08-31T21:43:41Z,NA
simfinapi,"Through simfinapi, you can intuitively access the
    'SimFin' Web-API (<https://simfin.com/>) to make 'SimFin' data easily
    available in R. To obtain an 'SimFin' API key (and thus to use this
    package), you need to register at <https://simfin.com/login>.",2020-09-28,Matthias Gomolka,https://github.com/Plebejer/simfinapi,TRUE,https://github.com/plebejer/simfinapi,4079,7,2021-08-09T13:15:49Z,582.7142857142857
simfinR,"Uses the 'SimFin' (SIMmplifying FINnance) api at <https://simfin.com/data/access/api> to download financial data straight into your R session. 
    It includes financial statements -- balance sheet, cash flow and income statement -- and adjusted daily price of stocks.
    The available data is comprehensive, going back to 2005 and available for quarters (Q1, Q2, Q3, Q4) and years (FY).",2021-04-02,Marcelo S. Perlin,https://github.com/msperlin/simfinR/,TRUE,https://github.com/msperlin/simfinr,10847,7,2021-06-16T11:49:35Z,1549.5714285714287
simglm,"Simulates regression models,
    including both simple regression and generalized linear mixed
    models with up to three level of nesting. Power simulations that are
    flexible allowing the specification of missing data, unbalanced designs,
    and different random error distributions are built into the package.",2021-08-09,Brandon LeBeau,https://github.com/lebebr01/simglm,TRUE,https://github.com/lebebr01/simglm,18092,34,2021-08-04T20:07:30Z,532.1176470588235
simhelpers,"Calculates performance criteria measures and associated Monte Carlo standard errors for simulation results. Includes functions to help run simulation studies. Our derivation and explanation of formulas and our general simulation workflow is closely aligned with the approach described by Morris, White, and Crowther (2019) <DOI: 10.1002/sim.8086>. ",2021-02-14,Megha Joshi,https://meghapsimatrix.github.io/simhelpers/index.html,TRUE,https://github.com/meghapsimatrix/simhelpers,5999,6,2021-06-06T23:32:01Z,999.8333333333334
SimInf,"Provides an efficient and very flexible framework to
    conduct data-driven epidemiological modeling in realistic large
    scale disease spread simulations. The framework integrates
    infection dynamics in subpopulations as continuous-time Markov
    chains using the Gillespie stochastic simulation algorithm and
    incorporates available data such as births, deaths and movements
    as scheduled events at predefined time-points. Using C code for
    the numerical solvers and 'OpenMP' (if available) to divide work
    over multiple processors ensures high performance when simulating
    a sample outcome. One of our design goals was to make the package
    extendable and enable usage of the numerical solvers from other R
    extension packages in order to facilitate complex epidemiological
    research. The package contains template models and can be extended
    with user-defined models. For more details see the paper by
    Widgren, Bauer, Eriksson and Engblom (2019)
    <doi:10.18637/jss.v091.i12>. The package also provides
    functionality to fit models to time series data using the
    Approximate Bayesian Computation Sequential Monte Carlo
    ('ABC-SMC') algorithm of Toni and others (2009)
    <doi:10.1098/rsif.2008.0172>.",2021-06-30,Stefan Widgren,https://github.com/stewid/SimInf,TRUE,https://github.com/stewid/siminf,24532,18,2021-06-30T16:49:20Z,1362.888888888889
simlandr,A set of tools for constructing potential landscapes for dynamical systems using Monte-Carlo simulation. Especially suitable for psychological formal models.,2021-08-16,Jingmeng Cui,https://github.com/Sciurus365/simlandr,TRUE,https://github.com/sciurus365/simlandr,212,0,2021-08-16T21:43:36Z,NA
simmer,"A process-oriented and trajectory-based Discrete-Event Simulation
    (DES) package for R. It is designed as a generic yet powerful framework. The
    architecture encloses a robust and fast simulation core written in 'C++' with
    automatic monitoring capabilities. It provides a rich and flexible R API that
    revolves around the concept of trajectory, a common path in the simulation
    model for entities of the same type. Documentation about 'simmer' is provided
    by several vignettes included in this package, via the paper by Ucar, Smeets
    & Azcorra (2019, <doi:10.18637/jss.v090.i02>), and the paper by Ucar,
    Hernández, Serrano & Azcorra (2018, <doi:10.1109/MCOM.2018.1700960>);
    see 'citation(""simmer"")' for details.",2021-08-11,Iñaki Ucar,"https://r-simmer.org, https://github.com/r-simmer/simmer",TRUE,https://github.com/r-simmer/simmer,49605,177,2021-08-18T11:08:03Z,280.2542372881356
simmer.bricks,Provides wrappers for common activity patterns in 'simmer' trajectories.,2019-01-09,Iñaki Ucar,"http://r-simmer.org, https://github.com/r-simmer/simmer.bricks",TRUE,https://github.com/r-simmer/simmer.bricks,13776,5,2021-08-10T15:08:35Z,2755.2
simmer.plot,A set of plotting methods for 'simmer' trajectories and simulations.,2020-04-25,Iñaki Ucar,"http://r-simmer.org, https://github.com/r-simmer/simmer.plot",TRUE,https://github.com/r-simmer/simmer.plot,30650,9,2021-08-10T15:03:43Z,3405.5555555555557
simmr,"Fits Stable Isotope Mixing Models (SIMMs) and is meant as a longer term replacement to the previous widely-used package SIAR. SIMMs are used to infer dietary proportions of organisms consuming various food sources from observations on the stable isotope values taken from the organisms' tissue samples. However SIMMs can also be used in other scenarios, such as in sediment mixing or the composition of fatty acids. The main functions are simmr_load and simmr_mcmc. The two vignettes contain a quick start and a full listing of all the features. The methods used are detailed in the papers Parnell et al 2010 <doi:10.1371/journal.pone.0009672>, and Parnell et al 2013 <doi:10.1002/env.2221>.",2021-02-27,Andrew Parnell,"https://github.com/andrewcparnell/simmr,
https://andrewcparnell.github.io/simmr/",TRUE,https://github.com/andrewcparnell/simmr,25644,14,2021-06-30T15:33:44Z,1831.7142857142858
simPH,"Simulates and plots quantities of interest (relative
    hazards, first differences, and hazard ratios) for linear coefficients,
    multiplicative interactions, polynomials, penalised splines, and
    non-proportional hazards, as well as stratified survival curves from Cox
    Proportional Hazard models. It also simulates and plots marginal effects
    for multiplicative interactions. Methods described in Gandrud (2015)
    <doi:10.18637/jss.v065.i03>.",2021-01-10,Christopher Gandrud,https://CRAN.R-project.org/package=simPH,TRUE,https://github.com/christophergandrud/simph,27579,14,2021-01-28T18:01:00Z,1969.9285714285713
simpleCache,"Provides intuitive functions for caching R objects, encouraging
    reproducible, restartable, and distributed R analysis. The user selects a
    location to store caches, and then provides  nothing more than a cache name
    and instructions (R code) for how to produce the R object. Also
    provides some advanced options like environment assignments, recreating or
    reloading caches, and cluster compute bindings (using the 'batchtools'
    package) making it flexible enough for use in large-scale data analysis
    projects.",2021-04-17,Nathan Sheffield,https://github.com/databio/simpleCache,TRUE,https://github.com/databio/simplecache,17869,30,2021-04-19T21:58:52Z,595.6333333333333
simplecolors,"A curated set of colors that are called using
    a standardized syntax: saturation + hue + lightness. For example, 
    ""brightblue4"" and ""mutedred2"". Functions exists to return individual colors 
    by name or to build palettes across or within hues. Most functions allow you 
    to visualize the palettes in addition to returning the desired hex codes.",2020-10-27,Jake Riley,https://github.com/rjake/simplecolors,TRUE,https://github.com/rjake/simplecolors,8175,7,2020-10-27T04:24:10Z,1167.857142857143
simplegraphdb,This is a graph database in 'SQLite'.  It is inspired by Denis Papathanasiou's Python simple-graph project on 'GitHub'.,2021-03-12,Michael Silva,https://github.com/mikeasilva/simplegraphdb,TRUE,https://github.com/mikeasilva/simplegraphdb,1812,0,2021-03-11T02:51:36Z,NA
simplePHENOTYPES,"The number of studies involving correlated traits and the availability of tools to handle this type of data has increased considerably in the last decade. With such a demand, we need tools for testing hypotheses related to single and multi-trait (correlated) phenotypes based on many genetic settings. Thus, we implemented various options for simulation of pleiotropy and Linkage Disequilibrium under additive, dominance and epistatic models. The simulation currently takes a marker data set as an input and then uses it for simulating multiple traits as described in Fernandes and Lipka (2020) <doi:10.1186/s12859-020-03804-y>.",2021-01-20,Samuel Fernandes,https://github.com/samuelbfernandes/simplePHENOTYPES,TRUE,https://github.com/samuelbfernandes/simplephenotypes,8703,5,2021-01-21T05:07:51Z,1740.6
simplevis,Wrapper functions to make 'ggplot2' and 'leaflet' visualisation easier with less brainpower required.,2021-08-06,David Hodge,"https://statisticsnz.github.io/simplevis/,
https://github.com/statisticsnz/simplevis/",TRUE,https://github.com/statisticsnz/simplevis,13940,41,2021-08-12T02:44:13Z,340
simplextree,"Provides an interface to a Simplex Tree data structure, which is 
  a data structure aimed at enabling efficient manipulation of simplicial complexes 
  of any dimension. The Simplex Tree data structure was originally introduced by 
  Jean-Daniel Boissonnat and Clément Maria (2014) <doi:10.1007/s00453-014-9887-3>. ",2020-09-12,Matt Piekenbrock,https://github.com/peekxc/simplextree,TRUE,https://github.com/peekxc/simplextree,4432,9,2021-07-21T18:09:33Z,492.44444444444446
simPop,"Tools and methods to simulate populations for surveys based
    on auxiliary data. The tools include model-based methods, calibration and
    combinatorial optimization algorithms, see Templ, Kowarik and Meindl (2017) <doi:10.18637/jss.v079.i10>) and
    Templ (2017) <doi:10.1007/978-3-319-50272-4>. The package was developed with support of
    the International Household Survey Network, DFID Trust Fund TF011722 and funds
    from the World bank.",2020-11-14,Matthias Templ,https://github.com/statistikat/simPop,TRUE,https://github.com/statistikat/simpop,21524,17,2021-08-30T14:27:40Z,1266.1176470588234
simputation,"Easy to use interfaces to a number of imputation methods
        that fit in the not-a-pipe operator of the 'magrittr' package.",2021-01-25,Mark van der Loo,https://github.com/markvanderloo/simputation,TRUE,https://github.com/markvanderloo/simputation,63091,71,2021-09-01T14:38:50Z,888.6056338028169
simr,"Calculate power for generalised linear mixed models, using
    simulation. Designed to work with models fit using the 'lme4' package.
    Described in Green and MacLeod, 2016 <doi:10.1111/2041-210X.12504>.",2019-01-29,Green Peter,https://github.com/pitakakariki/simr,TRUE,https://github.com/pitakakariki/simr,44515,47,2021-06-01T05:04:17Z,947.1276595744681
simrec,"Simulation of recurrent event data for non-constant baseline
    hazard in the total time model with risk-free intervals and possibly a competing event.
    Possibility to cut the data to an interim data set. Data can be plotted.
    Details about the method can be found in Jahn-Eimermacher, A. et al. (2015) <doi:10.1186/s12874-015-0005-2>.",2020-11-20,Federico Marini,https://github.com/federicomarini/simrec,TRUE,https://github.com/federicomarini/simrec,3008,1,2020-11-18T08:45:54Z,3008
simstandard,Creates simulated data from structural equation models with standardized loading. Data generation methods are described in Schneider (2013) <doi:10.1177/0734282913478046>.,2021-05-07,W. Joel Schneider,https://github.com/wjschne/simstandard,TRUE,https://github.com/wjschne/simstandard,13581,2,2021-05-10T18:01:28Z,6790.5
simstudy,"Simulates data sets in order to explore modeling
    techniques or better understand data generating processes. The user
    specifies a set of relationships between covariates, and generates
    data based on these specifications. The final data sets can represent
    data from randomized control trials, repeated measure (longitudinal)
    designs, and cluster randomized trials. Missingness can be generated
    using various mechanisms (MCAR, MAR, NMAR).",2020-10-07,Keith Goldfeld,"https://github.com/kgoldfeld/simstudy,
https://kgoldfeld.github.io/simstudy/,
https://kgoldfeld.github.io/simstudy/dev/",TRUE,https://github.com/kgoldfeld/simstudy,37740,38,2021-07-08T14:49:50Z,993.1578947368421
simsurv,"Simulate survival times from standard parametric survival
    distributions (exponential, Weibull, Gompertz), 2-component mixture
    distributions, or a user-defined hazard, log hazard, cumulative hazard,
    or log cumulative hazard function. Baseline covariates can be included
    under a proportional hazards assumption.
    Time dependent effects (i.e. non-proportional hazards) can be included by
    interacting covariates with linear time or a user-defined function of time.
    Clustered event times are also accommodated.
    The 2-component mixture distributions can allow for a variety of flexible
    baseline hazard functions reflecting those seen in practice.
    If the user wishes to provide a user-defined
    hazard or log hazard function then this is possible, and the resulting
    cumulative hazard function does not need to have a closed-form solution.
    For details see the supporting paper <doi:10.18637/jss.v097.i03>.
    Note that this package is modelled on the 'survsim' package available in
    the 'Stata' software (see Crowther and Lambert (2012)
    <https://www.stata-journal.com/sjpdf.html?articlenum=st0275> or
    Crowther and Lambert (2013) <doi:10.1002/sim.5823>).",2021-01-13,Sam Brilleman,NA,TRUE,https://github.com/sambrilleman/simsurv,26483,12,2021-01-18T08:06:33Z,2206.9166666666665
SimSurvey,"Simulate age-structured populations that vary in space and time and 
    explore the efficacy of a range of built-in or user-defined sampling 
    protocols to reproduce the population parameters of the known population. 
    (See Regular et al. (2020) <doi:10.1371/journal.pone.0232822> for more
    details).",2021-01-13,Paul Regular,https://paulregular.github.io/SimSurvey/,TRUE,https://github.com/paulregular/simsurvey,4403,10,2021-05-31T13:32:21Z,440.3
simTool,"Tool for statistical simulations that have two components. 
    One component generates the data and the other one
    analyzes the data. The main aims of the package are the reduction
    of the administrative source code (mainly loops and management code for the
    results) and a simple applicability of the package that allows the user to
    quickly learn how to work with it. Parallel computing is
    also supported. Finally, convenient functions are provided to summarize the
    simulation results.",2020-09-22,Marsel Scheer,https://github.com/MarselScheer/simTool,TRUE,https://github.com/marselscheer/simtool,18002,6,2020-09-23T16:18:53Z,3000.3333333333335
simtrait,"Simulate complex traits given a SNP genotype matrix and model parameters (the desired heritability, number of causal loci, and either the true ancestral allele frequencies used to generate the genotypes or the mean kinship for a real dataset).  Emphasis on avoiding common biases due to the use of estimated allele frequencies.  The code selects random loci to be causal, constructs coefficients for these loci and random independent non-genetic effects.  Traits can follow three models: random coefficients, fixed effect sizes, and infinitesimal (multivariate normal).  GWAS method benchmarking functions are also provided.  Partially described in Yao and Ochoa (2019) <doi:10.1101/858399>.",2021-08-16,Alejandro Ochoa,https://github.com/OchoaLab/simtrait,TRUE,https://github.com/ochoalab/simtrait,205,1,2021-08-12T14:50:57Z,205
simts,"A system contains easy-to-use tools as a support for time series analysis courses. In particular, it incorporates a technique called Generalized Method of Wavelet Moments (GMWM) as well as its robust implementation for fast and robust parameter estimation of time series models which is described, for example, in Guerrier et al. (2013) <doi: 10.1080/01621459.2013.799920>. More details can also be found in the paper linked to via the URL below.",2019-07-21,Stéphane Guerrier,"https://github.com/SMAC-Group/simts,
https://arxiv.org/pdf/1607.04543.pdf",TRUE,https://github.com/smac-group/simts,12501,8,2021-02-16T16:01:54Z,1562.625
simukde,"Generates random values from a univariate and multivariate continuous distribution by using kernel density estimation based on a sample. Duong (2017) <doi:10.18637/jss.v021.i07>, Christian P. Robert and George Casella (2010 ISBN:978-1-4419-1575-7) <doi:10.1007/978-1-4419-1576-4>.",2021-05-20,MAKHGAL Ganbold,https://github.com/galaamn/simukde,TRUE,https://github.com/galaamn/simukde,13385,0,2021-05-20T10:33:46Z,NA
sinar,"Implementation of the Conditional Least Square (CLS) estimates and
    its covariance matrix for the first-order spatial integer-valued
    autoregressive model (SINAR(1,1)) proposed by Ghodsi (2012)
    <doi:10.1080/03610926.2011.560739>.",2020-11-06,Gilberto P. Sassi,NA,TRUE,https://github.com/gilberto-sassi/sinar,3106,0,2020-11-02T14:32:06Z,NA
sinew,"Create 'roxygen2' skeleton populated with information scraped from the
         within the function script. Also creates field entries for imports in the
         'DESCRIPTION' and import in the 'NAMESPACE' files. Can be run from the R
         console or through the 'RStudio' 'addin' menu.",2018-08-31,Jonathan Sidi,https://github.com/metrumresearchgroup/sinew,TRUE,https://github.com/metrumresearchgroup/sinew,18983,130,2020-11-20T11:36:43Z,146.02307692307693
singcar,"When comparing single cases to control populations and no parameters are known researchers and clinicians must estimate these with a control sample. This is often done when testing a case's abnormality on some variable or testing abnormality of the discrepancy between two variables. Appropriate frequentist and Bayesian methods for doing this are here implemented, including tests allowing for the inclusion of covariates. These have been developed first and foremost by John Crawford and Paul Garthwaite, e.g. in Crawford and Howell (1998) <doi:10.1076/clin.12.4.482.7241>, Crawford and Garthwaite (2005) <doi:10.1037/0894-4105.19.3.318>, Crawford and Garthwaite (2007) <doi:10.1080/02643290701290146> and Crawford, Garthwaite and Ryan (2011) <doi:10.1016/j.cortex.2011.02.017>. The package is also equipped with power calculators for each method. ",2021-03-01,Jonathan Rittmo,https://github.com/jorittmo/singcar,TRUE,https://github.com/jorittmo/singcar,4175,1,2021-03-01T10:37:30Z,4175
SingleCaseES,"
  Provides R functions for calculating basic effect size indices for 
  single-case designs, including several non-overlap measures and parametric 
  effect size measures, and for estimating the gradual effects model developed 
  by Swan and Pustejovsky (2018) <DOI:10.1080/00273171.2018.1466681>. 
  Standard errors and confidence intervals (based on the assumption that the outcome 
  measurements are mutually independent) are provided for the subset of effect sizes 
  indices with known sampling distributions.",2021-04-30,James E. Pustejovsky,https://github.com/jepusto/SingleCaseES,TRUE,https://github.com/jepusto/singlecasees,18044,6,2021-08-03T20:36:29Z,3007.3333333333335
siplab,"A platform for computing competition indices and experimenting
    with spatially explicit individual-based vegetation models.",2020-09-04,Oscar Garcia,https://github.com/ogarciav/siplab/,TRUE,https://github.com/ogarciav/siplab,22812,0,2020-09-04T20:33:56Z,NA
sirt,"
    Supplementary functions for item response models aiming
    to complement existing R packages. The functionality includes among others
    multidimensional compensatory and noncompensatory IRT models
    (Reckase, 2009, <doi:10.1007/978-0-387-89976-3>), 
    MCMC for hierarchical IRT models and testlet models
    (Fox, 2010, <doi:10.1007/978-1-4419-0742-4>), 
    NOHARM (McDonald, 1982, <doi:10.1177/014662168200600402>), 
    Rasch copula model (Braeken, 2011, <doi:10.1007/s11336-010-9190-4>;
    Schroeders, Robitzsch & Schipolowski, 2014, <doi:10.1111/jedm.12054>),
    faceted and hierarchical rater models (DeCarlo, Kim & Johnson, 2011,
    <doi:10.1111/j.1745-3984.2011.00143.x>),
    ordinal IRT model (ISOP; Scheiblechner, 1995, <doi:10.1007/BF02301417>), 
    DETECT statistic (Stout, Habing, Douglas & Kim, 1996, 
    <doi:10.1177/014662169602000403>), local structural equation modeling 
    (LSEM; Hildebrandt, Luedtke, Robitzsch, Sommer & Wilhelm, 2016,
    <doi:10.1080/00273171.2016.1142856>).",2020-02-17,Alexander Robitzsch,"https://github.com/alexanderrobitzsch/sirt,
https://sites.google.com/site/alexanderrobitzsch2/software",TRUE,https://github.com/alexanderrobitzsch/sirt,275941,15,2021-06-25T07:58:49Z,18396.066666666666
sistec,"The Brazilian system for diploma registration and validation on technical and superior
    courses are managing by 'Sistec' platform, see <https://sistec.mec.gov.br/>. This package provides 
    tools for Brazilian institutions to update the student's registration and make data analysis 
    about their situation, retention and drop out.",2020-10-26,Samuel Macêdo,https://github.com/r-ifpe/sistec,TRUE,https://github.com/r-ifpe/sistec,5719,6,2021-05-18T18:45:48Z,953.1666666666666
sitar,"Functions for fitting and plotting SITAR (Super
    Imposition by Translation And Rotation) growth curve models. SITAR is
    a shape-invariant model with a regression B-spline mean curve and
    subject-specific random effects on both the measurement and age
    scales.  The model was first described by Lindstrom (1995)
    <doi:10.1002/sim.4780141807> and developed as the SITAR method by Cole
    et al (2010) <doi:10.1093/ije/dyq115>.",2021-04-22,Tim Cole,https://github.com/statist7/sitar,TRUE,https://github.com/statist7/sitar,29431,6,2021-08-24T17:00:58Z,4905.166666666667
siteymlgen,"The goal of 'siteymlgen' is to make it easy to organise
  the building of your 'R Markdown' website.
  The init() function placed within the first code chunk of the index.Rmd
  file of an 'R' project directory will initiate the generation of an automatically
  written _site.yml file. 'siteymlgen' recommends a specific naming
  convention for your 'R Markdown' files. This naming will ensure that
  your navbar layout is ordered according to a hierarchy.",2020-05-08,Adam Cribbs,https://github.com/Acribbs/siteymlgen,TRUE,https://github.com/acribbs/siteymlgen,4582,1,2021-08-18T11:53:31Z,4582
SITH,Implements a three-dimensional stochastic model of cancer growth and mutation similar to the one described in Waclaw et al. (2015) <doi:10.1038/nature14971>. Allows for interactive 3D visualizations of the simulated tumor. Provides a comprehensive summary of the spatial distribution of mutants within the tumor. Contains functions which create synthetic sequencing datasets from the generated tumor. ,2021-01-05,Phillip B. Nicol,https://github.com/phillipnicol/SITH,TRUE,https://github.com/phillipnicol/sith,4756,5,2021-03-27T13:57:37Z,951.2
sivs,"An iterative feature selection method (manuscript submitted) that
    internally utilizes various Machine Learning methods that have embedded
    feature reduction in order to shrink down the feature space into a small
    and yet robust set.",2021-07-20,Mehrad Mahmoudian,"https://github.com/mmahmoudian/sivs,
https://doi.org/10.1093/bioinformatics/btab501",TRUE,https://github.com/mmahmoudian/sivs,4210,3,2021-08-04T09:33:25Z,1403.3333333333333
sjlabelled,"Collection of functions dealing with labelled data, like reading and 
    writing data between R and other statistical software packages like 'SPSS',
    'SAS' or 'Stata', and working with labelled data. This includes easy ways 
    to get, set or change value and variable label attributes, to convert 
    labelled vectors into factors or numeric (and vice versa), or to deal with 
    multiple declared missing values.",2021-05-11,Daniel Lüdecke,https://strengejacke.github.io/sjlabelled/,TRUE,https://github.com/strengejacke/sjlabelled,1023880,63,2021-07-29T10:09:05Z,16252.063492063493
sjmisc,"Collection of miscellaneous utility functions, supporting data 
    transformation tasks like recoding, dichotomizing or grouping variables, 
    setting and replacing missing values. The data transformation functions 
    also support labelled data, and all integrate seamlessly into a 
    'tidyverse'-workflow.",2021-05-12,Daniel Lüdecke,https://strengejacke.github.io/sjmisc/,TRUE,https://github.com/strengejacke/sjmisc,942864,146,2021-05-12T08:11:48Z,6457.972602739726
sjPlot,"Collection of plotting and table output functions for data
    visualization. Results of various statistical analyses (that are commonly used
    in social sciences) can be visualized using this package, including simple and
    cross tabulated frequencies, histograms, box plots, (generalized) linear models,
    mixed effects models, principal component analysis and correlation matrices, 
    cluster analyses, scatter plots, stacked scales, effects plots of regression 
    models (including interaction terms) and much more. This package supports
    labelled data.",2021-07-10,Daniel Lüdecke,https://strengejacke.github.io/sjPlot/,TRUE,https://github.com/strengejacke/sjplot,641852,470,2021-08-09T08:26:42Z,1365.6425531914895
sjstats,"Collection of convenient functions for common statistical computations,
             which are not directly provided by R's base or stats packages.
             This package aims at providing, first, shortcuts for statistical measures, 
             which otherwise could only be calculated with additional effort 
             (like Cramer's V, Phi, or effect size statistics like Eta or Omega squared), 
             or for which currently no functions available. Second, another focus 
             lies on weighted variants of common statistical measures and tests 
             like weighted standard error, mean, t-test, correlation, and more.",2021-01-09,Daniel Lüdecke,https://strengejacke.github.io/sjstats/,TRUE,https://github.com/strengejacke/sjstats,676185,168,2021-07-22T10:26:07Z,4024.910714285714
skedastic,"Implements numerous methods for detecting heteroskedasticity 
    (sometimes called heteroscedasticity) in the classical linear regression 
    model. These include a test based on Anscombe (1961) 
    <https://projecteuclid.org/euclid.bsmsp/1200512155>, Ramsey's (1969) 
    BAMSET Test <doi:10.1111/j.2517-6161.1969.tb00796.x>, the tests of Bickel 
    (1978) <doi:10.1214/aos/1176344124>, Breusch and Pagan (1979)  
    <doi:10.2307/1911963> with and without the modification 
    proposed by Koenker (1981) <doi:10.1016/0304-4076(81)90062-2>, Carapeto and 
    Holt (2003) <doi:10.1080/0266476022000018475>, Cook and Weisberg (1983) 
    <doi:10.1093/biomet/70.1.1> (including their graphical methods), Diblasi 
    and Bowman (1997) <doi:10.1016/S0167-7152(96)00115-0>, Dufour, Khalaf, 
    Bernard, and Genest (2004) <doi:10.1016/j.jeconom.2003.10.024>, Evans and 
    King (1985) <doi:10.1016/0304-4076(85)90085-5> and Evans and King (1988) 
    <doi:10.1016/0304-4076(88)90006-1>, Glejser (1969) 
    <doi:10.1080/01621459.1969.10500976> as formulated by 
    Mittelhammer, Judge and Miller (2000, ISBN: 0-521-62394-4), Godfrey and 
    Orme (1999) <doi:10.1080/07474939908800438>, Goldfeld and Quandt 
    (1965) <doi:10.1080/01621459.1965.10480811>, Harrison and McCabe (1979) 
    <doi:10.1080/01621459.1979.10482544>, Harvey (1976) <doi:10.2307/1913974>, 
    Honda (1989) <doi:10.1111/j.2517-6161.1989.tb01749.x>, Horn (1981) 
    <doi:10.1080/03610928108828074>, Li and Yao (2019) 
    <doi:10.1016/j.ecosta.2018.01.001> with and without the modification of 
    Bai, Pan, and Yin (2016) <doi:10.1007/s11749-017-0575-x>, Rackauskas and 
    Zuokas (2007) <doi:10.1007/s10986-007-0018-6>, Simonoff and Tsai (1994) 
    <doi:10.2307/2986026> with and without the modification of Ferrari, 
    Cysneiros, and Cribari-Neto (2004) <doi:10.1016/S0378-3758(03)00210-6>, 
    Szroeter (1978) <doi:10.2307/1913831>, Verbyla (1993) 
    <doi:10.1111/j.2517-6161.1993.tb01918.x>, White (1980) 
    <doi:10.2307/1912934>, Wilcox and Keselman (2006) 
    <doi:10.1080/10629360500107923>, Yuce (2008) 
    <https://dergipark.org.tr/en/pub/iuekois/issue/8989/112070>, and Zhou, 
    Song, and Thompson (2015) <doi:10.1002/cjs.11252>. Besides these 
    heteroskedasticity tests, there are supporting functions that compute the 
    BLUS residuals of Theil (1965) <doi:10.1080/01621459.1965.10480851>, the 
    conditional two-sided p-values of Kulinskaya (2008) <arXiv:0810.2124v1>, 
    and probabilities for the nonparametric trend statistic of Lehmann (1975, 
    ISBN: 0-816-24996-1). Homoskedasticity refers to the assumption of 
    constant variance that is imposed on the model errors (disturbances); 
    heteroskedasticity is the violation of this assumption.",2021-06-14,Thomas Farrar,https://github.com/tjfarrar/skedastic,TRUE,https://github.com/tjfarrar/skedastic,19061,5,2021-06-13T18:54:24Z,3812.2
sketch,"Creates static / animated / interactive visualisations embeddable
    in R Markdown documents. It implements an R-to-JavaScript transpiler and
    enables users to write JavaScript applications using the syntax of R.",2020-10-08,Chun Fung Kwok,NA,TRUE,https://github.com/kcf-jackson/sketch,3647,65,2021-07-01T21:48:48Z,56.10769230769231
sketcher,"An implementation of image processing effects that convert a photo into a line drawing image. 
    For details, please refer to Tsuda, H. (2020). sketcher: An R package for converting a photo into a sketch style image. 
    <doi:10.31234/osf.io/svmw5>.",2020-05-25,Hiroyuki Tsuda,https://htsuda.net/sketcher/,TRUE,https://github.com/tsuda16k/sketcher,5053,32,2021-04-13T17:59:41Z,157.90625
skewlmm,"It fits scale mixture of skew-normal linear mixed models using an expectation–maximization (EM) type algorithm, including some possibilities for modeling the within-subject dependence. Details can be found in Schumacher, Lachos and Matos (2021) <doi:10.1002/sim.8870>.",2021-02-03,Fernanda L. Schumacher,https://github.com/fernandalschumacher/skewlmm,TRUE,https://github.com/fernandalschumacher/skewlmm,8949,1,2021-06-02T03:42:27Z,8949
skimr,"A simple to use summary function that can be used with pipes
    and displays nicely in the console. The default summary statistics may
    be modified by the user as can the default formatting.  Support for
    data frames and vectors is included, and users can implement their own
    skim methods for specific object types as described in a vignette.
    Default summaries include support for inline spark graphs.
    Instructions for managing these on specific operating systems are
    given in the ""Using skimr"" vignette and the README.",2021-03-07,Elin Waring,"https://docs.ropensci.org/skimr/ (website),
https://github.com/ropensci/skimr/",TRUE,https://github.com/ropensci/skimr,708992,936,2021-04-27T17:04:53Z,757.4700854700855
sknifedatar,"Extension of the 'modeltime' ecosystem. In
    addition. Allows fitting of multiple models over multiple time series.
    It also provides a bridge for using the 'workflowsets' package with
    'modeltime'. It includes some functionalities for spatial data and
    visualization.",2021-06-01,Rafael Zambrano,https://github.com/rafzamb/sknifedatar,TRUE,https://github.com/rafzamb/sknifedatar,1370,18,2021-07-18T16:35:20Z,76.11111111111111
skpr,"Generates and evaluates D, I, A, Alias, E, T, and G optimal designs. Supports generation and evaluation of blocked and split/split-split/.../N-split plot designs. Includes parametric and Monte Carlo power evaluation functions, and supports calculating power for censored responses. Provides a framework to evaluate power using functions provided in other packages or written by the user. Includes a Shiny graphical user interface that displays the underlying code used to create and evaluate the design to improve ease-of-use and make analyses more reproducible. For details, see Morgan-Wall et al. (2021) <doi:10.18637/jss.v099.i01>.",2021-08-17,Tyler Morgan-Wall,https://github.com/tylermorganwall/skpr,TRUE,https://github.com/tylermorganwall/skpr,31673,79,2021-08-17T03:10:12Z,400.9240506329114
skynet,"A flexible tool that allows generating bespoke
    air transport statistics for urban studies based on publicly available
    data from the Bureau of Transport Statistics (BTS) in the United States
    <https://www.transtats.bts.gov/databases.asp?Mode_ID=1&Mode_Desc=Aviation&Subject_ID2=0>.",2020-10-07,Filipe Teixeira,https://github.com/ropensci/skynet,TRUE,https://github.com/ropensci/skynet,16074,8,2020-11-03T08:26:53Z,2009.25
slackr,"'Slack' <https://slack.com/> provides a service for teams to
    collaborate by sharing messages, images, links, files and more.
    Functions are provided that make it possible to interact with the
    'Slack' platform 'API'. When you need to share information or data
    from R, rather than resort to copy/ paste in e-mails or other services
    like 'Skype' <https://www.skype.com/en/>, you can use this package to
    send well-formatted output from multiple R objects and expressions to
    all teammates at the same time with little effort. You can also send
    images from the current graphics device, R objects, and upload files.",2021-08-20,Bob Rudis,"https://github.com/mrkaye97/slackr,
https://mrkaye97.github.io/slackr/",TRUE,https://github.com/mrkaye97/slackr,128787,279,2021-09-02T01:04:12Z,461.60215053763443
sleepr,"Use behavioural variables to score activity and infer sleep from bouts of immobility. 
    It is primarily designed to score sleep in fruit flies from Drosophila Activity Monitor (TriKinetics) and Ethoscope data.
    It implements sleep scoring using the ""five-minute rule"" (Hendricks et al. (2000) <DOI:10.1016/S0896-6273(00)80877-6>),
    activity classification for Ethoscopes (Geissmann et al. (2017) <DOI:10.1371/journal.pbio.2003026>) 
    and a new algorithm to detect when animals are dead.",2018-10-30,Quentin Geissmann,https://github.com/rethomics/sleepr,TRUE,https://github.com/rethomics/sleepr,12354,5,2021-06-11T08:43:47Z,2470.8
sleepwalk,"A tool to interactively explore the
  embeddings created by dimension reduction methods such as 
  Principal Components Analysis (PCA), Multidimensional Scaling (MDS), 
  T-distributed Stochastic Neighbour Embedding (t-SNE),
  Uniform Manifold Approximation and Projection (UMAP) or any other.",2020-09-29,Svetlana Ovchinnikova,https://anders-biostat.github.io/sleepwalk/,TRUE,https://github.com/anders-biostat/sleepwalk,13140,83,2021-06-25T12:44:38Z,158.3132530120482
SLEMI,"The implementation of the algorithm for estimation of mutual information and channel capacity from experimental data by classification procedures (logistic regression). Technically, it allows to estimate information-theoretic measures between finite-state input and multivariate, continuous output. Method described in Jetka et al. (2019) <doi:10.1371/journal.pcbi.1007132>.",2021-02-22,Tomasz Jetka,https://github.com/TJetka/SLEMI,TRUE,https://github.com/tjetka/slemi,6767,3,2021-02-22T10:12:52Z,2255.6666666666665
slga,Provides access to soil and landscape grid of Australia raster datasets via existing open geospatial consortium web coverage services. See <https://www.clw.csiro.au/aclep/soilandlandscapegrid/index.html>.  ,2021-06-12,Lauren OBrien,https://github.com/obrl-soil/slga,TRUE,https://github.com/obrl-soil/slga,10521,17,2021-06-16T17:32:06Z,618.8823529411765
slickR,"Create and customize interactive carousels
      using the 'Slick' 'JavaScript' library and the
      'htmlwidgets' package. The carousels can contain
      plots produced in R, images, 'iframes', videos and
      other 'htmlwidgets'.  These carousels can be created
      directly from the R console, and viewed in the 'RStudio' 
      internal viewer, in 'Shiny' apps and R Markdown documents.",2020-12-18,Jonathan Sidi,https://github.com/yonicd/slickR,TRUE,https://github.com/yonicd/slickr,30739,129,2021-06-04T20:45:26Z,238.28682170542635
slider,"Provides type-stable rolling window functions over any R data
    type. Cumulative and expanding windows are also supported. For more
    advanced usage, an index can be used as a secondary vector that
    defines how sliding windows are to be created.",2021-07-01,Davis Vaughan,https://github.com/DavisVaughan/slider,TRUE,https://github.com/davisvaughan/slider,1001676,196,2021-07-01T19:49:44Z,5110.591836734694
SLOPE,"Efficient implementations for Sorted L-One Penalized Estimation 
    (SLOPE): generalized linear models regularized with the sorted L1-norm 
    (Bogdan et al. (2015) <doi:10/gfgwzt>). Supported models include ordinary 
    least-squares regression, binomial regression, multinomial regression, and 
    Poisson regression. Both dense and sparse  predictor matrices are supported.
    In addition, the package features predictor screening rules that enable fast
    and efficient solutions to high-dimensional problems.",2021-03-17,Johan Larsson,"https://jolars.github.io/SLOPE/, https://github.com/jolars/SLOPE",TRUE,https://github.com/jolars/slope,31833,13,2021-06-01T19:24:10Z,2448.6923076923076
slouch,"An implementation of a phylogenetic comparative method. It can fit univariate among-species Ornstein-Uhlenbeck models of phenotypic trait evolution, where the trait evolves towards a primary optimum. The optimum can be modelled as a single parameter, as multiple discrete regimes on the phylogenetic tree, and/or with continuous covariates. See also Hansen (1997) <doi:10.2307/2411186>, Butler & King (2004) <doi:10.1086/426002>, Hansen et al. (2008) <doi:10.1111/j.1558-5646.2008.00412.x>.",2020-02-21,Bjørn Tore Kopperud,http://github.com/kopperud/slouch,TRUE,https://github.com/kopperud/slouch,12893,1,2021-08-17T13:07:06Z,12893
slowraker,"A mostly pure-R implementation of the RAKE algorithm (Rose, S., Engel, D., Cramer, N. and Cowley, W. (2010) <doi:10.1002/9780470689646.ch1>), which can be used to extract keywords from documents without any training data.",2017-11-02,Christopher Baker,https://crew102.github.io/slowraker/index.html,TRUE,https://github.com/crew102/slowraker,15145,5,2021-05-19T00:51:42Z,3029
SLTCA,"Conduct latent trajectory class analysis with longitudinal data. Our method supports longitudinal continuous, binary and count data. For more methodological details, please refer to Hart, K.R., Fei, T. and Hanfelt, J.J. (2020), Scalable and robust latent trajectory class analysis using artificial likelihood. Biometrics <doi:10.1111/biom.13366>.",2020-09-23,Teng Fei,NA,TRUE,https://github.com/tengfei-emory/sltca,3722,0,2020-10-01T22:12:38Z,NA
slurmR,"'Slurm', Simple Linux Utility for Resource Management
          <https://slurm.schedmd.com/>, is a popular 'Linux' based software used to 
          schedule jobs in 'HPC' (High Performance Computing) clusters. This R package
          provides a specialized lightweight wrapper of 'Slurm' with a syntax similar to
          that found in the 'parallel' R package. The package also includes a method for
          creating socket cluster objects spanning multiple nodes that can be used with
          the 'parallel' package.",2021-09-03,George Vega Yon,"https://github.com/USCbiostats/slurmR, https://slurm.schedmd.com/",TRUE,https://github.com/uscbiostats/slurmr,8827,32,2021-09-03T00:15:04Z,275.84375
SmallCountRounding,"A statistical disclosure control tool to protect frequency tables in cases where small values are sensitive. The function PLSrounding() performs small count rounding of necessary inner cells so that all small frequencies of cross-classifications to be published (publishable cells) are rounded. This is equivalent to changing micro data since frequencies of unique combinations are changed. Thus, additivity and consistency are guaranteed. The methodology is described in Langsrud and Heldal (2018) <https://www.researchgate.net/publication/327768398_An_Algorithm_for_Small_Count_Rounding_of_Tabular_Data>.",2021-04-26,Øyvind Langsrud,https://github.com/statisticsnorway/SmallCountRounding,TRUE,https://github.com/statisticsnorway/smallcountrounding,14748,2,2021-07-02T06:41:42Z,7374
smam,"Animal movement models including moving-resting process
    with embedded Brownian motion according to
    Yan et al. (2014) <doi:10.1007/s10144-013-0428-8>,
    Pozdnyakov et al. (2017) <doi:10.1007/s11009-017-9547-6>,
    Brownian motion with measurement error according to
    Pozdnyakov et al. (2014) <doi:10.1890/13-0532.1>,
    and moving-resting-handling process with embedded Brownian motion,
    Pozdnyakov et al. (2018) <arXiv:1806.00849>.",2021-07-11,Chaoran Hu,https://github.com/ChaoranHu/smam,TRUE,https://github.com/chaoranhu/smam,19865,2,2021-08-10T00:51:37Z,9932.5
smapr,"
    Facilitates programmatic access to NASA Soil Moisture Active
    Passive (SMAP) data with R. It includes functions to search for, acquire,
    and extract SMAP data.",2019-04-22,Maxwell Joseph,https://github.com/ropensci/smapr,TRUE,https://github.com/ropensci/smapr,17914,61,2021-02-15T15:06:14Z,293.672131147541
SMARTAR,"Primary data analysis for sequential multiple assignment randomization trial (SMART) and calibration tools for clinical trial planning purposes. \n 
             The methods used for this package include:  \n
             (1) Likelihood-based global test (hypothesis test, power calculation) by in Zhong X., Cheng, B., Qian M., Cheung Y.K. (2019) <doi: 10.1016/j.cct.2019.105830>. \n
             (2) IPWE-based global test (hypotehsis test, power calculation) by Ogbagaber S.B., Karp J., Wahed A.S. (2016) <doi:10.1002/sim.6747>. \n
             (3) G estimates (pairwise comparison, power calculation) by  Lavori R., Dawson P.W. (2012) <doi: 10.1093/biostatistics/kxr016>. \n
             (4) IPW estimates (pairwise comparison, power calculation) by Murphy S.A. (2005) <doi: 10.1002/sim.2022>. \n
             (5) SAMRT with adaptive randomization by Cheung Y.K. (2015) <doi: 10.1111/biom.12258>.",2020-07-30,Tony Zhong,https://github.com/tonizhong/SMARTAR/,TRUE,https://github.com/tonizhong/smartar,8824,0,2020-10-27T13:01:12Z,NA
SmartEDA,"Exploratory analysis on any input data describing the structure and the relationships present in the data. The package automatically select the variable and does related descriptive statistics. Analyzing information value, weight of evidence, custom tables, summary statistics, graphical techniques will be performed for both numeric and categorical predictors.",2021-06-05,Dayanand Ubrangala,https://daya6489.github.io/SmartEDA/,TRUE,https://github.com/daya6489/smarteda,30983,21,2021-06-22T07:35:20Z,1475.3809523809523
smcfcs,"Implements multiple imputation of missing covariates by
    Substantive Model Compatible Fully Conditional Specification.
    This is a modification of the popular FCS/chained equations
    multiple imputation approach, and allows imputation of missing
    covariate values from models which are compatible with the user
    specified substantive model.",2021-06-17,Jonathan Bartlett,https://github.com/jwb133/smcfcs,TRUE,https://github.com/jwb133/smcfcs,51403,7,2021-06-17T16:41:33Z,7343.285714285715
smerc,"Implements statistical methods for analyzing the counts of areal data,
    with a focus on the detection of spatial clusters and clustering.  The package has a heavy emphasis on spatial scan methods, which were first introduced by Kulldorff and Nagarwalla (1995) <doi:10.1002/sim.4780140809> and Kulldorff (1997) <doi:10.1080/03610929708831995>.",2021-02-13,Joshua French,NA,TRUE,https://github.com/jfrench/smerc,24478,3,2021-01-14T18:53:53Z,8159.333333333333
smnet,"Fits flexible additive models to data on stream networks, taking account of flow-connectivity of the network.  Models are fitted using penalised least squares.",2020-11-09,Alastair Rushworth,https://github.com/alastairrushworth/smnet/,TRUE,https://github.com/alastairrushworth/smnet,16372,1,2020-11-09T07:42:23Z,16372
smoof,"Provides generators for a high number of both single- and multi-
    objective test functions which are frequently used for the benchmarking of
    (numerical) optimization algorithms. Moreover, it offers a set of convenient
    functions to generate, plot and work with objective functions.",2020-02-18,Jakob Bossek,https://github.com/jakobbossek/smoof,TRUE,https://github.com/jakobbossek/smoof,141695,28,2021-07-19T16:04:14Z,5060.535714285715
smooth,"Functions implementing Single Source of Error state space models for purposes of time series analysis and forecasting.
             The package includes ADAM (Svetunkov, 2021, <https://openforecast.org/adam/>),
             Exponential Smoothing (Hyndman et al., 2008, <doi: 10.1007/978-3-540-71918-2>),
             SARIMA (Svetunkov & Boylan, 2019 <doi: 10.1080/00207543.2019.1600764>),
             Complex Exponential Smoothing (Svetunkov & Kourentzes, 2018, <doi: 10.13140/RG.2.2.24986.29123>),
             Simple Moving Average (Svetunkov & Petropoulos, 2018 <doi: 10.1080/00207543.2017.1380326>)
             and several simulation functions. It also allows dealing with intermittent demand based on the
             iETS framework (Svetunkov & Boylan, 2019, <doi: 10.13140/RG.2.2.35897.06242>).",2021-06-14,"Ivan Svetunkov  (Lecturer at Centre for Marketing Analytics
    and Forecasting",https://github.com/config-i1/smooth,TRUE,https://github.com/config-i1/smooth,401100,63,2021-09-03T12:01:10Z,6366.666666666667
smoothr,"Tools for smoothing and tidying spatial features
    (i.e. lines and polygons) to make them more aesthetically pleasing.
    Smooth curves, fill holes, and remove small fragments from lines and
    polygons.",2021-06-22,Matthew Strimas-Mackey,"https://strimas.com/smoothr/, https://github.com/mstrimas/smoothr",TRUE,https://github.com/mstrimas/smoothr,29997,80,2021-06-22T12:46:05Z,374.9625
smovie,"Provides movies to help students to understand statistical 
  concepts.  The 'rpanel' package  <https://cran.r-project.org/package=rpanel> 
  is used to create interactive plots that move to illustrate key statistical 
  ideas and methods.  There are movies to: visualise probability distributions
  (including user-supplied ones); illustrate sampling distributions of the
  sample mean (central limit theorem), the median, the sample maximum 
  (extremal types theorem) and (the Fisher transformation of the) 
  product moment correlation coefficient; examine the influence of an 
  individual observation in simple linear regression; illustrate key concepts 
  in statistical hypothesis testing. Also provided are dpqr functions for the 
  distribution of the Fisher transformation of the correlation coefficient 
  under sampling from a bivariate normal distribution.",2020-11-05,Paul J. Northrop,"https://paulnorthrop.github.io/smovie/,
https://github.com/paulnorthrop/smovie/",TRUE,https://github.com/paulnorthrop/smovie,12392,0,2021-04-09T21:03:53Z,NA
snahelper,"'RStudio' addin which provides a GUI to visualize and analyse networks. 
    After finishing a session, the code to produce the plot is inserted in the current script.
    Alternatively, the function SNAhelperGadget() can be used directly from the console.
    Additional addins include the Netreader() for reading network files, Netbuilder() to create
    small networks via point and click, and the Componentlayouter() to layout networks with many components manually.",2020-09-13,David Schoch,https://github.com/schochastics/snahelper,TRUE,https://github.com/schochastics/snahelper,12957,60,2021-09-03T15:40:48Z,215.95
SnowballC,"An R interface to the C 'libstemmer' library that implements
  Porter's word stemming algorithm for collapsing words to a common
  root to aid comparison of vocabulary. Currently supported languages are
  Danish, Dutch, English, Finnish, French, German, Hungarian, Italian,
  Norwegian, Portuguese, Romanian, Russian, Spanish, Swedish
  and Turkish.",2020-04-01,Milan Bouchet-Valat,https://github.com/nalimilan/R.TeMiS,TRUE,https://github.com/nalimilan/r.temis,1869150,20,2021-05-12T18:07:40Z,93457.5
snp.plotter,"Creates plots of p-values using single SNP and/or haplotype data.
    Main features of the package include options to display a linkage
    disequilibrium (LD) plot and the ability to plot multiple datasets
    simultaneously. Plots can be created using global and/or individual
    haplotype p-values along with single SNP p-values. Images are created as
    either PDF/EPS files.",2014-02-26,Augustin Luna,"https://github.com/cannin/snp_plotter or
http://cbdb.nimh.nih.gov/~kristin/snp.plotter.html",TRUE,https://github.com/cannin/snp_plotter,18862,15,2021-08-13T20:03:40Z,1257.4666666666667
snpEnrichment,Implements classes and methods for large scale SNP enrichment analysis (e.g. SNPs associated with genes expression in a GWAS signal).,2015-10-01,Mickael Canouil,https://github.com/mcanouil/snpEnrichment,TRUE,https://github.com/mcanouil/snpenrichment,16818,2,2020-12-02T18:10:59Z,8409
snpsettest,"The goal of 'snpsettest' is to provide simple tools that perform
             set-based association tests (e.g., gene-based association tests)
             using GWAS (genome-wide association study) summary statistics. A
             set-based association test in this package is based on the
             statistical model described in VEGAS (versatile gene-based
             association study), which combines the effects of a set of SNPs
             accounting for linkage disequilibrium between markers. This package
             uses a different approach from the original VEGAS implementation to
             compute set-level p values more efficiently, as described in
             <https://github.com/HimesGroup/snpsettest/wiki/Statistical-test-in-snpsettest>.",2021-03-16,Jaehyun Joo,https://github.com/HimesGroup/snpsettest,TRUE,https://github.com/himesgroup/snpsettest,1719,2,2021-05-16T21:09:59Z,859.5
soc.ca,"Specific and class specific multiple correspondence analysis on
    survey-like data. Soc.ca is optimized to the needs of the social scientist and
    presents easily interpretable results in near publication ready quality.",2021-09-02,"Anton Grau Larsen and Jacob Lunding with contributions from Christoph Ellersgaard and
    Stefan Andrade",https://github.com/Rsoc/soc.ca,TRUE,https://github.com/rsoc/soc.ca,19583,10,2021-06-30T11:00:52Z,1958.3
sociome,"Accesses raw data via API and calculates social
    determinants of health measures for user-specified locations in the
    US, returning them in tidyverse- and sf-compatible data frames.",2021-02-24,Nik Krieger,NA,TRUE,https://github.com/nikkrieger/sociome,12496,9,2021-02-24T19:49:10Z,1388.4444444444443
soilDB,A collection of functions for reading data from USDA-NCSS soil databases.,2021-08-21,Andrew Brown,http://ncss-tech.github.io/AQP/,TRUE,https://github.com/ncss-tech/soildb,38403,38,2021-08-28T00:48:37Z,1010.6052631578947
SoilTaxonomy,"Taxonomic dictionaries, formative element lists, and functions related to the maintenance, development and application of U.S. Soil Taxonomy. 
   Data and functionality are based on official U.S. Department of Agriculture sources including the latest edition of the Keys to Soil Taxonomy. Descriptions and metadata are obtained from the National Soil Information System or Soil Survey Geographic databases. Other sources are referenced in the data documentation. 
   Provides tools for understanding and interacting with concepts in the U.S. Soil Taxonomic System. Most of the current utilities are for working with taxonomic concepts at the ""higher"" taxonomic levels: Order, Suborder, Great Group, and Subgroup.",2021-08-05,Andrew Brown,https://github.com/ncss-tech/SoilTaxonomy,TRUE,https://github.com/ncss-tech/soiltaxonomy,2496,5,2021-09-01T19:14:54Z,499.2
Sojourn,"Provides a simple way for utilizing Sojourn methods for
    accelerometer processing, as detailed in Lyden K, Keadle S,
    Staudenmayer J, & Freedson P (2014) <doi:10.1249/MSS.0b013e3182a42a2d>,
    Ellingson LD, Schwabacher IJ, Kim Y, Welk GJ, & Cook DB (2016)
    <doi:10.1249/MSS.0000000000000915>, and Hibbing PR, Ellingson LD,
    Dixon PM, & Welk GJ (2018) <doi:10.1249/MSS.0000000000001486>.",2021-05-07,Paul R. Hibbing,https://github.com/paulhibbing/Sojourn,TRUE,https://github.com/paulhibbing/sojourn,12279,3,2021-05-07T19:41:44Z,4093
Sojourn.Data,"Stores objects (e.g. neural networks) that are needed for
    using Sojourn accelerometer methods. For more information, see
    Lyden K, Keadle S, Staudenmayer J, & Freedson P (2014)
    <doi:10.1249/MSS.0b013e3182a42a2d>, Ellingson LD, Schwabacher IJ,
    Kim Y, Welk GJ, & Cook DB (2016) <doi:10.1249/MSS.0000000000000915>,
    and Hibbing PR, Ellingson LD, Dixon PM, & Welk GJ (2018)
    <doi:10.1249/MSS.0000000000001486>.",2021-05-03,Paul R. Hibbing,https://github.com/paulhibbing/Sojourn.Data,TRUE,https://github.com/paulhibbing/sojourn.data,9699,0,2021-05-20T04:28:25Z,NA
solaR,Calculation methods of solar radiation and performance of photovoltaic systems from daily and intradaily irradiation data sources.,2020-01-19,Oscar Perpiñán Lamigueiro,http://oscarperpinan.github.io/solar/,TRUE,https://github.com/oscarperpinan/solar,42752,28,2020-11-22T16:25:56Z,1526.857142857143
solartime,"Provide utilities to work with solar time, 
  i.e. where noon is exactly when sun culminates.
  Provides functions for computing sun position and times of sunrise and sunset.",2021-04-22,Thomas Wutzler,https://github.com/bgctw/solartime,TRUE,https://github.com/bgctw/solartime,19573,0,2021-09-02T09:40:14Z,NA
solitude,"Isolation forest is anomaly detection method introduced by the paper Isolation based Anomaly Detection (Liu, Ting and Zhou <doi:10.1145/2133360.2133363>).",2021-07-29,Komala Sheshachala Srikanth,https://github.com/talegari/solitude,TRUE,https://github.com/talegari/solitude,34707,19,2021-07-29T19:12:18Z,1826.6842105263158
solrium,"Provides a set of functions for querying and parsing data
    from 'Solr' (<https://solr.apache.org/>) 'endpoints' (local and
    remote), including search, 'faceting', 'highlighting', 'stats', and
    'more like this'. In addition, some functionality is included for
    creating, deleting, and updating documents in a 'Solr' 'database'.",2021-05-19,Scott Chamberlain,"https://github.com/ropensci/solrium (devel),
https://docs.ropensci.org/solrium/ (user manual)",TRUE,https://github.com/ropensci/solrium,106113,61,2021-06-30T23:30:33Z,1739.5573770491803
solvebio,"R language bindings for SolveBio's API.
    SolveBio is a biomedical knowledge hub that enables life science
    organizations to collect and harmonize the complex, disparate
    ""multi-omic"" data essential for today's R&D and BI needs.
    For more information, visit <https://www.solvebio.com>.",2021-05-07,David Caplan,https://github.com/solvebio/solvebio-r,TRUE,https://github.com/solvebio/solvebio-r,22734,2,2021-05-07T15:02:04Z,11367
SOMbrero,"The stochastic (also called on-line) version of the Self-Organising
             Map (SOM) algorithm is provided. Different versions of the
             algorithm are implemented, for numeric and relational data and for
             contingency tables as described, respectively, in Kohonen (2001)
             <isbn:3-540-67921-9>, Olteanu & Villa-Vialaneix (2005)
             <doi:10.1016/j.neucom.2013.11.047> and Cottrell et al (2004)
             <doi:10.1016/j.neunet.2004.07.010>. The package also contains many
             plotting features (to help the user interpret the results) and a
             graphical user interface based on 'shiny'.",2020-08-11,Nathalie Vialaneix,NA,TRUE,https://github.com/tuxette/sombrero,22306,15,2021-04-15T14:02:14Z,1487.0666666666666
SOMEnv,"Analysis of multivariate environmental high frequency data by Self-Organizing Map and k-means clustering algorithms. By means of the graphical user interface it provides a comfortable way to elaborate by self-organizing map algorithm rather big datasets (txt files up to 100 MB ) obtained by environmental high-frequency monitoring by sensors/instruments. The functions present in the package are based on 'kohonen' and 'openair' packages implemented by functions embedding Vesanto et al. (2001) <http://www.cis.hut.fi/projects/somtoolbox/package/papers/techrep.pdf>  heuristic rules for map initialization parameters, k-means clustering algorithm and map features visualization. Cluster profiles visualization as well as graphs dedicated to the visualization of time-dependent variables Licen et al. (2020) <doi:10.4209/aaqr.2019.08.0414> are provided.",2021-07-26,Sabina Licen,https://github.com/SomEnv/somenv,TRUE,https://github.com/somenv/somenv,2824,0,2021-01-21T17:23:56Z,NA
sortable,"Enables drag-and-drop behaviour in Shiny apps, by exposing the 
    functionality of the 'SortableJS' <https://sortablejs.github.io/Sortable/> 
    JavaScript library as an 'htmlwidget'. 
    You can use this in Shiny apps and widgets, 'learnr' tutorials as well as 
    R Markdown. In addition, provides a custom 'learnr' question type - 
    'question_rank()' - that allows ranking questions with drag-and-drop.",2020-09-17,Andrie de Vries,"https://github.com/rstudio/sortable,
https://rstudio.github.io/sortable/",TRUE,https://github.com/rstudio/sortable,22487,102,2021-04-22T06:07:15Z,220.4607843137255
sotkanet,"Access statistical information on welfare and health in Finland 
    from the Sotkanet open data portal <https://sotkanet.fi/sotkanet/fi/index>.",2021-05-03,Leo Lahti,https://ropengov.github.io/sotkanet/,TRUE,https://github.com/ropengov/sotkanet,16704,5,2021-09-01T12:07:01Z,3340.8
sotu,"The President of the United States is constitutionally obligated to provide
  a report known as the 'State of the Union'. The report summarizes the current challenges
  facing the country and the president's upcoming legislative agenda. While historically
  the State of the Union was often a written document, in recent decades it has always
  taken the form of an oral address to a joint session of the United States Congress.
  This package provides the raw text from every such address with the intention of
  being used for meaningful examples of text analysis in R. The corpus is well suited
  to the task as it is historically important, includes material intended to be read
  and material intended to be spoken, and it falls in the public domain. As the corpus
  spans over two centuries it is also a good test of how well various methods hold up
  to the idiosyncrasies of historical texts. Associated data about each address, such
  as the year, president, party, and format, are also included.",2017-05-22,Taylor B. Arnold,http://github.com/statsmaths/sotu/issues,TRUE,https://github.com/statsmaths/sotu,17891,1,2020-10-20T00:03:36Z,17891
soundgen,"Performs parametric synthesis of sounds with harmonic and noise 
    components such as animal vocalizations or human voice. Also offers tools 
    for pitch tracking, spectral analysis, audio segmentation, self-similarity 
    matrices, modulation spectra, morphing, etc., as well as interactive web 
    apps for manually corrected pitch tracking and formant measurement. 
    Reference: Anikin (2019) <doi:10.3758/s13428-018-1095-7>.",2021-08-07,Andrey Anikin,http://cogsci.se/soundgen.html,TRUE,https://github.com/tatters/soundgen,33932,16,2021-09-03T09:37:33Z,2120.75
SoundShape,"Implement a promising, and yet little explored protocol for bioacoustical analysis, the eigensound method by MacLeod, Krieger and Jones (2013) <doi:10.4404/hystrix-24.1-6299>. Eigensound is a multidisciplinary method focused on the direct comparison between stereotyped sounds from different species. 'SoundShape', in turn, provide the tools required for anyone to go from sound waves to Principal Components Analysis, using tools extracted from traditional bioacoustics (i.e. 'tuneR' and 'seewave' packages), geometric morphometrics (i.e. 'geomorph' package) and multivariate analysis (e.g. 'stats' package). For more information, please see Rocha and Romano (2021) and check 'SoundShape' repository on GitHub for news and updates <https://github.com/p-rocha/SoundShape>.",2021-03-31,Pedro Rocha,https://github.com/p-rocha/SoundShape,TRUE,https://github.com/p-rocha/soundshape,6165,3,2021-03-31T13:34:15Z,2055
SoupX,"Quantify, profile and remove ambient mRNA contamination (the ""soup"") from droplet based single cell RNA-seq experiments.  Implements the method described in Young et al. (2018) <doi:10.1101/303727>.",2021-05-17,Matthew Daniel Young,https://github.com/constantAmateur/SoupX,TRUE,https://github.com/constantamateur/soupx,9251,100,2021-05-14T13:06:39Z,92.51
sourcetools,"Tools for the reading and tokenization of R code. The
    'sourcetools' package provides both an R and C++ interface for the tokenization
    of R code, and helpers for interacting with the tokenized representation of R
    code.",2018-04-25,Kevin Ushey,NA,TRUE,https://github.com/kevinushey/sourcetools,9325716,73,2021-05-29T05:04:13Z,127749.53424657535
sovereign,"A set of tools for state-dependent 
  empirical analysis through both VAR- and local projection-based 
  state-dependent forecasts, impulse response functions, 
  historical decompositions, and forecast error variance decompositions.   ",2021-07-23,Tyler J. Pike,"https://github.com/tylerJPike/sovereign,
https://tylerjpike.github.io/sovereign/",TRUE,https://github.com/tylerjpike/sovereign,2072,1,2021-07-23T12:39:21Z,2072
sp,"Classes and methods for spatial
  data; the classes document where the spatial location information
  resides, for 2D or 3D data. Utility functions are provided, e.g. for
  plotting data as maps, spatial selection, as well as methods for
  retrieving coordinates, for subsetting, print, summary, etc.",2021-01-10,Edzer Pebesma,https://github.com/edzer/sp/ https://edzer.github.io/sp/,TRUE,https://github.com/edzer/sp,10837960,104,2021-08-27T10:44:02Z,104211.15384615384
SP2000,"A programmatic interface to <http://sp2000.org.cn>, re-written based on an accompanying 'Species 2000' API. Access tables describing catalogue of the Chinese known species of animals, plants, fungi, micro-organisms, and more. This package also supports access to catalogue of life global <http://catalogueoflife.org>, China animal scientific database <http://zoology.especies.cn> and catalogue of life Taiwan <https://taibnet.sinica.edu.tw/home_eng.php>. The development of 'SP2000' package were supported by Biodiversity Survey and Assessment Project of the Ministry of Ecology and Environment, China <2019HJ2096001006>,Yunnan University's ""Double First Class"" Project <C176240405> and Yunnan University's Research Innovation Fund for Graduate Students <2019227>.",2020-10-26,Liuyong Ding,https://otoliths.github.io/SP2000/,TRUE,https://github.com/otoliths/sp2000,9603,7,2021-06-11T04:38:18Z,1371.857142857143
spaa,"Miscellaneous functions for analysing species association
        and niche overlap.",2016-06-09,Jinlong Zhang,https://github.com/helixcn/spaa,TRUE,https://github.com/helixcn/spaa,27648,8,2021-08-12T03:42:24Z,3456
spacefillr,"Generates random and quasi-random space-filling sequences. Supports the following sequences: 'Halton', 'Sobol', 'Owen'-scrambled 'Sobol',  'Owen'-scrambled 'Sobol' with errors distributed as blue noise, progressive jittered, progressive multi-jittered ('PMJ'), 'PMJ' with blue noise, 'PMJ02', and 'PMJ02' with blue noise. Includes a 'C++' 'API'. Methods derived from ""Constructing Sobol sequences with better two-dimensional projections"" (2012) <doi:10.1137/070709359> S. Joe and F. Y. Kuo, ""Progressive Multi-Jittered Sample Sequences"" (2018) <https://graphics.pixar.com/library/ProgressiveMultiJitteredSampling/paper.pdf> Christensen, P., Kensler, A. and Kilpatrick, C., and ""A Low-Discrepancy Sampler that Distributes Monte Carlo Errors as a Blue Noise in Screen Space"" (2019) E. Heitz, B. Laurent, O. Victor, C. David and I. Jean-Claude, <doi:10.1145/3306307.3328191>. ",2021-02-23,Tyler Morgan-Wall,https://github.com/tylermorganwall/spacefillr,TRUE,https://github.com/tylermorganwall/spacefillr,9985,4,2021-06-29T03:48:33Z,2496.25
spacetime,"Classes and methods for spatio-temporal data, including space-time regular lattices, sparse lattices, irregular data, and trajectories; utility functions for plotting data as map sequences (lattice or animation) or multiple time series; methods for spatial and temporal selection and subsetting, as well as for spatial/temporal/spatio-temporal matching or aggregation, retrieving coordinates, print, summary, etc.",2021-06-14,Edzer Pebesma,https://github.com/edzer/spacetime,TRUE,https://github.com/edzer/spacetime,573741,52,2021-08-15T12:43:41Z,11033.48076923077
spacey,"One of the remaining pain points in making beautiful maps via 
    packages like 'rayshader' is both obtaining and processing spatial data
    to build from. 'spacey' aims to make it easier to obtain and use this data
    for locations within the United States, providing utilities to download 
    'USGS' and 'ESRI' geospatial data and quickly turn it into maps.",2020-03-14,Michael Mahoney,"https://github.com/mikemahoney218/spacey,
https://mikemahoney218.github.io/spacey/",TRUE,https://github.com/mikemahoney218/spacey,7475,10,2020-11-04T03:50:51Z,747.5
spAddins,"A set of RStudio addins that are designed to be used in
             combination with user-defined RStudio keyboard shortcuts. These
             addins either:
             1) insert text at a cursor position (e.g. insert
             operators %>%, <<-, %$%, etc.),
             2) replace symbols in selected pieces of text (e.g., convert
             backslashes to forward slashes which results in stings like
             ""c:\data\"" converted into ""c:/data/"") or
             3) enclose text with special symbols (e.g., converts ""bold"" into
             ""**bold**"") which is convenient for editing R Markdown files.",2017-12-14,Vilmantas Gegzna,https://github.com/GegznaV/spAddins,TRUE,https://github.com/gegznav/spaddins,15094,9,2020-11-20T16:18:21Z,1677.111111111111
SpaDES,"Metapackage for implementing a variety of event-based models, with
    a focus on spatially explicit models. These include raster-based,
    event-based, and agent-based models. The core simulation components
    (provided by 'SpaDES.core') are built upon a discrete event simulation (DES;
    see Matloff (2011) ch 7.8.3 <https://nostarch.com/artofr.htm>)
    framework that facilitates modularity, and easily enables the user to
    include additional functionality by running user-built simulation modules
    (see also 'SpaDES.tools'). Included are numerous tools to visualize rasters
    and other maps (via 'quickPlot'), and caching methods for reproducible
    simulations (via 'reproducible'). Tools for running simulation experiments are
    provided by 'SpaDES.experiment'. Additional functionality is provided by
    the 'SpaDES.addins' and 'SpaDES.shiny' packages.",2021-06-14,Alex M Chubaty,"https://spades.predictiveecology.org,
https://github.com/PredictiveEcology/SpaDES",TRUE,https://github.com/predictiveecology/spades,24900,38,2021-06-11T16:07:29Z,655.2631578947369
SpaDES.addins,"Provides 'RStudio' addins for 'SpaDES' packages and 'SpaDES' module
    development. See '?SpaDES.addins' for an overview of the tools provided.",2021-06-11,Alex M Chubaty,"http://spades-addins.predictiveecology.org/,
https://github.com/PredictiveEcology/SpaDES.addins",TRUE,https://github.com/predictiveecology/spades.addins,18477,1,2021-06-10T20:47:49Z,18477
SpaDES.core,"Provides the core framework for a discrete event system (DES) to 
    implement a complete data-to-decisions, reproducible workflow.
    The core DES components facilitate modularity, and easily enable the user
    to include additional functionality by running user-built modules.
    Includes conditional scheduling, restart after interruption, packaging of
    reusable modules, tools for developing arbitrary automated workflows,
    automated interweaving of modules of different temporal resolution,
    and tools for visualizing and understanding the DES project.",2021-06-10,Alex M Chubaty,"https://spades-core.predictiveecology.org/,
https://github.com/PredictiveEcology/SpaDES.core",TRUE,https://github.com/predictiveecology/spades.core,35168,7,2021-06-09T14:24:16Z,5024
SpaDES.tools,"Provides GIS and map utilities, plus additional modeling tools for
    developing cellular automata, dynamic raster models,  and agent based models
    in 'SpaDES'.
    Included are various methods for spatial spreading, spatial agents, GIS
    operations, random map generation, and others.
    See '?SpaDES.tools' for an categorized overview of these additional tools.",2021-06-07,Alex M Chubaty,"https://spades-tools.predictiveecology.org,
https://github.com/PredictiveEcology/SpaDES.tools",TRUE,https://github.com/predictiveecology/spades.tools,31778,3,2021-06-07T14:06:23Z,10592.666666666666
spaero,"Implements methods for anticipating the emergence and eradication
    of infectious diseases from surveillance time series. Also provides support
    for computational experiments testing the performance of such methods.",2020-09-26,Eamon ODea,NA,TRUE,https://github.com/e3bo/spaero,16670,0,2020-09-27T00:01:36Z,NA
spanish,"Character vector to numerical translation in Euros from Spanish
    spelled monetary quantities. Reverse translation from integer to Spanish.
    Upper limit is up to the millions range. Geocoding via Cadastral web site.",2019-05-10,Jose Manuel Vera Oteo,https://ropenspain.github.io/spanish/,TRUE,https://github.com/verajosemanuel/spanish,14522,1,2021-01-15T08:41:14Z,14522
spant,"Tools for reading, visualising and processing Magnetic Resonance
    Spectroscopy data. The package includes methods for spectral fitting: Wilson
    (2021) <DOI:10.1002/mrm.28385> and spectral alignment: Wilson (2018)
    <DOI:10.1002/mrm.27605>. ",2021-07-23,Martin Wilson,"https://martin3141.github.io/spant/,
https://github.com/martin3141/spant/",TRUE,https://github.com/martin3141/spant,26154,11,2021-08-13T12:41:15Z,2377.6363636363635
spark.sas7bdat,Read in 'SAS' Data ('.sas7bdat' Files) into 'Apache Spark' from R. 'Apache Spark' is an open source cluster computing framework available at <http://spark.apache.org>. This R package uses the 'spark-sas7bdat' 'Spark' package (<https://spark-packages.org/package/saurfang/spark-sas7bdat>) to import and process 'SAS' data in parallel using 'Spark'. Hereby allowing to execute 'dplyr' statements in parallel on top of 'SAS' data.,2021-04-19,Jan Wijffels,https://github.com/bnosac/spark.sas7bdat,TRUE,https://github.com/bnosac/spark.sas7bdat,25800,22,2021-04-19T07:18:34Z,1172.7272727272727
sparklyr,"R interface to Apache Spark, a fast and general
    engine for big data processing, see <http://spark.apache.org>. This
    package supports connecting to local and remote Apache Spark clusters,
    provides a 'dplyr' compatible back-end, and provides an interface to
    Spark's built-in machine learning algorithms.",2021-06-17,Yitao Li,https://spark.rstudio.com/,TRUE,https://github.com/sparklyr/sparklyr,6930610,806,2021-09-02T18:05:16Z,8598.771712158808
sparklyr.flint,"This sparklyr extension makes 'Flint' time series
    library functionalities (<https://github.com/twosigma/flint>) easily
    accessible through R.",2021-03-01,Yitao Li,<https://github.com/r-spark/sparklyr.flint>,TRUE,https://github.com/r-spark/sparklyr.flint,6552,7,2021-03-01T22:01:40Z,936
sparklyr.nested,A 'sparklyr' extension adding the capability to work easily with nested data.,2018-11-14,Matt Pollock,NA,TRUE,https://github.com/mitre/sparklyr.nested,143676,27,2021-07-12T16:24:13Z,5321.333333333333
sparkwarc,"Load WARC (Web ARChive) files into Apache Spark using 'sparklyr'. This
    allows to read files from the Common Crawl project <http://commoncrawl.org/>.",2020-12-15,Yitao Li,NA,TRUE,https://github.com/r-spark/sparkwarc,14554,13,2021-07-08T20:10:51Z,1119.5384615384614
sparr,"Provides functions to estimate kernel-smoothed spatial and spatio-temporal densities and relative risk functions, and perform subsequent inference. Methodological details can be found in the accompanying tutorial: Davies et al. (2018) <DOI:10.1002/sim.7577>.",2021-03-16,Tilman M. Davies,"https://tilmandavies.github.io/sparr/,
https://github.com/tilmandavies/sparr/",TRUE,https://github.com/tilmandavies/sparr,34302,3,2021-03-16T09:33:29Z,11434
sparrpowR,"Calculate the statistical power to detect clusters using kernel-based 
        spatial relative risk functions that are estimated using the 'sparr' package.
        Details about the 'sparr' package methods can be found in the tutorial: Davies
        et al. (2018) <doi:10.1002/sim.7577>.  Details about kernel density estimation 
        can be found in J. F. Bithell (1990) <doi:10.1002/sim.4780090616>.  More 
        information about relative risk functions using kernel density estimation can 
        be found in J. F. Bithell (1991) <doi:10.1002/sim.4780101112>.",2021-08-02,Ian D. Buller,https://github.com/machiela-lab/sparrpowR,TRUE,https://github.com/machiela-lab/sparrpowr,5628,2,2021-08-31T18:36:54Z,2814
SparseBiplots,"'HJ-Biplot' is a multivariate method that allow represent multivariate data on a subspace of low dimension, in such a way that most of the variability of the information is captured in a few dimensions. This package implements three new techniques and constructs in each case the 'HJ-Biplot', adapting restrictions to reduce weights and / or produce zero weights in the dimensions, based on the regularization theories. It implements three methods of regularization: Ridge, LASSO and Elastic Net.",2020-06-28,Mitzi Isabel Cubilla-Montilla,https://github.com/mitzicubillamontilla/SparseBiplots,TRUE,https://github.com/mitzicubillamontilla/sparsebiplots,9654,2,2021-07-05T01:01:50Z,4827
sparsediscrim,"A collection of sparse and regularized discriminant analysis
    methods intended for small-sample, high-dimensional data sets. The package
    features the High-Dimensional Regularized Discriminant Analysis classifier
    from Ramey et al. (2017) <arXiv:1602.01182>. Other classifiers include
    those from Dudoit et al. (2002) <doi:10.1198/016214502753479248>, Pang et
    al. (2009) <doi:10.1111/j.1541-0420.2009.01200.x>, and Tong et al. (2012)
    <doi:10.1093/bioinformatics/btr690>.",2021-07-01,Max Kuhn,"https://github.com/topepo/sparsediscrim,
https://topepo.github.io/sparsediscrim/",TRUE,https://github.com/topepo/sparsediscrim,3659,1,2021-06-28T00:27:34Z,3659
sparseLRMatrix,"Provides an S4 class for representing and
    interacting with sparse plus rank matrices. At the moment the
    implementation is quite spare, but the plan is eventually subclass
    Matrix objects.",2021-03-02,Alex Hayes,"https://rohelab.github.io/sparseLRMatrix/,
https://github.com/RoheLab/sparseLRMatrix",TRUE,https://github.com/rohelab/sparselrmatrix,1951,1,2021-03-02T18:50:58Z,1951
SPARSEMODr,"Implementation of spatially-explicit, stochastic disease models with customizable time windows that describe how parameter values fluctuate during outbreaks (e.g., in response to public health or conservation interventions).",2021-07-01,Joseph Mihaljevic  (C code,https://github.com/NAU-CCL/SPARSEMODr,TRUE,https://github.com/nau-ccl/sparsemodr,2999,0,2021-06-18T21:26:49Z,NA
sparsepp,"Provides interface to 'sparsepp' - fast, memory efficient hash map. 
    It is derived from Google's excellent 'sparsehash' implementation.
    We believe 'sparsepp' provides an unparalleled combination of performance and memory usage, 
    and will outperform your compiler's unordered_map on both counts. 
    Only Google's 'dense_hash_map' is consistently faster, at the cost of much greater 
    memory usage (especially when the final size of the map is not known in advance).",2018-09-22,Dmitriy Selivanov,"https://github.com/greg7mdp/sparsepp,
https://github.com/dselivanov/r-sparsepp",TRUE,https://github.com/greg7mdp/sparsepp,79445,1070,2021-07-21T20:56:47Z,74.24766355140187
sparsestep,"Implements the SparseStep model for solving regression
    problems with a sparsity constraint on the parameters. The SparseStep
    regression model was proposed in Van den Burg, Groenen, and Alfons (2017)
    <arXiv:1701.06967>. In the model, a regularization term is added to the
    regression problem which approximates the counting norm of the parameters.
    By iteratively improving the approximation a sparse solution to the
    regression problem can be obtained.  In this package both the standard
    SparseStep algorithm is implemented as well as a path algorithm which uses
    golden section search to determine solutions with different values for the
    regularization parameter.",2021-01-10,Gertjan van den Burg,"https://github.com/GjjvdBurg/SparseStep,
https://arxiv.org/abs/1701.06967",TRUE,https://github.com/gjjvdburg/sparsestep,13737,1,2021-01-12T16:31:08Z,13737
sparsevar,"A wrapper for sparse VAR/VECM time series models estimation
             using penalties like ENET (Elastic Net), SCAD (Smoothly Clipped 
             Absolute Deviation) and MCP (Minimax Concave Penalty). 
             Based on the work of Sumanta Basu and George Michailidis 
             <doi:10.1214/15-AOS1315>.",2021-04-18,Simone Vazzoler,https://github.com/svazzole/sparsevar,TRUE,https://github.com/svazzole/sparsevar,20352,9,2021-04-16T14:26:27Z,2261.3333333333335
sparta,Fast Multiplication and Marginalization of Sparse Tables.,2021-07-05,Mads Lindskou,https://github.com/mlindsk/sparta,TRUE,https://github.com/mlindsk/sparta,5636,0,2021-07-08T05:48:34Z,NA
spatialEco,"Utilities to support spatial data manipulation, query, sampling
    and modelling. Functions include models for species population density, download
    utilities for climate and global deforestation spatial products, spatial
    smoothing, multivariate separability, point process model for creating pseudo-
    absences and sub-sampling, polygon and point-distance landscape metrics,
    auto-logistic model, sampling models, cluster optimization, statistical
    exploratory tools and raster-based metrics.",2021-05-14,Jeffrey S. Evans,https://github.com/jeffreyevans/spatialEco,TRUE,https://github.com/jeffreyevans/spatialeco,84788,47,2021-06-11T16:33:19Z,1804
SpatialEpi,Methods and data for cluster detection and disease mapping.,2021-08-02,Albert Y. Kim,https://github.com/rudeboybert/SpatialEpi,TRUE,https://github.com/rudeboybert/spatialepi,66363,16,2021-08-06T04:10:09Z,4147.6875
SpatialKDE,"Calculate Kernel Density Estimation (KDE) for spatial data. 
  The algorithm is inspired by the tool 'Heatmap' from 'QGIS'. The method is described by:
  Hart, T., Zandbergen, P. (2014) <doi:10.1108/PIJPSM-04-2013-0039>, 
  Nelson, T. A., Boots, B. (2008) <doi:10.1111/j.0906-7590.2008.05548.x>,
  Chainey, S., Tompson, L., Uhlig, S.(2008) <doi:10.1057/palgrave.sj.8350066>.",2020-11-28,Jan Caha,"https://jancaha.github.io/SpatialKDE/index.html,
https://github.com/JanCaha/SpatialKDE",TRUE,https://github.com/jancaha/spatialkde,9848,3,2021-04-22T09:08:13Z,3282.6666666666665
SpatialPosition,"Computes spatial position models: the potential model as defined
    by Stewart (1941) <doi:10.1126/science.93.2404.89> and catchment areas as
    defined by Reilly (1931) or Huff (1964) <doi:10.2307/1249154>.",2021-06-14,Timothée Giraud,https://github.com/riatelab/SpatialPosition,TRUE,https://github.com/riatelab/spatialposition,30639,27,2021-06-14T12:56:47Z,1134.7777777777778
spatialreg,"A collection of all the estimation functions for spatial cross-sectional models (on lattice/areal data using spatial weights matrices) contained up to now in 'spdep', 'sphet' and 'spse'. These model fitting functions include maximum likelihood methods for cross-sectional models proposed by 'Cliff' and 'Ord' (1973, ISBN:0850860369) and (1981, ISBN:0850860814), fitting methods initially described by 'Ord' (1975) <doi:10.1080/01621459.1975.10480272>. The models are further described by 'Anselin' (1988) <doi:10.1007/978-94-015-7799-1>. Spatial two stage least squares and spatial general method of moment models initially proposed by 'Kelejian' and 'Prucha' (1998) <doi:10.1023/A:1007707430416> and (1999) <doi:10.1111/1468-2354.00027> are provided. Impact methods and MCMC fitting methods proposed by 'LeSage' and 'Pace' (2009) <doi:10.1201/9781420064254> are implemented for the family of cross-sectional spatial regression models. Methods for fitting the log determinant term in maximum likelihood and MCMC fitting are compared by 'Bivand et al.' (2013) <doi:10.1111/gean.12008>, and model fitting methods by 'Bivand' and 'Piras' (2015) <doi:10.18637/jss.v063.i18>; both of these articles include extensive lists of references. 'spatialreg' >= 1.1-* correspond to 'spdep' >= 1.1-1, in which the model fitting functions are deprecated and pass through to 'spatialreg', but will mask those in 'spatialreg'. From versions 1.2-*, the functions will be made defunct in 'spdep'.",2021-05-03,Roger Bivand,"https://github.com/r-spatial/spatialreg/,
https://r-spatial.github.io/spatialreg/",TRUE,https://github.com/r-spatial/spatialreg,206635,27,2021-06-03T10:22:07Z,7653.148148148148
spatialrisk,"Methods for spatial risk calculations. It offers an efficient approach to determine the sum of all observations within a 
     circle of a certain radius. This might be beneficial for insurers who are required (by a recent European Commission regulation) to determine 
     the maximum value of insured fire risk policies of all buildings that are partly or fully located within a circle of a radius of 200m. See 
     Church (1974) <doi:10.1007/BF01942293> for a description of the problem.",2021-05-26,Martin Haringa,"https://github.com/mharinga/spatialrisk,
https://mharinga.github.io/spatialrisk/",TRUE,https://github.com/mharinga/spatialrisk,19234,5,2021-08-10T15:19:02Z,3846.8
spatialsample,"Functions and classes for spatial resampling to use with the 
    'rsample' package, such as spatial cross-validation (Brenning, 2012) 
    <doi:10.1109/IGARSS.2012.6352393>. The scope of 'rsample' and 
    'spatialsample' is to provide the basic building blocks for creating and
    analyzing resamples of a spatial data set, but neither package includes
    functions for modeling or computing statistics. The resampled spatial data
    sets created by 'spatialsample' do not contain much overhead in memory.",2021-03-04,Julia Silge,"https://github.com/tidymodels/spatialsample,
https://spatialsample.tidymodels.org",TRUE,https://github.com/tidymodels/spatialsample,2270,18,2021-03-17T17:47:24Z,126.11111111111111
spatialTIME,"Visualization and analysis  of Vectra Immunoflourescent
    data. Options for calculating both the univariate and bivariate Ripley's K
    are included. Calculations are performed using a permutation-based 
    approach presented by Wilson et al. <doi:10.1101/2021.04.27.21256104>. ",2021-08-21,Fridley Lab,https://github.com/FridleyLab/spatialTIME,TRUE,https://github.com/fridleylab/spatialtime,1202,1,2021-09-02T18:41:56Z,1202
spatialwarnings,Tools to compute and assess significance of early-warnings signals (EWS) of ecosystem degradation on raster data sets. EWS are metrics derived from the observed spatial structure of an ecosystem -- e.g. spatial autocorrelation -- that increase before an ecosystem undergoes a non-linear transition (Genin et al. (2018) <doi:10.1111/2041-210X.13058>).,2021-05-20,Alain Danet,https://github.com/spatial-ews/spatialwarnings,TRUE,https://github.com/spatial-ews/spatialwarnings,15655,7,2021-06-22T12:38:06Z,2236.4285714285716
spatPomp,"Inference on panel data using spatiotemporal partially-observed Markov process (SpatPOMP) models. To do so, it relies on and extends a number of facilities that the 'pomp' package provides for inference on time series data using partially-observed Markov process (POMP) models. Implemented methods include filtering and inference methods in Park and Ionides (2020) <doi:10.1007/s11222-020-09957-3>, Rebeschini and van Handel (2015) <doi:10.1214/14-AAP1061>, Evensen and van Leeuwen (1996) <doi:10.1029/94JC00572> and Ionides et al. (2021) <arXiv:2002.05211v2>. Pre-print statistical software article: Asfaw et al. (2021) <arXiv:2101.01157>.",2021-07-27,Kidus Asfaw,https://github.com/kidusasfaw/spatPomp,TRUE,https://github.com/kidusasfaw/spatpomp,1577,4,2021-07-27T16:15:50Z,394.25
spatsoc,"Detects spatial and temporal groups in GPS relocations 
    (Robitaille et al. (2020) <doi:10.1111/2041-210X.13215>). 
    It can be used to convert GPS relocations to 
    gambit-of-the-group format to build proximity-based social networks 
    In addition, the randomizations function provides data-stream 
    randomization methods suitable for GPS data. ",2021-02-24,Alec L. Robitaille,"https://docs.ropensci.org/spatsoc/,
https://github.com/ropensci/spatsoc,
http://spatsoc.robitalec.ca",TRUE,https://github.com/ropensci/spatsoc,16111,23,2021-08-09T00:02:31Z,700.4782608695652
spatstat,"Comprehensive open-source toolbox for analysing Spatial Point Patterns. Focused mainly on two-dimensional point patterns, including multitype/marked points, in any spatial region. Also supports three-dimensional point patterns, space-time point patterns in any number of dimensions, point patterns on a linear network, and patterns of other geometrical objects. Supports spatial covariate data such as pixel images. 
	Contains over 2000 functions for plotting spatial data, exploratory data analysis, model-fitting, simulation, spatial sampling, model diagnostics, and formal inference. 
	Data types include point patterns, line segment patterns, spatial windows, pixel images, tessellations, and linear networks. 
	Exploratory methods include quadrat counts, K-functions and their simulation envelopes, nearest neighbour distance and empty space statistics, Fry plots, pair correlation function, kernel smoothed intensity, relative risk estimation with cross-validated bandwidth selection, mark correlation functions, segregation indices, mark dependence diagnostics, and kernel estimates of covariate effects. Formal hypothesis tests of random pattern (chi-squared, Kolmogorov-Smirnov, Monte Carlo, Diggle-Cressie-Loosmore-Ford, Dao-Genton, two-stage Monte Carlo) and tests for covariate effects (Cox-Berman-Waller-Lawson, Kolmogorov-Smirnov, ANOVA) are also supported.
	Parametric models can be fitted to point pattern data using the functions ppm(), kppm(), slrm(), dppm() similar to glm(). Types of models include Poisson, Gibbs and Cox point processes, Neyman-Scott cluster processes, and determinantal point processes. Models may involve dependence on covariates, inter-point interaction, cluster formation and dependence on marks. Models are fitted by maximum likelihood, logistic regression, minimum contrast, and composite likelihood methods. 
	A model can be fitted to a list of point patterns (replicated point pattern data) using the function mppm(). The model can include random effects and fixed effects depending on the experimental design, in addition to all the features listed above.
	Fitted point process models can be simulated, automatically. Formal hypothesis tests of a fitted model are supported (likelihood ratio test, analysis of deviance, Monte Carlo tests) along with basic tools for model selection (stepwise(), AIC()) and variable selection (sdr). Tools for validating the fitted model include simulation envelopes, residuals, residual plots and Q-Q plots, leverage and influence diagnostics, partial residuals, and added variable plots.",2021-06-23,Adrian Baddeley,http://spatstat.org/,TRUE,https://github.com/spatstat/spatstat,905337,134,2021-07-29T02:13:49Z,6756.246268656716
spatstat.core,"Functionality for data analysis and modelling of
	     spatial data, mainly spatial point patterns,
	     in the 'spatstat' family of packages.
	     (Excludes analysis of spatial data on a linear network,
	     which is covered by the separate package 'spatstat.linnet'.)
	     Exploratory methods include quadrat counts, K-functions and their simulation envelopes, nearest neighbour distance and empty space statistics, Fry plots, pair correlation function, kernel smoothed intensity, relative risk estimation with cross-validated bandwidth selection, mark correlation functions, segregation indices, mark dependence diagnostics, and kernel estimates of covariate effects. Formal hypothesis tests of random pattern (chi-squared, Kolmogorov-Smirnov, Monte Carlo, Diggle-Cressie-Loosmore-Ford, Dao-Genton, two-stage Monte Carlo) and tests for covariate effects (Cox-Berman-Waller-Lawson, Kolmogorov-Smirnov, ANOVA) are also supported.
	Parametric models can be fitted to point pattern data using the functions ppm(), kppm(), slrm(), dppm() similar to glm(). Types of models include Poisson, Gibbs and Cox point processes, Neyman-Scott cluster processes, and determinantal point processes. Models may involve dependence on covariates, inter-point interaction, cluster formation and dependence on marks. Models are fitted by maximum likelihood, logistic regression, minimum contrast, and composite likelihood methods. 
	A model can be fitted to a list of point patterns (replicated point pattern data) using the function mppm(). The model can include random effects and fixed effects depending on the experimental design, in addition to all the features listed above.
	Fitted point process models can be simulated, automatically. Formal hypothesis tests of a fitted model are supported (likelihood ratio test, analysis of deviance, Monte Carlo tests) along with basic tools for model selection (stepwise(), AIC()) and variable selection (sdr). Tools for validating the fitted model include simulation envelopes, residuals, residual plots and Q-Q plots, leverage and influence diagnostics, partial residuals, and added variable plots.",2021-07-16,Adrian Baddeley,http://spatstat.org/,TRUE,https://github.com/spatstat/spatstat.core,228143,2,2021-08-31T07:59:08Z,114071.5
spatstat.data,Contains all the datasets for the 'spatstat' family of packages.,2021-03-21,Adrian Baddeley,http://spatstat.org/,TRUE,https://github.com/spatstat/spatstat.data,752644,5,2021-05-16T02:52:45Z,150528.8
spatstat.geom,"Defines types of spatial data such as point patterns,
	     mainly in two dimensions, but also in higher dimensions.
	     Provides class support, and functions for geometrical operations
	     on spatial data, used in the 'spatstat' family of packages.
	     Excludes spatial data on a linear network, which are covered by
	     the separate package 'spatstat.linnet'.",2021-07-12,Adrian Baddeley,http://spatstat.org/,TRUE,https://github.com/spatstat/spatstat.geom,237046,1,2021-09-03T14:58:29Z,237046
spatstat.linnet,"Defines types of spatial data on a linear network
	     and provides functionality for geometrical operations,
	     data analysis and modelling of data on a linear network,
	     in the 'spatstat' family of packages.
	     Contains definitions and support for linear networks, including creation of networks, geometrical measurements, topological connectivity, geometrical operations such as inserting and deleting vertices, intersecting a network with another object, and interactive editing of networks.
	     Data types defined on a network include point patterns, pixel images, functions, and tessellations.
	     Exploratory methods include kernel estimation of intensity on a network, K-functions and pair correlation functions on a network, simulation envelopes, nearest neighbour distance and empty space distance, relative risk estimation with cross-validated bandwidth selection. Formal hypothesis tests of random pattern (chi-squared, Kolmogorov-Smirnov, Monte Carlo, Diggle-Cressie-Loosmore-Ford, Dao-Genton, two-stage Monte Carlo) and tests for covariate effects (Cox-Berman-Waller-Lawson, Kolmogorov-Smirnov, ANOVA) are also supported.
	Parametric models can be fitted to point pattern data using the function lppm() similar to glm(). Only Poisson models are implemented so far. Models may involve dependence on covariates and dependence on marks. Models are fitted by maximum likelihood.
	Fitted point process models can be simulated, automatically. Formal hypothesis tests of a fitted model are supported (likelihood ratio test, analysis of deviance, Monte Carlo tests) along with basic tools for model selection (stepwise(), AIC()) and variable selection (sdr). Tools for validating the fitted model include simulation envelopes, residuals, residual plots and Q-Q plots, leverage and influence diagnostics, partial residuals, and added variable plots.
	Random point patterns on a network can be generated using a variety of models.",2021-07-17,Adrian Baddeley,http://spatstat.org/,TRUE,https://github.com/spatstat/spatstat.linnet,143175,2,2021-08-25T08:39:41Z,71587.5
spatstat.sparse,"Defines sparse three-dimensional arrays
	     and supports standard operations on them.
	     The package also includes utility functions for
	     matrix calculations that are common in
	     statistics, such as quadratic forms.",2021-03-16,Adrian Baddeley,http://spatstat.org/,TRUE,https://github.com/spatstat/spatstat.sparse,172975,0,2021-05-16T02:53:19Z,NA
spatstat.utils,"Contains utility functions for the 'spatstat' family of packages
             which may also be useful for other purposes.",2021-06-14,Adrian Baddeley,http://spatstat.org/,TRUE,https://github.com/spatstat/spatstat.utils,799330,4,2021-08-26T10:27:00Z,199832.5
spbabel,"Tools to convert from specific formats to more general forms of 
    spatial data. Using tables to store the actual entities present in spatial
    data provides flexibility, and the functions here deliberately 
    minimize the level of interpretation applied, leaving that for specific 
    applications. Includes support for simple features,  round-trip for 'Spatial' classes and long-form 
    tables, analogous to 'ggplot2::fortify'. There is also a more 'normal form' representation
    that decomposes simple features and their kin to tables of objects, parts, and unique coordinates. ",2020-09-15,Michael D. Sumner,https://mdsumner.github.io/spbabel/,TRUE,https://github.com/mdsumner/spbabel,42008,19,2020-09-15T08:47:15Z,2210.9473684210525
spData,"Diverse spatial datasets for demonstrating, benchmarking and teaching spatial data analysis. 
    It includes R data of class sf (defined by the package 'sf'), Spatial ('sp'), and nb ('spdep').
    Unlike other spatial data packages such as 'rnaturalearth' and 'maps', 
    it also contains data stored in a range of file formats including GeoJSON, ESRI Shapefile and GeoPackage. 
    Some of the datasets are designed to illustrate specific analysis techniques.
    cycle_hire() and cycle_hire_osm(), for example, is designed to illustrate point pattern analysis techniques.",2021-06-18,Roger Bivand,https://nowosad.github.io/spData/,TRUE,https://github.com/nowosad/spdata,1226158,50,2021-08-25T08:24:18Z,24523.16
spdep,"A collection of functions to create spatial weights matrix
  objects from polygon 'contiguities', from point patterns by distance and
  tessellations, for summarizing these objects, and for permitting their
  use in spatial data analysis, including regional aggregation by minimum
  spanning tree; a collection of tests for spatial 'autocorrelation',
  including global 'Morans I' and 'Gearys C' proposed by 'Cliff' and 'Ord'
  (1973, ISBN: 0850860369) and (1981, ISBN: 0850860814), 'Hubert/Mantel'
  general cross product statistic, Empirical Bayes estimates and
  'Assunção/Reis' (1999) <doi:10.1002/(SICI)1097-0258(19990830)18:16%3C2147::AID-SIM179%3E3.0.CO;2-I> Index, 'Getis/Ord' G ('Getis' and 'Ord' 1992)
  <doi:10.1111/j.1538-4632.1992.tb00261.x> and multicoloured
  join count statistics, 'APLE' ('Li 'et al.' )
  <doi:10.1111/j.1538-4632.2007.00708.x>, local 'Moran's I'
  ('Anselin' 1995) <doi:10.1111/j.1538-4632.1995.tb00338.x> and
  'Getis/Ord' G ('Ord' and 'Getis' 1995)
  <doi:10.1111/j.1538-4632.1995.tb00912.x>,
  'saddlepoint' approximations ('Tiefelsdorf' 2002)
  <doi:10.1111/j.1538-4632.2002.tb01084.x> and exact tests
  for global and local 'Moran's I' ('Bivand et al.' 2009)
  <doi:10.1016/j.csda.2008.07.021> and 'LOSH' local indicators
  of spatial heteroscedasticity ('Ord' and 'Getis')
  <doi:10.1007/s00168-011-0492-y>. The implementation of most of
  the measures is described in 'Bivand' and 'Wong' (2018)
  <doi:10.1007/s11749-018-0599-x>.
  'spdep' >= 1.1-1 corresponds to 'spatialreg' >= 1.1-1, in which the model
  fitting functions are deprecated and pass through to 'spatialreg', but
  will mask those in 'spatialreg'. From versions 1.2-1, the functions will
  be made defunct in 'spdep'.
  For now 'spatialreg' only has functions from 'spdep', where they are shown
  as deprecated. 'spatialreg' only loads the namespace of 'spdep'; if you
  attach 'spdep', the same functions in the other package will be masked.
  Some feed through adequately, others do not.",2021-05-23,Roger Bivand,https://github.com/r-spatial/spdep/,TRUE,https://github.com/r-spatial/spdep,1355405,76,2021-08-26T19:33:07Z,17834.276315789473
spduration,"An implementation of split-population duration regression models. 
    Unlike regular duration models, split-population duration models are
    mixture models that accommodate the presence of a sub-population that is 
    not at risk for failure, e.g. cancer patients who have been cured by 
    treatment. This package implements Weibull and Loglogistic forms for the 
    duration component, and focuses on data with time-varying covariates. 
    These models were originally formulated in Boag (1949) 
    <http://www.jstor.org/stable/2983694> and Berkson and Gage (1952) 
    <http://www.jstor.org/stable/2281318>, and extended in Schmidt and Witte 
    (1989) <doi:10.1016/0304-4076(89)90034-1>.",2018-05-04,Andreas Beger,"https://github.com/andybega/spduration,
https://andybeger.com/spduration",TRUE,https://github.com/andybega/spduration,16599,4,2020-12-09T08:19:21Z,4149.75
speakr,"It allows running 'Praat' scripts from R and it provides some
    wrappers for basic plotting. It also adds support for literate markdown
    tangling. The package is designed to bring reproducible phonetic research
    into R.",2021-07-22,Stefano Coretta  (<https://orcid.org/0000-0001-9627-5532>,https://github.com/stefanocoretta/speakr,TRUE,https://github.com/stefanocoretta/speakr,3004,11,2021-07-21T20:35:46Z,273.09090909090907
specmine,"Provides a set of methods for metabolomics 
	data analysis, including data loading in different formats, 
	pre-processing, metabolite identification, univariate and multivariate 
	data analysis, machine learning, feature selection and pathway analysis. Case studies 
	can be found on the website: <http://bio.di.uminho.pt/metabolomicspackage/index.html>.
	This package suggests 'rcytoscapejs', a package not in mainstream repositories. If you need to install it,
	use: devtools::install_github('cytoscape/r-cytoscape.js@v0.0.7').",2021-05-16,Miguel Rocha,https://github.com/BioSystemsUM/specmine,TRUE,https://github.com/biosystemsum/specmine,8187,1,2021-03-08T14:45:44Z,8187
specmine.datasets,"Provides the data sets used to exemplify 'specmine'. The data sets were formerly distributed with 'specmine', however they exceed current CRAN policy for package size.",2021-02-18,Miguel Rocha,https://github.com/BioSystemsUM/specmine.datasets,TRUE,https://github.com/biosystemsum/specmine.datasets,3416,0,2021-02-17T19:12:00Z,NA
specr,"Provides utilities for conducting specification curve analyses (Simonsohn, Simmons & Nelson (2015, <doi: 10.2139/ssrn.2694998>) or multiverse analyses (Steegen, Tuerlinckx, Gelman & Vanpaemel, 2016, <doi: 10.1177/1745691616658637>) including functions to setup, run, evaluate, and plot all specifications.",2020-03-26,Philipp K. Masur,"https://masurp.github.io/specr/, https://github.com/masurp/specr",TRUE,https://github.com/masurp/specr,6194,41,2021-02-17T17:54:36Z,151.0731707317073
spectacles,"Stores and eases the manipulation of spectra and associated data,
    with dedicated classes for spatial and soil-related data.",2021-01-11,Pierre Roudier,https://github.com/pierreroudier/spectacles/,TRUE,https://github.com/pierreroudier/spectacles,15577,6,2021-01-11T00:35:05Z,2596.1666666666665
spectralGraphTopology,"In the era of big data and hyperconnectivity, learning
    high-dimensional structures such as graphs from data has become a prominent
    task in machine learning and has found applications in many fields such as
    finance, health care, and networks. 'spectralGraphTopology' is an open source,
    documented, and well-tested R package for learning graphs from data. It
    provides implementations of state of the art algorithms such as Combinatorial
    Graph Laplacian Learning (CGL), Spectral Graph Learning (SGL), Graph Estimation
    based on Majorization-Minimization (GLE-MM), and Graph Estimation based on
    Alternating Direction Method of Multipliers (GLE-ADMM). In addition, graph
    learning has been widely employed for clustering, where specific algorithms
    are available in the literature. To this end, we provide an implementation of
    the Constrained Laplacian Rank (CLR) algorithm.",2019-10-12,Ze Vinicius,"https://github.com/dppalomar/spectralGraphTopology,
https://mirca.github.io/spectralGraphTopology,
https://www.danielppalomar.com",TRUE,https://github.com/dppalomar/spectralgraphtopology,11761,47,2021-08-28T08:30:18Z,250.2340425531915
speech,"Converts the floor speeches of Uruguayan legislators, extracted from the 
    parliamentary minutes, to tidy data.frame where each observation is the intervention of a single legislator.",2019-12-22,Nicolas Schmidt,https://github.com/Nicolas-Schmidt/speech,TRUE,https://github.com/nicolas-schmidt/speech,5944,4,2021-09-03T13:38:11Z,1486
sperrorest,"Implements spatial error estimation and
    permutation-based variable importance measures for predictive models
    using spatial cross-validation and spatial block bootstrap.",2021-04-18,Alexander Brenning,"https://giscience-fsu.github.io/sperrorest/,
https://github.com/giscience-fsu/sperrorest",TRUE,https://github.com/giscience-fsu/sperrorest,25696,10,2021-06-29T08:09:06Z,2569.6
spex,"Functions to produce a fully fledged 'geo-spatial' object extent as a
    'SpatialPolygonsDataFrame'. Also included are functions to generate polygons
    from raster data using 'quadmesh' techniques, a round number buffered extent, and
    general spatial-extent and 'raster-like' extent helpers missing from the originating
    packages. Some latitude-based tools for polar maps are included. ",2020-06-20,Michael D. Sumner,https://mdsumner.github.io/spex/,TRUE,https://github.com/mdsumner/spex,23569,20,2020-09-29T13:59:05Z,1178.45
spfda,"Implements a group-bridge penalized function-on-scalar regression
    model proposed by Wang et al. (2020) <arXiv:2006.10163>, to simultaneously
    estimate functional coefficient and recover the local sparsity.",2020-11-25,Zhengjia Wang,"https://github.com/dipterix/spfda,
https://dipterix.github.io/spfda/",TRUE,https://github.com/dipterix/spfda,3400,0,2021-08-24T12:13:19Z,NA
spfilteR,Tools to decompose (transformed) spatial connectivity matrices and perform supervised or unsupervised semiparametric spatial filtering in a regression framework. The package supports unsupervised spatial filtering in standard linear as well as some generalized linear regression models.,2021-05-12,Sebastian Juhl,https://github.com/sjuhl/spfilteR,TRUE,https://github.com/sjuhl/spfilter,2964,1,2021-08-17T06:44:43Z,2964
sphunif,"Implementation of uniformity tests on the circle and
    (hyper)sphere. The main function of the package is unif_test(), which
    conveniently collects more than 30 tests for assessing uniformity on
    S^{p-1}={x in R^p : ||x||=1}, p >= 2. The test statistics are implemented
    in the unif_stat() function, which allows computing several statistics to
    several samples within a single call, thus facilitating Monte Carlo
    experiments. Furthermore, the unif_stat_MC() function allows
    parallelizing them in a simple way. The asymptotic null distributions of
    the statistics are available through the function unif_stat_distr(). The
    core of 'sphunif' is coded in C++ by relying on the 'Rcpp' package.
    The package also provides several novel datasets and gives the
    reproducibility for the data application in García-Portugués,
    Navarro-Esteban and Cuesta-Albertos (2020) <arXiv:2008.09897>.",2021-09-02,Eduardo García-Portugués,https://github.com/egarpor/sphunif,TRUE,https://github.com/egarpor/sphunif,0,2,2021-09-02T07:53:42Z,0
spind,"Functions for spatial methods based on generalized estimating equations (GEE) and
  wavelet-revised methods (WRM), functions for scaling by wavelet multiresolution regression (WMRR),
  conducting multi-model inference, and stepwise model selection. Further, contains functions 
  for spatially corrected model accuracy measures.",2020-10-30,Gudrun Carl,https://github.com/levisc8/spind,TRUE,https://github.com/levisc8/spind,21008,3,2020-10-30T14:11:45Z,7002.666666666667
spinifex,"Data visualization tours animates linear projection 
  of multivariate data as its basis (ie. orientation) changes. The 'spinifex' 
  packages generates paths for manual tours by manipulating the contribution of 
  a single variable at a time ['Cook' & 'Buja' (1997) <doi:10.2307/1390747>]. 
  Other types of tours, such as grand (random walk) and guided (optimizing some
  objective function) are available in the
  'tourr' package ['Wickham' 'et' 'al.' (2011) <doi:10.18637/jss.v040.i02>]. 
  'spinifex' builds on 'tourr' and can render tours with 'gganimate' and 
  'plotly' graphics, and allows for exporting as an .html widget and as an .gif, 
  respectively. This work is fully discussed at ['Spyrison' & 'Cook' (2020) 
  <doi:10.32614/RJ-2020-027>.",2021-04-18,Nicholas Spyrison,https://github.com/nspyrison/spinifex/,TRUE,https://github.com/nspyrison/spinifex,12664,1,2021-08-26T07:18:06Z,12664
spiR,"In 2015, The 17 United Nations' Sustainable Development Goals were adopted. 'spiR' is a wrapper of several open datasets published by the Social Progress Imperative (<https://www.socialprogress.org/>), including the Social Progress Index (a synthetic measure of human development across the world). 'spiR''s goal is to provide data to help policymakers and researchers prioritize actions that accelerate social progress across the world in the context of the Sustainable Development Goals. Please cite: Warin, Th. (2019) ""spiR: An R Package for the Social Progress Index"", <doi:10.6084/m9.figshare.11421573.v2>.",2021-03-05,Thierry Warin,https://github.com/warint/spiR/,TRUE,https://github.com/warint/spir,7783,2,2021-08-20T14:00:20Z,3891.5
spiralize,"It visualizes data along an Archimedean spiral <https://en.wikipedia.org/wiki/Archimedean_spiral>. 
    It has two major advantages for visualization: 1. It is able to visualize data with very long axis with high 
    resolution. 2. It is efficient for time series data to reveal periodic patterns.",2021-07-08,Zuguang Gu,https://github.com/jokergoo/spiralize,TRUE,https://github.com/jokergoo/spiralize,883,29,2021-07-09T15:10:43Z,30.448275862068964
splash,"This program calculates bioclimatic indices and fluxes (radiation, 
    evapotranspiration, soil moisture) for use in studies of ecosystem function, 
    species distribution, and vegetation dynamics under changing climate 
    scenarios. Predictions are based on a minimum of required inputs: latitude, 
    precipitation, air temperature, and cloudiness. 
    Davis et al. (2017) <doi:10.5194/gmd-10-689-2017>.",2021-09-03,Wolfgang Cramer,"https://github.com/villegar/splash/,
https://splash.robertovillegas-diaz.com/,
https://bitbucket.org/labprentice/splash/",TRUE,https://github.com/villegar/splash,0,0,2021-09-02T22:57:34Z,NA
splines2,"Constructs basis matrix of B-splines, M-splines,
    I-splines, convex splines (C-splines), periodic M-splines,
    natural cubic splines, generalized Bernstein polynomials,
    and their integrals (except C-splines) and derivatives
    of given order by close-form recursive formulas.
    It also contains a C++ head-only library integrated with Rcpp.
    See Wang and Yan (2021) <doi:10.6339/21-JDS1020> for details.",2021-08-16,Wenjie Wang,"https://wwenjie.org/splines2,
https://github.com/wenjie2wang/splines2",TRUE,https://github.com/wenjie2wang/splines2,123427,15,2021-08-20T01:07:50Z,8228.466666666667
splithalf,"Estimate the internal consistency of your tasks with a permutation based split-half reliability approach.
    Unofficial release name: ""Kitten Mittens"".",2021-01-08,Sam Parsons,https://github.com/sdparsons/splithalf,TRUE,https://github.com/sdparsons/splithalf,16970,9,2021-04-23T12:38:19Z,1885.5555555555557
splithalfr,"Estimates split-half reliabilities for scoring algorithms of cognitive tasks and questionnaires. The 'splithalfr' supports researcher-provided scoring algorithms, with six vignettes illustrating how on included datasets. The package provides four splitting methods (first-second, odd-even, permutated, Monte Carlo), the option to stratify splits by task design, a number of reliability coefficients, and the option to sub-sample data.",2021-06-25,Thomas Pronk,https://github.com/tpronk/splithalfr,TRUE,https://github.com/tpronk/splithalfr,8658,0,2021-06-25T11:02:25Z,NA
splitTools,"Fast, lightweight toolkit for data splitting. Data
    sets can be partitioned into disjoint groups (e.g. into training,
    validation, and test) or into (repeated) k-folds for subsequent
    cross-validation. Besides basic splits, the package supports
    stratified, grouped as well as blocked splitting. Furthermore,
    cross-validation folds for time series data can be created. See e.g.
    Hastie et al. (2001) <doi:10.1007/978-0-387-84858-7> for the basic
    background on data partitioning and cross-validation.",2020-10-13,Michael Mayer,https://github.com/mayer79/splitTools,TRUE,https://github.com/mayer79/splittools,14800,5,2021-05-06T16:36:03Z,2960
splot,"Automates common plotting tasks to ease data exploration.
  Makes density plots (potentially overlaid on histograms),
  scatter plots with prediction lines, or bar or line plots with error bars.
  For each type, y, or x and y variables can be plotted at levels of other variables,
  all with minimal specification.",2020-06-24,Micah Iserman,https://miserman.github.io/splot,TRUE,https://github.com/miserman/splot,16639,0,2021-03-11T21:45:00Z,NA
splus2R,"Currently there are many functions in S-PLUS that are
  missing in R. To facilitate the conversion of S-PLUS packages to R packages,
  this package provides some missing S-PLUS functionality in R.",2021-01-30,William Constantine,https://github.com/spkaluzny/splus2r,TRUE,https://github.com/spkaluzny/splus2r,238128,1,2021-01-29T18:21:04Z,238128
splusTimeDate,"A collection of classes and methods for working with
  times and dates. The code was originally available in S-PLUS.",2021-04-27,Stephen Kaluzny,https://github.com/spkaluzny/splusTimeDate,TRUE,https://github.com/spkaluzny/splustimedate,30355,0,2021-04-26T23:21:00Z,NA
splusTimeSeries,"A collection of classes and methods for working with time series.
  The code was originally available in S-PLUS.",2021-04-30,Stephen Kaluzny,https://github.com/spkaluzny/splusTimeSeries,TRUE,https://github.com/spkaluzny/splustimeseries,24648,0,2021-04-28T23:19:45Z,NA
spNetwork,"Perform spatial analysis on network.
    Allow to calculate Network Kernel Density Estimate, and to build
    spatial matrices ('listw' objects like in 'spdep' package) to conduct any kind of traditional 
    spatial analysis with spatial weights based on reticular distances. K functions on network
    are also available but still experimental. References: Okabe et al (2019) <doi:10.1080/13658810802475491>;
    Okabe et al (2012, ISBN:978-0470770818);Baddeley el al (2015, ISBN:9781482210200).",2021-01-21,Jeremy Gelb,https://github.com/JeremyGelb/spNetwork,TRUE,https://github.com/jeremygelb/spnetwork,2258,3,2021-08-31T03:32:47Z,752.6666666666666
spocc,"A programmatic interface to many species occurrence data sources,
    including Global Biodiversity Information Facility ('GBIF'), 'USGSs'
    Biodiversity Information Serving Our Nation ('BISON'), 'iNaturalist',
    'eBird', Integrated Digitized
    'Biocollections' ('iDigBio'), 'VertNet', Ocean 'Biogeographic' Information
    System ('OBIS'), and Atlas of Living Australia ('ALA'). Includes
    functionality for retrieving species occurrence data, and combining
    those data.",2021-01-05,Scott Chamberlain,"https://github.com/ropensci/spocc (devel),
https://docs.ropensci.org/spocc/ (user manual)",TRUE,https://github.com/ropensci/spocc,70512,86,2021-07-26T13:02:41Z,819.9069767441861
spoiler,"It can be useful to temporarily hide some text or other HTML elements 
    in 'Shiny' applications. Building on 'Spoiler-Alert.js', it is possible to select the
    elements to hide at startup, to partially reveal them by hovering them, and to 
    completely show them when clicking on them.",2021-06-07,Etienne Bacher,https://github.com/etiennebacher/spoiler,TRUE,https://github.com/etiennebacher/spoiler,998,1,2021-06-26T19:45:39Z,998
spongebob,"Convert text (and text in R objects) to Mocking SpongeBob case 
             <https://knowyourmeme.com/memes/mocking-spongebob> and show them 
             off in fun ways. 
             CoNVErT TexT (AnD TeXt In r ObJeCtS) To MOCkINg SpoNgebOb CAsE
             <https://knowyourmeme.com/memes/mocking-spongebob> aND shOw tHem 
             OFf IN Fun WayS. ",2019-03-02,Jay Qi,https://github.com/jayqi/spongebob,TRUE,https://github.com/jayqi/spongebob,12177,22,2021-04-15T17:47:13Z,553.5
sportyR,"Create scaled 'ggplot' representations of playing surfaces.
    Playing surfaces are drawn pursuant to rule-book specifications.
    This package should be used as a baseline plot for displaying player
    tracking data.",2021-04-20,Ross Drucker,https://github.com/rossdrucker/sportyR,TRUE,https://github.com/rossdrucker/sportyr,1657,49,2021-05-29T01:40:08Z,33.816326530612244
spotifyr,"An R wrapper for pulling data from the 'Spotify' Web API 
  <https://developer.spotify.com/documentation/web-api/> in bulk, or post items on a
  'Spotify' user's playlist.",2021-06-17,Daniel Antal,https://github.com/charlie86/spotifyr,TRUE,https://github.com/charlie86/spotifyr,20501,278,2021-06-23T14:48:41Z,73.74460431654676
spotoroo,An algorithm to cluster satellite hot spot data spatially and temporally.,2021-03-31,Weihao Li,"https://tengmcing.github.io/spotoroo/,
https://github.com/TengMCing/spotoroo/",TRUE,https://github.com/tengmcing/spotoroo,1864,2,2021-09-03T13:27:50Z,932
spray,Sparse arrays interpreted as multivariate polynomials.,2021-02-14,Robin K. S. Hankin,https://github.com/RobinHankin/spray,TRUE,https://github.com/robinhankin/spray,37836,1,2021-08-09T23:28:26Z,37836
spreval,"Processing and analysis of field collected or simulated sprinkler 
    system catch data (depths) to characterize irrigation uniformity and efficiency using
    standard and other measures. Standard measures include the Christiansen coefficient
    of uniformity (CU) as found in Christiansen, J.E.(1942, ISBN:0138779295,
    ""Irrigation by Sprinkling""); and distribution uniformity (DU), potential
    efficiency of the low quarter (PELQ), and application efficiency of the low quarter (AELQ)
    that are implementations of measures of the same notation in Keller, J. and Merriam, 
    J.L. (1978) ""Farm Irrigation System Evaluation: A Guide for Management""
    <https://pdf.usaid.gov/pdf_docs/PNAAG745.pdf>. spreval::DU.lh is similar to spreval::DU
    but is the distribution uniformity of the low half instead of low quarter as in DU.
    spreval::PELQT is a version of spreval::PELQ adapted for traveling systems instead
    of lateral move or solid-set sprinkler systems. The function spreval::eff is
    analogous to the method used to compute application efficiency for furrow irrigation
    presented in Walker, W. and Skogerboe, G.V. (1987,ISBN:0138779295, ""Surface
    Irrigation: Theory and Practice""),that uses piecewise integration of infiltrated
    depth compared against soil-moisture deficit (SMD), when the argument ""target""
    is set equal to SMD.  The other functions contained in the package provide 
    graphical representation of sprinkler system uniformity, and other standard
    univariate parametric and non-parametric statistical measures as applied to
    sprinkler system catch depths. A sample data set of field test data spreval::catchcan
    (catch depths) is provided and is used in examples and vignettes.",2021-07-06,Garry Grabow,https://glgrabow.github.io/spreval/,TRUE,https://github.com/glgrabow/spreval,702,0,2021-07-08T20:38:23Z,NA
sprex,"Calculate species richness functions for rarefaction and
    extrapolation.",2016-04-16,Eric Archer,NA,TRUE,https://github.com/ericarcher/sprex,15278,0,2021-05-22T12:54:27Z,NA
springer,"Recently, regularized variable selection has emerged as a powerful tool to identify and dissect gene-environment interactions. Nevertheless, in longitudinal studies with high dimensional genetic factors, regularization methods for G×E interactions have not been systematically developed. In this package, we provide the implementation of sparse group variable selection, based on both the quadratic inference function (QIF) and generalized estimating equation (GEE), to accommodate the bi-level selection for longitudinal G×E studies with high dimensional genomic features. Alternative methods conducting only the group or individual level selection have also been included. The core modules of the package have been developed in C++. ",2021-07-26,Fei Zhou,https://github.com/feizhoustat/springer,TRUE,https://github.com/feizhoustat/springer,4507,0,2021-04-08T00:12:22Z,NA
sprtt,"The seq_ttest() function is the implementation of Abraham
    Wald’s (1947) <doi:10.2134/agronj1947.00021962003900070011x> Sequential Probability Ratio Test (SPRT) for the test of
    a normal mean (difference) with unknown variance in R (R Core Team, 2018).
    It performs sequential t tests developed by Rushton (1950) <doi:10.2307/2332385>, Rushton (1952) <doi:10.2307/2334026> and
    Hajnal (1961) <doi:10.2307/2333131>, based on the SPRT. Specifically, seq_ttest() performs
    one-sample, two-sample, and paired t tests for testing one- and
    two-sided hypotheses.  The test is to be applied to the data during
    the sampling process, ideally after each observation. At any stage, it
    will return a decision to either continue sampling or terminate and
    accept one of the specified hypotheses. For more information on the
    SPRT t test, see Schnuerch & Erdfelder (2019) <doi:10.1037/met0000234>.",2021-08-06,Meike Steinhilber,https://meikesteinhilber.github.io/sprtt/,TRUE,https://github.com/meikesteinhilber/sprtt,372,1,2021-08-11T15:02:17Z,372
sps,"Sequential Poisson sampling is a method for drawing probability-proportional-to-size samples with a given number of units, and is commonly used for price-index surveys. This package gives functions to draw a stratified sequential Poisson sample according to the method by Ohlsson (1998, ISSN:0282-423X), and generate bootstrap replicate weights according to the generalized bootstrap method by Beaumont and Patak (2012, <doi:10.1111/j.1751-5823.2011.00166.x>).",2021-08-20,Steve Martin,https://github.com/marberts/sps,TRUE,https://github.com/marberts/sps,154,0,2021-08-19T03:46:37Z,NA
spsann,"Methods to optimize sample configurations using spatial simulated annealing. Multiple objective 
    functions are implemented for various purposes, such as variogram estimation, spatial trend estimation 
    and spatial interpolation. A general purpose spatial simulated annealing function enables the user to 
    define his/her own objective function. Solutions for augmenting existing sample configurations and solving
    multi-objective optimization problems are available as well.",2019-04-29,Alessandro Samuel-Rosa,https://github.com/samuel-rosa/spsann/,TRUE,https://github.com/samuel-rosa/spsann,19593,6,2020-09-11T03:08:57Z,3265.5
spsComps,"The systemPipeShiny (SPS) framework comes with many UI and server components. However, installing the whole framework is heavy and takes some time. If you would like to use UI and server components from SPS in your own Shiny apps, do not hesitate to try this package.",2021-05-18,Le Zhang,https://github.com/lz100/spsComps,TRUE,https://github.com/lz100/spscomps,2412,6,2021-08-13T07:15:21Z,402
SPSP,"An implementation of the feature Selection procedure by Partitioning the entire Solution Paths
            (namely SPSP) to identify the relevant features rather than using a single tuning parameter. 
            By utilizing the entire solution paths, this procedure can obtain better selection accuracy than 
            the commonly used approach of selecting only one tuning parameter based on existing criteria, 
            cross-validation (CV), generalized CV, AIC, BIC, and extended BIC (Liu, Y., & Wang, P. (2018) 
            <doi:10.1214/18-EJS1434>). It is more stable and accurate (low false positive and 
            false negative rates) than other variable selection approaches. In addition, it can be flexibly 
            coupled with the solution paths of Lasso, adaptive Lasso, ridge regression, and other penalized 
            estimators.",2021-08-02,Xiaorui (Jeremy) Zhu,https://github.com/XiaoruiZhu/SPSP,TRUE,https://github.com/xiaoruizhu/spsp,390,0,2021-08-09T15:29:37Z,NA
spsur,"A collection of functions to test and estimate Seemingly 
    Unrelated Regression (usually called SUR) models, with spatial structure, by maximum 
    likelihood and three-stage least squares. The package estimates the 
    most common spatial specifications, that is, SUR with Spatial Lag of 
    X regressors (called SUR-SLX), SUR with Spatial Lag Model (called SUR-SLM), 
    SUR with Spatial Error Model (called SUR-SEM), SUR with Spatial Durbin Model (called SUR-SDM), 
    SUR with Spatial Durbin Error Model (called SUR-SDEM), 
    SUR with Spatial Autoregressive terms and Spatial Autoregressive 
    Disturbances (called SUR-SARAR), SUR-SARAR with Spatial Lag of X 
    regressors (called SUR-GNM) and SUR with Spatially Independent Model (called SUR-SIM).
    The methodology of these models can be found in next references
    Mur, J., Lopez, F., and Herrera, M. (2010) <doi:10.1080/17421772.2010.516443> 
    Lopez, F.A., Mur, J., and Angulo, A. (2014) <doi:10.1007/s00168-014-0624-2>.",2021-06-25,Roman Minguez,https://CRAN.R-project.org/package=spsur,TRUE,https://github.com/rominsal/spsur,14721,9,2021-04-15T11:49:18Z,1635.6666666666667
spsUtil,"The systemPipeShiny (SPS) framework comes with many useful utility functions. However, installing the whole framework is heavy and takes some time. If you like only a few useful utility functions from SPS, install this package is enough.",2021-08-11,Le Zhang,https://github.com/lz100/spsUtil,TRUE,https://github.com/lz100/spsutil,2349,1,2021-08-18T01:58:51Z,2349
sptotal,"Performs predictions of totals and weighted sums, or finite population block kriging, on spatial data using the methods in Ver Hoef (2008) <doi:10.1007/s10651-007-0035-y>. The primary outputs are an estimate of the total, mean, or weighted sum in the region, an estimated prediction variance, and a plot of the predicted and observed values. This is useful primarily to users with ecological data that are counts or densities measured on some sites in a finite area of interest. Spatial prediction for the total count or average density in the entire region can then be done using the functions in this package. ",2021-07-06,Higham Matt,https://highamm.github.io/sptotal/index.html,TRUE,https://github.com/highamm/sptotal,4675,2,2021-07-07T18:56:33Z,2337.5
SPUTNIK,"A set of tools for the peak filtering of mass spectrometry
  imaging data (MSI or IMS) based on spatial distribution of signal. Given a 
  region-of-interest (ROI), representing the spatial region where the informative
  signal is expected to be localized, a series of filters determine which peak
  signals are characterized by an implausible spatial distribution. The filters
  reduce the dataset dimensionality and increase its information vs noise ratio,
  improving the quality of the unsupervised analysis results, reducing data
  dimensionality and simplifying the chemical interpretation.",2021-05-17,Paolo Inglese,https://github.com/paoloinglese/SPUTNIK,TRUE,https://github.com/paoloinglese/sputnik,15357,1,2021-05-17T09:51:33Z,15357
SPYvsSPY,"Data on the Spy vs. Spy comic strip of Mad magazine, created and
   written by Antonio Prohias.",2017-10-02,Steven E. Pav,https://github.com/shabbychef/SPYvsSPY,TRUE,https://github.com/shabbychef/spyvsspy,16216,8,2021-04-07T05:41:08Z,2027
SqlRender,"A rendering tool for parameterized SQL that also translates into
  different SQL dialects.  These dialects include 'Microsoft Sql Server', 'Oracle', 
  'PostgreSql', 'Amazon RedShift', 'Apache Impala', 'IBM Netezza', 'Google BigQuery', 'Microsoft PDW', 'Apache Spark', and 'SQLite'.",2021-09-02,Martijn Schuemie,"https://ohdsi.github.io/SqlRender/,
https://github.com/OHDSI/SqlRender",TRUE,https://github.com/ohdsi/sqlrender,53813,45,2021-08-31T07:47:05Z,1195.8444444444444
squashinformr,"Scrape SquashInfo <http://www.squashinfo.com/> for data on the Professional Squash Association World Tour and other squash events. 'squashinformr' functions scrape, parse, and clean data associated with players, tournaments, and rankings.",2021-08-08,Hayden MacDonald,https://github.com/HaydenMacDonald/squashinformr,TRUE,https://github.com/haydenmacdonald/squashinformr,6781,2,2021-08-08T11:26:00Z,3390.5
squid,"A simulation-based tool made to help researchers to become familiar with
    multilevel variations, and to build up sampling designs for their study. 
    This tool has two main objectives: First, it provides an educational tool useful for students, 
    teachers and researchers who want to learn to use mixed-effects models. 
    Users can experience how the mixed-effects model framework can be used to understand 
    distinct biological phenomena by interactively exploring simulated multilevel data. 
    Second, it offers research opportunities to those who are already familiar with 
    mixed-effects models, as it enables the generation of data sets that users may download 
    and use for a range of simulation-based statistical analyses such as power 
    and sensitivity analysis of multilevel and multivariate data [Allegue, H., Araya-Ajoy, Y.G., Dingemanse, 
    N.J., Dochtermann N.A., Garamszegi, L.Z., Nakagawa, S., Reale, D., Schielzeth, H. and Westneat, D.F. (2016) 
    <doi: 10.1111/2041-210X.12659>].",2019-08-06,Hassen Allegue,https://github.com/hallegue/squid,TRUE,https://github.com/hallegue/squid,14475,0,2021-03-07T18:05:51Z,NA
srm,"
    Provides functionality for structural equation modeling for
    the social relations model (Kenny & La Voie, 1984;
    <doi:10.1016/S0065-2601(08)60144-6>; Warner, Kenny, & Soto, 1979,
    <doi:10.1037/0022-3514.37.10.1742>). Maximum likelihood
    estimation (Gill & Swartz, 2001, <doi:10.2307/3316080>;
    Nestler, 2018, <doi:10.3102/1076998617741106>) and
    least squares estimation is supported (Bond & Malloy, 2018,
    <doi:10.1016/B978-0-12-811967-9.00014-X>).",2019-12-15,Alexander Robitzsch,"https://github.com/alexanderrobitzsch/srm,
https://sites.google.com/site/alexanderrobitzsch2/software",TRUE,https://github.com/alexanderrobitzsch/srm,9567,4,2021-03-17T16:34:00Z,2391.75
srt,"Read 'SubRip'
    <https://sourceforge.net/projects/subrip/> subtitle files as data
    frames for easy text analysis or manipulation.  Easily shift numeric
    timings and export subtitles back into valid 'SubRip' timestamp format
    to sync subtitles and audio.",2021-09-02,Kiernan Nicholls,"https://github.com/kiernann/srt, https://kiernann.com/srt/",TRUE,https://github.com/kiernann/srt,3244,0,2021-09-02T17:17:13Z,NA
srvyr,"Use piping, verbs like 'group_by' and 'summarize', and other
    'dplyr' inspired syntactic style when calculating summary statistics on survey
    data using functions from the 'survey' package.",2021-03-28,Greg Freedman Ellis,"http://gdfe.co/srvyr/, https://github.com/gergness/srvyr/",TRUE,https://github.com/gergness/srvyr,92731,170,2021-08-28T19:42:06Z,545.4764705882353
ss3sim,"Develops a framework for fisheries stock assessment simulation
    testing with Stock Synthesis (SS) as described in Anderson et al.
    (2014) <doi:10.1371/journal.pone.0092725>.",2019-11-08,Kelli F. Johnson,https://github.com/ss3sim/ss3sim,TRUE,https://github.com/ss3sim/ss3sim,16953,33,2021-08-10T18:07:41Z,513.7272727272727
SSBtools,"Functions used by other packages from Statistics Norway are gathered. General data manipulation functions, and functions for hierarchical computations are included (Langsrud, 2020) <doi:10.13140/RG.2.2.27313.61283>. The hierarchy specification functions are useful within statistical disclosure control.",2021-07-02,Øyvind Langsrud,https://github.com/statisticsnorway/SSBtools,TRUE,https://github.com/statisticsnorway/ssbtools,20952,3,2021-09-03T08:33:43Z,6984
ssdtools,"Species sensitivity distributions are 
  cumulative probability distributions which are fitted to 
  toxicity concentrations for different species as described by
  Posthuma et al.(2001) <isbn:9781566705783>.
  The ssdtools package uses Maximum Likelihood to fit distributions 
  such as the log-normal, gamma, log-logistic, 
  log-Gumbel, Gompertz and Weibull.
  The user can provide custom distributions.
  Multiple distributions can be averaged using Information Criteria.
  Confidence intervals on hazard concentrations and proportions are produced by 
  parametric bootstrapping.",2021-05-14,Joe Thorley,https://github.com/bcgov/ssdtools,TRUE,https://github.com/bcgov/ssdtools,17762,21,2021-07-30T00:36:32Z,845.8095238095239
ssh,"Connect to a remote server over SSH to transfer files via SCP, 
    setup a secure tunnel, or run a command or script on the host while 
    streaming stdout and stderr directly to the client.",2021-05-03,Jeroen Ooms,"https://docs.ropensci.org/ssh/ (website),
https://github.com/ropensci/ssh (devel)",TRUE,https://github.com/ropensci/ssh,93684,114,2021-05-03T14:23:00Z,821.7894736842105
sss,"Tools to import survey files
    in the .sss (triple-s) format. The package provides the function
    read.sss() that reads the .asc (or .csv) and .sss files of a
    triple-s survey data file.",2017-04-01,Andrie de Vries,https://github.com/andrie/sss,TRUE,https://github.com/andrie/sss,17854,7,2020-09-18T09:10:25Z,2550.5714285714284
stabm,"An implementation of many measures for the
    assessment of the stability of feature selection. Both simple measures
    and measures which take into account the similarities between features
    are available, see Bommert (2020) <doi:10.17877/DE290R-21906>.",2021-04-07,Andrea Bommert,"https://bommert.github.io/stabm/, https://github.com/bommert/stabm",TRUE,https://github.com/bommert/stabm,12306,6,2021-04-06T12:15:15Z,2051
stabs,"Resampling procedures to assess the stability of selected variables
    with additional finite sample error control for high-dimensional variable
    selection procedures such as Lasso or boosting. Both, standard stability
    selection (Meinshausen & Buhlmann, 2010, <doi:10.1111/j.1467-9868.2010.00740.x>) 
    and complementary pairs stability selection with improved error bounds 
    (Shah & Samworth, 2013, <doi:10.1111/j.1467-9868.2011.01034.x>) are
    implemented. The package can be combined with arbitrary user specified
    variable selection approaches.",2021-01-29,Benjamin Hofner,https://github.com/hofnerb/stabs,TRUE,https://github.com/hofnerb/stabs,149014,21,2021-01-28T09:57:24Z,7095.9047619047615
stacks,"Model stacking is an ensemble technique
    that involves training a model to combine the outputs of many 
    diverse statistical models, and has been shown to improve 
    predictive performance in a variety of settings. 'stacks' 
    implements a grammar for 'tidymodels'-aligned model stacking.",2021-07-23,Simon Couch,"https://stacks.tidymodels.org/,
https://github.com/tidymodels/stacks",TRUE,https://github.com/tidymodels/stacks,6900,211,2021-07-23T17:11:55Z,32.70142180094787
stagedtrees,"Creates and fits staged event tree probability models, 
             which are probabilistic graphical models capable of representing 
             asymmetric conditional independence statements 
             for categorical variables. 
             Includes functions to create, plot and fit staged 
             event trees from data, as well as many efficient structure 
             learning algorithms.
             References:
             Collazo R. A., Görgen C. and Smith J. Q. 
             (2018, ISBN:9781498729604).
             Görgen C., Bigatti A., Riccomagno E. and Smith J. Q. (2018) 
             <arXiv:1705.09457>.
             Thwaites P. A., Smith, J. Q. (2017) <arXiv:1510.00186>.
             Barclay L. M., Hutton J. L. and Smith J. Q. (2013) 
             <doi:10.1016/j.ijar.2013.05.006>.
             Smith J. Q. and Anderson P. E. (2008) 
             <doi:10.1016/j.artint.2007.05.004>.",2021-06-21,Gherardo Varando,https://github.com/gherardovarando/stagedtrees,TRUE,https://github.com/gherardovarando/stagedtrees,10619,2,2021-06-21T15:38:28Z,5309.5
StAMPP,"Allows users to calculate pairwise Nei's Genetic Distances (Nei 1972), pairwise Fixation
 Indexes (Fst) (Weir & Cockerham 1984) and also Genomic Relationship matrixes following Yang et al. (2010) in mixed and single
 ploidy populations. Bootstrapping across loci is implemented during Fst calculation to generate confidence intervals and p-values
 around pairwise Fst values. StAMPP utilises SNP genotype data of any ploidy level (with the ability to handle missing data) and is coded to  
 utilise multithreading where available to allow efficient analysis of large datasets. StAMPP is able to handle genotype data from genlight objects 
 allowing integration with other packages such adegenet.
 Please refer to LW Pembleton, NOI Cogan & JW Forster, 2013, Molecular Ecology Resources, 13(5), 946-952. <doi:10.1111/1755-0998.12129> for the appropriate citation and user manual. Thank you in advance.",2021-08-08,LW Pembleton,https://github.com/lpembleton/StAMPP,TRUE,https://github.com/lpembleton/stampp,35168,1,2021-08-08T16:02:22Z,35168
standardize,"Tools which allow regression variables to be placed on similar
    scales, offering computational benefits as well as easing interpretation of
    regression output.",2021-03-05,Christopher D. Eager,https://github.com/CDEager/standardize,TRUE,https://github.com/cdeager/standardize,36087,18,2021-03-05T06:57:20Z,2004.8333333333333
standartox,"The <http://standartox.uni-landau.de> database offers cleaned,
    harmonized and aggregated ecotoxicological test data, which can
    be used for assessing effects and risks of chemical concentrations
    found in the environment.",2021-05-05,Andreas Scharmüller,https://github.com/andschar/standartox,TRUE,https://github.com/andschar/standartox,1384,7,2021-05-10T08:17:42Z,197.71428571428572
stanette,"Expansion and additions to 'rstan' to facilitate pharmacokinetics (PK)
    and pharmacodynamics (PD) modeling with 'rstan'.  A PKPD model often is specified
    via a set of ordinary differential equations(ODEs) and requires flexible and 
    different routes of drug administrations.  These features make PKPD modeling
    with plain 'rstan' challenging and tedious to code.  'stanette' provides a powerful 
    Stan-compatible ODE solver ('LSODA') and mechanism/utilities that make easy
    specification of flexible dosing records.",2021-08-16,Wenping Wang,"https://mc-stan.org/rstan/, https://discourse.mc-stan.org",TRUE,https://github.com/stan-dev/rstan,145,803,2021-02-08T17:39:07Z,0.18057285180572852
stapler,"An implementation of Simultaneous Truth and 
    Performance Level Estimation (STAPLE) <doi:10.1109/TMI.2004.828354>.  This
    method is used when there are multiple raters for an object, typically an
    image, and this method fuses these ratings into one rating.  It uses an
    expectation-maximization method to estimate this rating and the individual
    specificity/sensitivity for each rater.",2020-01-09,John Muschelli,https://github.com/muschellij2/stapler,TRUE,https://github.com/muschellij2/stapler,15103,0,2021-04-01T16:04:13Z,NA
staplr,"Provides function to manipulate PDF files: 
    fill out PDF forms;
    merge multiple PDF files into one; 
    remove selected pages from a file;
    rename multiple files in a directory;
    rotate entire pdf document; 
    rotate selected pages of a pdf file;
    Select pages from a file;
    splits single input PDF document into individual pages;
    splits single input PDF document into parts from given points.
    'staplr' requires Java 8 installed on your system.",2021-01-11,Priyanga Dilini Talagala,NA,TRUE,https://github.com/pridiltal/staplr,28608,225,2021-01-11T09:01:13Z,127.14666666666666
staRdom,"This is a user-friendly way to run a parallel factor (PARAFAC) analysis (Harshman, 1971) <doi:10.1121/1.1977523> on excitation emission matrix (EEM) data from dissolved organic matter (DOM) samples (Murphy et al., 2013) <doi:10.1039/c3ay41160e>. The analysis includes profound methods for model validation. Some additional functions allow the calculation of absorbance slope parameters and create beautiful plots.",2021-04-20,Matthias Pucher,https://cran.r-project.org/package=staRdom,TRUE,https://github.com/matthiaspucher/stardom,20579,9,2021-04-19T07:14:22Z,2286.5555555555557
starnet,"Implements stacked elastic net regression (Rauschenberger 2020, <doi:10.1093/bioinformatics/btaa535>). The elastic net generalises ridge and lasso regularisation (Zou 2005, <doi:10.1111/j.1467-9868.2005.00503.x>). Instead of fixing or tuning the mixing parameter alpha, we combine multiple alpha by stacked generalisation (Wolpert 1992 <doi:10.1016/S0893-6080(05)80023-1>).",2020-11-24,Armin Rauschenberger,https://github.com/rauschenberger/starnet,TRUE,https://github.com/rauschenberger/starnet,5481,4,2021-04-19T07:07:16Z,1370.25
stars,"Reading, manipulating, writing and plotting
    spatiotemporal arrays (raster and vector data cubes) in 'R', using 'GDAL'
    bindings provided by 'sf', and 'NetCDF' bindings by 'ncmeta' and 'RNetCDF'.",2021-06-08,Edzer Pebesma,"https://r-spatial.github.io/stars/,
https://github.com/r-spatial/stars/",TRUE,https://github.com/r-spatial/stars,705199,395,2021-09-01T08:10:20Z,1785.313924050633
starsExtra,"Miscellaneous functions for working with 'stars' objects, mainly single-band rasters. Currently includes functions for: (1) focal filtering, (2) detrending of Digital Elevation Models, (3) calculating flow length, (4) calculating the Convergence Index, (5) calculating topographic aspect and topographic slope.",2021-06-15,Michael Dorman,"https://michaeldorman.github.io/starsExtra/,
https://github.com/michaeldorman/starsExtra/",TRUE,https://github.com/michaeldorman/starsextra,7913,18,2021-06-15T06:39:24Z,439.6111111111111
starter,"Get started with new projects by dropping a skeleton of a new
    project into a new or existing directory, initialise git repositories,
    and create reproducible environments with the 'renv' package. The
    package allows for dynamically named files, folders, file content, as
    well as the functionality to drop individual template files into
    existing projects.",2021-07-22,Daniel D. Sjoberg,"https://github.com/ddsjoberg/starter,
http://www.danieldsjoberg.com/starter/index.html",TRUE,https://github.com/ddsjoberg/starter,1136,5,2021-07-27T20:24:35Z,227.2
startup,Adds support for R startup configuration via '.Renviron.d' and '.Rprofile.d' directories in addition to '.Renviron' and '.Rprofile' files.  This makes it possible to keep private / secret environment variables separate from other environment variables.  It also makes it easier to share specific startup settings by simply copying a file to a directory.,2020-09-03,Henrik Bengtsson,https://github.com/HenrikBengtsson/startup,TRUE,https://github.com/henrikbengtsson/startup,28139,111,2021-08-25T21:52:24Z,253.5045045045045
starvars,"Allows the user to estimate a vector logistic smooth transition autoregressive model via maximum log-likelihood or nonlinear least squares. It further permits to test for linearity in the multivariate framework against a vector logistic smooth transition autoregressive model with a single transition variable. The estimation method is discussed in Terasvirta and Yang (2014, <doi:10.1108/S0731-9053(2013)0000031008>). Also, realized covariances can be constructed from stock market prices or returns, as explained in Andersen et al. (2001, <doi:10.1016/S0304-405X(01)00055-1>).",2021-01-11,Andrea Bucci,https://github.com/andbucci/starvars,TRUE,https://github.com/andbucci/starvars,6168,3,2021-09-03T16:12:08Z,2056
starvz,"Performance analysis workflow that combines the power of the R
    language (and the tidyverse realm) and many auxiliary tools to
    provide a consistent, flexible, extensible, fast, and versatile
    framework for the performance analysis of task-based applications
    that run on top of the StarPU runtime (with its MPI (Message
    Passing Interface) layer for multi-node support).  Its goal is to
    provide a fruitful prototypical environment to conduct performance
    analysis hypothesis-checking for task-based applications that run
    on heterogeneous (multi-GPU, multi-core) multi-node HPC
    (High-performance computing) platforms.",2021-03-25,Lucas Mello Schnorr,https://github.com/schnorr/starvz,TRUE,https://github.com/schnorr/starvz,4463,6,2021-08-28T22:23:36Z,743.8333333333334
starwarsdb,"Provides data about the 'Star Wars' movie franchise
    in a set of relational tables or as a complete 'DuckDB' database. All
    data was collected from the open source 'Star Wars' API
    <https://swapi.dev/>.",2020-11-02,Garrick Aden-Buie,https://github.com/gadenbuie/starwarsdb,TRUE,https://github.com/gadenbuie/starwarsdb,4100,30,2021-07-27T15:35:28Z,136.66666666666666
statar,"A set of tools inspired by 'Stata' to explore data.frames ('summarize',
    'tabulate', 'xtile', 'pctile', 'binscatter', elapsed quarters/month, lead/lag).",2020-11-19,Matthieu Gomez,https://github.com/matthieugomez/statar,TRUE,https://github.com/matthieugomez/statar,35546,47,2021-05-03T18:22:47Z,756.2978723404256
statcanR,"An easy connection with R to Statistics Canada's Web Data Service. Open economic data (formerly known as CANSIM tables, now identified by Product IDs (PID)) are accessible as a data frame, directly in the user's R environment.
    Warin, Le Duc (2019) <doi:10.6084/m9.figshare.10544735>.",2021-03-03,Thierry Warin,https://github.com/warint/statcanR/,TRUE,https://github.com/warint/statcanr,8447,8,2021-08-24T16:40:41Z,1055.875
states,"Create panel data consisting of independent states from 1816 to
    the present. The package includes the Gleditsch & Ward (G&W) and Correlates
    of War (COW) lists of independent states, as well as helper functions for 
    working with state panel data and standardizing other data sources to 
    create country-year/month/etc. data. ",2021-04-14,Andreas Beger,"https://github.com/andybega/states,
https://www.andybeger.com/states/",TRUE,https://github.com/andybega/states,21319,12,2021-04-14T12:20:45Z,1776.5833333333333
statespacer,"A tool that makes estimating models in state space form 
    a breeze. See ""Time Series Analysis by State Space Methods"" by 
    Durbin and Koopman (2012, ISBN: 978-0-19-964117-8) for details 
    about the algorithms implemented.",2020-11-19,Dylan Beijers,"https://DylanB95.github.io/statespacer/,
https://github.com/DylanB95/statespacer/",TRUE,https://github.com/dylanb95/statespacer,7888,2,2020-12-29T12:43:36Z,3944
statgenGWAS,"Fast single trait Genome Wide Association Studies (GWAS) following 
    the method described in Kang et al. (2010), <doi:10.1038/ng.548>.        
    One of a series of statistical genetic packages for streamlining the 
    analysis of typical plant breeding experiments developed by Biometris.",2021-07-29,Bart-Jan van Rossum,"https://biometris.github.io/statgenGWAS/index.html,
https://github.com/Biometris/statgenGWAS/",TRUE,https://github.com/biometris/statgengwas,9558,5,2021-07-27T12:04:23Z,1911.6
statgenGxE,"Analysis of multi environment data of plant breeding experiments
    following the analyses described in Malosetti, Ribaut, 
    and van Eeuwijk (2013), <doi:10.3389/fphys.2013.00044>.
    One of a series of statistical genetic packages for streamlining the analysis of 
    typical plant breeding experiments developed by Biometris.
    Some functions have been created to be used in conjunction with the R 
    package 'asreml' for the 'ASReml' software, which can be obtained upon 
    purchase from 'VSN' international (<https://www.vsni.co.uk/software/asreml-r>).",2021-01-07,Bart-Jan van Rossum,"https://biometris.github.io/statgenGxE/index.html,
https://github.com/Biometris/statgenGxE/",TRUE,https://github.com/biometris/statgengxe,4722,1,2021-01-08T10:41:55Z,4722
statgenHTP,"Phenotypic analysis of data coming from high throughput 
    phenotyping (HTP) platforms, including different types of outlier detection,
    spatial analysis, and parameter estimation. The package is being developed
    within the EPPN2020 project (<https://eppn2020.plant-phenotyping.eu/>).
    Some functions have been created to be used in conjunction with the R 
    package 'asreml' for the 'ASReml' software, which can be obtained upon 
    purchase from 'VSN' international (<https://www.vsni.co.uk/software/asreml>).",2021-08-20,Bart-Jan van Rossum,"https://biometris.github.io/statgenHTP/index.html,
https://github.com/Biometris/statgenHTP/",TRUE,https://github.com/biometris/statgenhtp,2619,0,2021-08-16T06:48:25Z,NA
statgenSTA,"Phenotypic analysis of field trials using mixed models with and 
    without spatial components. One of a series of statistical genetic packages 
    for streamlining the analysis of typical plant breeding experiments developed
    by Biometris.    
    Some functions have been created to be used in conjunction with the R 
    package 'asreml' for the 'ASReml' software, which can be obtained upon 
    purchase from 'VSN' international (<https://www.vsni.co.uk/software/asreml-r>). ",2021-05-25,Bart-Jan van Rossum,"https://biometris.github.io/statgenSTA/index.html,
https://github.com/Biometris/statgenSTA/",TRUE,https://github.com/biometris/statgensta,11700,1,2021-05-26T09:16:28Z,11700
statnet,"Statnet is a collection of packages for statistical network analysis that are 
  designed to work together because they share common data representations and 'API' 
  design.  They provide an integrated set of tools for the representation, 
  visualization, analysis, and simulation of many different forms of network data.  
  This package is designed to make it easy to install and load the 
  key 'statnet' packages in a single step.  Learn more about 'statnet' 
  at <http://www.statnet.org>.  Tutorials for many packages can be found 
  at <https://github.com/statnet/Workshops/wiki>.  For an introduction to functions in this package, 
  type help(package='statnet').",2019-06-14,Martina Morris,http://statnet.org,TRUE,https://github.com/statnet/statnet,135230,22,2021-06-26T06:16:54Z,6146.818181818182
statnet.common,Non-statistical utilities used by the software developed by the Statnet Project. They may also be of use to others.,2021-06-05,Pavel N. Krivitsky  (<https://orcid.org/0000-0002-9101-3362>,https://statnet.org,TRUE,https://github.com/statnet/statnet.common,772120,8,2021-07-12T06:17:35Z,96515
statnipokladna,"Get programmatic access to data from the Czech public
    budgeting and accounting database, Státní pokladna
    <https://monitor.statnipokladna.cz/>.",2021-05-26,Petr Bouchal,"https://github.com/petrbouchal/statnipokladna,
https://petrbouchal.xyz/statnipokladna/",TRUE,https://github.com/petrbouchal/statnipokladna,6620,3,2021-06-14T14:12:45Z,2206.6666666666665
stats19,"Tools to help download, process and analyse the UK road collision data collected using the
  'STATS19' form. The data are provided as 'CSV' files with detailed road safety data about the
  circumstances of car crashes and other incidents on the roads resulting in 
  casualties in Great Britain from 1979, the types
  (including make and model) of vehicles involved and the consequential casualties.  The
  statistics relate only to personal casualties on public roads that are reported
  to the police, and subsequently recorded, using the 'STATS19' accident reporting form. See
  the Department for Transport website 
  <https://data.gov.uk/dataset/cb7ae6f0-4be6-4935-9277-47e5ce24a11f/road-safety-data> for more
  information on these data.",2021-07-21,Robin Lovelace,"https://github.com/ropensci/stats19,
https://docs.ropensci.org/stats19/",TRUE,https://github.com/ropensci/stats19,44403,39,2021-07-21T07:24:25Z,1138.5384615384614
statsExpressions,"Utilities for producing dataframes with rich details for the
    most common types of statistical approaches and tests: parametric,
    nonparametric, robust, and Bayesian t-test, one-way ANOVA, correlation
    analyses, contingency table analyses, and meta-analyses. The
    functions are pipe-friendly and provide a consistent syntax to work
    with tidy data. These dataframes additionally contain expressions with
    statistical details, and can be used in graphing packages. This
    package also forms the statistical processing backend for
    'ggstatsplot'.",2021-05-30,Indrajeet Patil,"https://indrajeetpatil.github.io/statsExpressions/,
https://github.com/IndrajeetPatil/statsExpressions",TRUE,https://github.com/indrajeetpatil/statsexpressions,111532,220,2021-08-16T16:01:04Z,506.96363636363634
statsr,"Data and functions to support Bayesian and frequentist inference and decision making 
            for the Coursera Specialization ""Statistics with R"".
            See <https://github.com/StatsWithR/statsr> for more information.",2021-01-22,Merlise Clyde,https://github.com/StatsWithR/statsr,TRUE,https://github.com/statswithr/statsr,95288,51,2021-01-22T19:40:05Z,1868.3921568627452
SteinerNet,"A set of functions for finding and analysing Steiner trees. It has applications in
    biological pathway network analysis. Sadeghi (2013) <doi:10.1186/1471-2105-14-144>.",2020-09-07,Afshin Sadeghi,https://github.com/krashkov/SteinerNet,TRUE,https://github.com/krashkov/steinernet,10595,6,2020-09-05T00:56:01Z,1765.8333333333333
STEPCAM,"Collection of model estimation, and model plotting functions 
             related to the STEPCAM family of community assembly models. 
             STEPCAM is a STEPwise Community Assembly Model that infers 
             the relative contribution of Dispersal Assembly, Habitat Filtering 
             and Limiting Similarity from a dataset consisting of the 
             combination of trait and abundance data. See also <http://onlinelibrary.wiley.com/wol1/doi/10.1890/14-0454.1/abstract> for more information.",2016-09-21,Thijs Janzen,https://github.com/thijsjanzen/STEPCAM,TRUE,https://github.com/thijsjanzen/stepcam,15847,5,2020-12-16T18:18:07Z,3169.4
stevedata,"This is a collection of various kinds of data with broad uses for teaching. 
    My students, and academics like me who teach the same topics I teach, should find 
    this useful if their teaching workflow is also built around the R programming 
    language. The applications are multiple but mostly cluster on topics of statistical
    methodology, international relations, and political economy.",2021-04-21,Steve Miller,http://svmiller.com/stevedata/,TRUE,https://github.com/svmiller/stevedata,4216,3,2021-08-30T14:55:25Z,1405.3333333333333
stevemisc,"These are miscellaneous functions that I find useful for my research and teaching.
    The contents include themes for plots, functions for simulating
    quantities of interest from regression models, functions for simulating various 
    forms of fake data for instructional/research purposes, and many more. All told, the functions
    provided here are broadly useful for data organization, data presentation, data recoding, 
    and data simulation. ",2021-07-27,Steve Miller,NA,TRUE,https://github.com/svmiller/stevemisc,2657,2,2021-07-27T13:27:06Z,1328.5
stfit,A general spatiotemporal satellite image imputation method based on sparse functional data analytic techniques. The imputation method applies and extends the Functional Principal Analysis by Conditional Estimation (PACE). The underlying idea for the proposed procedure is to impute a missing pixel by borrowing information from temporally and spatially contiguous pixels based on the best linear unbiased prediction.  ,2020-10-02,Weicheng Zhu,NA,TRUE,https://github.com/mingsnu/stfit,3714,0,2020-10-25T20:23:13Z,NA
stickr,"Download and use R hex stickers. Stickers made 
  available in standardized locations in GitHub repositories, as well as those in the 
  <https://github.com/rstudio/hex-stickers> repository will be available.",2020-07-03,Alex Gold,https://github.com/akgold/stickr,TRUE,https://github.com/akgold/stickr,3969,30,2020-10-26T15:37:53Z,132.3
stlcsb,"The Citizens' Service Bureau of the City of St. Louis is a clearing house for 
    non-emergency service requests. This package provides functions for downloading, 
    categorizing problem requests, cleaning and subsetting CSB data, and projecting the data
    using the x and y coordinates included with CSB data releases.",2019-02-22,Christopher Prener,https://github.com/slu-openGIS/stlcsb,TRUE,https://github.com/slu-opengis/stlcsb,10669,5,2020-12-18T01:50:44Z,2133.8
stm,"The Structural Topic Model (STM) allows researchers 
  to estimate topic models with document-level covariates. 
  The package also includes tools for model selection, visualization,
  and estimation of topic-covariate regressions. Methods developed in
  Roberts et. al. (2014) <doi:10.1111/ajps.12103> and 
  Roberts et. al. (2016) <doi:10.1080/01621459.2016.1141684>. Vignette
  is Roberts et. al. (2019) <doi:10.18637/jss.v091.i02>.",2020-09-18,Brandon Stewart,http://www.structuraltopicmodel.com/,TRUE,https://github.com/bstewart/stm,183346,314,2020-09-18T13:05:26Z,583.9044585987261
stminsights,"This app enables interactive validation, interpretation and visualization of structural topic models from the 'stm' package by Roberts and others (2014) <doi:10.1111/ajps.12103>. It also includes helper functions for model diagnostics and extracting data from effect estimates.",2021-03-29,Carsten Schwemmer,https://github.com/cschwem2er/stminsights,TRUE,https://github.com/cschwem2er/stminsights,17857,86,2021-03-29T13:50:49Z,207.63953488372093
stochQN,"Implementations of stochastic, limited-memory quasi-Newton optimizers,
	similar in spirit to the LBFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) algorithm,
	for smooth stochastic optimization. Implements the following methods:
	oLBFGS (online LBFGS) (Schraudolph, N.N., Yu, J. and Guenter, S., 2007 <http://proceedings.mlr.press/v2/schraudolph07a.html>),
	SQN (stochastic quasi-Newton) (Byrd, R.H., Hansen, S.L., Nocedal, J. and Singer, Y., 2016 <arXiv:1401.7020>),
	adaQN (adaptive quasi-Newton) (Keskar, N.S., Berahas, A.S., 2016, <arXiv:1511.01169>).
	Provides functions for easily creating R objects
	with partial_fit/predict methods from some given objective/gradient/predict functions.
	Includes an example stochastic logistic regression using these optimizers.
	Provides header files and registered C routines for using it directly from C/C++.",2019-09-05,David Cortes,https://github.com/david-cortes/stochQN,TRUE,https://github.com/david-cortes/stochqn,13320,10,2021-04-15T15:41:10Z,1332
stochvol,Efficient algorithms for fully Bayesian estimation of stochastic volatility (SV) models with and without asymmetry (leverage) via Markov chain Monte Carlo (MCMC) methods. Methodological details are given in Kastner and Frühwirth-Schnatter (2014) <doi:10.1016/j.csda.2013.01.002> and Hosszejni and Kastner (2019) <doi:10.1007/978-3-030-30611-3_8>; the most common use cases are described in Kastner (2016) <doi:10.18637/jss.v069.i05> and the package vignette.,2021-07-12,Darjus Hosszejni,https://gregorkastner.github.io/stochvol/,TRUE,https://github.com/gregorkastner/stochvol,49704,6,2021-07-12T14:21:06Z,8284
stochvolTMB,"Parameter estimation for stochastic volatility models using maximum likelihood. The latent log-volatility is 
    integrated out of the likelihood using the Laplace approximation. The models are fitted via 'TMB' (Template Model
    Builder) (Kristensen, Nielsen, Berg, Skaug, and Bell (2016) <doi:10.18637/jss.v070.i05>). ",2021-08-13,Jens Christian Wahl,https://github.com/JensWahl/stochvolTMB,TRUE,https://github.com/jenswahl/stochvoltmb,4584,7,2021-08-14T13:08:18Z,654.8571428571429
stockfish,"An implementation of the UCI open communication protocol that ships
    with 'Stockfish 11' <https://stockfishchess.org/>, a very popular, open
    source, powerful chess engine written in C++.",2021-05-04,Caio Lente,https://github.com/curso-r/stockfish,TRUE,https://github.com/curso-r/stockfish,1801,24,2021-06-10T00:32:21Z,75.04166666666667
stokes,"Provides functionality for working with tensors, alternating
           tensors, wedge products, Stokes's theorem, and related concepts
	   from the exterior calculus.  Functionality for Grassman algebra
	   is provided.  The canonical reference would be:
	   M. Spivak (1965, ISBN:0-8053-9021-9) ""Calculus on Manifolds"".",2021-05-10,Robin K. S. Hankin,https://github.com/RobinHankin/stokes,TRUE,https://github.com/robinhankin/stokes,7218,2,2021-08-27T09:05:05Z,3609
stopwords,"Provides multiple sources of stopwords, for use in text analysis and natural language processing.",2021-02-10,Kenneth Benoit,https://github.com/quanteda/stopwords,TRUE,https://github.com/quanteda/stopwords,969524,93,2021-02-10T12:35:08Z,10424.989247311827
storr,"Creates and manages simple key-value stores.  These can
    use a variety of approaches for storing the data.  This package
    implements the base methods and support for file system, in-memory
    and DBI-based database stores.",2020-12-01,Rich FitzJohn,https://github.com/richfitz/storr,TRUE,https://github.com/richfitz/storr,105407,114,2020-12-02T07:07:43Z,924.6228070175439
storywranglr,"An interface to explore trends in Twitter data using the 
    'Storywrangler' Application Programming Interface (API), which can be found
    here: <https://github.com/janeadams/storywrangler>.",2021-08-13,Christopher Belanger,https://github.com/chris31415926535/storywranglr,TRUE,https://github.com/chris31415926535/storywranglr,302,0,2021-08-13T13:00:15Z,NA
stplanr,"Tools for transport planning with an emphasis on spatial transport
    data and non-motorized modes. Enables common transport planning tasks including:
    downloading and cleaning transport datasets; creating geographic ""desire lines""
    from origin-destination (OD) data; route assignment, locally and via
    interfaces to routing services such as <https://cyclestreets.net/> and
    calculation of route segment attributes such as bearing.
    The package implements the 'travel flow aggregration' method
    described in Morgan and Lovelace (2020) <doi:10.1177/2399808320942779>.
    Further information on the package's aim and scope can be found
    in the vignettes and in a paper in the R Journal
    (Lovelace and Ellison 2018) <doi:10.32614/RJ-2018-053>.",2021-07-22,Robin Lovelace,"https://github.com/ropensci/stplanr,
https://docs.ropensci.org/stplanr/",TRUE,https://github.com/ropensci/stplanr,81441,354,2021-09-02T10:55:54Z,230.0593220338983
strand,"Provides a framework for performing discrete (share-level) simulations of
  investment strategies. Simulated portfolios optimize exposure to an input signal subject
  to constraints such as position size and factor exposure. For background see L. Chincarini
  and D. Kim (2010, ISBN:978-0-07-145939-6) ""Quantitative Equity Portfolio Management"".",2020-11-19,Jeff Enos,https://github.com/strand-tech/strand,TRUE,https://github.com/strand-tech/strand,5832,15,2020-11-19T01:57:45Z,388.8
stratamatch,"A pilot matching design to automatically 
    stratify and match large datasets.  The manual_stratify() function allows
    users to manually stratify a dataset based on categorical variables of 
    interest, while the auto_stratify() function does automatically by
    allocating a held-aside (pilot) data set, fitting a prognostic score  
    (see Hansen (2008) <doi:10.1093/biomet/asn004>) on the pilot set, and stratifying the data set based
    on prognostic score quantiles.  The strata_match() function then does optimal
    matching of the data set in parallel within strata.",2021-05-26,Rachael C. Aikens,https://github.com/raikens1/stratamatch,TRUE,https://github.com/raikens1/stratamatch,13080,3,2021-05-26T00:10:33Z,4360
StratifiedSampling,"Integrating a stratified structure in the population in a sampling design can considerably reduce the variance of the Horvitz-Thompson estimator. We propose in this package different methods to handle the selection of a balanced sample in stratified population. For more details see Raphaël Jauslin, Esther Eustache and Yves Tillé (2021) <arXiv:2101.05568>. The package propose also a method based on optimal transport and balanced sampling, see Raphaël Jauslin and Yves Tillé <arXiv:2105.08379>.",2021-05-20,Raphael Jauslin,https://github.com/RJauslin/StratifiedSampling,TRUE,https://github.com/rjauslin/stratifiedsampling,2832,0,2021-05-20T11:30:46Z,NA
stray,"
    This is a modification of 'HDoutliers' package. The 'HDoutliers' algorithm is a powerful 
    unsupervised algorithm for detecting anomalies in high-dimensional data, with a 
    strong theoretical foundation. However, it suffers from some limitations that 
    significantly hinder its performance level, under certain circumstances. This package 
    implements the algorithm proposed in Talagala, Hyndman and Smith-Miles (2019) 
    <arXiv:1908.04000>  for detecting anomalies in high-dimensional data
    that addresses these limitations of 'HDoutliers' algorithm. We define an anomaly as an observation that deviates markedly from the majority
    with a large distance gap. An approach based on extreme value theory is used 
    for the anomalous threshold calculation.",2020-06-29,Priyanga Dilini Talagala,NA,TRUE,https://github.com/pridiltal/stray,9067,54,2020-10-25T22:23:12Z,167.90740740740742
stream,A framework for data stream modeling and associated data mining tasks such as clustering and classification. The development of this package was supported in part by NSF IIS-0948893 and NIH R21HG005912. Hahsler et al (2017) <doi:10.18637/jss.v076.i14>.,2020-12-02,Michael Hahsler,https://github.com/mhahsler/stream,TRUE,https://github.com/mhahsler/stream,27772,29,2021-08-26T19:24:03Z,957.6551724137931
streamDepletr,"Implementation of analytical models for estimating streamflow 
    depletion due to groundwater pumping, and other related tools. Functions
    are broadly split into two groups: (1) analytical streamflow depletion
    models, which estimate streamflow depletion for a single stream reach
    resulting from groundwater pumping; and (2) depletion apportionment 
    equations, which distribute estimated streamflow depletion among multiple
    stream reaches within a stream network. See Zipper et al. (2018) <doi:10.1029/2018WR022707>
    for more information on depletion apportionment equations and Zipper et
    al. (2019) <doi:10.1029/2018WR024403> for more information on analytical
    depletion functions, which combine analytical models and depletion apportionment
    equations.",2020-03-25,Samuel C. Zipper,https://github.com/FoundrySpatial/streamDepletr,TRUE,https://github.com/foundryspatial/streamdepletr,8248,6,2021-05-07T20:47:07Z,1374.6666666666667
streamMOA,"Interface for data stream clustering algorithms implemented in the MOA (Massive Online Analysis) framework (Albert Bifet, Geoff Holmes, Richard Kirkby, Bernhard Pfahringer (2010). MOA: Massive Online Analysis, Journal of Machine Learning Research 11: 1601-1604).",2020-12-03,Michael Hahsler,NA,TRUE,https://github.com/mhahsler/streammoa,18891,8,2020-12-03T21:20:17Z,2361.375
strex,"There are some things that I wish were easier with
    the 'stringr' or 'stringi' packages. The foremost of these is the
    extraction of numbers from strings. 'stringr' and 'stringi' make you
    figure out the regular expression for yourself; 'strex' takes care of
    this for you. There are many other handy functionalities in 'strex'.
    Contributions to this package are encouraged: it is intended as a
    miscellany of string manipulation functions that cannot be found in
    'stringi' or 'stringr'.",2021-04-18,Rory Nolan,"https://rorynolan.github.io/strex/,
https://github.com/rorynolan/strex",TRUE,https://github.com/rorynolan/strex,149562,36,2021-04-18T16:58:26Z,4154.5
string2path,"Extract glyph information from a font file, and translate the
    outline curves to flattened paths or tessellated polygons. The converted
    data is returned as a 'data.frame' in easy-to-plot format.",2021-08-09,Hiroaki Yutani,"https://yutannihilation.github.io/string2path/,
https://github.com/yutannihilation/string2path",TRUE,https://github.com/yutannihilation/string2path,386,51,2021-08-21T09:00:30Z,7.568627450980392
stringb,"Base R already ships with string handling capabilities 'out-
    of-the-box' but lacks streamlined function names and workflow. The
    'stringi' ('stringr') package on the other hand has well named functions,
    extensive Unicode support and allows for a streamlined workflow. On the other
    hand it adds dependencies and regular expression interpretation between base R
    functions and 'stringi' functions might differ. This packages aims at providing
    a solution to the use case of unwanted dependencies on the one hand but the need
    for streamlined text processing on the other. The packages' functions are solely
    based on wrapping base R functions into 'stringr'/'stringi' like function names.
    Along the way it adds one or two extra functions and last but not least provides
    all functions as generics, therefore allowing for adding methods for other text
    structures besides plain character vectors.",2021-01-25,Peter Meissner,https://github.com/petermeissner/stringb,TRUE,https://github.com/petermeissner/stringb,17667,26,2021-01-27T05:22:38Z,679.5
stringdist,"Implements an approximate string matching version of R's native
    'match' function. Also offers fuzzy text search based on various string
     distance measures. Can calculate various string distances based on edits
    (Damerau-Levenshtein, Hamming, Levenshtein, optimal sting alignment), qgrams (q-
    gram, cosine, jaccard distance) or heuristic metrics (Jaro, Jaro-Winkler). An
    implementation of soundex is provided as well. Distances can be computed between
    character vectors while taking proper care of encoding or between integer
    vectors representing generic sequences. This package is built for speed and
    runs in parallel by using 'openMP'. An API for C or C++ is exposed as well.
    Reference: MPJ van der Loo (2014) <doi:10.32614/RJ-2014-011>.",2021-07-28,Mark van der Loo,https://github.com/markvanderloo/stringdist,TRUE,https://github.com/markvanderloo/stringdist,1330996,251,2021-07-28T13:51:10Z,5302.772908366534
stringfish,"Provides an extendable, performant and multithreaded 'alt-string' implementation backed by 'C++' vectors and strings.",2021-07-24,Travers Ching,https://github.com/traversc/stringfish,TRUE,https://github.com/traversc/stringfish,66097,38,2021-07-24T07:03:12Z,1739.3947368421052
stringi,"A multitude of character string/text/natural language
    processing tools: pattern searching (e.g., with 'Java'-like regular
    expressions or the 'Unicode' collation algorithm), random string generation,
    case mapping, string transliteration, concatenation, sorting, padding,
    wrapping, Unicode normalisation, date-time formatting and parsing,
    and many more. They are fast, consistent, convenient, and -
    thanks to 'ICU' (International Components for Unicode) -
    portable across all locales and platforms.",2021-08-25,Marek Gagolewski,"https://stringi.gagolewski.com/ http://site.icu-project.org/
https://www.unicode.org/",TRUE,https://github.com/gagolews/stringi,25781897,238,2021-09-03T00:34:10Z,108327.29831932773
stringr,"A consistent, simple and easy to use set of
    wrappers around the fantastic 'stringi' package. All function and
    argument names (and positions) are consistent, all functions deal with
    ""NA""'s and zero length vectors in the same way, and the output from
    one function is easy to feed into the input of another.",2019-02-10,Hadley Wickham,"http://stringr.tidyverse.org, https://github.com/tidyverse/stringr",TRUE,https://github.com/tidyverse/stringr,25112741,430,2021-08-03T18:18:01Z,58401.72325581395
stringx,"English is the native language for only 5% of the World population.
    Also, only 17% of us can understand this text. Moreover, the Latin alphabet
    is the main one for merely 36% of the total.
    The early computer era, now a very long time ago, was dominated by the US.
    Due to the proliferation of the internet, smartphones, social media, and
    other technologies and communication platforms, this is no longer the case.
    This package replaces base R string functions (such as grep(),
    tolower(), sprintf(), and strptime()) with ones that fully
    support the Unicode standards related to natural language and
    date-time processing. It also fixes some long-standing inconsistencies,
    and introduces some new, useful features.
    Thanks to 'ICU' (International Components for Unicode) and 'stringi',
    they are fast, reliable, and portable across different platforms.",2021-09-03,Marek Gagolewski,https://stringx.gagolewski.com/,TRUE,https://github.com/gagolews/stringx,270,13,2021-09-03T00:26:51Z,20.76923076923077
strm,"Implements a spatio-temporal regression model based on Chi, G. and Zhu, J. (2019) Spatial Regression Models for the Social Sciences <isbn:9781544302072>. The approach here fits a spatial error model while incorporating a temporally lagged response variable and temporally lagged explanatory variables. This package builds on the errorsarlm() function from the spatialreg package.",2021-05-05,Maria Kamenetsky,NA,TRUE,https://github.com/mkamenet3/strm,3586,0,2021-05-05T01:56:27Z,NA
strucchangeRcpp,"A fast implementation with additional experimental features for
             testing, monitoring and dating structural changes in (linear)
             regression models. 'strucchangeRcpp' features tests/methods from
	     the generalized fluctuation test framework as well as from
	     the F test (Chow test) framework. This includes methods to
             fit, plot and test fluctuation processes (e.g. cumulative/moving
             sum, recursive/moving estimates) and F statistics, respectively.
             These methods are described in Zeileis et al. (2002)
             <doi:10.18637/jss.v007.i02>.
             Finally, the breakpoints in regression models with structural
             changes can be estimated together with confidence intervals,
             and their magnitude as well as the model fit can be evaluated
             using a variety of statistical measures.",2021-05-26,Dainius Masiliunas,https://github.com/bfast2/strucchangeRcpp/,TRUE,https://github.com/bfast2/strucchangercpp,6025,3,2021-05-24T22:18:40Z,2008.3333333333333
strvalidator,"An open source platform for validation and process control.
    Tools to analyze data from internal validation of forensic short tandem
    repeat (STR) kits are provided. The tools are developed to provide
    the necessary data to conform with guidelines for internal validation
    issued by the European Network of Forensic Science Institutes (ENFSI)
    DNA Working Group, and the Scientific Working Group on DNA Analysis Methods
    (SWGDAM). A front-end graphical user interface is provided.
    More information about each function can be found in the
    respective help documentation.",2020-07-10,Oskar Hansson,https://sites.google.com/site/forensicapps/strvalidator,TRUE,https://github.com/oskarhansson/strvalidator,21924,4,2021-05-13T06:12:43Z,5481
studentlife,"Download, navigate and analyse the Student-Life dataset. 
    The Student-Life dataset contains passive and automatic sensing data 
    from the phones of a class of 48 Dartmouth college students. 
    It was collected over a 10 week term. Additionally, the dataset contains ecological 
    momentary assessment results along with pre-study and post-study mental  
    health surveys. The intended use is to assess 
    mental health, academic performance and behavioral trends. 
    The raw dataset and additional information is 
    available at <https://studentlife.cs.dartmouth.edu/>.",2020-11-01,Daniel Fryer,https://github.com/Frycast/studentlife,TRUE,https://github.com/frycast/studentlife,10400,7,2021-04-22T00:28:49Z,1485.7142857142858
STV,"Implementations of the Single Transferable Vote counting 
    system. By default, it uses the Cambridge method for surplus allocation
    and Droop method for quota calculation.  Fractional surplus allocation
    and the Hare quota are available as options.",2021-02-01,John Emerson,https://github.com/jayemerson/STV,TRUE,https://github.com/jayemerson/stv,12303,3,2021-02-01T00:12:06Z,4101
styler,"Pretty-prints R code without changing the user's formatting
    intent.",2021-07-13,Lorenz Walthert,"https://github.com/r-lib/styler, https://styler.r-lib.org",TRUE,https://github.com/r-lib/styler,1142269,505,2021-08-21T14:06:21Z,2261.918811881188
stylest,Estimates distinctiveness in speakers' (authors') style. Fits models that can be used for predicting speakers of new texts. Methods developed in Huang et al (2020) <doi:10.1017/pan.2019.49>.,2021-03-04,Leslie Huang,https://github.com/leslie-huang/stylest,TRUE,https://github.com/leslie-huang/stylest,11883,29,2021-03-04T18:17:31Z,409.7586206896552
stylo,"Supervised and unsupervised multivariate methods, supplemented by GUI and some visualizations, to perform various analyses in the field of computational stylistics, authorship attribution, etc. For further reference, see Eder et al. (2016), <https://journal.r-project.org/archive/2016/RJ-2016-007/index.html>. You are also encouraged to visit the Computational Stylistics Group's website <https://computationalstylistics.github.io/>, where a reasonable amount of information about the package and related projects are provided.",2020-12-06,Maciej Eder,https://github.com/computationalstylistics/stylo,TRUE,https://github.com/computationalstylistics/stylo,51419,106,2021-08-06T15:31:37Z,485.08490566037733
subplex,"The subplex algorithm for unconstrained optimization, developed by Tom Rowan <http://www.netlib.org/opt/subplex.tgz>.",2020-02-23,Aaron A. King,https://github.com/kingaa/subplex/,TRUE,https://github.com/kingaa/subplex,169073,4,2021-09-01T08:14:30Z,42268.25
SubtypeDrug,"A systematic biology tool was developed to prioritize cancer subtype-specific drugs by integrating genetic perturbation, drug action, biological pathway, and cancer subtype. 
    The capabilities of this tool include inferring patient-specific subpathway activity profiles in the context of gene expression profiles with subtype labels, calculating differentially 
    expressed subpathways based on cultured human cells treated with drugs in the 'cMap' (connectivity map) database, prioritizing cancer subtype specific drugs according to drug-disease 
    reverse association score based on subpathway, and visualization of results (Castelo (2013) <doi:10.1186/1471-2105-14-7>; Han et al (2019) <doi:10.1093/bioinformatics/btz894>; Lamb and Justin (2006) <doi:10.1126/science.1132939>). Please cite using <doi:10.1093/bioinformatics/btab011>.",2021-05-17,Xudong Han,NA,TRUE,https://github.com/hanjunwei-lab/subtypedrug,3903,3,2021-01-22T10:28:55Z,1301
sudachir,"Interface to 'Sudachi' <https://github.com/WorksApplications/Sudachi>,
    a Japanese morphological analyzer. This is a port of what is available in Python.",2020-11-10,Shinya Uryu,https://github.com/uribo/sudachir,TRUE,https://github.com/uribo/sudachir,3157,5,2021-04-15T00:30:52Z,631.4
suddengains,"Identify sudden gains based on the three criteria outlined by Tang and DeRubeis (1999) <doi:10.1037/0022-006X.67.6.894> to a selection of repeated measures. Sudden losses, defined as the opposite of sudden gains can also be identified. Two different datasets can be created, one including all sudden gains/losses and one including one selected sudden gain/loss for each case. It can extract scores around sudden gains/losses. It can plot the average change around sudden gains/losses and trajectories of individual cases.",2020-05-22,Milan Wiedemann,https://milanwiedemann.github.io/suddengains/,TRUE,https://github.com/milanwiedemann/suddengains,12828,4,2021-07-27T11:32:41Z,3207
sugarbag,"Create a hexagon tile map display from spatial polygons. Each 
    polygon is represented by a hexagon tile, placed as close to it's original
    centroid as possible, with a focus on maintaining spatial relationship to
    a focal point. Developed to aid visualisation and analysis of spatial 
    distributions across Australia, which can be challenging due to the 
    concentration of the population on the coast and wide open interior.",2020-10-26,Stephanie Kobakian,"https://srkobakian.github.io/sugarbag/,
https://github.com/srkobakian/sugarbag",TRUE,https://github.com/srkobakian/sugarbag,9867,29,2020-10-26T03:57:55Z,340.2413793103448
sugrrants,"Provides 'ggplot2' graphics for analysing time
    series data. It aims to fit into the 'tidyverse' and grammar of
    graphics framework for handling temporal data.",2020-10-05,Earo Wang,https://pkg.earo.me/sugrrants/,TRUE,https://github.com/earowang/sugrrants,33747,74,2020-10-03T07:00:34Z,456.0405405405405
summariser,"Functions to speed up the exploratory analysis of simple
    datasets using 'dplyr'. Functions are provided to do the 
    common tasks of calculating confidence intervals.",2020-03-30,Conor Neilson,https://github.com/condwanaland/summariser,TRUE,https://github.com/condwanaland/summariser,18588,0,2021-08-29T04:42:25Z,NA
summarytools,"Data frame summaries, cross-tabulations,
  weight-enabled frequency tables and common descriptive
  (univariate) statistics in concise tables available in a
  variety of formats (plain ASCII, Markdown and HTML). A good 
  point-of-entry for exploring data, both for experienced
  and new R users.",2021-07-28,Dominic Comtois,https://github.com/dcomtois/summarytools,TRUE,https://github.com/dcomtois/summarytools,344400,409,2021-08-11T04:56:57Z,842.0537897310513
SUMMER,"Provides methods for spatial and spatio-temporal smoothing of demographic and health indicators using survey data, with particular focus on estimating and projecting under-five mortality rates, described in Mercer et al. (2015) <doi:10.1214/15-AOAS872>, Li et al. (2019) <doi:10.1371/journal.pone.0210645> and Li et al. (2020) <arXiv:2007.05117>. ",2021-07-06,Zehang R Li,https://github.com/richardli/SUMMER,TRUE,https://github.com/richardli/summer,16489,12,2021-08-24T16:17:41Z,1374.0833333333333
sunburstR,"Make interactive 'd3.js' sequence sunburst diagrams in R with the
    convenience and infrastructure of an 'htmlwidget'.",2020-10-08,Mike Bostock,https://github.com/timelyportfolio/sunburstR,TRUE,https://github.com/timelyportfolio/sunburstr,274871,187,2020-10-08T01:39:44Z,1469.8983957219252
sundialr,"Provides a way to call the functions in 'SUNDIALS' C ODE solving library (<https://computing.llnl.gov/projects/sundials>). Currently the serial version of ODE solver, 'CVODE', sensitivity calculator 'CVODES' and differential algebraic solver 'IDA' from the 'SUNDIALS' library are implemented. The package requires ODE to be written as an 'R' or 'Rcpp' function and does not require the 'SUNDIALS' library to be installed on the local machine.",2021-05-16,Satyaprakash Nayak,https://github.com/sn248/sundialr,TRUE,https://github.com/sn248/sundialr,14417,8,2021-05-16T23:59:22Z,1802.125
SUNGEO,Tools for integrating spatially-misaligned GIS datasets. Part of the Sub-National Geospatial Data Archive System (<https://www.nsf.gov/awardsearch/showAward?AWD_ID=1925693&HistoricalAwards=false>).,2021-08-12,Jason Byers,<https://github.com/zhukovyuri/SUNGEO>,TRUE,https://github.com/zhukovyuri/sungeo,5089,1,2021-08-12T12:21:41Z,5089
supclust,"Methodology for supervised grouping aka ""clustering"" of
   potentially many predictor variables, such as genes etc, implementing
   algorithms 'PELORA' and 'WILMA'.",2020-10-15,Marcel Dettling <marcel.dettling@zhaw.ch> and Martin Maechler,https://github.com/mmaechler/supclust,TRUE,https://github.com/mmaechler/supclust,19040,0,2020-10-15T10:58:22Z,NA
superb,"
    Computes standard error and confidence interval of various descriptive statistics under 
    various designs and sampling schemes. The main function, superbPlot(), can either return a plot 
    or a dataframe with the statistic and its precision interval so that other plotting package
    can be used. See Cousineau (2017) <doi:10.5709/acp-0214-z> for a review or Cousineau (2005)
    <doi:10.20982/tqmp.01.1.p042>, Morey (2008) <doi:10.20982/tqmp.04.2.p061>, Baguley (2012)
    <doi:10.3758/s13428-011-0123-7>, Cousineau & Laurencelle (2016) <doi:10.1037/met0000055>,
    Cousineau & O'Brien (2014) <doi:10.3758/s13428-013-0441-z>, Calderini & Harding 
    <doi:10.20982/tqmp.15.1.p001>.",2021-06-23,Denis Cousineau,https://dcousin3.github.io/superb/,TRUE,https://github.com/dcousin3/superb,2147,12,2021-08-26T10:37:48Z,178.91666666666666
SuperExactTest,"Identification of sets of objects with shared features is a common operation in all disciplines. Analysis of intersections among multiple sets is fundamental for in-depth understanding of their complex relationships. This package implements a theoretical framework for efficient computation of statistical distributions of multi-set intersections based upon combinatorial theory, and provides multiple scalable techniques for visualizing the intersection statistics. The statistical algorithm behind this package was published in Wang et al. (2015) <doi:10.1038/srep16923>.",2019-06-21,Minghui Wang,https://github.com/mw201608/SuperExactTest/,TRUE,https://github.com/mw201608/superexacttest,18527,12,2021-08-29T04:35:47Z,1543.9166666666667
SuperLearner,"Implements the super learner prediction method and contains a
    library of prediction algorithms to be used in the super learner.",2021-05-10,Eric Polley,https://github.com/ecpolley/SuperLearner,TRUE,https://github.com/ecpolley/superlearner,106236,215,2021-03-28T19:31:11Z,494.1209302325581
supernova,"Produces ANOVA tables in the format used by Judd, McClelland, and Ryan 
    (2017, ISBN: 978-1138819832) in their introductory textbook, Data Analysis. This includes 
    proportional reduction in error and formatting to improve ease the transition between the book 
    and R.",2021-07-25,Ji Son,https://github.com/UCLATALL/supernova,TRUE,https://github.com/uclatall/supernova,18310,60,2021-07-24T23:24:01Z,305.1666666666667
superpc,"Does prediction in the case of a censored survival outcome, or a regression outcome, using the ""supervised principal component"" approach. 'Superpc' is especially useful for high-dimensional data when the number of features p dominates the number of samples n (p >> n paradigm), as generated, for instance, by high-throughput technologies.",2020-10-19,Jean-Eudes Dazard,"http://www-stat.stanford.edu/~tibs/superpc,
https://github.com/jedazard/superpc",TRUE,https://github.com/jedazard/superpc,204526,2,2020-10-19T18:42:30Z,102263
SuperpixelImageSegmentation,"Image Segmentation using Superpixels, Affinity Propagation and Kmeans Clustering. The R code is based primarily on the article ""Image Segmentation using SLIC Superpixels and Affinity Propagation Clustering, Bao Zhou, International Journal of Science and Research (IJSR), 2013"" <https://www.ijsr.net/archive/v4i4/SUB152869.pdf>. ",2021-05-04,Lampros Mouselimis,https://github.com/mlampros/SuperpixelImageSegmentation,TRUE,https://github.com/mlampros/superpixelimagesegmentation,13751,9,2021-05-04T19:26:26Z,1527.888888888889
Superpower,"Functions to perform simulations of ANOVA designs of up to three factors. Calculates the observed power and average observed effect size for all main effects and interactions in the ANOVA, and all simple comparisons between conditions. Includes functions for analytic power calculations and additional helper functions that compute effect sizes for ANOVA designs, observed error rates in the simulations, and functions to plot power curves. Please see Lakens, D., & Caldwell, A. R. (2021). ""Simulation-Based Power Analysis for Factorial Analysis of Variance Designs"". <doi:10.1177/2515245920951503>.",2021-05-25,Aaron Caldwell,https://aaroncaldwell.us/SuperpowerBook/,TRUE,https://github.com/arcaldwell49/superpower,11966,35,2021-05-25T00:22:17Z,341.8857142857143
suppdata,"Downloads data supplementary materials from manuscripts,
    using papers' DOIs as references. Facilitates open, reproducible
    research workflows: scientists re-analyzing published datasets can
    work with them as easily as if they were stored on their own
    computer, and others can track their analysis workflow
    painlessly. The main function suppdata() returns a (temporary)
    location on the user's computer where the file is stored, making
    it simple to use suppdata() with standard functions like
    read.csv().",2020-12-17,William D. Pearse,"https://docs.ropensci.org/suppdata/,
https://github.com/ropensci/suppdata/",TRUE,https://github.com/ropensci/suppdata,11742,29,2020-12-14T21:40:25Z,404.8965517241379
supreme,"A modeling tool helping users better structure 'Shiny'
  applications developed with 'Shiny' modules. Users are able to: 1. Visualize
  relationship of modules in existing applications 2. Design new applications
  from scratch.",2020-07-08,Metin Yazici,https://strboul.github.io/supreme/,TRUE,https://github.com/strboul/supreme,4384,52,2021-06-05T12:40:31Z,84.3076923076923
sureLDA,A statistical learning method to simultaneously predict a range of target phenotypes using codified and natural language processing (NLP)-derived Electronic Health Record (EHR) data. See Ahuja et al (2020) JAMIA <doi:10.1093/jamia/ocaa079> for details.,2020-11-10,Yuri Ahuja,https://github.com/celehs/sureLDA,TRUE,https://github.com/celehs/surelda,3124,3,2021-02-23T06:58:59Z,1041.3333333333333
surf,"Estimation of gross flows under non-response and complex sampling designs, using Gutiérrez, Nascimento Silva and Trujillo (2014) <https://www150.statcan.gc.ca/n1/pub/12-001-x/2014002/article/14113-eng.pdf> complex sampling extension of the non-response model developed by Stasny (1987) <https://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/some-markov-chain-models-for-nonresponse-in-estimating-gross-labor-force-flows.pdf>. 
    It estimates the gross flows process under non-response by modelling the observable cross-tabulation counts as a two-stage Markov Chain process, combining (1) the unobservable Markov Chain describing the transition of states; and (2) the non-response process, given by the initial response probabilities and the response/non-response transition probabilities.",2021-04-06,Guilherme Jacob,NA,TRUE,https://github.com/guilhermejacob/surf,1547,1,2021-04-16T13:52:15Z,1547
SurfaceTortoise,"Create sampling designs using the surface reconstruction algorithm.
  Original method by: Olsson, D. 2002. A method to optimize soil sampling from 
  ancillary data. Poster presenterad at: NJF seminar no. 336, 
  Implementation of Precision Farming in Practical Agriculture, 10-12 
  June 2002, Skara, Sweden.",2020-10-02,Kristin Piikki,https://CRAN.R-project.org/package=SurfaceTortoise,TRUE,https://github.com/soilmapper/surfacetortoise,12897,0,2020-10-30T12:29:27Z,NA
surveydata,"Data obtained from surveys contains information not only about the
    survey responses, but also the survey metadata, e.g. the original survey
    questions and the answer options. The 'surveydata' package makes it easy to
    keep track of this metadata, and to easily extract columns with
    specific questions.",2020-09-15,Andrie de Vries,"https://github.com/andrie/surveydata,
https://andrie.github.io/surveydata",TRUE,https://github.com/andrie/surveydata,22947,19,2020-09-16T06:53:41Z,1207.7368421052631
surveysd,Calculate point estimates and their standard errors in complex household surveys using bootstrap replicates. Bootstrapping considers survey design with a rotating panel. A comprehensive description of the methodology can be found under <https://statistikat.github.io/surveysd/articles/methodology.html>.,2020-12-10,Johannes Gussenbauer,https://github.com/statistikat/surveysd,TRUE,https://github.com/statistikat/surveysd,12713,5,2020-12-18T10:31:10Z,2542.6
survHE,"Contains a suite of functions for survival analysis in health economics. 
    These can be used to run survival models under a frequentist (based on maximum likelihood) 
    or a Bayesian approach (both based on Integrated Nested Laplace Approximation or Hamiltonian 
    Monte Carlo). The user can specify a set of parametric models using a common notation and 
    select the preferred mode of inference. The results can also be post-processed to produce 
    probabilistic sensitivity analysis and can be used to export the output to an Excel 
    file (e.g. for a Markov model, as often done by modellers and practitioners). <doi:10.18637/jss.v095.i14>.",2021-02-09,Gianluca Baio,"https://github.com/giabaio/survHE,
http://www.statistica.it/gianluca/",TRUE,https://github.com/giabaio/survhe,19756,29,2021-02-08T23:27:11Z,681.2413793103449
survival,"Contains the core survival analysis routines, including
	     definition of Surv objects, 
	     Kaplan-Meier and Aalen-Johansen (multi-state) curves, Cox models,
	     and parametric accelerated failure time models.",2021-08-24,Terry M Therneau,https://github.com/therneau/survival,TRUE,https://github.com/therneau/survival,4734717,192,2021-08-23T15:09:01Z,24659.984375
survivalmodels,"Implementations of classical and machine learning models for survival analysis, including deep neural networks via 'keras' and 'tensorflow'. Each model includes a separated fit and predict interface with consistent prediction types for predicting risk, survival probabilities, or survival distributions with 'distr6' <https://CRAN.R-project.org/package=distr6>. Models are either implemented from 'Python' via 'reticulate' <https://CRAN.R-project.org/package=reticulate>, from code in GitHub packages, or novel implementations using 'Rcpp' <https://CRAN.R-project.org/package=Rcpp>. Novel machine learning survival models wil be included in the package in near-future updates. Neural networks are implemented from the 'Python' package 'pycox' <https://github.com/havakv/pycox> and are detailed by Kvamme et al. (2019) <https://jmlr.org/papers/v20/18-424.html>. The 'Akritas' estimator is defined in Akritas (1994) <doi:10.1214/aos/1176325630>. 'DNNSurv' is defined in Zhao and Feng (2020) <arXiv:1908.02337>.",2021-04-17,Raphael Sonabend,https://github.com/RaphaelS1/survivalmodels/,TRUE,https://github.com/raphaels1/survivalmodels,7941,12,2021-04-19T09:50:43Z,661.75
survivoR,"Several datasets which detail the results and events of each season of Survivor. This includes 
  details on the cast, voting history, immunity and reward challenges, jury votes and viewers. This data is 
  useful for practicing data wrangling, graph analytics and analysing how each season of Survivor played out. 
  Includes 'ggplot2' scales and colour palettes for visualisation.",2021-05-11,Daniel Oehm,https://github.com/doehm/survivoR,TRUE,https://github.com/doehm/survivor,2199,15,2021-07-18T03:10:27Z,146.6
SurvMetrics,"An implementation of popular evaluation metrics that are commonly used in survival prediction 
  including Concordance Index, Brier Score, Integrated Brier Score, 
  Integrated Square Error, Integrated Absolute Error and Mean Absolute Error.
  For a detailed information, see (Ishwaran H, Kogalur UB, Blackstone EH and Lauer MS (2008) <doi:10.1214/08-AOAS169>) and
  (Moradian H, Larocque D and Bellavance F (2017) <doi:10.1007/s10985-016-9372-1>) for different evaluation metrics.",2021-08-10,Hanpu Zhou,https://github.com/skyee1/SurvMetrics,TRUE,https://github.com/skyee1/survmetrics,275,0,2021-08-10T06:25:57Z,NA
survminer,"Contains the function 'ggsurvplot()' for drawing easily beautiful
    and 'ready-to-publish' survival curves with the 'number at risk' table
    and 'censoring count plot'. Other functions are also available to plot 
    adjusted curves for `Cox` model and to visually examine 'Cox' model assumptions.",2021-03-09,Alboukadel Kassambara,https://rpkgs.datanovia.com/survminer/index.html,TRUE,https://github.com/kassambara/survminer,680898,351,2021-03-09T08:36:17Z,1939.8803418803418
survMS,"Package enables the data simulation from different survival models (Cox, AFT, and AH models). The simulated data will have various levels of complexity according to the survival model considered. The implemented methods for the Cox model are described in Ralf Bender, Thomas Augustin, Maria Blettner (2004) <doi:10.1002/sim.2059>.",2021-04-16,Mathilde Sautreuil,https://github.com/mathildesautreuil/survMS/,TRUE,https://github.com/mathildesautreuil/survms,1433,3,2021-04-21T09:09:40Z,477.6666666666667
survParamSim,"Perform survival simulation with parametric survival model generated from 'survreg' function in 'survival' package.
    In each simulation coefficients are resampled from variance-covariance matrix of parameter estimates to 
    capture uncertainty in model parameters.
    Prediction intervals of Kaplan-Meier estimates and hazard ratio of treatment effect can be further calculated using simulated survival data.",2021-04-26,Kenta Yoshida,https://github.com/yoshidk6/survParamSim,TRUE,https://github.com/yoshidk6/survparamsim,9688,1,2021-08-10T16:36:54Z,9688
survPen,"Fits hazard and excess hazard models with multidimensional penalized splines allowing for 
        time-dependent effects, non-linear effects and interactions between several continuous covariates. In survival and net survival analysis, in addition to modelling the effect of time (via the baseline hazard), one has often to deal with several continuous covariates and model their functional forms, their time-dependent effects, and their interactions. Model specification becomes therefore a complex problem and penalized regression splines represent an appealing solution to that problem as splines offer the required flexibility while penalization limits overfitting issues. Current implementations of penalized survival models can be slow or unstable and sometimes lack some key features like taking into account expected mortality to provide net survival and excess hazard estimates. In contrast, survPen provides an automated, fast, and stable implementation (thanks to explicit calculation of the derivatives of the likelihood) and offers a unified framework for 
        multidimensional penalized hazard and excess hazard models. survPen may be of interest to those who 1) analyse any kind of time-to-event data: mortality, disease relapse, machinery breakdown, unemployment, etc 2) wish to describe the associated hazard and to understand which predictors impact its dynamics. 
	See Fauvernier et al. (2019a) <doi:10.21105/joss.01434> for an overview of the package and Fauvernier et al. (2019b) <doi:10.1111/rssc.12368> for the method.",2020-09-25,Mathieu Fauvernier,https://github.com/fauvernierma/survPen,TRUE,https://github.com/fauvernierma/survpen,14936,3,2020-09-25T12:50:21Z,4978.666666666667
susieR,"Implements methods for variable selection in linear
    regression based on the ""Sum of Single Effects"" (SuSiE) model, as
    described in Wang et al (2020) <DOI:10.1101/501114>. These methods
    provide simple summaries, called ""Credible Sets"", for accurately
    quantifying uncertainty in which variables should be selected.
    The methods are motivated by genetic fine-mapping applications,
    and are particularly well-suited to settings where variables are
    highly correlated and detectable effects are sparse. The fitting
    algorithm, a Bayesian analogue of stepwise selection methods
    called ""Iterative Bayesian Stepwise Selection"" (IBSS), is simple
    and fast, allowing the SuSiE model be fit to large data sets
    (thousands of samples and hundreds of thousands of variables).",2021-06-14,Peter Carbonetto,https://github.com/stephenslab/susieR,TRUE,https://github.com/stephenslab/susier,2932,89,2021-08-31T15:51:10Z,32.943820224719104
svDialogs,"Quickly construct standard dialog boxes for your GUI, including 
  message boxes, input boxes, list, file or directory selection, ... In case R
  cannot display GUI dialog boxes, a simpler command line version of these
  interactive elements is also provided as fallback solution.",2021-04-17,Philippe Grosjean,"https://github.com/SciViews/svDialogs,
https://www.sciviews.org/svDialogs/",TRUE,https://github.com/sciviews/svdialogs,115511,3,2021-04-16T17:21:28Z,38503.666666666664
svglite,"A graphics device for R that produces 'Scalable Vector Graphics'.
  'svglite' is a fork of the older 'RSvgDevice' package.",2021-02-20,Thomas Lin Pedersen,"https://svglite.r-lib.org, https://github.com/r-lib/svglite",TRUE,https://github.com/r-lib/svglite,1763869,148,2021-07-23T12:18:31Z,11918.033783783783
svGUI,"The 'SciViews' 'svGUI' package eases the management of Graphical
  User Interfaces (GUI) in R. It is independent from any particular GUI widgets
  ('Tk', 'Gtk2', native, ...). It centralizes info about GUI elements currently
  used, and it dispatches GUI calls to the particular toolkits in use in
  function of the context (is R run at the terminal, within a 'Tk' application,
  a HTML page?).",2021-04-16,Philippe Grosjean,"https://github.com/SciViews/svGUI, https://www.sciviews.org/svGUI/",TRUE,https://github.com/sciviews/svgui,100765,1,2021-04-16T15:41:05Z,100765
svMisc,"Miscellaneous functions for 'SciViews' or general use: manage a
  temporary environment attached to the search path for temporary variables you
  do not want to save() or load(), test if 'Aqua', 'Mac', 'Win', ... Show
  progress bar, etc.",2021-04-16,Philippe Grosjean,"https://github.com/SciViews/svMisc,
https://www.sciviews.org/svMisc/",TRUE,https://github.com/sciviews/svmisc,79793,0,2021-08-05T11:44:54Z,NA
svSocket,Implements a socket server allowing to connect clients to R.,2021-05-05,Philippe Grosjean,"https://github.com/SciViews/svSocket,
https://www.sciviews.org/svSocket/",TRUE,https://github.com/sciviews/svsocket,26498,8,2021-05-05T08:02:48Z,3312.25
svUnit,A complete unit test system and functions to implement its GUI part.,2021-04-19,Philippe Grosjean,"https://github.com/SciViews/svUnit,
https://www.sciviews.org/svUnit/",TRUE,https://github.com/sciviews/svunit,161568,1,2021-04-18T19:34:56Z,161568
swag,"An algorithm that trains a meta-learning procedure that combines 
    screening and wrapper methods to find a set of extremely low-dimensional attribute 
    combinations. This package works on top of the 'caret' package and proceeds in a 
    forward-step manner. More specifically, it builds and tests learners starting 
    from very few attributes until it includes a maximal number of attributes by 
    increasing the number of attributes at each step. Hence, for each fixed number
    of attributes, the algorithm tests various (randomly selected) learners and 
    picks those with the best performance in terms of training error. Throughout,
    the algorithm uses the information coming from the best learners at the previous
    step to build and test learners in the following step. In the end, it outputs
    a set of strong low-dimensional learners.",2020-11-10,Samuel Orso,https://github.com/SMAC-Group/SWAG-R-Package/,TRUE,https://github.com/smac-group/swag-r-package,2275,0,2021-06-30T09:32:09Z,NA
swagger,"A collection of 'HTML', 'JavaScript', and 'CSS' assets that
  dynamically generate beautiful documentation from a 'Swagger' compliant API:
  <https://swagger.io/specification/>.",2020-10-02,Barret Schloerke,https://github.com/rstudio/swagger,TRUE,https://github.com/rstudio/swagger,1575554,50,2020-10-02T21:38:13Z,31511.08
sweidnumbr,"Structural handling of identity numbers used in the Swedish
    administration such as personal identity numbers ('personnummer') and
    organizational identity numbers ('organisationsnummer').",2020-03-29,Mans Magnusson and Erik Bulow,https://github.com/rOpenGov/sweidnumbr/,TRUE,https://github.com/ropengov/sweidnumbr,18066,7,2020-11-23T20:55:36Z,2580.8571428571427
swephR,"The Swiss Ephemeris (version 2.08) is a high precision ephemeris based upon the
    DE431 ephemerides from NASA's JPL. It covers the time range 13201 BCE to
    17191 CE. This package uses the semi-analytic theory by Steve Moshier.
    For faster and more accurate calculations, the compressed Swiss Ephemeris
    data is available in the 'swephRdata' package. To access this data package,
    run 'install.packages(""swephRdata"", repos = ""https://rstub.github.io/drat/"",
    type = ""source"")'. The size of the 'swephRdata' package is approximately
    115 MB. The user can also use the original JPL DE431 data.",2019-08-28,Ralf Stubner,"https://github.com/rstub/swephR/, https://rstub.github.io/swephR/,
http://www.astro.com/swisseph/",TRUE,https://github.com/rstub/swephr,11610,8,2021-04-10T09:10:15Z,1451.25
swfscAirDAS,"Process and summarize aerial survey 'DAS' data (AirDAS) 
    <https://swfsc-publications.fisheries.noaa.gov/publications/TM/SWFSC/NOAA-TM-NMFS-SWFSC-185.PDF>
    collected using an aerial survey program from the 
    Southwest Fisheries Science Center (SWFSC) 
    <https://www.fisheries.noaa.gov/west-coast/science-data/california-current-marine-mammal-assessment-program>.
    PDF files detailing the relevant AirDAS data formats are included in this package.",2021-01-10,Sam Woodman,"https://smwoodman.github.io/swfscAirDAS/,
https://github.com/smwoodman/swfscAirDAS/",TRUE,https://github.com/smwoodman/swfscairdas,4295,0,2021-03-22T19:13:15Z,NA
swfscDAS,"Process and summarize shipboard 
    'DAS' <https://swfsc-publications.fisheries.noaa.gov/publications/TM/SWFSC/NOAA-TM-NMFS-SWFSC-305.PDF> data
    produced by the Southwest Fisheries Science Center (SWFSC) program 'WinCruz' 
    <https://www.fisheries.noaa.gov/west-coast/science-data/california-current-marine-mammal-assessment-program>.
    This package standardizes and streamlines basic DAS data processing,
    and includes a PDF with the DAS data format requirements.",2021-05-27,Sam Woodman,"https://smwoodman.github.io/swfscDAS/,
https://github.com/smwoodman/swfscDAS/",TRUE,https://github.com/smwoodman/swfscdas,5534,1,2021-08-10T23:03:45Z,5534
swfscMisc,"Collection of conversion, analytical, geodesic, mapping, and
    plotting functions. Used to support packages and code written by
    researchers at the Southwest Fisheries Science Center of the National
    Oceanic and Atmospheric Administration.",2021-05-21,Eric Archer,https://github.com/EricArcher/swfscMisc,TRUE,https://github.com/ericarcher/swfscmisc,35834,0,2021-06-04T02:03:04Z,NA
SWIM,"An efficient sensitivity analysis for stochastic models based on 
    Monte Carlo samples. Provides weights on simulated scenarios from a 
    stochastic model, such that stressed random variables fulfil given 
    probabilistic constraints (e.g. specified values for risk measures), 
    under the new scenario weights. Scenario weights are selected by 
    constrained minimisation of the relative entropy to the baseline model. 
    The 'SWIM' package is based on Pesenti S.M, Millossovich P., Tsanakas A. 
    (2019) ""Reverse Sensitivity Testing: What does it take to break the model"", 
    <openaccess.city.ac.uk/id/eprint/18896/>.",2020-09-22,Silvana M. Pesenti,"https://github.com/spesenti/SWIM,
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3515274,
https://utstat.toronto.edu/pesenti/?page_id=138",TRUE,https://github.com/spesenti/swim,10989,4,2020-09-21T18:06:39Z,2747.25
swissdd,"Builds upon the real time data service as well as the archive for national votes <https://opendata.swiss/api/3/action/package_show?id=echtzeitdaten-am-abstimmungstag-zu-eidgenoessischen-abstimmungsvorlagen> and cantonal votes <https://opendata.swiss/api/3/action/package_show?id=echtzeitdaten-am-abstimmungstag-zu-kantonalen-abstimmungsvorlagen>. It brings the results of Swiss popular votes, aggregated at the geographical level of choice, into R. Additionally, it allows to retrieve data from the Swissvotes-Database, one of the most comprehensive data platforms on Swiss referendums and initiatives <https://swissvotes.ch/page/dataset/swissvotes_dataset.csv>. ",2021-07-17,Thomas Lo Russo,https://github.com/politanch/swissdd,TRUE,https://github.com/politanch/swissdd,10756,13,2021-08-31T12:39:38Z,827.3846153846154
swissparl,"Retrieves the most important data on parliamentary activities of the Swiss Federal Assembly via 
    an open, machine-readable interface (see <https://ws.parlament.ch/odata.svc/>). ",2020-04-14,David Zumbach,https://www.parlament.ch/en/services/open-data-webservices,TRUE,https://github.com/zumbov2/swissparl,10561,18,2021-04-16T07:26:21Z,586.7222222222222
SWMPr,"Tools for retrieving, organizing, and analyzing environmental
    data from the System Wide Monitoring Program of the National Estuarine
    Research Reserve System <http://cdmo.baruch.sc.edu/>. These tools
    address common challenges associated with continuous time series data
    for environmental decision making.",2021-09-02,Marcus W. Beck,NA,TRUE,https://github.com/fawda123/swmpr,19202,9,2021-09-03T11:41:08Z,2133.5555555555557
SWMPrExtension,"Tools for performing routine analysis and plotting tasks with environmental
    data from the System Wide Monitoring Program of the National Estuarine
    Research Reserve System <http://cdmo.baruch.sc.edu/>. This package builds
    on the functionality of the SWMPr package <https://cran.r-project.org/package=SWMPr>,
    which is used to retrieve and organize the data. The combined set of tools
    address common challenges associated with continuous time series data
    for environmental decision making, and are intended for use in annual reporting activities.
    References:
    Beck, Marcus W. (2016) <ISSN 2073-4859><https://journal.r-project.org/archive/2016-1/beck.pdf>
    Rudis, Bob (2014) <https://rud.is/b/2014/11/16/moving-the-earth-well-alaska-hawaii-with-r/>.
    United States Environmental Protection Agency (2015) <https://cfpub.epa.gov/si/si_public_record_Report.cfm?Lab=OWOW&dirEntryId=327030>.
    United States Environmental Protection Agency (2012) <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.646.1973&rep=rep1&type=pdf>.",2020-08-30,Dave Eslinger,NA,TRUE,https://github.com/noaa-ocm/swmprextension,15742,4,2021-07-16T15:37:30Z,3935.5
sylly,"Provides the hyphenation algorithm used for 'TeX'/'LaTeX' and similar software, as proposed by Liang (1983, <https://tug.org/docs/liang/>). Mainly contains the
                    function hyphen() to be used for hyphenation/syllable counting of text objects. It was originally developed for and part of the 'koRpus' package, but later
                    released as a separate package so it's lighter to have this particular functionality available for other packages. Support for various languages needs be added
                    on-the-fly or by plugin packages (<https://undocumeantit.github.io/repos/>); this package does not include any language specific data. Due to some restrictions
                    on CRAN, the full package sources are only available from the project homepage. To ask for help, report bugs, request features, or discuss the development of
                    the package, please subscribe to the koRpus-dev mailing list (<http://korpusml.reaktanz.de>).",2020-09-20,Meik Michalke,https://reaktanz.de/?c=hacking&s=sylly,TRUE,https://github.com/undocumeantit/sylly,138909,1,2020-09-19T22:11:16Z,138909
symengine,"
    Provides an R interface to 'SymEngine' <https://github.com/symengine/>,
    a standalone 'C++' library for fast symbolic manipulation. The package has functionalities
    for symbolic computation like calculating exact mathematical expressions, solving
    systems of linear equations and code generation.",2020-07-06,Jialin Ma,https://github.com/symengine/symengine.R,TRUE,https://github.com/symengine/symengine.r,10681,17,2021-06-18T20:16:09Z,628.2941176470588
symSEM,"A collection of functions for symbolic computation using 'Ryacas'
  package for structural equation models. This package includes
  functions to calculate model-implied covariance (and correlation)
  matrix and sampling covariance matrix of functions of variables using
  the first-order Taylor approximation. Reference: McArdle and McDonald (1984) <doi:10.1111/j.2044-8317.1984.tb00802.x>.",2020-07-16,Mike Cheung,https://github.com/mikewlcheung/symsem,TRUE,https://github.com/mikewlcheung/symsem,4712,0,2021-07-05T01:08:50Z,NA
syn,"Generates synonyms from a given word drawing from a synonym list
    from the 'moby' project <http://moby-thesaurus.org/>.",2019-12-20,Nicholas Tierney,"http://syn.njtierney.com/, https://github.com/ropenscilabs/syn",TRUE,https://github.com/ropenscilabs/syn,7747,43,2021-07-05T07:31:21Z,180.1627906976744
synoptReg,"Set of functions to compute different types of synoptic classification methods and for analysing their effect on environmental variables. More information about the methods used in Lemus-Canovas et al. 2019 <DOI:10.1016/j.atmosres.2019.01.018>,  Martin-Vide et al. 2008 <DOI:10.5194/asr-2-99-2008>, Jenkinson and Collison 1977.",2021-04-21,Marc Lemus-Canovas,<https://lemuscanovas.github.io/synoptreg/>,TRUE,https://github.com/lemuscanovas/synoptreg,15037,5,2021-04-21T09:39:41Z,3007.4
SynthETIC,"Creation of an individual claims simulator which generates various
    features of non-life insurance claims. An initial set of test parameters,
    designed to mirror the experience of an Auto Liability portfolio, were set
    up and applied by default to generate a realistic test data set of
    individual claims (see vignette). The simulated data set then allows
    practitioners to back-test the validity of various reserving models and to
    prove and/or disprove certain actuarial assumptions made in claims
    modelling. The distributional assumptions used to generate this data set can
    be easily modified by users to match their experiences. Reference: Avanzi B,
    Taylor G, Wang M, Wong B (2020) ""SynthETIC: an individual insurance claim
    simulator with feature control"" <arXiv:2008.05693>.",2021-09-02,Melantha Wang,https://github.com/agi-lab/SynthETIC,TRUE,https://github.com/agi-lab/synthetic,4714,3,2021-08-27T09:37:52Z,1571.3333333333333
sys,"Drop-in replacements for the base system2() function with fine control
    and consistent behavior across platforms. Supports clean interruption, timeout, 
    background tasks, and streaming STDIN / STDOUT / STDERR over binary or text 
    connections. Arguments on Windows automatically get encoded and quoted to work 
    on different locales.",2020-07-23,Jeroen Ooms,https://github.com/jeroen/sys,TRUE,https://github.com/jeroen/sys,14537465,86,2020-09-10T12:37:08Z,169040.2906976744
sysfonts,"Loading system fonts and Google Fonts
    <https://fonts.google.com/> into R, in order to
    support other packages such as 'R2SWF' and 'showtext'.",2021-08-09,"Yixuan Qiu and authors/contributors of the
    included fonts. See file AUTHORS for details.",https://github.com/yixuan/sysfonts,TRUE,https://github.com/yixuan/sysfonts,671798,17,2021-08-09T10:40:52Z,39517.529411764706
systemfonts,"Provides system native access to the font catalogue. As font
    handling varies between systems it is difficult to correctly locate 
    installed fonts across different operating systems. The 'systemfonts' 
    package provides bindings to the native libraries on Windows, macOS and 
    Linux for finding font files that can then be used further by e.g. graphic
    devices. The main use is intended to be from compiled code but 'systemfonts'
    also provides access from R.",2021-05-11,Thomas Lin Pedersen,https://github.com/r-lib/systemfonts,TRUE,https://github.com/r-lib/systemfonts,3153200,70,2021-05-11T12:09:35Z,45045.71428571428
syt,"Deals with standard Young tableaux (field of combinatorics). Performs enumeration, counting, random generation, the Robinson-Schensted correspondence, and conversion to and from paths on the Young lattice. ",2021-01-16,Stéphane Laurent,https://github.com/stla/syt,TRUE,https://github.com/stla/syt,11766,2,2021-01-16T09:47:48Z,5883
syuzhet,"Extracts sentiment and sentiment-derived plot arcs
    from text using a variety of sentiment dictionaries conveniently
    packaged for consumption by R users.  Implemented dictionaries include
    ""syuzhet"" (default) developed in the Nebraska Literary Lab
    ""afinn"" developed by Finn Årup Nielsen, ""bing"" developed by Minqing Hu
    and Bing Liu, and ""nrc"" developed by Mohammad, Saif M. and Turney, Peter D.
    Applicable references are available in README.md and in the documentation
    for the ""get_sentiment"" function.  The package also provides a hack for
    implementing Stanford's coreNLP sentiment parser. The package provides
    several methods for plot arc normalization.",2020-11-24,Matthew Jockers,https://github.com/mjockers/syuzhet,TRUE,https://github.com/mjockers/syuzhet,722766,300,2021-04-12T16:34:49Z,2409.22
T4cluster,"Cluster analysis is one of the most fundamental problems in data science. We provide a variety of algorithms from clustering to the learning on the space of partitions. See Hennig, Meila, and Rocci (2016, ISBN:9781466551886) for general exposition to cluster analysis.",2021-08-16,Kisung You,https://kisungyou.com/T4cluster/,TRUE,https://github.com/kisungyou/t4cluster,4371,3,2021-08-14T02:59:37Z,1457
taber,Sometimes you need to split your data and work on the two chunks independently before bringing them back together. 'Taber' allows you to do that with its two functions.,2020-10-04,Seth Wenchel,https://github.com/restonslacker/taber,TRUE,https://github.com/restonslacker/taber,15091,4,2020-10-04T11:28:30Z,3772.75
table.glue,"Translate double and integer valued data into
    character values formatted for tabulation in manuscripts
    or other types of academic reports. ",2020-10-31,Byron Jaeger,https://github.com/bcjaeger/table.glue,TRUE,https://github.com/bcjaeger/table.glue,4013,4,2021-02-11T16:25:27Z,1003.25
table1,"Create HTML tables of descriptive statistics, as one would expect
    to see as the first table (i.e. ""Table 1"") in a medical/epidemiological journal
    article.",2021-06-06,Benjamin Rich,https://github.com/benjaminrich/table1,TRUE,https://github.com/benjaminrich/table1,102780,43,2021-08-21T06:03:04Z,2390.232558139535
tableHTML,"A tool to create and style HTML tables with CSS. These can
    be exported and used in any application that accepts HTML (e.g. 'shiny',
    'rmarkdown', 'PowerPoint'). It also provides functions to create CSS files
    (which also work with shiny).",2021-03-21,Theo Boutaris,https://github.com/LyzandeR/tableHTML,TRUE,https://github.com/lyzander/tablehtml,51274,20,2021-03-21T15:33:37Z,2563.7
tableone,"Creates 'Table 1', i.e., description of baseline patient
    characteristics, which is essential in every medical research.
    Supports both continuous and categorical variables, as well as
    p-values and standardized mean differences. Weighted data are
    supported via the 'survey' package.",2021-07-04,Kazuki Yoshida,https://github.com/kaz-yos/tableone,TRUE,https://github.com/kaz-yos/tableone,220218,159,2021-07-04T13:28:15Z,1385.0188679245282
tableschema.r,"Allows to work with 'Table Schema' (<http://specs.frictionlessdata.io/table-schema/>). 'Table Schema' is well suited for use cases around handling and validating tabular data in text formats such as 'csv', but its utility extends well beyond this core usage, towards a range of applications where data benefits from a portable schema format. The 'tableschema.r' package can load and validate any table schema descriptor, allow the creation and modification of descriptors, expose methods for reading and streaming data that conforms to a 'Table Schema' via the 'Tabular Data Resource' abstraction.",2020-03-12,Kleanthis Koupidis,https://github.com/frictionlessdata/tableschema-r,TRUE,https://github.com/frictionlessdata/tableschema-r,12960,21,2021-09-02T12:21:21Z,617.1428571428571
tablesgg,"Presentation-quality tables are displayed as plots on an R 
  graphics device.  Although there are other packages that format tables 
  for display, this package is unique in combining two features: (a) It is 
  aware of the logical structure of the table being presented, and makes 
  use of that for automatic layout and styling of the table.  This avoids 
  the need for most manual adjustments to achieve an attractive result. 
  (b) It displays tables using 'ggplot2' graphics.  Therefore a table can 
  be presented anywhere a graph could be, with no more effort.  External 
  software such as LaTeX or HTML or their viewers is not required.  The 
  package provides a full set of tools to control the style and appearance 
  of tables, including titles, footnotes and reference marks, horizontal 
  and vertical rules, and spacing of rows and columns.  Methods are included 
  to display matrices; data frames; tables created by R's ftable(), table(), 
  and xtabs() functions; and tables created by the 'tables' and 'xtable' 
  packages.  Methods can be added to display other table-like objects.  A 
  vignette is included that illustrates usage and options available in the 
  package.",2021-06-02,Richard Raubertas,https://github.com/rrprf/tablesgg,TRUE,https://github.com/rrprf/tablesgg,3376,0,2021-06-03T14:45:34Z,NA
tablet,"Creates a table of descriptive statistics
 for factor and numeric columns in a data frame. Displays
 these by groups, if any. Highly customizable, with support
 for 'html' and 'pdf' provided by 'kableExtra'. Respects
 original column order, column labels, and factor level order.
 See ?tablet.data.frame and vignettes.",2021-08-21,Tim Bergsma,NA,TRUE,https://github.com/bergsmat/tablet,3926,1,2021-08-21T02:10:51Z,3926
tabnet,"Implements the 'TabNet' model by Sercan O. Arik et al (2019) <arXiv:1908.07442>
    and provides a consistent interface for fitting and creating predictions. It's
    also fully compatible with the 'tidymodels' ecosystem.",2021-06-22,Daniel Falbel,https://github.com/mlverse/tabnet,TRUE,https://github.com/mlverse/tabnet,3735,50,2021-06-22T22:58:21Z,74.7
tabr,"Provides a music notation syntax and a collection of music programming functions for generating, manipulating, organizing and analyzing musical information in R. 
    The music notation framework facilitates creating and analyzing music data in notation form.
    Music data can be viewed, manipulated and analyzed while in different forms of representation based around different data structures: strings and data frames.
    Each representation offers advantages over the other for different use cases.
    Music syntax can be entered directly and represented in character strings to minimize the formatting overhead of data entry by using simple data structures, for example when wanting to quickly enter and transcribe short pieces of music to sheet music or tablature.
    The package contains functions for directly performing various mathematical, logical and organizational operations and musical transformations on special object classes that facilitate working with music data and notation.
    The same music data can also be organized in tidy data frames, allowing for a more familiar and powerful approach to the analysis of large amounts of structured music data.
    Functions are available for mapping seamlessly between these data structures and their representations of musical information.
    The package also provides API wrapper functions for transcribing musical representations in R into guitar tablature (""tabs"") and basic sheet music using the 'LilyPond' backend (<http://lilypond.org>).
    'LilyPond' is open source music engraving software for generating high quality sheet music based on markup syntax. 
    The package generates 'LilyPond' files from R code and can pass them to 'LilyPond' to be rendered into sheet music pdf files.
    The package offers nominal MIDI file output support in conjunction with rendering sheet music. 
    The package can read MIDI files and attempts to structure the MIDI data to integrate as best as possible with the data structures and functionality found throughout the package.",2021-02-20,Matthew Leonawicz,https://github.com/leonawicz/tabr,TRUE,https://github.com/leonawicz/tabr,16053,93,2021-06-14T14:14:31Z,172.61290322580646
tabshiftr,"Helps the user to build and register schema descriptions of 
    disorganised (messy) tables. Disorganised tables are tables that are 
    not in a topologically coherent form, where packages such as 'tidyr' could 
    be used for reshaping. The schema description documents the arrangement of 
    input tables and is used to reshape them into a standardised (tidy) output 
    format.",2021-07-01,Steffen Ehrmann,https://github.com/EhrmannS/tabshiftr,TRUE,https://github.com/ehrmanns/tabshiftr,5468,1,2021-08-26T10:09:48Z,5468
tabula,"An easy way to examine archaeological count data. This
    package provides a convenient and reproducible toolkit for relative
    dating by matrix seriation (reciprocal ranking, CA-based seriation).
    It also provides several tests and measures of diversity:
    heterogeneity and evenness (Brillouin, Shannon, Simpson, etc.),
    richness and rarefaction (Chao1, Chao2, ACE, ICE, etc.), turnover and
    similarity (Brainerd-Robinson, etc.). The package make it easy to
    visualize count data and statistical thresholds: rank vs abundance
    plots, heatmaps, Ford (1962) and Bertin (1977) diagrams.",2021-05-25,Nicolas Frerebeau,"https://packages.tesselle.org/tabula/,
https://github.com/tesselle/tabula",TRUE,https://github.com/tesselle/tabula,11853,20,2021-08-25T16:07:14Z,592.65
tabularaster,"Facilities to work with vector and raster data in efficient 
 repeatable and systematic work flow. Missing functionality in existing packages 
 is included here to allow extraction from raster data with 'simple features' and 
 'Spatial' types and to make extraction consistent and straightforward. Extract cell 
 numbers from raster data and  return the cells as a data frame 
 rather than as lists of matrices or vectors. The functions here allow spatial data 
 to be used without special handling for the format currently in use. ",2021-03-15,Michael D. Sumner,https://github.com/hypertidy/tabularaster,TRUE,https://github.com/hypertidy/tabularaster,18800,47,2021-03-15T00:20:53Z,400
tabulizer,"Bindings for the 'Tabula' <http://tabula.technology/> 'Java'
    library, which can extract tables from PDF documents. The 'tabulizerjars'
    package <https://github.com/ropensci/tabulizerjars> provides versioned
    'Java' .jar files, including all dependencies, aligned to releases of
    'Tabula'.",2018-06-07,Thomas J. Leeper,https://github.com/ropensci/tabulizer,TRUE,https://github.com/ropensci/tabulizer,135470,426,2021-07-22T08:19:14Z,318.00469483568077
takos,"It includes functions for applying methodologies utilized for single-process kinetic analysis of solid-state processes were recently summarized and described in the Recommendation of ICTAC Kinetic Committee. These methods work with the basic kinetic equation. The Methodologies included refers to  Avrami, Friedman, Kissinger, Ozawa, OFM, Mo, Starink, isoconversional methodology (Vyazovkin) according to ICATAC Kinetics Committee recommendations as reported in Vyazovkin S, Chrissafis K, Di Lorenzo ML, et al. ICTAC Kinetics Committee recommendations for collecting experimental thermal analysis data for kinetic computations. Thermochim Acta. 2014;590:1-23. <doi:10.1016/J.TCA.2014.05.036> .",2020-10-19,Serena Berretta and Giorgio Luciano,https://github.com/sere3s/takos,TRUE,https://github.com/sere3s/takos,12174,2,2021-07-15T15:32:59Z,6087
TAM,"
    Includes marginal maximum likelihood estimation and joint maximum
    likelihood estimation for unidimensional and multidimensional 
    item response models. The package functionality covers the 
    Rasch model, 2PL model, 3PL model, generalized partial credit model, 
    multi-faceted Rasch model, nominal item response model, 
    structured latent class model, mixture distribution IRT models, 
    and located latent class models. Latent regression models and 
    plausible value imputation are also supported. For details see
    Adams, Wilson and Wang, 1997 <doi:10.1177/0146621697211001>,
    Adams, Wilson and Wu, 1997 <doi:10.3102/10769986022001047>,
    Formann, 1982 <doi:10.1002/bimj.4710240209>,
    Formann, 1992 <doi:10.1080/01621459.1992.10475229>.",2021-06-25,Alexander Robitzsch,"http://www.edmeasurementsurveys.com/TAM/Tutorials/,
https://github.com/alexanderrobitzsch/TAM,
https://sites.google.com/site/alexanderrobitzsch2/software",TRUE,https://github.com/alexanderrobitzsch/tam,153766,12,2021-06-25T13:22:39Z,12813.833333333334
tanaka,"The Tanaka method enhances the representation of topography on a map using shaded contour lines. In this simplified implementation of the method, north-west white contours represent illuminated topography and south-east black contours represent shaded topography. See Tanaka (1950) <doi:10.2307/211219>.",2020-04-14,Timothée Giraud,https://github.com/rcarto/tanaka/,TRUE,https://github.com/rcarto/tanaka,11846,54,2021-01-20T15:03:25Z,219.37037037037038
tangram,"Provides an extensible formula system to quickly and easily create
    production quality tables. The steps of the process are formula parser,
    statistical content generation from data, to rendering. Each step of the process
    is separate and user definable thus creating a set of building blocks for
    highly extensible table generation. A user is not limited by any of the 
    choices of the package creator other than the formula grammar. For example,
    one could chose to add a different S3 rendering function and output a format
    not provided in the default package. Or possibly one would rather have Gini
    coefficients for their statistical content. Routines to achieve New England
    Journal of Medicine style, Lancet style and Hmisc::summaryM() statistics are
    provided. The package contains rendering for HTML5, Rmarkdown and an indexing
    format for use in tracing and tracking are provided.",2020-04-29,Shawn Garbett,https://github.com/spgarbet/tangram,TRUE,https://github.com/spgarbet/tangram,17102,56,2021-03-05T18:34:22Z,305.39285714285717
tangram.pipe,"Builds tables with customizable rows. Users can specify the type
             of data to use for each row, as well as how to handle missing
             data and the types of comparison tests to run on the table
             columns.",2021-07-05,Andrew Guide,https://github.com/thomasgstewart/tangram.pipe,TRUE,https://github.com/thomasgstewart/tangram.pipe,717,0,2021-09-02T21:21:35Z,NA
tapnet,"Functions to produce, fit and predict from bipartite networks with abundance, trait and phylogenetic information. Its methods are described in detail in Benadi, G., Dormann, C.F., Fruend, J., Stephan, R. & Vazquez, D.P. (2021) Quantitative prediction of interactions in bipartite networks based on traits, abundances, and phylogeny. The American Naturalist, in press.",2021-01-28,Carsten Dormann,https://github.com/biometry/tapnet,TRUE,https://github.com/biometry/tapnet,2355,0,2021-04-06T10:58:04Z,NA
tarchetypes,"Function-oriented Make-like declarative workflows for
  Statistics and data science are supported in the 'targets' R package.
  As an extension to 'targets', the 'tarchetypes' package provides
  convenient user-side functions to make 'targets' easier to use.
  By establishing reusable archetypes for common kinds of
  targets and pipelines, these functions help express complicated
  reproducible workflows concisely and compactly.
  The methods in this package were influenced by the 'drake' R package
  by Will Landau (2018) <doi:10.21105/joss.00550>.",2021-08-04,William Michael Landau,"https://docs.ropensci.org/tarchetypes/,
https://github.com/ropensci/tarchetypes",TRUE,https://github.com/ropensci/tarchetypes,8320,52,2021-08-04T01:37:13Z,160
targets,"As a pipeline toolkit for Statistics and data science in R,
  the 'targets' package brings together function-oriented programming and
  'Make'-like declarative workflows.
  It analyzes the dependency relationships among the tasks of a workflow,
  skips steps that are already up to date, runs the necessary
  computation with optional parallel workers, abstracts files as
  R objects, and provides tangible evidence that the results match
  the underlying code and data. The methodology in this package
  borrows from GNU 'Make' (2015, ISBN:978-9881443519)
  and 'drake' (2018, <doi:10.21105/joss.00550>).",2021-08-19,William Michael Landau,"https://docs.ropensci.org/targets/,
https://github.com/ropensci/targets",TRUE,https://github.com/ropensci/targets,16096,465,2021-09-03T02:52:35Z,34.61505376344086
taskscheduleR,"Schedule R scripts/processes with the Windows task scheduler. This
    allows R users to automate R processes on specific time points from R itself.",2021-04-16,Jan Wijffels,https://github.com/bnosac/taskscheduleR,TRUE,https://github.com/bnosac/taskscheduler,51023,273,2021-04-16T12:59:21Z,186.89743589743588
tastypie,"You only need to type 'why pie charts are bad' on Google to find
    thousands of articles full of (valid) reasons why other types of charts 
    should be preferred over this one.
    Therefore, because of the little use due to the reasons already mentioned,
    making pie charts (and related) in R is not straightforward, so other 
    functions are needed to simplify things.
    In this R package there are useful functions to make 'tasty' pie charts 
    immediately by exploiting the many cool templates provided.",2021-05-15,Paolo Dalena,https://paolodalena.github.io/tastypie/,TRUE,https://github.com/paolodalena/tastypie,2804,8,2021-05-16T22:43:59Z,350.5
taxa,"Provides classes for storing and manipulating taxonomic data. 
 Most of the classes can be treated like base R vectors (e.g. can be used 
 in tables as columns and can be named). Vectorized classes can store taxon names
 and authorities, taxon IDs from databases, taxon ranks, and other types of
 information. More complex classes are provided to store taxonomic trees and
 user-defined data associated with them.",2021-07-13,Scott Chamberlain,"https://docs.ropensci.org/taxa/, https://github.com/ropensci/taxa",TRUE,https://github.com/ropensci/taxa,35020,40,2021-07-14T16:52:41Z,875.5
taxalight,"Creates a local Lightning Memory-Mapped Database ('LMDB') 
             of many commonly used taxonomic authorities
             and provides functions that can quickly query this data.
             Supported taxonomic authorities include 
             the Integrated Taxonomic Information System ('ITIS'),
             National Center for Biotechnology Information ('NCBI'),
             Global Biodiversity Information Facility ('GBIF'), 
             Catalogue of Life ('COL'), and Open Tree Taxonomy ('OTT'). 
             Name and identifier resolution using 'LMDB' can
             be hundreds of times faster than either relational databases or
             internet-based queries. Precise data provenance information for
             data derived from naming providers is also included.",2021-08-03,Carl Boettiger,https://github.com/cboettig/taxalight,TRUE,https://github.com/cboettig/taxalight,1841,2,2021-08-03T16:15:51Z,920.5
taxize,"Interacts with a suite of web 'APIs' for taxonomic tasks,
    such as getting database specific taxonomic identifiers, verifying
    species names, getting taxonomic hierarchies, fetching downstream and
    upstream taxonomic names, getting taxonomic synonyms, converting
    scientific to common names and vice versa, and more.",2020-10-30,Scott Chamberlain,"https://docs.ropensci.org/taxize/ (website),
https://github.com/ropensci/taxize (devel), https://taxize.dev
(user manual)",TRUE,https://github.com/ropensci/taxize,133573,217,2021-05-27T16:56:35Z,615.5437788018434
taxizedb,"Tools for working with 'taxonomic' databases, including
    utilities for downloading databases, loading them into various
    'SQL' databases, cleaning up files, and providing a 'SQL' connection
    that can be used to do 'SQL' queries directly or used in 'dplyr'.",2021-01-15,Scott Chamberlain,"https://ropensci.github.io/taxizedb/,
https://github.com/ropensci/taxizedb",TRUE,https://github.com/ropensci/taxizedb,16597,20,2021-05-03T21:53:01Z,829.85
taxlist,"Handling taxonomic lists through objects of class 'taxlist'.
    This package provides functions to import species lists from 'Turboveg'
    (<https://www.synbiosys.alterra.nl/turboveg/>) and the possibility to create
    backups from resulting R-objects.
    Also quick displays are implemented as summary-methods.",2021-07-15,Miguel Alvarez,"https://cran.r-project.org/package=taxlist,
https://github.com/ropensci/taxlist,
https://docs.ropensci.org/taxlist/",TRUE,https://github.com/ropensci/taxlist,18861,6,2021-07-15T15:56:57Z,3143.5
taxonomizr,Functions for assigning taxonomy to NCBI accession numbers and taxon IDs based on NCBI's accession2taxid and taxdump files. This package allows the user to downloads NCBI data dumps and create a local database for fast and local taxonomic assignment.,2021-05-06,Scott Sherrill-Mix,NA,TRUE,https://github.com/sherrillmix/taxonomizr,19822,42,2021-05-06T16:06:51Z,471.95238095238096
taxotools,"Tools include matching and merging taxonomic lists, casting and 
  melting scientific names, managing taxonomic lists from GBIF and ITIS, 
  harvesting names from wikipedia and fuzzy matching.",2021-01-18,Vijay Barve,NA,TRUE,https://github.com/vijaybarve/taxotools,15802,4,2021-05-27T01:19:50Z,3950.5
taylor,"A comprehensive resource for data on Taylor Swift songs. Data is
    included for all officially released studio albums, extended plays (EPs),
    and individual singles are included. Data comes from
    'Genius' (lyrics) and 'Spotify' (song characteristics). Additional functions
    are included for easily creating data visualizations with color palettes
    inspired by Taylor Swift's album covers.",2021-08-17,W. Jake Thompson,"https://taylor.wjakethompson.com,
https://github.com/wjakethompson/taylor",TRUE,https://github.com/wjakethompson/taylor,213,0,2021-08-18T00:26:07Z,NA
tbl2xts,"Facilitate the movement between data frames to 'xts'. Particularly
    useful when moving from 'tidyverse' to the widely used 'xts' package, which is
    the input format of choice to various other packages. It also allows the user 
    to use a 'spread_by' argument for a character column 'xts' conversion.",2021-01-12,Nico Katzke,https://tbl2xts.nfkatzke.com,TRUE,https://github.com/nicktz/tbl2xts,23534,2,2021-01-12T15:11:46Z,11767
TBRDist,"Fast calculation of the Subtree Prune and Regraft (SPR),
  Tree Bisection and Reconnection (TBR) and Replug distances between 
  unrooted trees, using the algorithms of Whidden and 
  Matsen (2017) <arxiv:1511.07529>.",2020-09-17,Martin R. Smith,"https://ms609.github.io/TBRDist/,
https://github.com/ms609/TBRDist/,
https://github.com/cwhidden/uspr/",TRUE,https://github.com/ms609/tbrdist,5991,0,2021-07-13T14:16:39Z,NA
TCA,"Tensor Composition Analysis (TCA) allows the deconvolution of two-dimensional data (features by observations) coming from a mixture of heterogeneous sources into a three-dimensional matrix of signals (features by observations by sources). The TCA framework further allows to test the features in the data for different statistical relations with an outcome of interest while modeling source-specific effects; particularly, it allows to look for statistical relations between source-specific signals and an outcome. For example, TCA can deconvolve bulk tissue-level DNA methylation data (methylation sites by individuals) into a three-dimensional tensor of cell-type-specific methylation levels for each individual (i.e. methylation sites by individuals by cell types) and it allows to detect cell-type-specific statistical relations (associations) with phenotypes. For more details see Rahmani et al. (2019) <DOI:10.1038/s41467-019-11052-9>.",2021-02-14,Elior Rahmani,https://www.nature.com/articles/s41467-019-11052-9,TRUE,https://github.com/cozygene/tca,10540,10,2021-02-14T07:17:30Z,1054
TCIU,"Provide the core functionality to transform longitudinal data to
    complex-time (kime) data using analytic and numerical techniques, visualize the original 
    time-series and reconstructed kime-surfaces, perform model based (e.g., tensor-linear regression)
    and model-free classification and clustering methods in the book Dinov, ID and Velev, MV. (2021)
    ""Data Science: Time Complexity, Inferential Uncertainty, and Spacekime Analytics"", De Gruyter STEM Series,
    ISBN 978-3-11-069780-3. <https://www.degruyter.com/view/title/576646>.
    The package includes 18 core functions which can be separated into three groups.
    1) draw longitudinal data, such as fMRI time-series, and forecast or transform the time-series data.
    2) simulate real-valued time-series data, e.g., fMRI time-courses, detect the activated areas,
    report the corresponding p-values, and visualize the p-values in the 3D brain space.
    3) Laplace transform and kimesurface reconstructions of the fMRI data.",2021-04-30,Daniel Adrian [ctb,"https://github.com/SOCR/TCIU, https://spacekime.org,
https://tciu.predictive.space",TRUE,https://github.com/socr/tciu,4562,6,2021-06-24T15:00:38Z,760.3333333333334
tcsinvest,"R functions for Tinkoff Investments API <https://tinkoffcreditsystems.github.io/invest-openapi/>. Using this package, analysts and traders can interact with account and market data from within R.  Clients for both REST and Streaming protocols implemented.",2021-08-17,Vyacheslav Arbuzov,"https://github.com/arbuzovv/tcsinvest,tcsinvest.ru",TRUE,https://github.com/arbuzovv/tcsinvest,215,0,2021-08-18T21:28:02Z,NA
td,"The 'twelvedata' REST service offers access to current and historical
 data on stocks, standard as well as digital 'crypto' currencies, and other financial
 assets covering a wide variety of course and time spans. See <https://twelvedata.com/>
 for details, to create an account, and to request an API key for free-but-capped access
 to the data.",2021-06-05,Dirk Eddelbuettel,"https://dirk.eddelbuettel.com/code/td.html,
https://github.com/eddelbuettel/td",TRUE,https://github.com/eddelbuettel/td,2617,8,2021-06-05T14:01:22Z,327.125
TDAstats,"A comprehensive toolset for any
    useR conducting topological data analysis, specifically via the
    calculation of persistent homology in a Vietoris-Rips complex.
    The tools this package currently provides can be conveniently split
    into three main sections: (1) calculating persistent homology; (2)
    conducting statistical inference on persistent homology calculations;
    (3) visualizing persistent homology and statistical inference.
    The published form of TDAstats can be found in Wadhwa et al. (2018)
    <doi:10.21105/joss.00860>.   
    For a general background on computing persistent homology for
    topological data analysis, see Otter et al. (2017)
    <doi:10.1140/epjds/s13688-017-0109-5>.
    To learn more about how the permutation test is used for
    nonparametric statistical inference in topological data analysis,
    read Robinson & Turner (2017) <doi:10.1007/s41468-017-0008-7>.
    To learn more about how TDAstats calculates persistent homology,
    you can visit the GitHub repository for Ripser, the software that
    works behind the scenes at <https://github.com/Ripser/ripser>.
    This package has been published as Wadhwa et al. (2018)
    <doi:10.21105/joss.00860>.",2019-12-12,Raoul Wadhwa,https://github.com/rrrlw/TDAstats,TRUE,https://github.com/rrrlw/tdastats,16411,22,2021-05-14T15:45:40Z,745.9545454545455
tdaunif,"Uniform random samples from simple manifolds, sometimes with noise,
    are commonly used to test topological data analytic (TDA) tools.
    This package includes samplers powered by two techniques: analytic
    volume-preserving parameterizations, as employed by Arvo (1995)
    <doi:10.1145/218380.218500>, and rejection sampling, as employed by
    Diaconis, Holmes, and Shahshahani (2013) <doi:10.1214/12-IMSCOLL1006>.",2020-10-26,Jason Cory Brunson,https://corybrunson.github.io/tdaunif/,TRUE,https://github.com/corybrunson/tdaunif,3328,2,2020-11-28T20:12:43Z,1664
tdthap,"Functions and examples are provided for Transmission/disequilibrium tests
             for extended marker haplotypes, as in
             Clayton, D. and Jones, H. (1999) ""Transmission/disequilibrium tests
             for extended marker haplotypes"". Amer. J. Hum. Genet., 65:1161-1169,
             <doi:10.1086/302566>.",2019-08-22,David Clayton,https://github.com/jinghuazhao/R,TRUE,https://github.com/jinghuazhao/r,23565,6,2021-08-27T21:09:36Z,3927.5
teamcolors,"Provides color palettes corresponding to professional and amateur, 
    sports teams. These can be useful in creating data graphics that are themed 
    for particular teams. ",2020-01-22,Benjamin S. Baumer,http://github.com/beanumber/teamcolors,TRUE,https://github.com/beanumber/teamcolors,17650,43,2020-10-15T15:49:31Z,410.4651162790698
telegram,"R wrapper around the Telegram Bot API (http://core.telegram.org/bots/api) to access Telegram's messaging facilities with ease (e.g. you send messages, images, files from R to your smartphone).",2016-09-17,Luca Braglia,http://github.com/lbraglia/telegram,TRUE,https://github.com/lbraglia/telegram,29616,53,2021-06-25T07:52:21Z,558.7924528301887
telemac,"An R interface to the TELEMAC suite for modelling
    of free surface flow. This includes methods for model initialisation, simulation,
    and visualisation. So far only the TELEMAC-2D module for 2-dimensional hydrodynamic
    modelling is implemented. ",2021-02-19,Tobias Pilz,https://github.com/tpilz/telemac,TRUE,https://github.com/tpilz/telemac,1690,2,2021-04-12T07:57:16Z,845
tempdisagg,"Temporal disaggregation methods are used to disaggregate and
    interpolate a low frequency time series to a higher frequency series, where
    either the sum, the mean, the first or the last value of the resulting
    high frequency series is consistent with the low frequency series. Temporal
    disaggregation can be performed with or without one or more high frequency
    indicator series. Contains the methods of Chow-Lin, Santos-Silva-Cardoso,
    Fernandez, Litterman, Denton and Denton-Cholette, summarized in Sax and
    Steiner (2013) <doi:10.32614/RJ-2013-028>. Supports most R time series
    classes.",2020-02-07,Christoph Sax,https://journal.r-project.org/archive/2013-2/sax-steiner.pdf,TRUE,https://github.com/christophsax/tempdisagg,507271,26,2021-07-12T16:23:51Z,19510.423076923078
Tendril,"Compute the coordinates to produce a tendril plot. 
    In the tendril plot, each tendril (branch) represents a type of events, 
    and the direction of the tendril is dictated by on which treatment arm the 
    event is occurring. If an event is occurring on the first of the two 
    specified treatment arms, the tendril bends in a clockwise direction. 
    If an event is occurring on the second of the treatment arms, the
    tendril bends in an anti-clockwise direction. 
    Ref: Karpefors, M and Weatherall, J., ""The Tendril Plot - a novel visual summary 
    of the incidence, significance and temporal aspects of adverse events in 
    clinical trials"" - JAMIA 2018; 25(8): 1069-1073 <doi:10.1093/jamia/ocy016>.",2020-02-11,Martin Karpefors,https://github.com/Karpefors/Tendril,TRUE,https://github.com/karpefors/tendril,7246,5,2021-08-17T09:59:52Z,1449.2
tensorflow,"Interface to 'TensorFlow' <https://www.tensorflow.org/>,
  an open source software library for numerical computation using data
  flow graphs. Nodes in the graph represent mathematical operations,
  while the graph edges represent the multidimensional data arrays
  (tensors) communicated between them. The flexible architecture allows
  you to deploy computation to one or more 'CPUs' or 'GPUs' in a desktop,
  server, or mobile device with a single 'API'. 'TensorFlow' was originally
  developed by researchers and engineers working on the Google Brain Team
  within Google's Machine Intelligence research organization for the
  purposes of conducting machine learning and deep neural networks research,
  but the system is general enough to be applicable in a wide variety
  of other domains as well.",2021-08-19,Daniel Falbel [ctb,https://github.com/rstudio/tensorflow,TRUE,https://github.com/rstudio/tensorflow,1004300,1221,2021-08-19T18:47:15Z,822.5225225225225
tensorTS,"Factor and autoregressive models for matrix and tensor valued time series. 
    We provide functions for estimation, simulation and prediction. The models are discussed in Chen et al (2020) <DOI:10.1016/j.jeconom.2020.07.015>, 
    Chen et al (2020) <arXiv:1905.07530>, and Han et al (2020) <arXiv:2006.02611>.",2021-08-10,Zebang Li,https://github.com/zebang/tensorTS,TRUE,https://github.com/zebang/tensorts,1909,5,2021-08-13T21:33:17Z,381.8
tergm,"An integrated set of extensions to the 'ergm' package to analyze and simulate network evolution based on exponential-family random graph models (ERGM). 'tergm' is a part of the 'statnet' suite of packages for network analysis. See Krivitsky and Handcock (2014) <doi:10.1111/rssb.12014> and Carnegie, Krivitsky, Hunter, and Goodreau (2015) <doi:10.1080/10618600.2014.903087>.",2021-07-28,Pavel N. Krivitsky,https://statnet.org,TRUE,https://github.com/statnet/tergm,159090,13,2021-07-28T08:28:27Z,12237.692307692309
term,"Creates, manipulates, queries and repairs vectors
    of parameter terms.  Parameter terms are the labels used to reference
    values in vectors, matrices and arrays. They represent the names in
    coefficient tables and the column names in 'mcmc' and 'mcmc.list'
    objects.",2021-02-06,Joe Thorley,"https://poissonconsulting.github.io/term/,
https://github.com/poissonconsulting/term",TRUE,https://github.com/poissonconsulting/term,20803,8,2021-09-02T14:31:11Z,2600.375
Ternary,"Plots ternary diagrams (simplex plots / Gibbs triangles) using the
  standard graphics functions.
  An alternative to 'ggtern', which uses the 'ggplot2' family of plotting 
  functions.
  Includes a 'Shiny' user interface for point-and-click plotting.",2021-05-12,Martin R. Smith,"https://ms609.github.io/Ternary/,
https://github.com/ms609/Ternary/",TRUE,https://github.com/ms609/ternary,34764,14,2021-06-29T14:28:35Z,2483.1428571428573
terra,"Methods for spatial data analysis with raster and vector data. Raster methods allow for low-level data manipulation as well as high-level global, local, zonal, and focal computation. The predict and interpolate methods facilitate the use of regression type (interpolation, machine learning) models for spatial prediction, including with satellite remote sensing data. Processing of very large files is supported. See the manual and tutorials on <https://rspatial.org/terra/> to get started. 'terra' is very similar to the 'raster' package; but 'terra' can do more, is easier to use, and it is faster.",2021-08-20,Robert J. Hijmans,https://rspatial.org/terra/,TRUE,https://github.com/rspatial/terra,1139508,267,2021-09-03T07:25:21Z,4267.820224719101
terrainr,"Functions for the retrieval, manipulation, and visualization of 
    'geospatial' data, with an aim towards producing '3D' landscape 
    visualizations in the 'Unity' '3D' rendering engine. Functions are also 
    provided for retrieving elevation data and base map tiles from the 'USGS' 
    National Map ('<https://apps.nationalmap.gov/services/>').",2021-08-05,Michael Mahoney,"https://docs.ropensci.org/terrainr/,
https://github.com/ropensci/terrainr",TRUE,https://github.com/ropensci/terrainr,3041,40,2021-08-25T13:20:32Z,76.025
TestDesign,"Use the optimal test design approach by Birnbaum (1968, ISBN:9781593119348) and
    van der Linden (2018) <doi:10.1201/9781315117430> in constructing fixed and adaptive tests. Supports the following
    mixed-integer programming (MIP) solver packages: 'lpsymphony', 'Rsymphony', 'gurobi', 'lpSolve', and 'Rglpk'. The 'gurobi' package
    is not available from CRAN; see <https://www.gurobi.com/downloads/>. ",2021-07-12,Seung W. Choi,https://choi-phd.github.io/TestDesign/ (documentation),TRUE,https://github.com/choi-phd/testdesign,14655,1,2021-07-11T16:36:50Z,14655
testit,"Provides two convenience functions assert() and test_pkg() to
    facilitate testing R packages.",2021-04-14,Yihui Xie,https://github.com/yihui/testit,TRUE,https://github.com/yihui/testit,322662,44,2021-04-14T15:11:18Z,7333.227272727273
testthat,"Software testing is important, but, in part because
    it is frustrating and boring, many of us avoid it. 'testthat' is a
    testing framework for R that is easy to learn and use, and integrates
    with your existing 'workflow'.",2021-07-01,Hadley Wickham,"https://testthat.r-lib.org, https://github.com/r-lib/testthat",TRUE,https://github.com/r-lib/testthat,17855616,752,2021-08-05T22:43:37Z,23744.17021276596
TeXCheckR,"Checks LaTeX documents and .bib files for typing errors, such as spelling errors, incorrect quotation marks. Also provides useful functions for parsing and linting bibliography files.",2020-11-17,Hugh Parsonage,https://github.com/HughParsonage/TeXCheckR,TRUE,https://github.com/hughparsonage/texcheckr,24106,8,2020-12-14T10:37:10Z,3013.25
TexExamRandomizer,"Randomizing exams with 'LaTeX'.
    If you can compile your main document with 'LaTeX', the program should be able to compile the randomized
    versions without much extra effort when creating the document.",2018-02-13,Alejandro Gonzalez Recuenco,https://github.com/alexrecuenco/TexExamRandomizer,TRUE,https://github.com/alexrecuenco/texexamrandomizer,14114,1,2020-11-15T01:46:23Z,14114
texmex,"Statistical extreme value modelling of threshold excesses, maxima
    and multivariate extremes. Univariate models for threshold excesses and maxima
    are the Generalised Pareto, and Generalised Extreme Value model respectively.
    These models may be fitted by using maximum (optionally penalised-)likelihood,
    or Bayesian estimation, and both classes of models may be fitted with covariates
    in any/all model parameters. Model diagnostics support the fitting process.
    Graphical output for visualising fitted models and return level estimates is
    provided. For serially dependent sequences, the intervals declustering algorithm
    of Ferro and Segers (2003) <doi:10.1111/1467-9868.00401> is provided, with
    diagnostic support to aid selection of threshold and declustering horizon.
    Multivariate modelling is performed via the conditional approach of Heffernan
    and Tawn (2004) <doi:10.1111/j.1467-9868.2004.02050.x>, with graphical tools for
    threshold selection and to diagnose estimation convergence.",2020-12-04,Harry Southworth,https://github.com/harrysouthworth/texmex,TRUE,https://github.com/harrysouthworth/texmex,22573,5,2020-12-04T12:47:30Z,4514.6
texPreview,"Compile snippets of 'LaTeX' directly into images
    from the R console to view in the 'RStudio' viewer pane, Shiny apps
    and 'RMarkdown' documents.",2020-12-10,Jonathan Sidi,https://github.com/yonicd/texPreview,TRUE,https://github.com/yonicd/texpreview,33508,46,2020-12-10T11:30:39Z,728.4347826086956
texreg,"Converts coefficients, standard errors, significance stars, and goodness-of-fit statistics of statistical models into LaTeX tables or HTML tables/MS Word documents or to nicely formatted screen output for the R console for easy model comparison. A list of several models can be combined in a single table. The output is highly customizable. New model types can be easily implemented. (If the Zelig package, which this package enhances, cannot be found on CRAN, you can find it at <https://github.com/IQSS/Zelig>.)",2020-06-18,Philip Leifeld,http://github.com/leifeld/texreg/,TRUE,https://github.com/leifeld/texreg,415294,91,2021-02-20T16:57:30Z,4563.67032967033
text,"Transforms text variables to word embeddings; where the word embeddings are used to statistically test the mean difference between set of texts, compute semantic similarity scores between texts, predict numerical variables, and visual statistically significant words according to various dimensions etc. For more information see  <https://www.r-text.org>.",2020-12-14,Oscar Kjell,"https://r-text.org/, https://github.com/OscarKjell/text/",TRUE,https://github.com/oscarkjell/text,3364,34,2021-09-03T06:20:18Z,98.94117647058823
text.alignment,"Find similarities between texts using the Smith-Waterman algorithm. The algorithm performs local sequence alignment and determines similar regions between two strings.
    The Smith-Waterman algorithm is explained in the paper: ""Identification of common molecular subsequences"" by T.F.Smith and M.S.Waterman (1981), available at <doi:10.1016/0022-2836(81)90087-5>. 
    This package implements the same logic for sequences of words and letters instead of molecular sequences.",2020-09-08,Jan Wijffels,https://github.com/DIGI-VUB/text.alignment,TRUE,https://github.com/digi-vub/text.alignment,6952,2,2020-09-08T07:18:20Z,3476
text2speech,"Unifies different text to speech engines, such as
    Google, Microsoft, and Amazon.  Text synthesis can be done
    in any engine with a simple switch of an argument denoting
    the service requested.  The 'aws.polly' package has been
    orphaned and can be found from the CRAN archives.",2020-06-30,John Muschelli,https://github.com/muschellij2/text2speech,TRUE,https://github.com/muschellij2/text2speech,15348,10,2020-12-09T00:46:10Z,1534.8
text2vec,"Fast and memory-friendly tools for text vectorization, topic
    modeling (LDA, LSA), word embeddings (GloVe), similarities. This package
    provides a source-agnostic streaming API, which allows researchers to perform
    analysis of collections of documents which are larger than available RAM. All
    core functions are parallelized to benefit from multicore machines.",2020-02-18,Dmitriy Selivanov,http://text2vec.org,TRUE,https://github.com/dselivanov/text2vec,255039,738,2020-09-19T13:01:51Z,345.5813008130081
textdata,"Provides a framework to download, parse, and store text datasets
    on the disk and load them when needed. Includes various sentiment lexicons
    and labeled text data sets for classification and analysis.",2020-05-04,Emil Hvitfeldt,https://github.com/EmilHvitfeldt/textdata,TRUE,https://github.com/emilhvitfeldt/textdata,156909,63,2020-10-24T21:28:48Z,2490.6190476190477
texter,"Implement text and sentiment analysis with 'texter'. 
             Generate sentiment scores on text data and also visualize sentiments.
             'texter' allows you quickly generate insights on your data.
             It includes support for lexicons such as 'NRC' and 'Bing'.",2021-08-17,Simi Kafaru,https://github.com/simmieyungie/texter,TRUE,https://github.com/simmieyungie/texter,188,2,2021-08-19T21:26:43Z,94
textmineR,"An aid for text mining in R, with a syntax that
    should be familiar to experienced R users. Provides a wrapper for several 
    topic models that take similarly-formatted input and give similarly-formatted
    output. Has additional functionality for analyzing and diagnostics for
    topic models.",2021-06-28,Tommy Jones,https://www.rtextminer.com/,TRUE,https://github.com/tommyjones/textminer,64670,95,2021-06-28T19:58:04Z,680.7368421052631
textplot,"Visualise complex relations in texts. This is done by providing functionalities for displaying 
    text co-occurrence networks, text correlation networks, dependency relationships as well as text clustering and semantic text 'embeddings'. 
    Feel free to join the effort of providing interesting text visualisations.",2021-08-18,Jan Wijffels,https://github.com/bnosac/textplot,TRUE,https://github.com/bnosac/textplot,10590,44,2021-08-18T17:58:23Z,240.6818181818182
textrank,"The 'textrank' algorithm is an extension of the 'Pagerank' algorithm for text. The algorithm allows to summarize text by calculating how sentences are related to one another. This is done by looking at overlapping terminology used in sentences in order to set up links between sentences. The resulting sentence network is next plugged into the 'Pagerank' algorithm which identifies the most important sentences in your text and ranks them. 
    In a similar way 'textrank' can also be used to extract keywords. A word network is constructed by looking if words are following one another. On top of that network the 'Pagerank' algorithm is applied to extract relevant words after which relevant words which are following one another are combined to get keywords.  
    More information can be found in the paper from Mihalcea, Rada & Tarau, Paul (2004) <https://www.aclweb.org/anthology/W04-3252/>.",2020-10-12,Jan Wijffels,https://github.com/bnosac/textrank,TRUE,https://github.com/bnosac/textrank,32116,62,2020-10-12T11:33:30Z,518
textrecipes,"Converting text to numerical features requires
    specifically created procedures, which are implemented as steps
    according to the 'recipes' package. These steps allows for
    tokenization, filtering, counting (tf and tfidf) and feature hashing.",2021-07-11,Emil Hvitfeldt,"https://github.com/tidymodels/textrecipes,
https://textrecipes.tidymodels.org",TRUE,https://github.com/tidymodels/textrecipes,31762,119,2021-07-30T06:12:57Z,266.9075630252101
textshape,Tools that can be used to reshape and restructure text data.,2021-05-28,Tyler Rinker,https://github.com/trinker/textshape,TRUE,https://github.com/trinker/textshape,427309,40,2021-05-25T15:32:01Z,10682.725
textshaping,"Provides access to the text shaping functionality in the 'HarfBuzz'
    library and the bidirectional algorithm in the 'Fribidi' library. 
    'textshaping' is a low-level utility package mainly for graphic devices that 
    expands upon the font tool-set provided by the 'systemfonts' package.",2021-06-09,Thomas Lin Pedersen,https://github.com/r-lib/textshaping,TRUE,https://github.com/r-lib/textshaping,6721966,7,2021-06-09T13:57:05Z,960280.8571428572
textTinyR,"It offers functions for splitting, parsing, tokenizing and creating a vocabulary for big text data files. Moreover, it includes functions for building a document-term matrix and extracting information from those (term-associations, most frequent terms). It also embodies functions for calculating token statistics (collocations, look-up tables, string dissimilarities) and functions to work with sparse matrices. Lastly, it includes functions for Word Vector Representations (i.e. 'GloVe', 'fasttext') and incorporates functions for the calculation of (pairwise) text document dissimilarities. The source code is based on 'C++11' and exported in R through the 'Rcpp', 'RcppArmadillo' and 'BH' packages.",2021-05-05,Lampros Mouselimis,https://github.com/mlampros/textTinyR,TRUE,https://github.com/mlampros/texttinyr,21017,29,2021-05-05T18:34:26Z,724.7241379310345
textutils,"Utilities for handling character vectors
  that store human-readable text (either plain or with
  markup, such as HTML or LaTeX). The package provides,
  in particular, functions that help with the
  preparation of plain-text reports, e.g. for expanding
  and aligning strings that form the lines of such
  reports. The package also provides generic functions for
  transforming R objects to HTML and to plain text.",2021-04-01,Enrico Schumann,"http://enricoschumann.net/R/packages/textutils/,
https://github.com/enricoschumann/textutils",TRUE,https://github.com/enricoschumann/textutils,49732,9,2021-04-02T06:57:10Z,5525.777777777777
tfaddons,"'TensorFlow SIG Addons' <https://www.tensorflow.org/addons> is a repository 
             of community contributions that conform to well-established API patterns, 
             but implement new functionality not available in core 'TensorFlow'. 
             'TensorFlow' natively supports a large number of operators, layers, metrics, 
             losses, optimizers, and more. However, in a fast moving field like Machine Learning, 
             there are many interesting new developments that cannot be integrated into 
             core 'TensorFlow' (because their broad applicability is not yet clear, or 
             it is mostly used by a smaller subset of the community).",2020-06-02,Turgut Abdullayev,https://github.com/henry090/tfaddons,TRUE,https://github.com/henry090/tfaddons,4814,17,2021-05-31T12:17:28Z,283.1764705882353
tfarima,"Building customized transfer function and ARIMA models with multiple operators and parameter restrictions. Functions for model identification, model estimation (exact or conditional maximum likelihood), model diagnostic checking, automatic outlier detection, calendar effects, forecasting and seasonal adjustment. See Bell and Hillmer (1983) <doi:10.1080/01621459.1983.10478005>, Box, Jenkins, Reinsel and Ljung <ISBN:978-1-118-67502-1>, Box, Pierce and Newbold (1987) <doi:10.1080/01621459.1987.10478430>, Box and Tiao (1975) <doi:10.1080/01621459.1975.10480264>, Chen and Liu (1993) <doi:10.1080/01621459.1993.10594321>.",2021-05-29,Jose L. Gallego,https://github.com/gallegoj/tfarima,TRUE,https://github.com/gallegoj/tfarima,5143,0,2021-05-29T17:41:42Z,NA
tfautograph,Translate R control flow expressions into 'Tensorflow' graphs.,2021-08-19,Tomasz Kalinowski,https://t-kalinowski.github.io/tfautograph/,TRUE,https://github.com/t-kalinowski/tfautograph,25560,13,2021-08-19T18:14:42Z,1966.1538461538462
tfdatasets,"Interface to 'TensorFlow' Datasets, a high-level library for 
    building complex input pipelines from simple, re-usable pieces. 
    See <https://www.tensorflow.org/guide> for additional
    details.",2021-04-03,Daniel Falbel [ctb,https://github.com/rstudio/tfdatasets,TRUE,https://github.com/rstudio/tfdatasets,106877,28,2021-08-30T22:23:58Z,3817.035714285714
tfestimators,"Interface to 'TensorFlow' Estimators 
    <https://www.tensorflow.org/guide/estimator>, a high-level 
    API that provides implementations of many different model types 
    including linear models and deep neural networks. ",2021-08-09,Kevin Kuo,https://github.com/rstudio/tfestimators,TRUE,https://github.com/rstudio/tfestimators,120470,58,2021-08-27T13:15:08Z,2077.0689655172414
tfhub,"'TensorFlow' Hub is a library for the publication, discovery, and
    consumption of reusable parts of machine learning models. A module is a 
    self-contained piece of a 'TensorFlow' graph, along with its weights and 
    assets, that can be reused across different tasks in a process known as
    transfer learning. Transfer learning train a model with a smaller dataset,
    improve generalization, and speed up training.",2020-05-22,Daniel Falbel,https://github.com/rstudio/tfhub,TRUE,https://github.com/rstudio/tfhub,10050,27,2021-08-26T14:24:35Z,372.22222222222223
tfio,"Interface to 'TensorFlow IO', Datasets and filesystem extensions maintained by `TensorFlow SIG-IO` <https://github.com/tensorflow/community/blob/master/sigs/io/CHARTER.md>.",2019-12-19,TensorFlow IO Contributors,https://github.com/tensorflow/io,TRUE,https://github.com/tensorflow/io,11005,494,2021-09-02T21:11:59Z,22.277327935222672
tfprobability,"Interface to 'TensorFlow Probability', a 'Python' library built on 'TensorFlow'
    that makes it easy to combine probabilistic models and deep learning on modern hardware ('TPU', 'GPU').
    'TensorFlow Probability' includes a wide selection of probability distributions and bijectors, probabilistic layers,
    variational inference, Markov chain Monte Carlo, and optimizers such as Nelder-Mead, BFGS, and SGLD.",2021-05-20,Sigrid Keydana,https://github.com/rstudio/tfprobability,TRUE,https://github.com/rstudio/tfprobability,16532,44,2021-05-25T04:26:16Z,375.72727272727275
tfruns,"Create and manage unique directories for each 'TensorFlow' 
  training run. Provides a unique, time stamped directory for each run
  along with functions to retrieve the directory of the latest run or 
  latest several runs. ",2021-02-26,Daniel Falbel [ctb,https://github.com/rstudio/tfruns,TRUE,https://github.com/rstudio/tfruns,905970,31,2021-02-26T10:38:39Z,29224.83870967742
tgamtheme,"Theme and colour palettes for The Globe and Mail's graphics. Includes colour and fill scale functions, colour palette helpers and a Globe-styled 'ggplot2' theme object.",2021-02-05,Tom Cardoso,"https://github.com/globeandmail/tgamtheme,
https://globeandmail.github.io/tgamtheme/",TRUE,https://github.com/globeandmail/tgamtheme,2439,4,2021-02-05T22:24:20Z,609.75
theiaR,"Provides a simple interface to search available data provided by
    Theia (<https://theia.cnes.fr>), download it, and manage it. Data can be downloaded
    based on a search result or from a cart file downloaded from Theia website.",2020-11-19,Xavier Laviron,https://github.com/norival/theiaR,TRUE,https://github.com/norival/theiar,11034,3,2020-12-02T13:50:10Z,3678
themis,"A dataset with an uneven number of cases in each
    class is said to be unbalanced. Many models produce a subpar
    performance on unbalanced datasets. A dataset can be balanced by
    increasing the number of minority cases using SMOTE 2011
    <arXiv:1106.1813>, BorderlineSMOTE 2005 <doi:10.1007/11538059_91> and
    ADASYN 2008 <https://ieeexplore.ieee.org/document/4633969>. Or by
    decreasing the number of majority cases using NearMiss 2003
    <https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf> or Tomek
    link removal 1976 <https://ieeexplore.ieee.org/document/4309452>.",2021-06-12,Emil Hvitfeldt,"https://github.com/tidymodels/themis,
https://themis.tidymodels.org",TRUE,https://github.com/tidymodels/themis,22908,106,2021-08-01T06:57:02Z,216.11320754716982
Thermimage,"A collection of functions and routines for inputting thermal
    image video files, plotting and converting binary raw data into estimates of
    temperature.  First published 2015-03-26.  Written primarily for research purposes
    in biological applications of thermal images.  v1 included the base calculations 
    for converting thermal image binary values to temperatures. v2 included additional
    equations for providing heat transfer calculations and an import function for thermal
    image files (v2.2.3 fixed error importing thermal image to windows OS). v3. Added numerous
    functions for converting thermal image, videos, rewriting and exporting.  
    v3.1. Added new functions to convert files. v3.2.  Fixed the various functions related to finding frame times.
    v4.0. fixed an error in atmospheric attenuation constants, affecting raw2temp and temp2raw functions.
    Recommend update for use with long distance calculations.",2019-11-30,Glenn J. Tattersall,"https://cran.r-project.org/package=Thermimage,
https://github.com/gtatters/Thermimage",TRUE,https://github.com/gtatters/thermimage,24709,106,2021-04-07T22:52:16Z,233.10377358490567
thinkr,"Some tools for cleaning up messy 'Excel' files to
    be suitable for R. People who have been working with 'Excel' for years
    built more or less complicated sheets with names, characters, formats
    that are not homogeneous. To be able to use them in R nowadays, we
    built a set of functions that will avoid the majority of importation
    problems and keep all the data at best.",2020-07-07,Vincent Guyader,https://github.com/Thinkr-open/thinkr,TRUE,https://github.com/thinkr-open/thinkr,17090,16,2021-07-16T10:39:06Z,1068.125
thor,"Key-value store, implemented as a wrapper around 'LMDB';
    the ""lightning memory-mapped database"" <https://symas.com/lmdb/>.
    'LMDB' is a transactional key value store that uses a memory map
    for efficient access.  This package wraps the entire 'LMDB'
    interface (except duplicated keys), and provides objects for
    transactions and cursors.",2020-05-15,Rich FitzJohn,https://github.com/richfitz/thor,TRUE,https://github.com/richfitz/thor,15603,51,2021-06-17T17:51:31Z,305.94117647058823
thorn,"Creates some 'WebGL' shaders. They can be used as the background of a 'Shiny' app. They also can be visualized in the 'RStudio' viewer pane or included in 'Rmd' documents, but this is pretty useless, besides contemplating them.",2020-11-12,Stéphane Laurent,https://github.com/stla/thorn,TRUE,https://github.com/stla/thorn,3514,3,2020-11-12T18:53:31Z,1171.3333333333333
threejs,"Create interactive 3D scatter plots, network plots, and
    globes using the 'three.js' visualization library (<https://threejs.org>).",2020-01-21,B. W. Lewis,https://bwlewis.github.io/rthreejs,TRUE,https://github.com/bwlewis/rthreejs,560555,256,2021-08-31T02:43:52Z,2189.66796875
threesixtygiving,"Access open data from <https://www.threesixtygiving.org>, a 
    database of charitable grant giving in the UK operated by '360Giving'.
    The package provides functions to search and retrieve data on charitable 
    grant giving, and process that data into tidy formats. It relies on the 
    '360Giving' data standard, described at 
    <https://standard.threesixtygiving.org/>.",2020-12-02,Evan Odell,"https://docs.evanodell.com/threesixtygiving,
https://github.com/evanodell/threesixtygiving,",TRUE,https://github.com/evanodell/threesixtygiving,2960,2,2020-11-29T10:45:02Z,1480
threshr,"Provides functions for the selection of thresholds for use in 
    extreme value models, based mainly on the methodology in 
    Northrop, Attalides and Jonathan (2017) <doi:10.1111/rssc.12159>.
    It also performs predictive inferences about future extreme values, 
    based either on a single threshold or on a weighted average of inferences 
    from multiple thresholds, using the 'revdbayes' package 
    <https://cran.r-project.org/package=revdbayes>.   
    At the moment only the case where the data can be treated as 
    independent identically distributed observations is considered.",2020-09-14,Paul J. Northrop,"https://paulnorthrop.github.io/threshr/,
https://github.com/paulnorthrop/threshr",TRUE,https://github.com/paulnorthrop/threshr,16200,5,2020-09-14T22:08:32Z,3240
thurstonianIRT,"Fit Thurstonian Item Response Theory (IRT) models in R. This 
  package supports fitting Thurstonian IRT models and its extensions using 
  'Stan', 'lavaan', or 'Mplus' for the model estimation. Functionality for 
  extracting results, making predictions, and simulating data is provided as 
  well. References: 
  Brown & Maydeu-Olivares (2011) <doi:10.1177/0013164410375112>;
  Bürkner et al. (2019) <doi:10.1177/0013164419832063>.",2020-08-07,Paul-Christian Bürkner,https://github.com/paul-buerkner/thurstonianIRT,TRUE,https://github.com/paul-buerkner/thurstonianirt,11194,17,2021-08-31T16:03:00Z,658.4705882352941
tibble,"Provides a 'tbl_df' class (the 'tibble') with stricter checking and better formatting than the traditional
    data frame.",2021-08-25,Kirill Müller,"https://tibble.tidyverse.org/, https://github.com/tidyverse/tibble",TRUE,https://github.com/tidyverse/tibble,36670145,452,2021-08-26T00:15:56Z,81128.63938053098
tibbletime,"Built on top of the 'tibble' package, 'tibbletime' is an extension
  that allows for the creation of time aware tibbles. Some immediate
  advantages of this include: the ability to perform time-based subsetting
  on tibbles, quickly summarising and aggregating results by time periods,
  and creating columns that can be used as 'dplyr' time-based groups.",2020-07-21,Davis Vaughan,https://github.com/business-science/tibbletime,TRUE,https://github.com/business-science/tibbletime,333998,178,2021-02-18T19:52:26Z,1876.3932584269662
tibblify,"A tool to rectangle a nested list, that is to convert it into a
    tibble. This is done automatically or according to a given specification.
    A common use case is for nested lists coming from parsing JSON files or
    the JSON response of REST APIs. It is supported by the 'vctrs' package
    and therefore offers a wide support of vector types.",2020-09-23,Maximilian Girlich,https://github.com/mgirlich/tibblify,TRUE,https://github.com/mgirlich/tibblify,3839,12,2021-02-05T12:56:40Z,319.9166666666667
tidybayes,"Compose data for and extract, manipulate, and visualize posterior draws from Bayesian models
    ('JAGS', 'Stan', 'rstanarm', 'brms', 'MCMCglmm', 'coda', ...) in a tidy data format. Functions are provided
    to help extract tidy data frames of draws from Bayesian models and that generate point
    summaries and intervals in a tidy format. In addition, 'ggplot2' 'geoms' and 'stats' are provided for
    common visualization primitives like points with multiple uncertainty intervals, eye plots (intervals plus
    densities), and fit curves with multiple, arbitrary uncertainty bands.",2021-08-22,Matthew Kay,"https://mjskay.github.io/tidybayes/,
https://github.com/mjskay/tidybayes/",TRUE,https://github.com/mjskay/tidybayes,92948,615,2021-08-23T00:04:30Z,151.1349593495935
tidyBdE,"Tools to download data series from 'Banco de España' ('BdE')
    on 'tibble' format. 'Banco de España' is the national central bank
    and, within the framework of the Single Supervisory Mechanism ('SSM'),
    the supervisor of the Spanish banking system along with the European
    Central Bank. This package is in no way sponsored endorsed or
    administered by 'Banco de España'.",2021-08-04,Diego H. Herrero,"https://ropenspain.github.io/tidyBdE/,
https://github.com/rOpenSpain/tidyBdE",TRUE,https://github.com/ropenspain/tidybde,2123,3,2021-09-03T15:50:03Z,707.6666666666666
tidyboot,"Compute arbitrary non-parametric bootstrap statistics on data in
    tidy data frames.",2018-03-14,Mika Braginsky,https://github.com/langcog/tidyboot,TRUE,https://github.com/langcog/tidyboot,12832,16,2020-11-12T23:03:34Z,802
tidycat,Create additional rows and columns on broom::tidy() output to allow for easier control on categorical parameter estimates. ,2021-08-02,Guy J. Abel,https://guyabel.github.io/tidycat/,TRUE,https://github.com/guyabel/tidycat,5273,3,2021-08-18T05:19:13Z,1757.6666666666667
tidycensus,"An integrated R interface to the decennial US Census and American Community Survey APIs and
    the US Census Bureau's geographic boundary files. Allows R users to return Census and ACS data as
    tidyverse-ready data frames, and optionally returns a list-column with feature geometry for all Census 
    geographies. ",2021-05-19,Kyle Walker,https://github.com/walkerke/tidycensus,TRUE,https://github.com/walkerke/tidycensus,166666,457,2021-08-15T09:53:34Z,364.69584245076584
tidycode,"Analyze lines of R code using tidy principles. This allows you to 
    input lines of R code and output a data frame with one row per function 
    included. Additionally, it facilitates code classification via included lexicons.",2019-12-10,Lucy DAgostino McGowan,https://github.com/LucyMcGowan/tidycode,TRUE,https://github.com/lucymcgowan/tidycode,10328,30,2020-11-01T15:33:27Z,344.26666666666665
tidycomm,"Provides convenience functions for common data
    modification and analysis tasks in communication research. This
    includes functions for univariate and bivariate data analysis, index
    generation and reliability computation, and intercoder reliability
    tests. All functions follow the style and syntax of the tidyverse, and
    are construed to perform their computations on multiple variables at
    once. Functions for univariate and bivariate data analysis comprise
    summary statistics for continuous and categorical variables, as well
    as several tests of bivariate association including effect sizes.
    Functions for data modification comprise index generation and
    automated reliability analysis of index variables. Functions for
    intercoder reliability comprise tests of several intercoder
    reliability estimates, including simple and mean pairwise percent
    agreement, Krippendorff's Alpha (Krippendorff 2004, ISBN:
    9780761915454), and various Kappa coefficients (Brennan & Prediger
    1981 <doi: 10.1177/001316448104100307>; Cohen 1960 <doi:
    10.1177/001316446002000104>; Fleiss 1971 <doi: 10.1037/h0031619>).",2021-07-06,Julian Unkel,https://joon-e.github.io/tidycomm/,TRUE,https://github.com/joon-e/tidycomm,10995,8,2021-07-06T11:16:35Z,1374.375
tidycwl,"The Common Workflow Language <https://www.commonwl.org/> is an
    open standard for describing data analysis workflows. This package takes
    the raw Common Workflow Language workflows encoded in JSON or 'YAML'
    and turns the workflow elements into tidy data frames or lists.
    A graph representation for the workflow can be constructed and visualized
    with the parsed workflow inputs, outputs, and steps. Users can embed the
    visualizations in their 'Shiny' applications, and export them
    as HTML files or static images.",2020-10-19,Soner Koc,"https://sbg.github.io/tidycwl/, https://github.com/sbg/tidycwl",TRUE,https://github.com/sbg/tidycwl,8539,7,2020-10-15T05:30:30Z,1219.857142857143
tidydice,"Utils for basic statistical experiments, that can be used for teaching 
    introductory statistics. Each experiment generates a tibble.
    Dice rolls and coin flips are simulated using sample().
    The properties of the dice can be changed, like the number of sides.
    A coin flip is simulated using a two sided dice.
    Experiments can be combined with the pipe-operator.",2021-04-19,Roland Krasser,https://github.com/rolkra/tidydice/,TRUE,https://github.com/rolkra/tidydice,9294,2,2021-04-18T16:16:22Z,4647
tidyestimate,"The 'ESTIMATE' package infers tumor purity from expression data as a 
  function of immune and stromal infiltrate, but requires writing of intermediate 
  files, is un-pipeable, and performs poorly when presented with modern datasets 
  with current gene symbols. 'tidyestimate' a fast, tidy, modern reimagination of
  'ESTIMATE' (2013) <doi:10.1038/ncomms3612>.",2021-06-21,Kai Aragaki,https://github.com/KaiAragaki/tidyestimate,TRUE,https://github.com/kaiaragaki/tidyestimate,527,0,2021-06-21T16:43:55Z,NA
tidygate,"It interactively or programmatically label points within custom gates on two dimensions <https://github.com/stemangiola/tidygate>. 
    The information is added to your tibble. It is based on the package 'gatepoints' from Wajid Jawaid (who is also author of this package). The code of 'gatepoints' was nto integrated in 'tidygate'. 
    The benefits are (i) in interactive mode you can draw your gates on extensive 'ggplot'-like scatter plots; 
    (ii) you can draw multiple gates; and (iii) you can save your gates and apply the programmatically.",2021-05-03,Stefano Mangiola,https://github.com/stemangiola/tidygate,TRUE,https://github.com/stemangiola/tidygate,5253,9,2021-07-26T08:50:39Z,583.6666666666666
tidygenomics,"Handle genomic data within data frames just as you would with 'GRanges'.
    This packages provides method to deal with genomic intervals the ""tidy-way"" which makes
    it simpler to integrate in the the general data munging process. The API is inspired by the
    popular 'bedtools' and the genome_join() method from the 'fuzzyjoin' package.",2019-08-08,Constantin Ahlmann-Eltze,https://github.com/const-ae/tidygenomics,TRUE,https://github.com/const-ae/tidygenomics,16247,97,2021-04-15T07:00:02Z,167.49484536082474
tidygeocoder,An intuitive interface for getting data from geocoder services. ,2021-04-19,Jesse Cambon,"https://jessecambon.github.io/tidygeocoder/,
https://github.com/jessecambon/tidygeocoder",TRUE,https://github.com/jessecambon/tidygeocoder,57738,177,2021-08-31T01:48:46Z,326.20338983050846
tidygraph,"A graph, while not ""tidy"" in itself, can be thought of as two tidy
    data frames describing node and edge data respectively. 'tidygraph'
    provides an approach to manipulate these two virtual data frames using the
    API defined in the 'dplyr' package, as well as provides tidy interfaces to 
    a lot of common graph algorithms.",2020-05-12,Thomas Lin Pedersen,"https://tidygraph.data-imaginist.com,
https://github.com/thomasp85/tidygraph",TRUE,https://github.com/thomasp85/tidygraph,941489,422,2021-08-03T11:48:38Z,2231.016587677725
tidyHeatmap,"This is a tidy implementation for heatmap.  At the
    moment it is based on the (great) package 'ComplexHeatmap'.  The goal
    of this package is to interface a tidy data frame with this powerful
    tool.  Some of the advantages are: Row and/or columns colour
    annotations are easy to integrate just specifying one parameter
    (column names).  Custom grouping of rows is easy to specify providing
    a grouped tbl. For example: df %>% group_by(...).  Labels size
    adjusted by row and column total number.  Default use of Brewer and
    Viridis palettes.",2021-07-07,Stefano Mangiola,"https://www.r-project.org,
https://github.com/stemangiola/tidyHeatmap",TRUE,https://github.com/stemangiola/tidyheatmap,13767,166,2021-07-12T06:08:32Z,82.93373493975903
tidyhydat,"Provides functions to access historical and real-time national 'hydrometric'
    data from Water Survey of Canada data sources (<https://dd.weather.gc.ca/hydrometric/csv/> and
    <https://collaboration.cmc.ec.gc.ca/cmc/hydrometrics/www/>) and then applies tidy data principles.",2021-05-17,Sam Albers,https://docs.ropensci.org/tidyhydat/,TRUE,https://github.com/ropensci/tidyhydat,21867,58,2021-06-30T23:11:33Z,377.01724137931035
tidyjson,Turn complex 'JSON' data into tidy data frames.,2020-05-31,Cole Arendt,https://github.com/colearendt/tidyjson,TRUE,https://github.com/colearendt/tidyjson,56484,126,2021-01-30T13:11:51Z,448.2857142857143
tidylda,"Implements an algorithm for Latent Dirichlet
    Allocation (LDA), Blei et at. (2003) <https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf>,
    using style conventions from the 'tidyverse',
    Wickham et al. (2019)<doi:10.21105/joss.01686>,
    and 'tidymodels', Kuhn et al.<https://tidymodels.github.io/model-implementation-principles/>.
    Fitting is done via collapsed Gibbs sampling.
    Also implements several novel features for LDA such as guided models and
    transfer learning based on ongoing and, as yet, unpublished research.",2021-07-19,Tommy Jones,https://github.com/TommyJones/tidylda/,TRUE,https://github.com/tommyjones/tidylda,579,22,2021-07-21T03:25:39Z,26.318181818181817
tidylog,Provides feedback about 'dplyr' and 'tidyr' operations.,2020-07-03,Benjamin Elbers,https://github.com/elbersb/tidylog/,TRUE,https://github.com/elbersb/tidylog,104063,466,2021-02-04T18:21:37Z,223.31115879828326
tidyLPA,"An interface to the 'mclust' package to easily
    carry out latent profile analysis (""LPA""). Provides functionality to
    estimate commonly-specified models. Follows a tidy approach, in that
    output is in the form of a data frame that can subsequently be
    computed on. Also has functions to interface to the commercial 'MPlus'
    software via the 'MplusAutomation' package.",2020-08-17,Joshua M Rosenberg,https://data-edu.github.io/tidyLPA/,TRUE,https://github.com/data-edu/tidylpa,32861,35,2021-06-03T14:21:07Z,938.8857142857142
tidyMicro,"A reliable alternative to popular microbiome analysis R packages. We provide standard tools as well as novel extensions on standard analyses to improve interpretability and the analyst’s ability to communicate results, all while maintaining object malleability to encourage open source collaboration.",2020-09-13,Charlie Carpenter,NA,TRUE,https://github.com/charliecarpenter/tidymicro,6968,12,2021-04-19T13:58:11Z,580.6666666666666
tidymodels,"The tidy modeling ""verse"" is a collection of packages for 
    modeling and statistical analysis that share the underlying design
    philosophy, grammar, and data structures of the tidyverse.",2021-04-19,Max Kuhn,"https://tidymodels.tidymodels.org,
https://github.com/tidymodels/tidymodels",TRUE,https://github.com/tidymodels/tidymodels,483784,492,2021-08-27T18:18:02Z,983.30081300813
tidymv,"Provides functions for visualising generalised
    additive models and getting predicted values using tidy tools from the 'tidyverse' packages.",2021-04-21,Stefano Coretta,https://github.com/stefanocoretta/tidymv,TRUE,https://github.com/stefanocoretta/tidymv,22291,31,2021-04-25T08:29:19Z,719.0645161290323
tidyndr,"The goal is to simplify routine analysis of the Nigeria National  Data Repository (NDR) <https://ndr.shieldnigeriaproject.com> using the PEPFAR Monitoring, Evaluation, and Reporting (MER) indicators (see <https://datim.zendesk.com/hc/en-us/articles/360000084446-MER-Indicator-Reference-Guides>). It is designed to import in to R patient-level line-list downloaded as 'csv' file from the front-end of the NDR.",2021-03-23,Stephen Balogun,https://github.com/stephenbalogun/tidyndr,TRUE,https://github.com/stephenbalogun/tidyndr,1674,2,2021-03-24T02:25:38Z,837
tidypaleo,"Provides a set of functions with a common framework for age-depth model management, 
  stratigraphic visualization, and common statistical transformations. The focus of the
  package is stratigraphic visualization, for which 'ggplot2' components are provided
  to reproduce the scales, geometries, facets, and theme elements commonly used in
  publication-quality stratigraphic diagrams. Helpers are also provided to reproduce
  the exploratory statistical summaries that are frequently included on
  stratigraphic diagrams.",2021-04-07,Dewey Dunnington,"https://paleolimbot.github.io/tidypaleo/,
https://github.com/paleolimbot/tidypaleo",TRUE,https://github.com/paleolimbot/tidypaleo,2135,27,2021-03-31T11:44:42Z,79.07407407407408
tidyposterior,"Bayesian analysis used here to answer the question: ""when looking at resampling results, are the differences between models 'real'?"" To answer this, a model can be created were the performance statistic is the resampling statistics (e.g. accuracy or RMSE). These values are explained by the model types. In doing this, we can get parameter estimates for each model's affect on performance and make statistical (and practical) comparisons between models. The methods included here are similar to Benavoli et al (2017) <https://jmlr.org/papers/v18/16-305.html>.",2021-03-25,Max Kuhn,"https://tidyposterior.tidymodels.org,
https://github.com/tidymodels/tidyposterior",TRUE,https://github.com/tidymodels/tidyposterior,141616,93,2021-03-25T19:47:25Z,1522.752688172043
tidypredict,"It parses a fitted 'R' model object, and returns a formula
    in 'Tidy Eval' code that calculates the predictions.
    It works with several databases back-ends because it leverages 'dplyr'
    and 'dbplyr' for the final 'SQL' translation of the algorithm. It currently
    supports lm(), glm(), randomForest(), ranger(), earth(), xgb.Booster.complete(),
    cubist(), and ctree() models. ",2020-10-28,Max Kuhn,"https://tidypredict.tidymodels.org,
https://github.com/tidymodels/tidypredict",TRUE,https://github.com/tidymodels/tidypredict,104376,233,2021-03-24T22:51:38Z,447.9656652360515
tidyquant,"Bringing business and financial analysis to the 'tidyverse'. The 'tidyquant' 
    package provides a convenient wrapper to various 'xts', 'zoo', 'quantmod', 'TTR' 
    and 'PerformanceAnalytics' package 
    functions and returns the objects in the tidy 'tibble' format. The main 
    advantage is being able to use quantitative functions with the 'tidyverse'
    functions including 'purrr', 'dplyr', 'tidyr', 'ggplot2', 'lubridate', etc. See 
    the 'tidyquant' website for more information, documentation and examples.",2021-03-05,Matt Dancho,https://github.com/business-science/tidyquant,TRUE,https://github.com/business-science/tidyquant,576336,682,2021-03-04T18:47:13Z,845.067448680352
tidyquery,Use 'SQL' 'SELECT' statements to query 'R' data frames.,2021-02-06,Ian Cook,https://github.com/ianmcook/tidyquery,TRUE,https://github.com/ianmcook/tidyquery,11662,148,2021-08-07T18:05:10Z,78.79729729729729
tidyr,"Tools to help to create tidy data, where each
    column is a variable, each row is an observation, and each cell
    contains a single value.  'tidyr' contains tools for changing the
    shape (pivoting) and hierarchy (nesting and 'unnesting') of a dataset,
    turning deeply nested lists into rectangular data frames
    ('rectangling'), and extracting values out of string columns. It also
    includes tools for working with missing values (both implicit and
    explicit).",2021-03-03,Hadley Wickham,"https://tidyr.tidyverse.org, https://github.com/tidyverse/tidyr",TRUE,https://github.com/tidyverse/tidyr,25731744,1057,2021-08-27T13:56:46Z,24344.12866603595
tidyREDCap,"
    Helper functions for processing REDCap data in R. 'REDCap' (Research
    Electronic Data CAPture; <https://projectredcap.org>) is a web-enabled
    application for building and managing surveys and databases developed at
    Vanderbilt University.",2020-02-10,Raymond Balise,https://raymondbalise.github.io/tidyREDCap/index.html,TRUE,https://github.com/raymondbalise/tidyredcap,7927,2,2021-02-13T22:25:33Z,3963.5
tidyRSS,"
    With the objective of including data from RSS feeds into your analysis, 
    'tidyRSS' parses RSS, Atom and JSON feeds and returns a tidy data frame.",2020-09-13,Robert Myles McDonnell,https://github.com/RobertMyles/tidyrss,TRUE,https://github.com/robertmyles/tidyrss,55248,45,2020-09-11T14:02:15Z,1227.7333333333333
tidyselect,"A backend for the selecting functions of the 'tidyverse'.
    It makes it easy to implement select-like functions in your own
    packages in a way that is consistent with other 'tidyverse'
    interfaces for selection.",2021-04-30,Lionel Henry,"https://tidyselect.r-lib.org, https://github.com/r-lib/tidyselect",TRUE,https://github.com/r-lib/tidyselect,26016190,99,2021-04-30T07:14:53Z,262789.797979798
tidySEM,"A tidy workflow for generating, estimating, reporting,
    and plotting structural equation models using 'lavaan', 'OpenMx', or
    'Mplus'. Throughout this workflow, elements of syntax, results, and graphs
    are represented as 'tidy' data, making them easy to customize.",2021-07-13,Caspar J. van Lissa,https://cjvanlissa.github.io/tidySEM/,TRUE,https://github.com/cjvanlissa/tidysem,11343,30,2021-08-14T17:30:12Z,378.1
tidyseurat,"It creates an invisible layer that allow to see the 'Seurat' object 
    as tibble and interact seamlessly with the tidyverse.",2021-08-25,Stefano Mangiola,https://github.com/stemangiola/tidyseurat,TRUE,https://github.com/stemangiola/tidyseurat,5778,62,2021-08-19T02:06:20Z,93.19354838709677
tidysq,A tidy approach to analysis of biological sequences. All processing and data-storage functions are heavily optimized to allow the fastest and most efficient data storage.,2021-07-27,Dominik Rafacz,https://github.com/BioGenies/tidysq,TRUE,https://github.com/biogenies/tidysq,757,25,2021-07-24T11:44:23Z,30.28
tidystats,"Save the output of statistical tests in an organized file that can 
  be shared with others or used to report statistics in scientific papers.",2020-09-21,Willem Sleegers,https://willemsleegers.github.io/tidystats/,TRUE,https://github.com/willemsleegers/tidystats,16200,14,2020-11-21T13:05:56Z,1157.142857142857
tidytable,"A tidy interface to 'data.table' that is 'rlang' compatible,
  giving users the speed of 'data.table' with the clean syntax of the tidyverse.",2021-09-03,Mark Fairbanks,https://github.com/markfairbanks/tidytable,TRUE,https://github.com/markfairbanks/tidytable,24699,270,2021-09-03T04:22:32Z,91.47777777777777
tidytext,"Using tidy data principles can make many text mining tasks easier, 
    more effective, and consistent with tools already in wide use. Much of the 
    infrastructure needed for text mining with tidy data frames already exists 
    in packages like 'dplyr', 'broom', 'tidyr', and 'ggplot2'. In this package, 
    we provide functions and supporting data sets to allow conversion of text 
    to and from tidy formats, and to switch seamlessly between tidy tools and 
    existing text mining packages.",2021-04-10,Julia Silge,https://github.com/juliasilge/tidytext,TRUE,https://github.com/juliasilge/tidytext,1396846,992,2021-04-11T18:20:02Z,1408.1108870967741
tidytransit,Read General Transit Feed Specification (GTFS) zipfiles into a list of R dataframes. Perform validation of the data structure against the specification. Analyze the headways and frequencies at routes and stops. Create maps and perform spatial analysis on the routes and stops. Please see the GTFS documentation here for more detail: <http://gtfs.org/>.,2021-07-26,Flavio Poletti,https://github.com/r-transit/tidytransit,TRUE,https://github.com/r-transit/tidytransit,22461,87,2021-08-26T06:24:56Z,258.17241379310343
tidytreatment,"Functions for extracting tidy data from Bayesian treatment effect models, in particular BART, but extensions are possible. Functionality includes extracting tidy posterior summaries as in 'tidybayes' <https://github.com/mjskay/tidybayes>, estimating (average) treatment effects, common support calculations, and plotting useful summaries of these.",2021-07-26,Joshua J Bon,https://github.com/bonStats/tidytreatment,TRUE,https://github.com/bonstats/tidytreatment,1008,10,2021-07-26T23:15:16Z,100.8
tidytree,"Phylogenetic tree generally contains multiple components including node, edge, branch and associated data. 'tidytree' provides an approach to convert tree object to tidy data frame as well as provides tidy interfaces to manipulate tree data.",2021-05-22,Guangchuang Yu,https://yulab-smu.top/treedata-book/,TRUE,https://github.com/yulab-smu/tidytree,148677,29,2021-08-22T04:07:14Z,5126.793103448276
tidyUSDA,"Provides a consistent API to pull United States Department of
    Agriculture census and survey data from the National Agricultural
    Statistics Service (NASS) QuickStats service.",2021-08-01,Brad Lindblad,"https://bradlindblad.github.io/tidyUSDA/,
https://github.com/bradlindblad/tidyUSDA/",TRUE,https://github.com/bradlindblad/tidyusda,13946,33,2021-08-01T17:56:51Z,422.6060606060606
tidyverse,"The 'tidyverse' is a set of packages that work in harmony
    because they share common data representations and 'API' design. This
    package is designed to make it easy to install and load multiple
    'tidyverse' packages in a single step. Learn more about the
    'tidyverse' at <https://www.tidyverse.org>.",2021-04-15,Hadley Wickham,"https://tidyverse.tidyverse.org,
https://github.com/tidyverse/tidyverse",TRUE,https://github.com/tidyverse/tidyverse,31841608,1097,2021-05-03T15:41:17Z,29026.07839562443
tidyvpc,"Perform a Visual Predictive Check (VPC), while accounting for 
    stratification, censoring, and prediction correction. Using piping from 
    'magrittr', the intuitive syntax gives users a flexible and powerful method 
    to generate VPCs using both traditional binning and a new binless approach 
    Jamsen et al. (2018) <doi:10.1002/psp4.12319> with Additive Quantile 
    Regression (AQR) and Locally Estimated Scatterplot Smoothing (LOESS) 
    prediction correction. ",2020-09-29,James Craig,https://github.com/certara/tidyvpc,TRUE,https://github.com/certara/tidyvpc,6794,5,2021-08-24T19:39:07Z,1358.8
tidyxl,"Imports non-tabular from Excel files into R.  Exposes cell content,
    position and formatting in a tidy structure for further manipulation.
    Tokenizes Excel formulas.  Supports '.xlsx' and '.xlsm' via the embedded
    'RapidXML' C++ library <http://rapidxml.sourceforge.net>.  Does not support
    '.xlsb' or '.xls'.",2020-11-16,Duncan Garmonsway,https://github.com/nacnudus/tidyxl,TRUE,https://github.com/nacnudus/tidyxl,52058,190,2021-07-18T19:37:01Z,273.9894736842105
TIGERr,"
    The R implementation of TIGER. 
    TIGER integrates random forest algorithm into an innovative ensemble learning architecture. Benefiting from this advanced architecture, TIGER is resilient to outliers, free from model tuning and less likely to be affected by specific hyperparameters.
    TIGER supports targeted and untargeted metabolomics data and is competent to perform both intra- and inter-batch technical variation removal. TIGER can also be used for cross-kit adjustment to ensure data obtained from different analytical assays can be effectively combined and compared.
    Reference: Han S. et al. (2021) TIGER: technical variation elimination for metabolomics data using ensemble learning architecture (submitted).",2021-09-02,Siyu Han,NA,TRUE,https://github.com/han-siyu/tiger,0,0,2021-09-01T08:40:40Z,NA
tigris,"Download TIGER/Line shapefiles from the United States Census Bureau 
    (<https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html>) 
    and load into R as 'sf' objects.",2021-06-18,Kyle Walker,https://github.com/walkerke/tigris,TRUE,https://github.com/walkerke/tigris,279013,234,2021-08-19T10:05:13Z,1192.3632478632478
tikzDevice,"Provides a graphics output device for R that records plots
        in a LaTeX-friendly format. The device transforms plotting
        commands issued by R functions into LaTeX code blocks. When
        included in a LaTeX document, these blocks are interpreted with
        the help of 'TikZ'---a graphics package for TeX and friends
        written by Till Tantau. Using the 'tikzDevice', the text of R
        plots can contain LaTeX commands such as mathematical formula.
        The device also allows arbitrary LaTeX code to be inserted into
        the output stream.",2020-06-30,Ralf Stubner,https://github.com/daqana/tikzDevice,TRUE,https://github.com/daqana/tikzdevice,195619,109,2021-05-24T17:55:49Z,1794.6697247706422
tiledb,"The data science storage engine 'TileDB' introduces a
 powerful on-disk format for multi-dimensional arrays. It supports
 dense and sparse arrays, dataframes and key-values stores, cloud
 storage ('S3', 'GCS', 'Azure'), chunked arrays, multiple compression,
 encryption and checksum filters, uses a fully multi-threaded
 implementation, supports parallel I/O, data versioning ('time
 travel'), metadata and groups. It is implemented as an embeddable
 cross-platform C++ library with APIs from several languages, and
 integrations.",2021-08-10,Dirk Eddelbuettel,https://github.com/TileDB-Inc/TileDB-R,TRUE,https://github.com/tiledb-inc/tiledb-r,6412,64,2021-09-02T13:14:23Z,100.1875
tiler,"Creates geographic map tiles from geospatial map files or non-geographic map tiles from simple image files. 
    This package provides a tile generator function for creating map tile sets for use with packages such as 'leaflet'. 
    In addition to generating map tiles based on a common raster layer source, it also handles the non-geographic edge case, producing map tiles from arbitrary images.
    These map tiles, which have a non-geographic, simple coordinate reference system (CRS), can also be used with 'leaflet' when applying the simple CRS option.
    Map tiles can be created from an input file with any of the following extensions: tif, grd and nc for spatial maps and png, jpg and bmp for basic images.
    This package requires 'Python' and the 'gdal' library for 'Python'. 
    'Windows' users are recommended to install 'OSGeo4W' (<https://trac.osgeo.org/osgeo4w/>) as an easy way to obtain the required 'gdal' support for 'Python'.",2021-02-20,Matthew Leonawicz,"https://docs.ropensci.org/tiler/,
https://github.com/ropensci/tiler",TRUE,https://github.com/ropensci/tiler,19481,56,2021-02-20T16:51:14Z,347.875
timechange,"Efficient routines for manipulation of date-time objects while
   accounting for time-zones and daylight saving times. The package includes
   utilities for updating of date-time components (year, month, day etc.),
   modification of time-zones, rounding of date-times, period addition and
   subtraction etc. Parts of the 'CCTZ' source code, released under the Apache
   2.0 License, are included in this package. See
   <https://github.com/google/cctz> for more details.",2020-10-05,Vitalie Spinu,https://github.com/vspinu/timechange/,TRUE,https://github.com/vspinu/timechange,14418,19,2020-10-22T10:19:40Z,758.8421052631579
timeperiodsR,"Simple definition of time intervals for the current, previous, and next week, month, quarter and year.",2020-04-03,Alexey Seleznev,"https://selesnow.github.io/timeperiodsR, https://t.me/R4marketing,
https://www.youtube.com/playlist?list=PLD2LDq8edf4qed2KVKfXmKdh0OQcdj9gw",TRUE,https://github.com/selesnow/timeperiodsr,13928,2,2021-05-25T14:07:33Z,6964
timereg,"Programs for Martinussen and Scheike (2006), `Dynamic Regression
    Models for Survival Data', Springer Verlag.  Plus more recent developments.
    Additive survival model, semiparametric proportional odds model, fast
    cumulative residuals, excess risk models and more. Flexible competing risks
    regression including GOF-tests. Two-stage frailty modelling. PLS for the
    additive risk model. Lasso in the 'ahaz' package.",2021-05-20,Thomas Scheike with contributions from Torben Martinussen,https://github.com/scheike/timereg,TRUE,https://github.com/scheike/timereg,219026,20,2021-05-20T10:02:53Z,10951.3
TimeSeries.OBeu,"Estimate and return the needed parameters for visualizations designed for 'OpenBudgets.eu' <http://openbudgets.eu/> time series data. Calculate time series model and forecast parameters in budget time series data of municipalities across Europe, according to the 'OpenBudgets.eu' data model. There are functions for measuring deterministic and stochastic trend of the input time series data with 'ACF', 'PACF', 'Phillips Perron' test, 'Augmented Dickey Fuller (ADF)' test, 'Kwiatkowski-Phillips-Schmidt-Shin (KPSS)' test, 'Mann Kendall' test for monotonic trend and 'Cox and Stuart' trend test, decomposing with local regression models or 'stl' decomposition, fitting the appropriate 'arima' model and provide forecasts for the input 'OpenBudgets.eu' time series fiscal data. Also, can be used generally to extract visualization parameters convert them to 'JSON' format and use them as input in a different graphical interface.",2019-12-17,Kleanthis Koupidis,https://github.com/okgreece/TimeSeries.OBeu,TRUE,https://github.com/okgreece/timeseries.obeu,13930,1,2021-09-02T13:44:30Z,13930
timetk,"
    Easy visualization, wrangling, and feature engineering of time series data for 
    forecasting and machine learning prediction. Consolidates and extends time series functionality 
    from packages including 'dplyr', 'stats', 'xts', 'forecast', 'slider', 'padr', 'recipes', and 'rsample'.",2021-01-18,Matt Dancho,https://github.com/business-science/timetk,TRUE,https://github.com/business-science/timetk,919731,429,2021-08-12T08:40:46Z,2143.895104895105
timevis,"Create rich and fully interactive timeline visualizations.
    Timelines can be included in Shiny apps and R markdown documents, or viewed
    from the R console and 'RStudio' Viewer. 'timevis' includes an extensive API
    to manipulate a timeline after creation, and supports getting data out of
    the visualization into R. Based on the 'vis.js' Timeline module and the
    'htmlwidgets' R package.",2020-09-16,Dean Attali,"https://github.com/daattali/timevis,
https://daattali.com/shiny/timevis-demo/",TRUE,https://github.com/daattali/timevis,80474,493,2021-01-17T09:13:34Z,163.23326572008114
tint,"A 'tufte'-alike style for 'rmarkdown'.
 A modern take on the 'Tufte' design for pdf and html vignettes,
 building on the 'tufte' package with additional contributions
 from the 'knitr' and 'ggtufte' package, and also acknowledging
 the key influence of 'envisioned css'.",2020-07-18,Dirk Eddelbuettel and Jonathan Gilligan,http://dirk.eddelbuettel.com/code/tint.html,TRUE,https://github.com/eddelbuettel/tint,24883,241,2021-06-02T23:09:23Z,103.24896265560166
tinylabels,"Assign, extract, or remove variable labels from R vectors.
  Lightweight and dependency-free.",2021-03-27,Marius Barth,https://github.com/mariusbarth/tinylabels,TRUE,https://github.com/mariusbarth/tinylabels,5547,0,2021-03-27T14:02:56Z,NA
tinyscholar,"Provides functions to get personal 'Google Scholar'
    profile data from web API and show it in table or figure format.",2021-05-17,Shixiang Wang,https://github.com/ShixiangWang/tinyscholar,TRUE,https://github.com/shixiangwang/tinyscholar,4874,6,2021-05-16T15:10:13Z,812.3333333333334
tinyspotifyr,"An R wrapper for the 'Spotify' Web API 
  <https://developer.spotify.com/web-api/>.",2021-02-24,Troy Hernandez,https://github.com/TroyHernandez/tinyspotifyr,TRUE,https://github.com/troyhernandez/tinyspotifyr,1967,10,2021-02-21T13:49:16Z,196.7
tinytest,"Provides a lightweight (zero-dependency) and easy to use 
    unit testing framework. Main features: install tests with 
    the package. Test results are treated as data that can be stored and 
    manipulated. Test files are R scripts interspersed with test commands, that
    can be programmed over. Fully automated build-install-test sequence for 
    packages. Skip tests when not run locally (e.g. on CRAN). Flexible and 
    configurable output printing. Compare computed output with output stored 
    with the package. Run tests in parallel. Extensible by other packages.
    Report side effects.",2021-07-06,Mark van der Loo,https://github.com/markvanderloo/tinytest,TRUE,https://github.com/markvanderloo/tinytest,306718,178,2021-08-20T21:29:38Z,1723.1348314606741
tinytex,"Helper functions to install and maintain the 'LaTeX' distribution
  named 'TinyTeX' (<https://yihui.org/tinytex/>), a lightweight, cross-platform,
  portable, and easy-to-maintain version of 'TeX Live'. This package also
  contains helper functions to compile 'LaTeX' documents, and install missing
  'LaTeX' packages automatically.",2021-08-05,Yihui Xie,https://github.com/yihui/tinytex,TRUE,https://github.com/yihui/tinytex,21526567,651,2021-08-20T15:28:38Z,33066.923195084484
tippy,'Htmlwidget' of 'Tippyjs' to add tooltips to 'Shiny' apps and 'R markdown' documents.,2021-01-11,John Coene,https://tippy.john-coene.com/,TRUE,https://github.com/johncoene/tippy,19807,55,2021-07-07T17:22:02Z,360.1272727272727
TKCat,"Facilitate the management of data from knowledge
   resources that are frequently used alone or together
   in research environments.
   In 'TKCat', knowledge resources are manipulated as modeled database (MDB)
   objects. These objects provide access to the data tables along with a general
   description of the resource and a detail data model documenting the
   tables, their fields and their relationships.
   These MDB are then gathered in catalogs that can be easily
   explored an shared.
   Finally, 'TKCat' provides tools to easily subset, filter and combine MDBs and
   create new catalogs suited for specific needs.",2021-03-04,Patrice Godard,https://github.com/patzaw/TKCat,TRUE,https://github.com/patzaw/tkcat,1941,2,2021-08-24T08:21:19Z,970.5
tm.plugin.factiva,"Provides a 'tm' Source to create corpora from
  articles exported from the Dow Jones 'Factiva' content provider as
  XML or HTML files. It is able to read both text content and meta-data
  information (including source, date, title, author, subject,
  geographical coverage, company, industry, and various
  provider-specific fields).",2019-10-19,Milan Bouchet-Valat,https://github.com/nalimilan/R.TeMiS,TRUE,https://github.com/nalimilan/r.temis,24117,20,2021-05-12T18:07:40Z,1205.85
tm.plugin.koRpus,"Enhances 'koRpus' text object classes and methods to also support large
          corpora. Hierarchical ordering of corpus texts into arbitrary categories will
          be preserved. Provided classes and methods also improve the ability of using
          the 'koRpus' package together with the 'tm' package. To ask for help, report
          bugs, suggest feature improvements, or discuss the global development of the
          package, please subscribe to the koRpus-dev mailing list
          (<https://korpusml.reaktanz.de>).",2021-05-18,m.eik michalke,https://reaktanz.de/?c=hacking&s=koRpus,TRUE,https://github.com/undocumeantit/tm.plugin.korpus,2873,2,2021-05-18T13:11:53Z,1436.5
tm.plugin.lexisnexis,"Provides a 'tm' Source to create corpora from
  articles exported from the 'LexisNexis' content provider as
  HTML files. It is able to read both text content and meta-data
  information (including source, date, title, author and pages).
  Note that the file format is highly unstable: there is no warranty
  that this package will work for your corpus, and you may have
  to adjust the code to adapt it to your particular format.",2019-10-19,Milan Bouchet-Valat,https://github.com/nalimilan/R.TeMiS,TRUE,https://github.com/nalimilan/r.temis,22783,20,2021-05-12T18:07:40Z,1139.15
tmap,"Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.",2021-06-16,Martijn Tennekes,https://github.com/mtennekes/tmap,TRUE,https://github.com/mtennekes/tmap,752052,575,2021-08-27T08:52:31Z,1307.9165217391305
tmaptools,"Set of tools for reading and processing spatial data. The aim is to supply the workflow to create thematic maps. This package also facilitates 'tmap', the package for visualizing thematic maps.",2021-01-19,Martijn Tennekes,https://github.com/mtennekes/tmaptools,TRUE,https://github.com/mtennekes/tmaptools,613429,32,2021-01-19T17:48:39Z,19169.65625
tmbstan,"Enables all 'rstan' functionality for a 'TMB' model object, in particular MCMC sampling and chain visualization. Sampling can be performed with or without Laplace approximation for the random effects.",2019-05-18,Kasper Kristensen,NA,TRUE,https://github.com/kaskr/tmbstan,16198,22,2020-11-21T14:11:57Z,736.2727272727273
Tmisc,"Miscellaneous utility functions for data manipulation,
    data tidying, and working with gene expression data.",2020-09-16,Stephen Turner,"https://github.com/stephenturner/Tmisc,
https://stephenturner.github.io/Tmisc/",TRUE,https://github.com/stephenturner/tmisc,27510,6,2021-03-09T20:25:28Z,4585
tmuxr,"Create, control, and capture 'tmux' sessions, windows, and panes
    using a pipeable API.",2020-05-22,Jeroen Janssens,"https://datascienceworkshops.github.io/tmuxr,
https://github.com/datascienceworkshops/tmuxr",TRUE,https://github.com/datascienceworkshops/tmuxr,3245,45,2021-02-06T20:20:12Z,72.11111111111111
togglr,Use the <http://toggl.com> time tracker api through R.,2018-08-13,Vincent Guyader,https://github.com/ThinkR-open/togglr,TRUE,https://github.com/thinkr-open/togglr,15414,37,2021-06-27T18:58:02Z,416.5945945945946
tokenizers,"Convert natural language text into tokens. Includes tokenizers for
    shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs,
    characters, shingled characters, lines, tweets, Penn Treebank, regular
    expressions, as well as functions for counting characters, words, and sentences,
    and a function for splitting longer texts into separate documents, each with
    the same number of words.  The tokenizers have a consistent interface, and
    the package is built on the 'stringi' and 'Rcpp' packages for  fast
    yet correct tokenization in 'UTF-8'. ",2018-03-29,Lincoln Mullen,https://lincolnmullen.com/software/tokenizers/,TRUE,https://github.com/ropensci/tokenizers,1942070,166,2021-07-14T20:54:14Z,11699.216867469879
tongfen,"Several functions to allow comparisons of data across different geographies, in particular for Canadian census data from different censuses.",2021-07-24,Jens von Bergmann,"https://github.com/mountainMath/tongfen,
https://mountainmath.github.io/tongfen/",TRUE,https://github.com/mountainmath/tongfen,3625,21,2021-07-23T19:43:15Z,172.61904761904762
toOrdinal,Language specific cardinal to ordinal number conversion.,2019-02-24,Damian W. Betebenner,"https://CenterForAssessment.github.io/toOrdinal,
https://github.com/CenterForAssessment/toOrdinal,
https://cran.r-project.org/package=toOrdinal",TRUE,https://github.com/centerforassessment/toordinal,42147,6,2021-04-25T01:27:21Z,7024.5
TopDom,"The 'TopDom' method identifies topological domains in genomes from Hi-C sequence data (Shin et al., 2016 <doi:10.1093/nar/gkv1505>).  The authors published an implementation of their method as an R script (two different versions; also available in this package).  This package originates from those original 'TopDom' R scripts and provides help pages adopted from the original 'TopDom' PDF documentation.  It also provides a small number of bug fixes to the original code.",2021-05-06,Henrik Bengtsson,https://github.com/HenrikBengtsson/TopDom,TRUE,https://github.com/henrikbengtsson/topdom,1460,6,2021-05-06T15:50:52Z,243.33333333333334
torch,"Provides functionality to define and train neural networks similar to 
    'PyTorch' by Paszke et al (2019) <arXiv:1912.01703> but written entirely in R 
    using the 'libtorch' library. Also supports low-level tensor operations and 
    'GPU' acceleration.",2021-08-17,Daniel Falbel,"https://torch.mlverse.org/docs, https://github.com/mlverse/torch",TRUE,https://github.com/mlverse/torch,26669,260,2021-08-23T15:06:51Z,102.57307692307693
torchdatasets,"Provides datasets in a format that can be easily consumed by torch 'dataloaders'.
  Handles data downloading from multiple sources, caching and pre-processing so
  users can focus only on their model implementations.",2021-01-19,Daniel Falbel,"https://mlverse.github.io/torchdatasets/,
https://github.com/mlverse/torchdatasets",TRUE,https://github.com/mlverse/torchdatasets,2869,5,2021-08-24T01:33:04Z,573.8
torchvision,"Provides access to datasets, models and preprocessing
    facilities for deep learning with images. Integrates seamlessly
    with the 'torch' package and it's 'API' borrows heavily from 
    'PyTorch' vision package.",2021-08-17,Daniel Falbel,"https://torchvision.mlverse.org,
https://github.com/mlverse/torchvision",TRUE,https://github.com/mlverse/torchvision,7399,49,2021-08-24T13:49:39Z,151
tosca,"A framework for statistical analysis in content analysis. In addition to a pipeline for preprocessing text corpora and linking to the latent Dirichlet allocation from the 'lda' package, plots are offered for the descriptive analysis of text corpora and topic models. In addition, an implementation of Chang's intruder words and intruder topics is provided. Sample data for the vignette is included in the toscaData package, which is available on gitHub: <https://github.com/Docma-TU/toscaData>.",2021-04-20,Lars Koppers,"https://github.com/Docma-TU/tosca,
https://doi.org/10.5281/zenodo.3591068",TRUE,https://github.com/docma-tu/tosca,14463,12,2021-04-18T13:59:21Z,1205.25
tosr,"The goal of 'tosr' is to create the Tree of Science from 
             Web of Science (WoS) and Scopus data. It can read files 
             from both sources at the same time. More information 
             can be found in Valencia-Hernández (2020) 
             <https://revistas.unal.edu.co/index.php/ingeinv/article/view/77718>.",2021-07-14,Sebastian Robledo,"https://github.com/coreofscience/tosr,
https://coreofscience.github.io/tosr/",TRUE,https://github.com/coreofscience/tosr,662,1,2021-07-14T00:03:45Z,662
totalcensus,"Download summary files from Census Bureau <https://www2.census.gov/> 
    and extract data, in particular high resolution data at 
    block, block group, and tract level, from decennial census and 
    American Community Survey 1-year and 5-year estimates.",2021-06-14,Guanglai Li,https://github.com/GL-Li/totalcensus,TRUE,https://github.com/gl-li/totalcensus,60543,41,2021-07-14T15:40:27Z,1476.658536585366
TotalCopheneticIndex,"Measures the degree of balance for a given phylogenetic tree by 
  calculating the Total Cophenetic Index.
  Reference: A. Mir, F. Rossello, L. A. Rotger (2013).
  A new balance index for phylogenetic trees.
  Math. Biosci. 241, 125-136 <doi:10.1016/j.mbs.2012.10.005>.",2021-06-23,Martin R. Smith,https://github.com/ms609/tci/,TRUE,https://github.com/ms609/tci,15786,0,2021-06-23T07:07:48Z,NA
touch,"R implementation of the software tools developed in the H-CUP
    (Healthcare Cost and Utilization Project) <https://www.hcup-us.ahrq.gov>
    and AHRQ (Agency for Healthcare Research and Quality)
    <https://www.ahrq.gov>.  It currently contains functions for mapping ICD-9
    codes to the AHRQ comorbidity measures and translating ICD-9
    (resp. ICD-10) codes to ICD-10 (resp. ICD-9) codes based on GEM (General
    Equivalence Mappings) from CMS (Centers for Medicare and Medicaid
    Services).",2019-11-17,Wenjie Wang,https://github.com/wenjie2wang/touch,TRUE,https://github.com/wenjie2wang/touch,14521,3,2020-11-23T20:13:42Z,4840.333333333333
TouRnament,Contains two functions related to sports competitions. One to create league tables and one to create a match schedule.,2019-10-05,Tobias Wolfanger,NA,TRUE,https://github.com/captaincaracho/tournament,7970,1,2021-01-17T13:36:12Z,7970
tourr,"Implements geodesic interpolation and basis
    generation functions that allow you to create new tour
    methods from R.",2020-12-11,Hadley Wickham,https://github.com/ggobi/tourr,TRUE,https://github.com/ggobi/tourr,41795,55,2021-07-29T06:00:17Z,759.9090909090909
toxEval,"Data analysis package for estimating potential biological effects from chemical concentrations in environmental samples. Included are a set of functions to analyze, visualize, and organize measured concentration data as it relates to user-selected chemical-biological interaction benchmark data such as water quality criteria. The intent of these analyses is to develop a better understanding of the potential biological relevance of environmental chemistry data. Results can be used to prioritize which chemicals at which sites may be of greatest concern. These methods are meant to be used as a screening technique to predict potential for biological influence from chemicals that ultimately need to be validated with direct biological assays. A description of the analysis can be found in Blackwell et al. (2017) <doi:10.1021/acs.est.7b01613>.",2020-10-09,Laura DeCicco,NA,TRUE,https://github.com/usgs-r/toxeval,12114,9,2020-10-08T23:03:49Z,1346
Tplyr,A tool created to simplify the data manipulation necessary to create clinical reports.,2021-07-13,Eli Miller,https://github.com/atorus-research/Tplyr,TRUE,https://github.com/atorus-research/tplyr,7807,26,2021-07-13T15:10:05Z,300.2692307692308
TR8,"Plant ecologists often need to collect ""traits"" data
    about plant species which are often scattered among various
    databases: TR8 contains a set of tools which take care of
    automatically retrieving some of those functional traits data
    for plant species from publicly available databases (Biolflor,
    The Ecological Flora of the British Isles, LEDA traitbase, Ellenberg
    values for Italian Flora, Mycorrhizal intensity databases, Catminat, BROT,
    PLANTS, Jepson Flora Project).
    The TR8 name, inspired by ""car plates"" jokes, was chosen since
    it both reminds of the main object of the package and is
    extremely short to type.",2020-12-01,Gionata Bocci,https://github.com/GioBo/TR8,TRUE,https://github.com/giobo/tr8,18389,13,2020-12-01T22:34:19Z,1414.5384615384614
track2KBA,"Functions for preparing and analyzing animal tracking data, 
    with the intention of identifying areas which are potentially important at 
    the population level and therefore of conservation interest. Areas identified 
    using this package may be checked against global or regionally-defined criteria,
    such as those set by the Key Biodiversity Area program. The method
    published herein is an optimized and updated version of the method described in Lascelles
    et al. 2016 <doi:10.1111/ddi.12411>.",2021-08-07,Martin Beal,https://github.com/BirdLifeInternational/track2kba,TRUE,https://github.com/birdlifeinternational/track2kba,418,3,2021-08-20T10:39:58Z,139.33333333333334
trackdem,"Obtain population density and body size structure, using video material or image sequences as input. Functions assist in the creation of image sequences from videos, background detection and subtraction, particle identification and tracking. An artificial neural network can be trained for noise filtering. The goal is to supply accurate estimates of population size, structure and/or individual behavior, for use in  evolutionary and ecological studies.",2020-02-27,Marjolein Bruijning,https://github.com/marjoleinbruijning/trackdem,TRUE,https://github.com/marjoleinbruijning/trackdem,18800,5,2021-06-07T11:05:54Z,3760
trackdf,"Data frame class for storing collective movement data (e.g. fish
    schools, ungulate herds, baboon troops) collected from GPS trackers or
    computer vision tracking software. ",2021-01-13,Simon Garnier,"https://swarm-lab.github.io/trackdf/,
https://github.com/swarm-lab/trackdf",TRUE,https://github.com/swarm-lab/trackdf,10365,2,2021-01-12T12:04:22Z,5182.5
trackdown,"Collaborative writing and editing of R Markdown (or Sweave) documents. The local .Rmd (or .Rnw) is uploaded as a plain-text file to Google Drive. By taking advantage of the easily readable Markdown (or LaTeX) syntax and the well-known online interface offered by Google Docs, collaborators can easily contribute to the writing and editing process. After integrating all authors’ contributions, the final document can be downloaded and rendered locally.",2021-08-06,Emily Kothe,"https://github.com/claudiozandonella/trackdown/,
https://claudiozandonella.github.io/trackdown/",TRUE,https://github.com/claudiozandonella/trackdown,408,101,2021-08-27T16:49:32Z,4.03960396039604
tractor.base,"Functions for working with magnetic resonance images. Reading and
    writing of popular file formats (DICOM, Analyze, NIfTI-1, NIfTI-2, MGH);
    interactive and non-interactive visualisation; flexible image manipulation;
    metadata and sparse image handling.",2020-12-13,Jon Clayden,"http://www.tractor-mri.org.uk, https://github.com/tractor/tractor",TRUE,https://github.com/tractor/tractor,32716,26,2021-05-13T22:46:24Z,1258.3076923076924
trade,"A collection of tools for trade practitioners, including the ability to calibrate different consumer demand systems and simulate the effects of tariffs and quotas under different competitive regimes. These tools are derived from Anderson et al. (2001) <doi:10.1016/S0047-2727(00)00085-2> and Froeb et al. (2003) <doi:10.1016/S0304-4076(02)00166-5>.",2021-05-19,Charles Taragin,https://github.com/luciu5/trade,TRUE,https://github.com/luciu5/trade,11758,0,2021-05-06T16:14:54Z,NA
tradepolicy,"Datasets from An Advanced Guide to Trade Policy Analysis
    (Year: 2016, ISBN: 978-92-870-4367-2) by Yotov, et al. and functions to 
    report regression summaries with clustered robust standard errors.",2021-05-27,Mauricio Vargas,https://r.tiid.org/R_structural_gravity/,TRUE,https://github.com/pachamaltese/tradepolicy,955,2,2021-05-30T05:18:49Z,477.5
TrafficBDE,"Estimate and return either the traffic speed or the car entries in the city of Thessaloniki using historical traffic data. It's used in transport pilot <http://trafficstatusprediction.imet.gr/> of the 'BigDataEurope' project <https://www.big-data-europe.eu/>. There are functions for processing these data, training a neural network, select the most appropriate model and predict the traffic speed or the car entries for a selected time date.",2018-03-01,Aikaterini Chatzopoulou,https://github.com/okgreece/TrafficBDE,TRUE,https://github.com/okgreece/trafficbde,11416,0,2021-08-23T17:48:10Z,NA
trainR,"The goal of 'trainR' is to provide a simple interface to the 
    National Rail Enquiries (NRE) systems. There are few data feeds 
    available, the simplest of them is Darwin, which provides real-time 
    arrival and departure predictions, platform numbers, delay estimates, 
    schedule changes and cancellations. Other data feeds provide historical 
    data, Historic Service Performance (HSP), and much more. 'trainR' 
    simplifies the data retrieval, so that the users can focus on their 
    analyses. For more details visit 
    <https://www.nationalrail.co.uk/46391.aspx>. ",2021-01-20,Roberto Villegas-Diaz,"https://github.com/villegar/trainR/,
https://villegar.github.io/trainR/",TRUE,https://github.com/villegar/trainr,2891,2,2021-08-10T21:04:29Z,1445.5
traipse,"A collection of commonly used tools for animal movement and other tracking 
 data. Variously distance, angle, bearing, distance-to, bearing-to and speed are 
 provided for geographic data that can be used directly or within 'tidyverse' 
 syntax. Distances and bearings are calculated using modern geodesic methods as 
 provided by Charles F. F. Karney (2013) <doi:10.1007/s00190-012-0578-z> 
 via the 'geodist' and 'geosphere' packages. ",2020-10-25,Michael Sumner,https://github.com/Trackage/traipse,TRUE,https://github.com/trackage/traipse,14762,15,2020-10-23T10:42:47Z,984.1333333333333
traitdataform,"Assistance for handling ecological trait data and applying the 
    Ecological Trait-Data Standard terminology (Schneider et al. 2019
    <doi:10.1111/2041-210X.13288>). There are two major use cases: (1) preparation of
    own trait datasets for publication, and (2) harmonizing
    trait datasets from different sources by re-formatting them into a unified
    format. See 'traitdataform' website for full documentation. ",2021-09-02,Florian D. Schneider,"https://ecologicaltraitdata.github.io/traitdataform/,
https://github.com/ecologicaltraitdata/traitdataform",TRUE,https://github.com/ecologicaltraitdata/traitdataform,12472,25,2021-09-01T07:39:49Z,498.88
trajectories,"Classes and methods for trajectory data, with support for nesting individual Track objects in track sets (Tracks) and track sets for different entities in collections of Tracks. Methods include selection, generalization, aggregation, intersection, simulation, and plotting.",2021-03-16,Edzer Pebesma,https://github.com/edzer/trajectories,TRUE,https://github.com/edzer/trajectories,25112,27,2021-03-15T11:55:19Z,930.074074074074
trajr,"A toolbox to assist with statistical analysis of 2-dimensional animal trajectories.
    It provides simple access to algorithms for calculating and assessing a variety of 
    characteristics such as speed and acceleration, as well as multiple measures of 
    straightness or tortuosity. McLean & Skowron Volponi (2018) <doi:10.1111/eth.12739>.",2020-12-17,Jim McLean,https://github.com/JimMcL/trajr,TRUE,https://github.com/jimmcl/trajr,19591,17,2021-08-20T12:15:22Z,1152.4117647058824
transformr,"In order to smoothly animate the transformation of polygons and 
    paths, many aspects needs to be taken into account, such as differing number 
    of control points, changing center of rotation, etc. The 'transformr' 
    package provides an extensive framework for manipulating the shapes of 
    polygons and paths and can be seen as the spatial brother to the 'tweenr' 
    package.",2020-07-05,Thomas Lin Pedersen,https://github.com/thomasp85/transformr,TRUE,https://github.com/thomasp85/transformr,129582,93,2021-07-02T12:09:34Z,1393.3548387096773
tranSurv,"A latent, quasi-independent truncation time is assumed to be linked with the observed dependent truncation time, the event time, and an unknown transformation parameter via a structural transformation model. The transformation parameter is chosen to minimize the conditional Kendall's tau (Martin and Betensky, 2005) <doi:10.1198/016214504000001538> or the regression coefficient estimates (Jones and Crowley, 1992) <doi:10.2307/2336782>. The marginal distribution for the truncation time and the event time are completely left unspecified. The methodology is applied to survival curve estimation and regression analysis.",2021-01-12,Sy Han (Steven) Chiou,https://github.com/stc04003/tranSurv,TRUE,https://github.com/stc04003/transurv,12533,0,2021-07-17T17:33:17Z,NA
transx,"Univariate time series operations that follow an opinionated design. 
    The main principle of 'transx' is to keep the number of observations the same. 
    Operations that reduce this number have to fill the observations gap.",2020-11-27,Kostas Vasilopoulos,https://github.com/kvasilopoulos/transx,TRUE,https://github.com/kvasilopoulos/transx,2948,2,2021-01-24T19:40:13Z,1474
treasuryTR,"Generate Total Returns (TR) from bond yield data with fixed maturity, e.g. 
  reported treasury yields. The generated TR series are very close to alternative series 
  that can be purchased (e.g. CRSP, Bloomberg), suggesting they are a high-quality 
  alternative for those, see Swinkels (2019) <doi:10.3390/data4030091>.",2021-07-22,Martin Geissmann,https://github.com/mgei/treasuryTR,TRUE,https://github.com/mgei/treasurytr,3506,1,2021-05-07T08:37:14Z,3506
TreeBUGS,"User-friendly analysis of hierarchical multinomial processing tree (MPT) 
    models that are often used in cognitive psychology. Implements the latent-trait 
    MPT approach (Klauer, 2010) <DOI:10.1007/s11336-009-9141-0> and the beta-MPT 
    approach (Smith & Batchelder, 2010) <DOI:10.1016/j.jmp.2009.06.007> to model 
    heterogeneity of participants. MPT models are conveniently specified by an
    .eqn-file as used by other MPT software and data are provided by a .csv-file 
    or directly in R. Models are either fitted by calling JAGS or by an MPT-tailored 
    Gibbs sampler in C++ (only for nonhierarchical and beta MPT models). Provides 
    tests of heterogeneity and MPT-tailored summaries and plotting functions.
    A detailed documentation is available in Heck, Arnold, & Arnold (2018) 
    <DOI:10.3758/s13428-017-0869-7>.",2021-01-08,Daniel W. Heck,https://github.com/danheck/TreeBUGS,TRUE,https://github.com/danheck/treebugs,24379,8,2021-01-07T23:59:11Z,3047.375
treeclim,"Bootstrapped response and correlation functions,
    seasonal correlations and evaluation of reconstruction
    skills for use in dendroclimatology and dendroecology,
    see Zang and Biondi (2015) <doi:10.1111/ecog.01335>.",2020-10-08,Christian Zang,https://github.com/cszang/treeclim,TRUE,https://github.com/cszang/treeclim,25444,11,2020-10-08T13:19:01Z,2313.090909090909
treeDA,"Performs sparse discriminant analysis on a combination of
    node and leaf predictors when the predictor variables are
    structured according to a tree, as described in Fukuyama et
    al. (2017) <doi:10.1371/journal.pcbi.1005706>.",2021-05-14,Julia Fukuyama,https://github.com/jfukuyama/treeda,TRUE,https://github.com/jfukuyama/treeda,12930,0,2021-05-14T19:39:20Z,NA
TreeDist,"Implements measures of tree similarity, including 
  information-based generalized Robinson-Foulds distances
  (Phylogenetic Information Distance, Clustering Information Distance,
  Matching Split Information Distance; Smith, 2020)
  <doi:10.1093/bioinformatics/btaa614>; 
  Jaccard-Robinson-Foulds distances (Bocker et al. 2013)
  <doi:10.1007/978-3-642-40453-5_13>, 
  including the Nye et al. (2006) metric <doi:10.1093/bioinformatics/bti720>;
  the Matching Split Distance (Bogdanowicz & Giaro 2012)
  <doi:10.1109/TCBB.2011.48>;
  Maximum Agreement Subtree distances;
  the Kendall-Colijn (2016) distance <doi:10.1093/molbev/msw124>, and the
  Nearest Neighbour Interchange (NNI) distance, approximated per Li et al. 
  (1996) <doi:10.1007/3-540-61332-3_168>.
  Includes tools for visualizing mappings of tree space, for
  calculating the median of sets of trees, and for computing the information
  content of trees and splits.",2021-07-13,Martin R. Smith,"https://ms609.github.io/TreeDist/,
https://github.com/ms609/TreeDist/",TRUE,https://github.com/ms609/treedist,8753,8,2021-08-19T08:25:59Z,1094.125
treeheatr,"Creates interpretable decision tree visualizations 
    with the data represented as a heatmap at the tree's leaf nodes.
    'treeheatr' utilizes the customizable 'ggparty' package for 
    drawing decision trees.",2020-11-19,Trang Le,"https://trang1618.github.io/treeheatr/index.html,
https://trang1618.github.io/treeheatr-manuscript/",TRUE,https://github.com/trang1618/treeheatr,5704,47,2021-08-25T18:37:48Z,121.36170212765957
treemapify,Provides 'ggplot2' geoms for drawing treemaps.,2021-01-08,David Wilkins,https://wilkox.org/treemapify/,TRUE,https://github.com/wilkox/treemapify,614649,187,2021-03-13T06:24:38Z,3286.8930481283423
treenomial,"Provides functionality for creation and comparison of polynomials that uniquely
  describe trees as introduced in Liu (2019, <arXiv:1904.03332>). The core method
  converts rooted unlabeled phylo objects from 'ape' to the tree defining polynomials 
  described with coefficient matrices. Additionally, a conversion for rooted binary trees 
  with binary trait labels is also provided. Once the polynomials of trees are calculated 
  there are functions to calculate distances, distance matrices and plot different distance 
  trees from a target tree. Manipulation and conversion to the tree defining polynomials is 
  implemented in C++ with 'Rcpp' and 'RcppArmadillo'. Furthermore, parallel programming with 
  'RcppThread' is used to improve performance converting to polynomials and calculating distances. ",2020-10-05,Matthew Gould,https://github.com/mattgou1d/treenomial,TRUE,https://github.com/mattgou1d/treenomial,9331,1,2020-10-05T14:43:23Z,9331
treeplyr,"Matches phylogenetic trees and trait data, and
    allows simultaneous manipulation of the tree and data using 'dplyr'.",2020-09-17,Josef Uyeda,https://github.com/uyedaj/treeplyr,TRUE,https://github.com/uyedaj/treeplyr,15717,31,2020-09-14T13:07:37Z,507
TreeSearch,"Searches for phylogenetic trees that are optimal using a 
  user-defined criterion. 
  Handles inapplicable data using the algorithm of 
  Brazeau, Guillerme and Smith (2019) <doi:10.1093/sysbio/syy083>.
  Implements Profile Parsimony (Faith and Trueman, 2001) 
  <doi:10.1080/10635150118627>, and Successive Approximations (Farris, 1969) 
  <doi:10.2307/2412182>.",2020-07-09,Martin R. Smith,"https://ms609.github.io/TreeSearch,
https://github.com/ms609/TreeSearch",TRUE,https://github.com/ms609/treesearch,19999,1,2021-08-17T10:05:09Z,19999
treespace,"Tools for the exploration of distributions of phylogenetic trees.
    This package includes a 'shiny' interface which can be started from R using
    treespaceServer(). 
    For further details see Jombart et al. (2017) <DOI:10.1111/1755-0998.12676>.",2021-03-25,Thibaut Jombart,"https://cran.r-project.org/package=treespace,
https://github.com/thibautjombart/treespace",TRUE,https://github.com/thibautjombart/treespace,22410,20,2021-03-23T16:05:27Z,1120.5
TreeTools,"Efficient implementations of functions for the creation, 
  modification and analysis of phylogenetic trees.
  Applications include:
  generation of trees with specified shapes;
  analysis of tree shape;
  rooting of trees and extraction of subtrees;
  calculation and depiction of node support;
  calculation of ancestor-descendant relationships;
  import and export of trees from Newick, Nexus (Maddison et al. 1997)
  <doi:10.1093/sysbio/46.4.590>,
  and TNT <http://www.lillo.org.ar/phylogeny/tnt/> formats;
  and analysis of splits and cladistic information.",2021-06-23,Martin R. Smith,"https://ms609.github.io/TreeTools/,
https://github.com/ms609/TreeTools/",TRUE,https://github.com/ms609/treetools,18923,3,2021-08-19T06:40:04Z,6307.666666666667
treetop,Set of tools implemented into a shiny-based application for extracting and analyzing individual tree forest attributes from LiDAR (Light Detection and Ranging) data.,2021-03-02,Carlos Alberto Silva,https://github.com/carlos-alberto-silva/weblidar-treetop,TRUE,https://github.com/carlos-alberto-silva/weblidar-treetop,2063,70,2021-04-13T09:43:32Z,29.47142857142857
trekcolors,"Provides a dataset of predefined color palettes based on the Star Trek science fiction series, associated color palette functions, 
    and additional functions for generating customized palettes that are on theme. The package also offers functions for applying 
    the palettes to plots made using the 'ggplot2' package.",2021-06-01,Matthew Leonawicz,https://github.com/leonawicz/trekcolors,TRUE,https://github.com/leonawicz/trekcolors,9517,14,2021-05-29T18:53:21Z,679.7857142857143
trekfont,Provides a collection of true type and open type Star Trek-themed fonts.,2021-06-01,Matthew Leonawicz,https://github.com/leonawicz/trekfont,TRUE,https://github.com/leonawicz/trekfont,14852,11,2021-05-29T18:49:25Z,1350.1818181818182
trelliscopejs,"Trelliscope is a scalable, flexible, interactive approach to visualizing data (Hafen, 2013 <doi:10.1109/LDAV.2013.6675164>). This package provides methods that make it easy to create a Trelliscope display specification for TrelliscopeJS. High-level functions are provided for creating displays from within 'tidyverse' or 'ggplot2' workflows. Low-level functions are also provided for creating new interfaces.",2021-02-01,Ryan Hafen,https://github.com/hafen/trelliscopejs,TRUE,https://github.com/hafen/trelliscopejs,28781,217,2021-02-05T07:59:03Z,132.63133640552996
trelloR,"An R client for the Trello API. Supports free-tier features such as
    access to private boards, creating and updating cards and other resources,
    and downloading data in a structured way.",2021-04-22,Jakub Chromec,https://github.com/jchrom/trelloR,TRUE,https://github.com/jchrom/trellor,15826,24,2021-04-21T16:10:47Z,659.4166666666666
trendeval,"Provides a coherent interface for evaluating models fit with the
  trending package.  This package is part of the RECON
  (<https://www.repidemicsconsortium.org/>) toolkit for outbreak analysis.",2020-11-20,Dirk Schumacher,https://github.com/reconhub/trendeval,TRUE,https://github.com/reconhub/trendeval,5830,1,2021-08-27T20:40:43Z,5830
trending,"Provides a coherent interface to multiple modelling tools for
  fitting trends along with a standardised approach for generating confidence
  and prediction intervals.",2021-04-19,Dirk Schumacher,https://github.com/reconhub/trending,TRUE,https://github.com/reconhub/trending,7529,6,2021-08-27T12:12:32Z,1254.8333333333333
TRES,"Provides three estimators for tensor response regression (TRR) and tensor predictor regression (TPR) models with tensor envelope structure. The three types of estimation approaches are generic and can be applied to any envelope estimation problems. The full Grassmannian (FG) optimization is often associated with likelihood-based estimation but requires heavy computation and good initialization; the one-directional optimization approaches (1D and ECD algorithms) are faster, stable and does not require carefully chosen initial values; the SIMPLS-type is motivated by the partial least squares regression and is computationally the least expensive. For details of TRR, see Li L, Zhang X (2017) <doi:10.1080/01621459.2016.1193022>. For details of TPR, see Zhang X, Li L (2017) <doi:10.1080/00401706.2016.1272495>. For details of 1D algorithm, see Cook RD, Zhang X (2016) <doi:10.1080/10618600.2015.1029577>. For details of ECD algorithm, see Cook RD, Zhang X (2018) <doi:10.5705/ss.202016.0037>.",2021-06-25,Jing Zeng,https://github.com/leozeng15/TRES,TRUE,https://github.com/leozeng15/tres,12431,0,2021-06-25T03:36:11Z,NA
TREXr,"Performs data assimilation, processing and analyses on sap flow data obtained
  with the thermal dissipation method (TDM). The package includes functions for gap filling
  time-series data, detecting outliers, calculating data-processing uncertainties and
  generating uniform data output and visualisation. The package is designed to deal with
  large quantities of data and to apply commonly used data-processing methods. The functions
  have been validated on data collected from different tree species across
  the northern hemisphere (Peters et al. 2018 <doi:10.1111/nph.15241>).",2021-03-24,Richard Peters,https://the-hull.github.io/TREX/,TRUE,https://github.com/the-hull/trex,1697,0,2021-04-02T11:11:18Z,NA
trialr,"A collection of clinical trial designs and methods, implemented in 
    'rstan' and R, including: the Continual Reassessment Method by O'Quigley et 
    al. (1990) <doi:10.2307/2531628>; EffTox by Thall & Cook (2004) 
    <doi:10.1111/j.0006-341X.2004.00218.x>; the two-parameter logistic method of
    Neuenschwander, Branson & Sponer (2008) <doi:10.1002/sim.3230>; and the 
    Augmented Binary method by Wason & Seaman (2013) <doi:10.1002/sim.5867>; and
    more. We provide functions to aid model-fitting and analysis. 
    The 'rstan' implementations may also serve as a cookbook to anyone looking 
    to extend or embellish these models. We hope that this package encourages 
    the use of Bayesian methods in clinical trials. There is a preponderance of 
    early phase trial designs because this is where Bayesian methods are used 
    most. If there is a method you would like implemented, please get in touch.",2020-10-15,Kristian Brock,https://github.com/brockk/trialr,TRUE,https://github.com/brockk/trialr,22371,34,2020-10-15T15:23:33Z,657.9705882352941
triangle,"Provides the ""r, q, p, and d"" distribution functions for the triangle distribution.",2019-02-14,Rob Carnell,https://bertcarnell.github.io/triangle/,TRUE,https://github.com/bertcarnell/triangle,136314,2,2021-03-29T21:29:11Z,68157
TriDimRegression,"Fits 2D and 3D geometric transformations via 'Stan' probabilistic programming engine ( 
    Stan Development Team (2021) <https://mc-stan.org>). Returns posterior distribution for individual
    parameters of the fitted distribution. Allows for computation of LOO and WAIC information criteria 
    (Vehtari A, Gelman A, Gabry J (2017) <doi:10.1007/s11222-016-9696-4>) as well as Bayesian R-squared
    (Gelman A, Goodrich B, Gabry J, and Vehtari A (2018) <doi:10.1080/00031305.2018.1549100>).",2021-05-04,Alexander (Sasha) Pastukhov,https://github.com/alexander-pastukhov/tridim-regression,TRUE,https://github.com/alexander-pastukhov/tridim-regression,1399,0,2021-05-03T16:16:26Z,NA
trimetStops,"Information on all of the TriMet stops in the Portland Metro Area. It includes information such as the longitude, latitude, cross street, and direction of the stop. TriMet has catalogued these stops, 6880 in total. ",2020-08-27,Grayson White,https://github.com/graysonwhite/trimetStops,TRUE,https://github.com/graysonwhite/trimetstops,5455,0,2020-10-06T22:25:22Z,NA
trip,"Functions for accessing and manipulating spatial data for animal
    tracking, with straightforward coercion from and to other formats. Filter
    for speed and create time spent maps from animal track data. There are
    coercion methods to convert between 'trip' and 'ltraj' from 'adehabitatLT', 
    and between 'trip' and 'psp' and 'ppp' from 'spatstat'. Trip objects
    can be created from raw or grouped data frames, and from types in the 'sp', 
    'sf', 'amt', 'trackeR', 'mousetrap', and other packages. ",2021-03-16,Michael D. Sumner,https://github.com/Trackage/trip,TRUE,https://github.com/trackage/trip,36374,6,2021-02-11T23:18:57Z,6062.333333333333
triplot,"Tools for exploring effects of correlated features in predictive 
    models. The predict_triplot() function delivers instance-level explanations 
    that calculate the importance of the groups of explanatory variables. The 
    model_triplot() function delivers data-level explanations. The generic plot 
    function visualises in a concise way importance of hierarchical groups of 
    predictors. All of the the tools are model agnostic, therefore works for any
    predictive machine learning models. Find more details in Biecek (2018) 
    <arXiv:1806.08915>.",2020-07-13,Katarzyna Pekala,https://github.com/ModelOriented/triplot,TRUE,https://github.com/modeloriented/triplot,5286,8,2021-03-10T09:27:48Z,660.75
tripsAndDipR,"Uses read counts for biallelic single nucleotide polymorphisms (SNPs)
    to compare the likelihoods for the observed read counts given that a sample is 
    either diploid or triploid. It allows parameters to be specified to account for 
    sequencing error rates and allelic bias. For details of the algorithm, please see
    Delomas (2019) <doi:10.1111/1755-0998.13073>.",2019-08-28,Thomas Delomas,https://github.com/delomast/tripsAndDipR,TRUE,https://github.com/delomast/tripsanddipr,12816,2,2021-04-12T20:01:00Z,6408
troopdata,These functions generate data frames on troop deployments and military basing using U.S. Department of Defense data on overseas military deployments. This package provides functions for pulling country-year troop deployment and basing data. Subsequent versions will hopefully include cross-national data on deploying countries.,2021-09-02,Michael Flynn,https://github.com/meflynn/troopdata,TRUE,https://github.com/meflynn/troopdata,2184,20,2021-09-02T21:27:07Z,109.2
TropFishR,"A compilation of fish stock assessment methods for the
    analysis of length-frequency data in the context of data-poor
    fisheries. Includes methods and examples included in the FAO
    Manual by P. Sparre and S.C. Venema (1998), ""Introduction to tropical fish
    stock assessment""
    (<http://www.fao.org/documents/card/en/c/9bb12a06-2f05-5dcb-a6ca-2d6dd3080f65/>),
    as well as other more recent methods.",2020-01-28,Tobias K. Mildenberger,https://github.com/tokami/TropFishR,TRUE,https://github.com/tokami/tropfishr,27263,12,2021-08-30T09:29:56Z,2271.9166666666665
truelies,"Implements Bayesian methods, described in
    Hugh-Jones (2019) <doi:10.1007/s40881-019-00069-x>, for estimating the
    proportion of liars in coin flip-style experiments, where subjects
    report a random outcome and are paid for reporting a ""good"" outcome.",2019-08-26,David Hugh-Jones,https://github.com/hughjonesd/truelies,TRUE,https://github.com/hughjonesd/truelies,9211,0,2021-05-23T10:17:48Z,NA
TruncatedNormal,"A collection of functions to deal with the truncated univariate and multivariate normal and Student distributions, described in Botev (2017) <doi:10.1111/rssb.12162> and Botev and L'Ecuyer (2015) <doi:10.1109/WSC.2015.7408180>.",2020-05-17,Zdravko Botev,NA,TRUE,https://github.com/lbelzile/truncatednormal,37476,4,2020-12-30T20:36:28Z,9369
truthiness,"Data and functions for analyzing and simulating illusory
    truth datasets, developed as part of a longitudinal study by
    Henderson, Barr, and Simons (2020). The illusory truth effect is
    the observation that people rate repeated statements as more
    likely to be true than novel statements. We tested the trajectory
    of the illusory truth effect by collecting truth ratings for
    statements repeated across four time intervals: immediately, one
    day, one week, and one month following initial presentation. The
    package contains the anonymized data from the study along with
    stimulus materials, as well as functions for analyzing the data,
    running simulations, and calculating power. Further details about
    the project are available at <https://osf.io/nvugt/>, which
    includes Stage 1 of the Registered Report at the Journal of
    Cognition (<https://osf.io/vqnx2/>).",2021-05-24,Dale Barr,https://github.com/dalejbarr/truthiness,TRUE,https://github.com/dalejbarr/truthiness,2480,0,2021-06-08T14:16:05Z,NA
tryCatchLog,"Advanced tryCatch() and try() functions for better error handling
             (logging, stack trace with source code references and support for post-mortem analysis via dump files).",2021-05-21,Juergen Altfeld,https://github.com/aryoda/tryCatchLog,TRUE,https://github.com/aryoda/trycatchlog,28610,59,2021-05-21T00:28:37Z,484.91525423728814
ts.extend,"Stationary Gaussian ARMA processes and the stationary 'GARMA' distribution are fundamental in time series analysis. Here we give utilities to compute 
    the auto-covariance/auto-correlation for a stationary Gaussian ARMA process, as well as the probability functions (density, cumulative distribution, random 
    generation) for random vectors from this distribution.  We also give functions for the spectral intensity, and the permutation-spectrum test for testing 
    a time-series vector for the presence of a signal.",2020-11-14,Ben ONeill,https://github.com/ben-oneill/ts.extend,TRUE,https://github.com/ben-oneill/ts.extend,4030,1,2020-11-14T09:07:59Z,4030
tsbox,"Time series toolkit with identical behavior for all
  time series classes: 'ts','xts', 'data.frame', 'data.table', 'tibble', 'zoo',
  'timeSeries', 'tsibble', 'tis' or 'irts'. Also converts reliably between these classes.",2021-07-29,Christoph Sax,"https://www.tsbox.help, https://github.com/christophsax/tsbox",TRUE,https://github.com/christophsax/tsbox,83999,117,2021-07-28T09:28:53Z,717.9401709401709
tsdb,"A terribly-simple data base for numeric
  time series, written purely in R, so no external
  database-software is needed. Series are stored in
  plain-text files (the most-portable and enduring file
  type) in CSV format. Timestamps are encoded using R's
  native numeric representation for 'Date'/'POSIXct',
  which makes them fast to parse, but keeps them
  accessible with other software. The package provides
  tools for saving and updating series in this
  standardised format, for retrieving and joining data,
  for summarising files and directories, and for
  coercing series from and to other data types (such as
  'zoo' series).",2021-01-06,Enrico Schumann,"http://enricoschumann.net/R/packages/tsdb/,
https://github.com/enricoschumann/tsdb,
https://gitlab.com/enricoschumann/tsdb",TRUE,https://github.com/enricoschumann/tsdb,11643,9,2021-05-09T19:16:58Z,1293.6666666666667
tsensembler,"A framework for dynamically combining forecasting models for time series forecasting predictive tasks. It leverages machine learning models from other packages to automatically combine expert advice using metalearning and other state-of-the-art forecasting combination approaches. The predictive methods receive a data matrix as input, representing an embedded time series, and return a predictive ensemble model. The ensemble use generic functions 'predict()' and 'forecast()' to forecast future values of the time series. Moreover, an ensemble can be updated using methods, such as 'update_weights()' or 'update_base_models()'. A complete description of the methods can be found in: Cerqueira, V., Torgo, L., Pinto, F., and Soares, C. ""Arbitrated Ensemble for Time Series Forecasting."" to appear at: Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer International Publishing, 2017; and Cerqueira, V., Torgo, L., and Soares, C.: ""Arbitrated Ensemble for Solar Radiation Forecasting."" International Work-Conference on Artificial Neural Networks. Springer, 2017 <doi:10.1007/978-3-319-59153-7_62>.",2020-10-27,Vitor Cerqueira,https://github.com/vcerqueira/tsensembler,TRUE,https://github.com/vcerqueira/tsensembler,11786,25,2020-10-24T11:46:06Z,471.44
tsfeatures,"Methods for extracting various features from time series data. The features provided are those from Hyndman, Wang and Laptev (2013) <doi:10.1109/ICDMW.2015.104>, Kang, Hyndman and Smith-Miles (2017) <doi:10.1016/j.ijforecast.2016.09.004> and from Fulcher, Little and Jones (2013) <doi:10.1098/rsif.2013.0048>. Features include spectral entropy, autocorrelations, measures of the strength of seasonality and trend, and so on. Users can also define their own feature functions.",2020-06-07,Rob Hyndman,https://pkg.robjhyndman.com/tsfeatures/,TRUE,https://github.com/robjhyndman/tsfeatures,213305,209,2021-06-21T23:26:50Z,1020.5980861244019
tsfgrnn,"A general regression neural network (GRNN) is a variant of a
    Radial Basis Function Network characterized by a fast single-pass learning.
    'tsfgrnn' allows you to forecast time series using a GRNN model Francisco 
    Martinez et al. (2019) <doi:10.1007/978-3-030-20521-8_17> and Weizhong Yan
    (2012) <doi:10.1109/TNNLS.2012.2198074>. When the forecasting horizon
    is higher than 1, two multi-step ahead forecasting strategies can be used.
    The model built is autoregressive, that is, it is only based on the 
    observations of the time series. You can consult and plot how the
    prediction was done. It is also possible to assess the forecasting accuracy
    of the model using rolling origin evaluation.",2021-04-22,Francisco Martinez,https://github.com/franciscomartinezdelrio/tsfgrnn,TRUE,https://github.com/franciscomartinezdelrio/tsfgrnn,9093,5,2021-04-22T10:40:26Z,1818.6
tsfknn,"Allows to forecast time series using nearest neighbors regression
    Francisco Martinez, Maria P. Frias, Maria D. Perez-Godoy and Antonio J.
    Rivera (2017) <doi:10.1007/s10462-017-9593-z>. When the forecasting horizon
    is higher than 1, two multi-step ahead forecasting strategies can be used.
    The model built is autoregressive, that is, it is only based on the 
    observations of the time series. The nearest neighbors used in a prediction
    can be consulted and plotted.",2021-04-05,Francisco Martinez,https://github.com/franciscomartinezdelrio/tsfknn,TRUE,https://github.com/franciscomartinezdelrio/tsfknn,20866,7,2021-04-05T07:29:53Z,2980.8571428571427
tsibble,"Provides a 'tbl_ts' class (the 'tsibble') for
    temporal data in an data- and model-oriented format. The 'tsibble'
    provides tools to easily manipulate and analyse temporal data, such as
    filling in time gaps and aggregating over calendar periods.",2021-04-12,Earo Wang,https://tsibble.tidyverts.org,TRUE,https://github.com/tidyverts/tsibble,382177,463,2021-04-12T21:36:50Z,825.4362850971922
tsibbledata,"Provides diverse datasets in the 'tsibble' data structure. These datasets are useful for learning and demonstrating how tidy temporal data can tidied, visualised, and forecasted.",2021-03-16,Mitchell OHara-Wild,"https://tsibbledata.tidyverts.org/,
https://github.com/tidyverts/tsibbledata/",TRUE,https://github.com/tidyverts/tsibbledata,67978,17,2021-03-16T14:06:29Z,3998.705882352941
tsmp,"A toolkit implementing the Matrix Profile concept
    that was created by CS-UCR
    <http://www.cs.ucr.edu/~eamonn/MatrixProfile.html>.",2020-04-06,Francisco Bischoff,https://github.com/matrix-profile-foundation/tsmp,TRUE,https://github.com/matrix-profile-foundation/tsmp,20507,55,2020-12-27T23:25:39Z,372.8545454545455
TSP,"Basic infrastructure and some algorithms for the traveling
    salesperson problem (also traveling salesman problem; TSP).
    The package provides some simple algorithms and
    an interface to the Concorde TSP solver and its implementation of the
    Chained-Lin-Kernighan heuristic. The code for Concorde
    itself is not included in the package and has to be obtained separately.
    Hahsler and Hornik (2007) <doi:10.18637/jss.v023.i02>.",2020-04-17,Michael Hahsler,https://github.com/mhahsler/TSP,TRUE,https://github.com/mhahsler/tsp,892291,48,2021-07-28T09:13:51Z,18589.395833333332
tsviz,An 'RStudio' add-in to visualize time series. Time series are searched in the global environment as data.frame objects with a column of type date and a column of type numeric. Interactive charts are produced using 'plotly' package.,2019-07-26,Emanuele Fabbiani,https://github.com/donlelef/tsviz,TRUE,https://github.com/donlelef/tsviz,9533,33,2020-10-19T21:43:13Z,288.8787878787879
ttdo,"The 'tinytest' package offers a light-weight zero-dependency unit-testing
 framework to which this package adds support of the 'diffobj' package for 'diff'-style
 comparison of R objects.",2021-07-17,Dirk Eddelbuettel and Alton Barbehenn,"https://github.com/eddelbuettel/ttdo/,
https://dirk.eddelbuettel.com/code/ttdo.html",TRUE,https://github.com/eddelbuettel/ttdo,11022,17,2021-07-17T14:32:55Z,648.3529411764706
TTR,"A collection of over 50 technical indicators for creating technical trading rules. The package also provides fast implementations of common rolling-window functions, and several volatility calculations.",2020-09-01,Joshua Ulrich,https://github.com/joshuaulrich/TTR,TRUE,https://github.com/joshuaulrich/ttr,7948899,248,2021-08-22T13:34:55Z,32052.012096774193
ttt,"Create structured, formatted HTML tables of in a flexible and
    convenient way.",2021-05-07,Benjamin Rich,https://github.com/benjaminrich/ttt,TRUE,https://github.com/benjaminrich/ttt,1370,6,2021-06-06T21:05:57Z,228.33333333333334
ttTensor,"Tensor-train is a compact representation for higher-order tensors. Some algorithms for performing tensor-train decomposition are available such as TT-SVD, TT-WOPT, and TT-Cross. For the details of the algorithms, see I. V. Oseledets (2011) <doi:10.1137/090752286>, Yuan Longao, et al (2017) <arXiv:1709.02641>, I. V. Oseledets (2010) <doi:10.1016/j.laa.2009.07.024>.",2021-05-18,Koki Tsuyuzaki,https://github.com/rikenbit/ttTensor,TRUE,https://github.com/rikenbit/tttensor,9623,1,2021-05-18T07:14:40Z,9623
tuber,"Get comments posted on YouTube videos, information on how many 
    times a video has been liked, search for videos with particular content, and 
    much more. You can also scrape captions from a few videos. To learn more about
    the YouTube API, see <https://developers.google.com/youtube/v3/>.",2020-06-11,Gaurav Sood,http://github.com/soodoku/tuber,TRUE,https://github.com/soodoku/tuber,38579,157,2021-04-13T23:01:08Z,245.72611464968153
tufte,Provides R Markdown output formats to use Tufte styles for PDF and HTML output.,2021-05-17,Yihui Xie,https://github.com/rstudio/tufte,TRUE,https://github.com/rstudio/tufte,221141,301,2021-05-18T00:28:04Z,734.687707641196
tukeyGH,"It provides distribution, density and quantile functions of the
    Tukey's g-and-h probability distribution, as well as functions for random
    number generation, parameter estimation and testing.",2021-04-10,Flavio Santi,https://github.com/f-santi/tukeyGH,TRUE,https://github.com/f-santi/tukeygh,3065,1,2021-04-09T10:26:18Z,3065
tune,"The ability to tune models is important. 'tune' contains
    functions and classes to be used in conjunction with other
    'tidymodels' packages for finding reasonable values of
    hyper-parameters in models, pre-processing methods, and
    post-processing steps.",2021-07-21,Max Kuhn,"https://github.com/tidymodels/tune, https://tune.tidymodels.org",TRUE,https://github.com/tidymodels/tune,363742,165,2021-08-26T19:23:47Z,2204.4969696969697
tvR,"Provides tools for denoising noisy signal and images via
    Total Variation Regularization. Reducing the total variation of
    the given signal is known to remove spurious detail while preserving
    essential structural details. For the seminal work on the topic,
    see Rudin et al (1992) <doi:10.1016/0167-2789(92)90242-F>.",2021-08-22,Kisung You,https://github.com/kisungyou/tvR,TRUE,https://github.com/kisungyou/tvr,14981,3,2021-08-22T04:40:33Z,4993.666666666667
tvReg,"Fitting time-varying coefficient models for single and multi-equation regressions, using kernel smoothing techniques.",2021-03-09,Isabel Casas,https://github.com/icasas/tvReg,TRUE,https://github.com/icasas/tvreg,20613,5,2021-06-16T08:31:19Z,4122.6
tvthemes,"Contains various 'ggplot2' themes and color palettes based on TV shows 
    such as 'Game of Thrones', 'Brooklyn Nine-Nine', 'Avatar: The Last Airbender',
    'Spongebob Squarepants', and more.",2020-12-14,Ryo Nakagawara,https://github.com/Ryo-N7/tvthemes,TRUE,https://github.com/ryo-n7/tvthemes,13568,108,2020-12-14T09:12:36Z,125.62962962962963
tweenr,"In order to create smooth animation between states of data,
    tweening is necessary. This package provides a range of functions for
    creating tweened data that can be used as basis for animation. Furthermore 
    it adds a number of vectorized interpolaters for common R data 
    types such as numeric, date and colour.",2021-03-23,Thomas Lin Pedersen,https://github.com/thomasp85/tweenr,TRUE,https://github.com/thomasp85/tweenr,1621873,386,2021-07-02T12:23:26Z,4201.743523316062
twenty48,"Generates a game of 2048 that can be played in the
    console.  Supports grids of arbitrary sizes, undoing the last move,
    and resuming a game that was exited during the current session.",2021-04-24,Alexander Rossell Hayes,https://github.com/rossellhayes/twenty48,TRUE,https://github.com/rossellhayes/twenty48,3903,4,2021-04-24T03:15:06Z,975.75
TwitterAutomatedTrading,"Provides an integration to the 'metatrader 5'. 
    The functionalities carry out automated trading using
    sentiment indexes computed from 'twitter' and/or 'stockwits'. 
    The sentiment indexes are based on the ph.d. dissertation 
    ""Essays on Economic Forecasting Models"" (Godeiro,2018) <https://repositorio.ufpb.br/jspui/handle/123456789/15198>
    The integration between the 'R' and the 'metatrader 5' allows sending buy/sell orders to the brokerage. ",2020-05-31,Lucas Godeiro,https://github.com/lucasgodeiro/TwitterAutomatedTrading,TRUE,https://github.com/lucasgodeiro/twitterautomatedtrading,4466,5,2021-08-28T12:38:02Z,893.2
twitterwidget,"Include the Twitter status widgets in HTML pages created
  using R markdown. The package uses the Twitter javascript APIs to
  embed in your document Twitter cards associated to specific statuses.
  The main targets are regular HTML pages or dashboards.",2019-07-10,Guido Volpi,https://github.com/guivo/twitterwidget,TRUE,https://github.com/guivo/twitterwidget,9002,5,2020-09-22T14:44:05Z,1800.4
twn,"The TWN-list (Taxa Waterbeheer Nederland) is the Dutch standard for naming 
    taxons in Dutch Watermanagement. This package makes it easier to use the 
    TWN-list for ecological analyses. It  consists of two parts. First it makes the 
    TWN-list itself available in R. Second, it has a few functions that make it 
    easy to perform some basic and often recurring tasks for checking and consulting
    taxonomic data from the TWN-list. ",2021-03-21,Johan van Tent,https://github.com/RedTent/twn,TRUE,https://github.com/redtent/twn,4651,0,2021-08-18T14:19:45Z,NA
TwoRegression,"Application of two-regression algorithms for wearable
    research devices. It provides an easy way for users to read in
    device data files and apply an appropriate two-regression
    algorithm. More information is available from
    Hibbing PR, LaMunion SR, Kaplan AS, & Crouter SE (2017)
    <doi:10.1249/MSS.0000000000001532>.",2018-03-19,Paul R. Hibbing,https://github.com/paulhibbing/TwoRegression,TRUE,https://github.com/paulhibbing/tworegression,11948,0,2021-08-30T02:40:07Z,NA
txshift,"Efficient estimation of the population-level causal effects of
    stochastic interventions on a continuous-valued exposure. Both one-step and
    targeted minimum loss estimators are implemented for a causal parameter
    defined as the counterfactual mean of an outcome of interest under a
    stochastic intervention that may depend on the natural value of the
    exposure (i.e., a modified treatment policy). To accommodate settings in
    which two-phase sampling is employed, procedures for making use of inverse
    probability of censoring weights are provided to facilitate construction of
    inefficient and efficient one-step and targeted minimum loss estimators.
    The causal parameter and its estimation were first described by Díaz and van
    der Laan (2013) <doi:10.1111/j.1541-0420.2011.01685.x>, while the multiply
    robust estimation procedure and its application to data arising in two-phase
    sampling designs was detailed in NS Hejazi, MJ van der Laan, HE Janes, PB
    Gilbert, and DC Benkeser (2020) <doi:10.1111/biom.13375>. Estimation of
    nuisance parameters may be enhanced through the Super Learner ensemble model
    in 'sl3', available for download from GitHub using
    'remotes::install_github(""tlverse/sl3"")'.",2021-02-07,Nima Hejazi,https://github.com/nhejazi/txshift,TRUE,https://github.com/nhejazi/txshift,4154,7,2021-07-07T02:14:43Z,593.4285714285714
txtq,"This queue is a data structure that lets
  parallel processes send and receive messages,
  and it can help coordinate the work
  of complicated parallel tasks.
  Processes can push new messages to the queue,
  pop old messages, and obtain a
  log of all the messages ever pushed. File locking
  preserves the integrity of the data even when
  multiple processes access the queue simultaneously.",2021-03-27,William Michael Landau,https://github.com/wlandau/txtq,TRUE,https://github.com/wlandau/txtq,65438,19,2021-03-27T18:29:02Z,3444.1052631578946
typed,"A type system for R. It supports setting variable types in a script or the body of a function, so variables can't be assigned illegal values. Moreover it supports setting argument and return types for functions.",2021-03-19,Antoine Fabri,https://github.com/moodymudskipper/typed,TRUE,https://github.com/moodymudskipper/typed,1627,83,2021-03-22T11:52:47Z,19.602409638554217
typehint,"Type hints are special comments within a function body indicating the intended nature of the function's arguments in terms of data types, dimensions and permitted values. The actual parameters with which the function is called are evaluated against these type hint comments at run-time.",2021-08-10,Joachim Zuckarelli,https://github.com/jsugarelli/typehint/,TRUE,https://github.com/jsugarelli/typehint,272,0,2021-08-08T09:13:08Z,NA
tzdb,"Provides an up-to-date copy of the Internet Assigned Numbers
    Authority (IANA) Time Zone Database. It is updated periodically to
    reflect changes made by political bodies to time zone boundaries, UTC
    offsets, and daylight saving time rules. Additionally, this package
    provides a C++ interface for working with the 'date' library. 'date'
    provides comprehensive support for working with dates and date-times,
    which this package exposes to make it easier for other R packages to
    utilize. Headers are provided for calendar specific calculations,
    along with a limited interface for time zone manipulations.",2021-07-20,Davis Vaughan,"https://r-lib.github.io/tzdb/, https://github.com/r-lib/tzdb",TRUE,https://github.com/r-lib/tzdb,1006036,2,2021-07-20T17:50:20Z,503018
ubiquity,"Complete work flow for the analysis of pharmacokinetic pharmacodynamic (PKPD), physiologically-based pharmacokinetic (PBPK) and systems pharmacology models including: creation of ordinary differential equation-based models, pooled parameter estimation, individual/population based simulations, rule-based simulations for clinical trial design and modeling assays, deployment with a customizable 'Shiny' app, and non-compartmental analysis. System-specific analysis templates can be generated and each element includes integrated reporting with 'PowerPoint' and 'Word'. ",2021-09-03,John Harrold,https://ubiquity.tools/rworkflow,TRUE,https://github.com/john-harrold/ubiquity,10743,7,2021-09-03T01:28:47Z,1534.7142857142858
UBL,Provides a set of functions that can be used to obtain better predictive performance on cost-sensitive and cost/benefits tasks (for both regression and classification). This includes re-sampling approaches that modify the original data set biasing it towards the user preferences.,2021-03-29,Paula Branco,https://github.com/paobranco/UBL,TRUE,https://github.com/paobranco/ubl,40713,22,2021-05-04T21:45:11Z,1850.590909090909
ubms,"Fit Bayesian hierarchical models of animal abundance and occurrence
    via the 'rstan' package, the R interface to the 'Stan' C++ library.
    Supported models include single-season occupancy, dynamic occupancy, and
    N-mixture abundance models. Covariates on model parameters are specified
    using a formula-based interface similar to package 'unmarked', while also
    allowing for estimation of random slope and intercept terms. References:
    Carpenter et al. (2017) <doi:10.18637/jss.v076.i01>;
    Fiske and Chandler (2011) <doi:10.18637/jss.v043.i10>.",2021-02-02,Ken Kellner,https://kenkellner.com/ubms/,TRUE,https://github.com/kenkellner/ubms,2822,12,2021-08-22T13:16:30Z,235.16666666666666
UCSCXenaShiny,"Provides functions and a Shiny application for downloading,
    analyzing and visualizing datasets from UCSC Xena
    (<http://xena.ucsc.edu/>), which is a collection of UCSC-hosted public
    databases such as TCGA, ICGC, TARGET, GTEx, CCLE, and others.",2021-07-30,Shixiang Wang,https://github.com/openbiox/UCSCXenaShiny,TRUE,https://github.com/openbiox/ucscxenashiny,12139,43,2021-07-31T15:20:50Z,282.30232558139534
UCSCXenaTools,"Download and explore datasets from UCSC Xena data hubs, which are
    a collection of UCSC-hosted public databases such as TCGA, ICGC, TARGET, GTEx, CCLE, and others.
    Databases are normalized so they can be combined, linked, filtered, explored and downloaded.",2021-07-17,Shixiang Wang,"https://docs.ropensci.org/UCSCXenaTools/,
https://github.com/ropensci/UCSCXenaTools",TRUE,https://github.com/ropensci/ucscxenatools,31294,58,2021-07-17T07:54:36Z,539.551724137931
udpipe,"This natural language processing toolkit provides language-agnostic
    'tokenization', 'parts of speech tagging', 'lemmatization' and 'dependency
    parsing' of raw text. Next to text parsing, the package also allows you to train
    annotation models based on data of 'treebanks' in 'CoNLL-U' format as provided
    at <https://universaldependencies.org/format.html>. The techniques are explained
    in detail in the paper: 'Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0
    with UDPipe', available at <doi:10.18653/v1/K17-3009>. 
    The toolkit also contains functionalities for commonly used data manipulations on texts 
    which are enriched with the output of the parser. Namely functionalities and algorithms 
    for collocations, token co-occurrence, document term matrix handling, 
    term frequency inverse document frequency calculations,
    information retrieval metrics (Okapi BM25), handling of multi-word expressions,
    keyword detection (Rapid Automatic Keyword Extraction, noun phrase extraction, syntactical patterns) 
    sentiment scoring and semantic similarity analysis.",2021-06-01,Jan Wijffels,"https://bnosac.github.io/udpipe/en/index.html,
https://github.com/bnosac/udpipe",TRUE,https://github.com/bnosac/udpipe,140936,175,2021-09-01T10:13:39Z,805.3485714285714
uiucthemes,"A set of custom 'R' 'Markdown' templates for documents and
   presentations with the University of Illinois at Urbana-Champaign (UIUC)
   color scheme and identity standards.",2020-07-25,James Balamuta,"https://github.com/illinois-r/uiucthemes,
http://thecoatlessprofessor.com/projects/uiucthemes/",TRUE,https://github.com/illinois-r/uiucthemes,16298,45,2021-08-20T04:33:05Z,362.1777777777778
uk2us,Functions for converting between UK and US spellings of English words.,2021-02-19,Benjamin Davies,https://github.com/bldavies/uk2us,TRUE,https://github.com/bldavies/uk2us,2600,0,2021-02-19T21:41:57Z,NA
UKB.COVID19,"Process UK Biobank COVID-19 test result data for susceptibility, severity and mortality analyses, perform potential non-genetic COVID-19 risk factor and co-morbidity association tests. Wang et al. (2021) <doi:10.5281/zenodo.5174381>.",2021-08-10,Longfei Wang,https://github.com/bahlolab/UKB.COVID19,TRUE,https://github.com/bahlolab/ukb.covid19,258,1,2021-07-23T06:21:22Z,258
ukbabynames,"Full listing of UK baby names occurring more than three times per year between 1974 and 2020, and rankings of baby name popularity by decade from 1904 to 1994.",2021-04-22,Mine Çetinkaya-Rundel,https://mine-cetinkaya-rundel.github.io/ukbabynames/,TRUE,https://github.com/mine-cetinkaya-rundel/ukbabynames,17386,13,2021-04-23T22:35:03Z,1337.3846153846155
ukgasapi,Allows users to access live UK energy market information via various APIs.,2020-11-06,Timothy Wong,NA,TRUE,https://github.com/timothywong731/ukgasapi,22691,0,2020-10-26T23:51:07Z,NA
uklr,"Access data from Land Registry Open Data
    <http://landregistry.data.gov.uk/> through 'SPARQL' queries. 'uklr'
    supports the house price index, transaction and price paid data.",2021-05-25,Kostas Vasilopoulos,"https://kvasilopoulos.github.io/uklr/,
https://github.com/kvasilopoulos/uklr/",TRUE,https://github.com/kvasilopoulos/uklr,9393,3,2021-05-27T10:48:57Z,3131
ukpolice,"Downloads data from the 'UK Police' public data API, 
    the full docs of which are available at <https://data.police.uk/docs/>. 
    Includes data on police forces and police force areas, crime reports, 
    and the use of stop-and-search powers.",2020-08-03,Evan Odell,"https://github.com/EvanOdell/ukpolice/,
https://docs.evanodell.com/ukpolice",TRUE,https://github.com/evanodell/ukpolice,16710,4,2020-11-22T13:39:35Z,4177.5
umap,"Uniform manifold approximation and projection is a technique
    for dimension reduction. The algorithm was described by McInnes and
    Healy (2018) in <arXiv:1802.03426>. This package provides an interface
    for two implementations. One is written from scratch, including components
    for nearest-neighbor search and for embedding. The second implementation
    is a wrapper for 'python' package 'umap-learn' (requires separate
    installation, see vignette for more details).",2020-11-04,Tomasz Konopka,https://github.com/tkonopka/umap,TRUE,https://github.com/tkonopka/umap,192644,114,2020-11-03T06:57:21Z,1689.859649122807
umx,"Quickly create, run, and report structural equation and twin models.
    See '?umx' for help, and umx_open_CRAN_page(""umx"") for NEWS.
    Timothy C. Bates, Michael C. Neale, Hermine H. Maes, (2019). umx: A library for Structural Equation and Twin Modelling in R.
    Twin Research and Human Genetics, 22, 27-41. <doi:10.1017/thg.2019.2>.",2021-07-04,Timothy C. Bates,https://github.com/tbates/umx,TRUE,https://github.com/tbates/umx,46680,26,2021-08-30T11:11:00Z,1795.3846153846155
UncertainInterval,"Functions for the determination of an uncertain interval, that is, 
    a range of test scores that is inconclusive and does not allow a 
    classification other than 'Uncertain' (Reference: J.A. Landsheer (2016) 
    <doi:10.1371/journal.pone.0166007>).",2021-03-02,Hans Landsheer,https://github.com/HansLandsheer/UncertainInterval,TRUE,https://github.com/hanslandsheer/uncertaininterval,16921,0,2021-03-01T06:51:22Z,NA
uncmbb,Dataset contains select attributes for each match result since 1949-1950 season for UNC men's basketball team.,2021-01-04,Jay Lee,https://github.com/joongsup/uncmbb,TRUE,https://github.com/joongsup/uncmbb,14638,1,2021-01-04T08:08:40Z,14638
uncorbets,"Implements Minimum Torsion for portfolio diversification as
    described in Meucci, Attilio (2013) <doi:10.2139/ssrn.2276632>.",2021-07-12,Bernardo Reckziegel,https://github.com/Reckziegel/uncorbets,TRUE,https://github.com/reckziegel/uncorbets,682,0,2021-09-03T02:26:01Z,NA
UNF,"Computes a universal numeric fingerprint (UNF) for an R data
    object. UNF is a cryptographic hash or signature that can be used to uniquely
    identify (a version of) a rectangular dataset, or a subset thereof. UNF can
    be used, in tandem with a DOI, to form a persistent citation to a versioned
    dataset.",2021-05-16,Thomas J. Leeper,https://github.com/leeper/UNF,TRUE,https://github.com/leeper/unf,23343,18,2021-05-16T12:20:11Z,1296.8333333333333
ungroup,"Versatile method for ungrouping histograms (binned count data) 
 assuming that counts are Poisson distributed and that the underlying sequence 
 on a fine grid to be estimated is smooth. The method is based on the composite 
 link model and estimation is achieved by maximizing a penalized likelihood. 
 Smooth detailed sequences of counts and rates are so estimated from the binned 
 counts. Ungrouping binned data can be desirable for many reasons: Bins can be 
 too coarse to allow for accurate analysis; comparisons can be hindered when 
 different grouping approaches are used in different histograms; and the last 
 interval is often wide and open-ended and, thus, covers a lot of information 
 in the tail area. Age-at-death distributions grouped in age classes and 
 abridged life tables are examples of binned data. Because of modest assumptions, 
 the approach is suitable for many demographic and epidemiological applications. 
 For a detailed description of the method and applications see 
 Rizzi et al. (2015) <doi:10.1093/aje/kwv020>.",2021-06-28,Marius D. Pascariu,https://github.com/mpascariu/ungroup,TRUE,https://github.com/mpascariu/ungroup,20877,9,2021-06-28T21:57:37Z,2319.6666666666665
unheadr,"Verb-like functions to work with messy data, often derived from 
             spreadsheets or parsed PDF tables. Includes functions for unwrapping 
             values broken up across rows, relocating embedded grouping values, 
             and to annotate meaningful formatting in spreadsheet files.",2021-04-09,Luis D. Verde Arregoitia,"https://github.com/luisDVA/unheadr, https://unheadr.liomys.mx/",TRUE,https://github.com/luisdva/unheadr,9353,40,2021-04-09T21:49:21Z,233.825
unikn,"Define and use graphical elements of corporate design manuals in R. The 'unikn' package provides color functions (by defining dedicated colors and color palettes, and commands for changing, viewing, and using them) and styled text elements (e.g., for marking, underlining, or plotting colored titles). The pre-defined range of colors and text functions is based on the corporate design of the University of Konstanz <https://www.uni-konstanz.de/>, but can be adapted and extended for other institutions and purposes. ",2021-03-27,Hansjoerg Neth,https://CRAN.R-project.org/package=unikn,TRUE,https://github.com/hneth/unikn,19429,5,2021-08-09T14:37:00Z,3885.8
UniprotR,"Connect to Uniprot <https://www.uniprot.org/> to retrieve information about proteins using their accession number such information could be name or taxonomy information, For detailed information kindly read the publication <https://www.sciencedirect.com/science/article/pii/S1874391919303859>.",2021-07-10,Mohamed Soudy,https://github.com/Proteomicslab57357/UniprotR,TRUE,https://github.com/proteomicslab57357/uniprotr,21124,19,2021-08-10T09:51:14Z,1111.7894736842106
unitizer,"Simplifies regression tests by comparing objects produced by test
    code with earlier versions of those same objects.  If objects are unchanged
    the tests pass, otherwise execution stops with error details.  If in
    interactive mode, tests can be reviewed through the provided interactive
    environment.",2021-08-01,Brodie Gaslam,https://github.com/brodieG/unitizer,TRUE,https://github.com/brodieg/unitizer,53419,37,2021-08-02T10:53:21Z,1443.7567567567567
units,"Support for measurement units in R vectors, matrices
    and arrays: automatic propagation, conversion, derivation
    and simplification of units; raising errors in case of unit
    incompatibility. Compatible with the POSIXct, Date and difftime 
    classes. Uses the UNIDATA udunits library and unit database for 
    unit compatibility checking and conversion.
    Documentation about 'units' is provided in the paper by Pebesma, Mailund &
    Hiebert (2016, <doi:10.32614/RJ-2016-061>), included in this package as a
    vignette; see 'citation(""units"")' for details.",2021-06-08,Edzer Pebesma,https://github.com/r-quantities/units/,TRUE,https://github.com/r-quantities/units,12239119,117,2021-08-15T22:28:14Z,104607.8547008547
univariateML,"User-friendly maximum likelihood estimation (Fisher (1921) 
    <doi:10.1098/rsta.1922.0009>) of univariate densities.",2020-08-05,Jonas Moss,"https://github.com/JonasMoss/univariateML,
https://univariateml.netlify.com/",TRUE,https://github.com/jonasmoss/univariateml,25647,5,2021-08-31T16:50:16Z,5129.4
universals,"Provides S3 generic methods and some default implementations
    for Bayesian analyses that generate Markov Chain Monte Carlo (MCMC) samples.
    The purpose of 'universals' is to reduce package dependencies and conflicts.
    The 'nlist' package implements many of the methods for its 'nlist' class.",2020-09-24,Joe Thorley,"https://poissonconsulting.github.io/universals,
https://github.com/poissonconsulting/universals",TRUE,https://github.com/poissonconsulting/universals,18475,4,2021-03-17T22:53:17Z,4618.75
univOutl,"Well known outlier detection techniques in the univariate case. Methods to deal with skewed distribution are included too. The Hidiroglou-Berthelot (1986) method to search for outliers in ratios of historical data is implemented as well. When available, survey weights can be used in outliers detection.",2021-03-17,Marcello DOrazio,https://github.com/marcellodo/univOutl,TRUE,https://github.com/marcellodo/univoutl,66234,1,2021-03-17T08:52:29Z,66234
unix,"Bindings to system utilities found in most Unix systems such as
    POSIX functions which are not part of the Standard C Library.",2021-08-16,Jeroen Ooms,https://github.com/jeroen/unix,TRUE,https://github.com/jeroen/unix,25588,17,2021-08-04T20:49:42Z,1505.1764705882354
unnest,"Fast flattening of hierarchical data structures (e.g. JSON and XML
             documents) into data.frames with a flexible spec language.",2020-09-22,Vitalie Spinu,https://github.com/vspinu/unnest,TRUE,https://github.com/vspinu/unnest,4183,8,2021-07-14T12:01:29Z,522.875
unpivotr,"Tools for converting data from complex or irregular layouts to a
    columnar structure.  For example, tables with multilevel column or row
    headers, or spreadsheets.  Header and data cells are selected by their
    contents and position, as well as formatting and comments where available,
    and are associated with one other by their proximity in given directions.
    Functions for data frames and HTML tables are provided.",2021-08-22,Duncan Garmonsway,https://github.com/nacnudus/unpivotr,TRUE,https://github.com/nacnudus/unpivotr,26434,150,2021-08-21T20:23:34Z,176.22666666666666
unstruwwel,"Automatically converts language-specific verbal information, e.g., ""1st half of the 19th century,"" to its standardized numerical counterparts, e.g., ""1801-01-01/1850-12-31."" It follows the recommendations of the 'MIDAS' ('Marburger Informations-, Dokumentations- und Administrations-System'), see <doi:10.11588/artdok.00003770>.",2021-01-19,Stefanie Schneider,https://github.com/stefanieschneider/unstruwwel,TRUE,https://github.com/stefanieschneider/unstruwwel,2403,4,2021-01-20T07:46:11Z,600.75
unusualprofile,"Calculates a Mahalanobis distance for every row of a set of outcome variables (Mahalanobis, 1936 <doi:10.1007/s13171-019-00164-5>). The conditional Mahalanobis distance is calculated using a conditional covariance matrix (i.e., a covariance matrix of the outcome variables after controlling for a set of predictors). Plotting the output of the cond_maha() function can help identify which elements of a profile are unusual after controlling for the predictors.",2021-05-13,W. Joel Schneider,https://github.com/wjschne/unusualprofile,TRUE,https://github.com/wjschne/unusualprofile,1134,2,2021-05-12T16:05:43Z,567
unvotes,"Historical voting data of the United Nations General Assembly. This
    includes votes for each country in each roll call, as well as descriptions and
    topic classifications for each vote.",2021-03-09,David Robinson,https://github.com/dgrtwo/unvotes,TRUE,https://github.com/dgrtwo/unvotes,26405,44,2021-03-09T04:59:12Z,600.1136363636364
updog,"Implements empirical Bayes approaches to genotype
       polyploids from next generation sequencing data while
       accounting for allele bias, overdispersion, and sequencing
       error. The main functions are flexdog() and multidog(), 
       which allow the specification
       of many different genotype distributions. Also provided are functions to
       simulate genotypes, rgeno(), and read-counts, rflexdog(), as well as
       functions to calculate oracle genotyping error rates, oracle_mis(), and
       correlation with the true genotypes, oracle_cor(). These latter two
       functions are useful for read depth calculations. Run
       browseVignettes(package = ""updog"") in R for example usage. See
       Gerard et al. (2018) <doi:10.1534/genetics.118.301468> and
       Gerard and Ferrao (2020) <doi:10.1093/bioinformatics/btz852> for details 
       on the implemented methods.",2020-07-21,David Gerard,NA,TRUE,https://github.com/dcgerard/updog,15968,12,2021-07-20T13:23:56Z,1330.6666666666667
upsetjs,"'UpSet.js' is a re-implementation of 'UpSetR' to create interactive set visualizations for more than three sets.
  This is a 'htmlwidget' wrapper around the 'JavaScript' library 'UpSet.js'.",2021-02-15,Samuel Gratzl,https://github.com/upsetjs/upsetjs_r/,TRUE,https://github.com/upsetjs/upsetjs_r,6098,25,2021-02-14T21:57:58Z,243.92
UpSetR,"Creates visualizations of intersecting sets using a novel matrix
    design, along with visualizations of several common set, element and attribute
    related tasks (Conway 2017) <doi:10.1093/bioinformatics/btx364>.",2019-05-22,Jake Conway,http://github.com/hms-dbmi/UpSetR,TRUE,https://github.com/hms-dbmi/upsetr,572757,581,2020-12-28T03:04:52Z,985.8123924268502
upstartr,"Core functions necessary for using The Globe and Mail's R data journalism template, 'startr', along with utilities for day-to-day data journalism tasks, such as reading and writing files, producing graphics and cleaning up datasets.",2021-02-23,Tom Cardoso,"https://github.com/globeandmail/upstartr,
https://globeandmail.github.io/upstartr/",TRUE,https://github.com/globeandmail/upstartr,2506,3,2021-02-23T16:58:12Z,835.3333333333334
UPSvarApprox,"Variance approximations for the 
    Horvitz-Thompson total estimator in Unequal Probability Sampling
    using only first-order inclusion probabilities. 
    See Matei and Tillé (2005) and Haziza, Mecatti and Rao (2008) for details.",2020-10-14,Roberto Sichera,NA,TRUE,https://github.com/rhobis/upsvarapprox,14730,0,2021-05-23T18:11:54Z,NA
uptasticsearch,"
    'Elasticsearch' is an open-source, distributed, document-based datastore
    (<https://www.elastic.co/products/elasticsearch>).
    It provides an 'HTTP' 'API' for querying the database and extracting datasets, but that
    'API' was not designed for common data science workflows like pulling large batches of
    records and normalizing those documents into a data frame that can be used as a training
    dataset for statistical models. 'uptasticsearch' provides an interface for 'Elasticsearch'
    that is explicitly designed to make these data science workflows easy and fun.",2019-09-11,James Lamb,https://github.com/uptake/uptasticsearch,TRUE,https://github.com/uptake/uptasticsearch,36754,47,2021-05-25T03:41:18Z,782
urlchecker,"Provide the URL checking tools available in R 4.1+ as a package for
  earlier versions of R. Also uses concurrent requests so can be much faster than
  the serial versions.",2021-03-04,R Core team,https://github.com/r-lib/urlchecker,TRUE,https://github.com/r-lib/urlchecker,2429,30,2021-04-15T12:50:31Z,80.96666666666667
urlshorteneR,"Allows using two URL shortening services, which also provide
    expanding and analytic functions. Specifically developed for 'Bit.ly' (which requires OAuth2) and 'is.gd' (no API key).",2020-12-08,John Malc,https://github.com/dmpe/urlshorteneR,TRUE,https://github.com/dmpe/urlshortener,19300,19,2020-12-07T21:48:52Z,1015.7894736842105
urltools,"A toolkit for all URL-handling needs, including encoding and decoding,
    parsing, parameter extraction and modification. All functions are
    designed to be both fast and entirely vectorised. It is intended to be
    useful for people dealing with web-related datasets, such as server-side
    logs, although may be useful for other situations involving large sets of
    URLs.",2019-04-14,Os Keyes,https://github.com/Ironholds/urltools/,TRUE,https://github.com/ironholds/urltools,970109,122,2020-09-30T17:36:36Z,7951.713114754099
uroot,"Seasonal unit roots and seasonal stability tests.
    P-values based on response surface regressions are available for both tests.
    P-values based on bootstrap are available for seasonal unit root tests.",2020-09-04,Georgi N. Boshnakov,https://geobosh.github.io/uroot/,TRUE,https://github.com/geobosh/uroot,601233,2,2020-09-04T08:59:22Z,300616.5
ursa,"S3 classes and methods for manipulation with georeferenced raster data: reading/writing, processing, multi-panel visualization.",2021-05-22,Nikita Platonov,https://github.com/nplatonov/ursa,TRUE,https://github.com/nplatonov/ursa,10702,5,2021-05-28T14:22:51Z,2140.4
usa,"Updated versions of the 1970's ""US State Facts and
    Figures"" objects from the 'datasets' package included with R. The new
    data is compiled from a number of sources, primarily from United
    States Census Bureau or the relevant federal agency.",2020-02-23,Kiernan Nicholls,https://kiernann.com/usa https://github.com/kiernann/usa,TRUE,https://github.com/kiernann/usa,8114,2,2021-04-26T20:28:51Z,4057
usdampr,"Interface to easily access data via the United States Department of Agriculture (USDA)'s Livestock Mandatory Reporting ('LMR')
  Data API at <https://mpr.datamart.ams.usda.gov/>. The downloaded data can be saved for later off-line use. 
  Also provide relevant information and metadata for each of the input variables needed for sending the data inquiry.   ",2021-03-03,Bowen Chen,https://github.com/cbw1243/usdampr,TRUE,https://github.com/cbw1243/usdampr,4413,0,2021-03-06T05:47:05Z,NA
usdata,Demographic data on the United States at the county and state levels spanning multiple years.,2021-06-21,Mine Çetinkaya-Rundel,https://github.com/OpenIntroStat/usdata,TRUE,https://github.com/openintrostat/usdata,77470,2,2021-09-02T17:14:21Z,38735
usefun,"A set of general functions that I have used in various 
  projects and in other R packages. They support some miscellaneous operations 
  on data frames, matrices and vectors like adding a row on a ternary (3-value)
  data.frame based on positive and negative vector-indicators, rearranging a 
  list of data.frames by rownames, pruning rows or columns of a data.frame 
  that contain only one specific value given by the user,
  pruning and reordering a vector according to the common elements between its 
  names and elements of another given vector, finding the non-common elements 
  between two vectors (outer-section), 
  normalization of a vector, matrix or data.frame's numeric values in a specified range, 
  pretty printing of vector names and values in an R Markdown document.
  Also included is a function that returns the statistics needed for plotting a ROC curve.",2020-12-16,John Zobolas,https://github.com/bblodfon/usefun,TRUE,https://github.com/bblodfon/usefun,11615,0,2020-12-16T20:37:06Z,NA
usemodels,Code snippets to fit models using the tidymodels framework can be easily created for a given data set. ,2020-11-17,Max Kuhn,"https://usemodels.tidymodels.org/,
https://github.com/tidymodels/usemodels",TRUE,https://github.com/tidymodels/usemodels,7493,56,2021-06-14T15:41:50Z,133.80357142857142
usethis,"Automate package and project setup tasks that are
    otherwise performed manually. This includes setting up unit testing,
    test coverage, continuous integration, Git, 'GitHub', licenses,
    'Rcpp', 'RStudio' projects, and more.",2021-02-10,Hadley Wickham,"https://usethis.r-lib.org, https://github.com/r-lib/usethis",TRUE,https://github.com/r-lib/usethis,19674272,643,2021-08-30T14:08:34Z,30597.62363919129
usethis2,"Automate analytic project setup tasks that are otherwise performed 
    manually. This includes setting up docker, spinning up a microservice, and 
    more.",2021-08-10,Harel Lustiger,"https://tidylab.github.io/usethis2/,
https://github.com/tidylab/usethis2",TRUE,https://github.com/tidylab/usethis2,339,8,2021-08-11T09:00:07Z,42.375
USgas,Provides an overview of the demand for natural gas in the US by state and country level. Data source: US Energy Information Administration <https://www.eia.gov/>.,2021-05-26,Rami Krispin,https://github.com/RamiKrispin/USgas,TRUE,https://github.com/ramikrispin/usgas,5432,4,2021-05-27T15:36:50Z,1358
USgrid,"Provides a set of regular time-series datasets, describing the US electricity grid. That includes the total demand and supply, and as well as the demand by energy source (coal, solar, wind, etc.). Source: US Energy Information Administration (Dec 2019) <https://www.eia.gov/>.",2021-03-21,Rami Krispin,https://github.com/RamiKrispin/USgrid,TRUE,https://github.com/ramikrispin/usgrid,8270,17,2021-03-21T18:19:33Z,486.47058823529414
usl,"The Universal Scalability Law (Gunther 2007)
    <doi:10.1007/978-3-540-31010-5> is a model to predict hardware and
    software scalability. It uses system capacity as a function of load to
    forecast the scalability for the system.",2020-03-02,Stefan Moeding,NA,TRUE,https://github.com/smoeding/usl,19142,29,2021-07-18T10:45:54Z,660.0689655172414
usmap,"Obtain United States map data frames of varying region types (e.g. county, 
    state). The map data frames include Alaska and Hawaii conveniently placed to the
    bottom left, as they appear in most maps of the US. Convenience functions for plotting
    choropleths and working with FIPS codes are also provided.",2021-01-21,Paolo Di Lorenzo,https://usmap.dev,TRUE,https://github.com/pdil/usmap,140664,43,2021-04-09T03:10:07Z,3271.2558139534885
utf8,"Process and print 'UTF-8' encoded international
    text (Unicode). Input, validate, normalize, encode, format, and
    display.",2021-07-24,Kirill Müller,"https://ptrckprry.com/r-utf8/, https://github.com/patperry/r-utf8",TRUE,https://github.com/patperry/r-utf8,22520263,102,2021-07-29T04:13:53Z,220786.89215686274
utile.tables,A collection of functions to make building customized ready-to-export tables for publication purposes easier and creating summaries of large datasets for review a breeze.,2020-06-14,Eric Finnesgard,https://github.com/efinite/utile.tables,TRUE,https://github.com/efinite/utile.tables,11606,1,2021-05-28T20:32:50Z,11606
utile.tools,"A set of tools for preparing and summarizing data for publication purposes. Includes functions for tabulating models, means to produce human-readable summary statistics from raw data, macros for calculating duration of time, and simplistic hypothesis testing tools.",2020-06-04,Eric Finnesgard,https://github.com/efinite/utile.tools,TRUE,https://github.com/efinite/utile.tools,12734,1,2021-05-28T20:33:54Z,12734
utile.visuals,A small set of functions to aid in the production of visuals in ggplot2. Includes minimalist themes with transparent backgrounds and a suite of tools for building survival curves with risk tables.,2021-01-26,Eric Finnesgard,https://github.com/efinite/utile.visuals,TRUE,https://github.com/efinite/utile.visuals,11774,2,2021-05-28T20:33:17Z,5887
utiml,"Multi-label learning strategies and others procedures to support multi-
  label classification in R. The package provides a set of multi-label procedures such as
  sampling methods, transformation strategies, threshold functions, pre-processing 
  techniques and evaluation metrics. A complete overview of the matter can be seen in
  Zhang, M. and Zhou, Z. (2014) <doi:10.1109/TKDE.2013.39> and Gibaja, E. and 
  Ventura, S. (2015) A Tutorial on Multi-label Learning.",2021-05-31,Adriano Rivolli,https://github.com/rivolli/utiml,TRUE,https://github.com/rivolli/utiml,18912,22,2021-05-28T23:34:18Z,859.6363636363636
uwot,"An implementation of the Uniform Manifold Approximation and 
    Projection dimensionality reduction by McInnes et al. (2018) 
    <arXiv:1802.03426>. It also provides means to transform new data and to 
    carry out supervised dimensionality reduction. An implementation of the 
    related LargeVis method of Tang et al. (2016) <arXiv:1602.00370> is also 
    provided. This is a complete re-implementation in R (and C++, via the 'Rcpp'
    package): no Python installation is required. See the uwot website 
    (<https://github.com/jlmelville/uwot>) for more documentation and examples.",2020-12-15,James Melville,https://github.com/jlmelville/uwot,TRUE,https://github.com/jlmelville/uwot,340212,249,2021-08-22T05:03:21Z,1366.313253012048
V8,"An R interface to V8: Google's open source JavaScript and WebAssembly 
    engine. This package can be compiled either with V8 version 6 and up, a NodeJS
    shared library, or the legacy 3.14/3.15 branch of V8.",2021-05-01,Jeroen Ooms,https://github.com/jeroen/v8 (devel) https://v8.dev (upstream),TRUE,https://github.com/jeroen/v8,5022525,164,2021-07-22T11:45:04Z,30625.15243902439
vacuum,"An implementation of three procedures developed by 
    John Tukey: FUNOP (FUll NOrmal Plot), FUNOR-FUNOM 
    (FUll NOrmal Rejection-FUll NOrmal Modification), and vacuum cleaner. 
    Combined, they provide a way to identify, treat, and analyze outliers 
    in two-way (i.e., contingency) tables, as described in his 
    landmark paper ""The Future of Data Analysis"", Tukey, John W. (1962) 
    <https://www.jstor.org/stable/2237638>. ",2020-09-08,Ron Sielinski,https://github.com/Sielinski/vacuum,TRUE,https://github.com/sielinski/vacuum,4006,2,2020-09-11T13:00:50Z,2003
valaddin,"A set of basic tools to transform functions into functions with
    input validation checks, in a manner suitable for both programmatic and
    interactive use.",2021-01-08,Eugene Ha,https://github.com/egnha/valaddin,TRUE,https://github.com/egnha/valaddin,14690,26,2021-01-08T18:06:59Z,565
valhallr,"An interface to the 'Valhalla' routing engine’s
    application programming interfaces (APIs) for turn-by-turn routing,
    isochrones, and origin-destination analyses. Also includes several
    user-friendly functions for plotting outputs, and strives to follow
    ""tidy"" design principles. Please note that this package requires
    access to a running instance of 'Valhalla', which is open source and
    can be downloaded from <https://github.com/valhalla/valhalla>.",2021-03-09,Christopher Belanger,https://github.com/chris31415926535/valhallr,TRUE,https://github.com/chris31415926535/valhallr,2098,6,2021-05-07T12:45:40Z,349.6666666666667
validate,"Declare data validation rules and data quality indicators;
        confront data with them and analyze or visualize the results.
        The package supports rules that are per-field, in-record,
        cross-record or cross-dataset. Rules can be automatically
        analyzed for rule type and connectivity. See also Van der Loo
        and De Jonge (2018) <doi:10.1002/9781118897126>, Chapter 6
        and the JSS paper (2021) <doi:10.18637/jss.v097.i10>.",2021-04-29,Mark van der Loo,https://github.com/data-cleaning/validate,TRUE,https://github.com/data-cleaning/validate,72271,308,2021-08-24T15:23:00Z,234.6461038961039
validatedb,"Check whether records in a database table are valid using 
   validation rules in R syntax specified with R package 'validate'. 
   R validation checks are automatically translated in SQL using 'dbplyr'.",2021-03-18,Edwin de Jonge,https://github.com/data-cleaning/validatedb,TRUE,https://github.com/data-cleaning/validatedb,1426,14,2021-07-05T07:46:26Z,101.85714285714286
validatetools,"Rule sets with validation rules may contain redundancies or contradictions. 
  Functions for finding redundancies and problematic rules are provided, 
  given a set a rules formulated with 'validate'.",2020-02-06,Edwin de Jonge,https://github.com/data-cleaning/validatetools,TRUE,https://github.com/data-cleaning/validatetools,16769,11,2020-12-02T13:36:55Z,1524.4545454545455
valr,"Read and manipulate genome intervals and signals. Provides
             functionality similar to command-line tool suites within R,
             enabling interactive analysis and visualization of genome-scale data. 
             Riemondy et al. (2017) <doi:10.12688/f1000research.11997.1>.",2021-05-15,Jay Hesselberth,"https://github.com/rnabioco/valr/,
https://rnabioco.github.io/valr/",TRUE,https://github.com/rnabioco/valr,21384,67,2021-09-03T14:30:05Z,319.1641791044776
VancouvR,"Wrapper around the 'City of Vancouver' Open Data API <https://opendata.vancouver.ca/api/v2/console> to simplify and standardize access to 'City of Vancouver' open data. 
  Functionality to list the data catalogue and access data and geographic records.",2021-07-09,Jens von Bergmann,"https://github.com/mountainMath/VancouvR,
https://mountainmath.github.io/VancouvR/",TRUE,https://github.com/mountainmath/vancouvr,9480,14,2021-07-08T23:26:11Z,677.1428571428571
vanddraabe,"Identify and analyze conserved waters within crystallographic 
  protein structures and molecular dynamics simulation trajectories. Statistical 
  parameters for each water cluster, informative graphs, and a PyMOL session 
  file to visually explore the conserved waters and protein are returned. 
  Hydrophilicity is the propensity of waters to congregate near specific protein 
  atoms and is related to conserved waters. An informatics derived set of 
  hydrophilicity values are provided based on a large, high-quality X-ray 
  protein structure dataset.",2019-06-07,Emilio Xavier Esposito,"http://vanddraabe.com, https://github.com/exeResearch/vanddraabe/",TRUE,https://github.com/exeresearch/vanddraabe,8841,0,2021-06-10T20:58:26Z,NA
vapour,"Provides low-level access to 'GDAL' functionality for R packages.  
  'GDAL' is the 'Geospatial Data Abstraction Library' a translator for raster and vector geospatial data formats 
  that presents a single raster abstract data model and single vector abstract data model to the calling application 
  for all supported formats <https://gdal.org/>. ",2021-08-25,Michael Sumner,https://github.com/hypertidy/vapour,TRUE,https://github.com/hypertidy/vapour,15929,54,2021-08-23T01:20:30Z,294.98148148148147
vardpoor,"Generation of domain variables, linearization of several non-linear population statistics (the ratio of two totals, weighted income percentile, relative median income ratio, at-risk-of-poverty rate, at-risk-of-poverty threshold, Gini coefficient, gender pay gap, the aggregate replacement ratio, the relative median income ratio, median income below at-risk-of-poverty gap, income quintile share ratio, relative median at-risk-of-poverty gap), computation of regression residuals in case of weight calibration, variance estimation of sample surveys by the ultimate cluster method (Hansen, Hurwitz and Madow, Sample Survey Methods And Theory, vol. I: Methods and Applications; vol. II: Theory. 1953, New York: John Wiley and Sons), variance estimation for longitudinal, cross-sectional measures and measures of change for single and multistage stage cluster sampling designs (Berger, Y. G., 2015, <doi:10.1111/rssa.12116>). Several other precision measures are derived - standard error, the coefficient of variation, the margin of error, confidence interval, design effect.",2020-11-30,Martins Liberts,"https://csblatvia.github.io/vardpoor/,
https://github.com/CSBLatvia/vardpoor/",TRUE,https://github.com/csblatvia/vardpoor,39344,6,2021-03-17T13:56:22Z,6557.333333333333
varTestnlme,"An implementation of the Likelihood ratio Test (LRT) for testing that,
    in a (non)linear mixed effects model, the variances of a subset of the random
    effects are equal to zero. There is no restriction on the subset of variances
    that can be tested: for example, it is possible to test that all the variances
    are equal to zero. Note that the implemented test is asymptotic.
    This package should be used on model fits from packages 'nlme', 'lmer', and 'saemix'.
    Charlotte Baey, Paul-Henry Cournède and Estelle Kuhn (2019) <doi:10.1016/j.csda.2019.01.014>.",2021-01-07,Charlotte Baey,https://github.com/baeyc/varTestnlme/,TRUE,https://github.com/baeyc/vartestnlme,8633,0,2021-02-04T15:26:03Z,NA
varycoef,"Implements a maximum likelihood estimation (MLE)
    method for estimation and prediction of Gaussian process-based
    spatially varying coefficient (SVC) models 
    (Dambon et al. (2021a) <doi:10.1016/j.spasta.2020.100470>). 
    Covariance tapering (Furrer et al. (2006) <doi:10.1198/106186006X132178>) can 
    be applied such that the method scales to large data. Further, it implements
    a joint variable selection of the fixed and random effects (Dambon et al. 
    (2021b) <arXiv:2101.01932>).",2021-06-03,Jakob A. Dambon,https://github.com/jakobdambon/varycoef,TRUE,https://github.com/jakobdambon/varycoef,11898,4,2021-07-19T15:31:04Z,2974.5
vasicek,"Provide a collection of miscellaneous R functions
    related to the Vasicek distribution with the intent to make
    the lives of risk modelers easier.",2020-11-08,WenSui Liu,https://github.com/statcompute/vasicek,TRUE,https://github.com/statcompute/vasicek,5454,0,2020-10-24T20:28:36Z,NA
vaultr,"Provides an interface to a 'HashiCorp' vault server over
  its http API (typically these are self-hosted; see
  <https://www.vaultproject.io>).  This allows for secure storage and
  retrieval of secrets over a network, such as tokens, passwords and
  certificates.  Authentication with vault is supported through
  several backends including user name/password and authentication via
  'GitHub'.",2021-06-09,Rich FitzJohn,"https://github.com/vimc/vaultr,
https://www.vaccineimpact.org/vaultr/",TRUE,https://github.com/vimc/vaultr,14192,17,2021-06-09T13:51:09Z,834.8235294117648
VC2copula,"Provides new classes for (rotated) BB1, BB6, BB7, BB8, and 
  Tawn copulas, extends the existing Gumbel and Clayton families with 
  rotations, and allows to set up a vine copula model using the 'copula' API.
  Corresponding objects from the 'VineCopula' API can easily be converted.",2020-10-23,Thomas Nagler,https://github.com/tnagler/VC2copula,TRUE,https://github.com/tnagler/vc2copula,9701,3,2020-11-05T16:41:34Z,3233.6666666666665
vcdExtra,"Provides additional data sets, methods and documentation to complement the 'vcd' package for Visualizing Categorical Data
    and the 'gnm' package for Generalized Nonlinear Models.
	In particular, 'vcdExtra' extends mosaic, assoc and sieve plots from 'vcd' to handle 'glm()' and 'gnm()' models and
	adds a 3D version in 'mosaic3d'.  Additionally, methods are provided for comparing and visualizing lists of
	'glm' and 'loglm' objects. This package is now a support package for the book, ""Discrete Data Analysis with R"" by
  Michael Friendly and David Meyer.",2021-01-25,Michael Friendly,https://CRAN.R-project.org/package=vcdExtra,TRUE,https://github.com/friendly/vcdextra,180498,12,2021-01-25T16:05:14Z,15041.5
vcmeta,"Implements functions for varying coefficient meta-analysis methods. 
  These methods do not assume effect size homogeneity. Subgroup effect size 
  comparisons, general linear effect size contrasts, and linear models of 
  effect sizes based on varying coefficient methods can be used to describe 
  effect size heterogeneity. Varying coefficient meta-analysis methods do not 
  require the unrealistic assumptions of the traditional fixed-effect and 
  random-effects meta-analysis methods.  
  For details, see: 
  Bonett (2008) <doi:10.1037/a0012868>, 
  Bonett (2009) <doi:10.1037/a0016619>,
  Bonett (2010) <doi:10.1037/a0020142>,
  Bonett & Price (2014) <doi:10.1111/bmsp.12024>,
  Bonett & Price (2015) <doi:10.1111/bmsp.12024>,
  Bonett (2020) <doi:10.1111/bmsp.12189>,
  Bonett (2021) <doi:10.1177/1094428120911088>.",2021-08-21,Douglas G. Bonett,https://github.com/dgbonett/vcmeta,TRUE,https://github.com/dgbonett/vcmeta,130,0,2021-08-19T18:12:04Z,NA
vcr,"Record test suite 'HTTP' requests and replays them during
    future runs. A port of the Ruby gem of the same name
    (<https://github.com/vcr/vcr/>). Works by hooking into the 'webmockr'
    R package for matching 'HTTP' requests by various rules ('HTTP' method,
    'URL', query parameters, headers, body, etc.), and then caching
    real 'HTTP' responses on disk in 'cassettes'. Subsequent 'HTTP' requests
    matching any previous requests in the same 'cassette' use a cached
    'HTTP' response.",2021-05-31,Scott Chamberlain,"https://github.com/ropensci/vcr/ (devel)
https://books.ropensci.org/http-testing/ (user manual)",TRUE,https://github.com/ropensci/vcr,60969,58,2021-05-31T17:17:13Z,1051.1896551724137
vctrs,"Defines new notions of prototype and size that are
    used to provide tools for consistent and well-founded type-coercion
    and size-recycling, and are in turn connected to ideas of type- and
    size-stability useful for analysing function interfaces.",2021-04-29,Lionel Henry,https://vctrs.r-lib.org/,TRUE,https://github.com/r-lib/vctrs,36452680,215,2021-08-26T14:26:05Z,169547.3488372093
vdiffr,"An extension to the 'testthat' package that makes it easy
    to add graphical unit tests. It provides a Shiny application to
    manage the test cases.",2021-06-18,Lionel Henry,"https://vdiffr.r-lib.org/, https://github.com/r-lib/vdiffr",TRUE,https://github.com/r-lib/vdiffr,6210206,156,2021-06-29T17:56:44Z,39809.01282051282
vectorwavelet,"New wavelet methodology (vector wavelet coherence) (Oygur, T., Unal, G, 2020 <doi:10.1007/s40435-020-00706-y>) 
  to handle dynamic co-movements of multivariate time series via extending multiple and quadruple wavelet coherence methodologies. 
  This package can be used to perform multiple wavelet coherence, quadruple wavelet coherence, and n-dimensional vector wavelet coherence analyses.",2021-01-13,Tunc Oygur,https://github.com/toygur/vectorwavelet,TRUE,https://github.com/toygur/vectorwavelet,2574,2,2021-01-08T19:31:43Z,1287
vegan,"Ordination methods, diversity analysis and other
  functions for community and vegetation ecologists.",2020-11-28,Jari Oksanen,"https://cran.r-project.org, https://github.com/vegandevs/vegan",TRUE,https://github.com/vegandevs/vegan,1608122,269,2021-06-17T08:45:53Z,5978.148698884758
vegawidget,"'Vega' and 'Vega-Lite' parse text in 'JSON' notation to render 
  chart-specifications into 'HTML'. This package is used to facilitate the 
  rendering. It also provides a means to interact with signals, events,
  and datasets in a 'Vega' chart using 'JavaScript' or 'Shiny'.",2021-01-12,Ian Lyttle,https://github.com/vegawidget/vegawidget,TRUE,https://github.com/vegawidget/vegawidget,37172,50,2021-06-20T16:50:13Z,743.44
vegperiod,"Collection of common methods to determine growing season length in
  a simple manner. Start and end dates of the vegetation periods are calculated
  solely based on daily mean temperatures and the day of the year.",2021-02-02,Robert Nuske,https://github.com/rnuske/vegperiod,TRUE,https://github.com/rnuske/vegperiod,13909,2,2021-08-23T12:28:20Z,6954.5
vegtable,"Import and handling data from vegetation-plot databases, especially
    data stored in 'Turboveg' (<https://www.synbiosys.alterra.nl/turboveg>).
    Also import/export routines for exchange of data with 'Juice'
    (<http://www.sci.muni.cz/botany/juice>) are implemented.",2020-04-30,Miguel Alvarez,https://github.com/kamapu/vegtable,TRUE,https://github.com/kamapu/vegtable,15140,2,2021-05-28T08:50:47Z,7570
vein,"Elaboration of vehicular emissions inventories,
    consisting in four stages, pre-processing activity data, preparing 
    emissions factors, estimating the emissions and post-processing of emissions 
    in maps and databases. More details in Ibarra-Espinosa et al (2018) <doi:10.5194/gmd-11-2209-2018>.
    Before using VEIN you need to know the vehicular composition of your study area, in other words,
    the combination of of type of vehicles, size and fuel of the fleet. Then, it is recommended to
    start with the project to download a template to create a structure of directories and scripts.",2021-06-30,Sergio Ibarra-Espinosa,https://github.com/atmoschem/vein,TRUE,https://github.com/atmoschem/vein,22276,28,2021-09-02T23:37:45Z,795.5714285714286
vembedr,"A set of functions for generating HTML to
    embed hosted video in your R Markdown documents or Shiny applications.",2020-10-10,Ian Lyttle,https://github.com/ijlyttle/vembedr,TRUE,https://github.com/ijlyttle/vembedr,26347,49,2020-12-18T16:36:41Z,537.6938775510204
venn,"Draws and displays Venn diagrams up to 7 sets, and any Boolean union of set intersections.",2021-03-15,Adrian Dusa,https://github.com/dusadrian/venn,TRUE,https://github.com/dusadrian/venn,66436,6,2021-08-25T17:25:04Z,11072.666666666666
vetr,"Declarative template-based framework for verifying that objects
  meet structural requirements, and auto-composing error messages when they do
  not.",2021-05-03,Brodie Gaslam,https://github.com/brodieG/vetr,TRUE,https://github.com/brodieg/vetr,18650,60,2021-05-10T23:46:44Z,310.8333333333333
vfinputs,A set of visual input controls for Shiny apps to facilitate filtering across multiple outputs.,2020-10-13,Rafael Henkin,https://github.com/rhenkin/vfinputs,TRUE,https://github.com/rhenkin/vfinputs,3430,1,2020-10-19T13:33:23Z,3430
VFS,"Empirical models for runoff, erosion, and phosphorus loss 
    across a vegetated filter strip, given slope, soils, climate, and 
    vegetation (Gall et al., 2018) <doi:10.1007/s00477-017-1505-x>. 
    It also includes functions for deriving climate parameters from 
    measured daily weather data, and for simulating rainfall. Models 
    implemented include MUSLE (Williams, 1975) and APLE (Vadas et al., 
    2009 <doi:10.2134/jeq2008.0337>).",2018-10-12,Sarah Goslee,NA,TRUE,https://github.com/sgoslee/vfs,11040,0,2021-04-20T17:05:50Z,NA
viafr,"Provides direct access to linked names for the same entity across the world's major name authority files, including national and regional variations in language, character set, and spelling. For more information go to <https://viaf.org/>.",2020-04-22,Stefanie Schneider,https://github.com/stefanieschneider/viafr,TRUE,https://github.com/stefanieschneider/viafr,9265,5,2021-02-12T11:02:28Z,1853
vICC,"Compute group-specific intraclass correlation coefficients, 
    Bayesian testing of homogenous within-group variance, and spike-and-slab
    model selection to determine which groups share a common within-group 
    variance in a one-way random effects model <10.31234/osf.io/hpq7w>.",2020-12-08,Donald Williams,NA,TRUE,https://github.com/donaldrwilliams/vicc,2071,5,2020-12-13T20:54:02Z,414.2
VicmapR,"Easily interfaces R to spatial datasets available through 
  the Victorian Government's WFS (Web Feature Service): <https://services.land.vic.gov.au/catalogue/publicproxy/guest/dv_geoserver/wfs?request=getCapabilities>, 
  which allows users to read in 'sf' data from these sources. VicmapR uses the lazy querying approach and code developed by Teucher et al. (2021) for the 'bcdata' R package <doi:10.21105/joss.02927>.",2021-07-26,Justin Cally,"https://justincally.github.io/VicmapR/,
https://mapshare.vic.gov.au/vicplan/,
https://github.com/justincally/VicmapR/",TRUE,https://github.com/justincally/vicmapr,1258,13,2021-07-26T08:32:41Z,96.76923076923077
VIM,"New tools for the visualization of missing and/or imputed values
    are introduced, which can be used for exploring the data and the structure of
    the missing and/or imputed values. Depending on this structure of the missing
    values, the corresponding methods may help to identify the mechanism generating
    the missing values and allows to explore the data including missing values.
    In addition, the quality of imputation can be visually explored using various
    univariate, bivariate, multiple and multivariate plot methods. A graphical user
    interface available in the separate package VIMGUI allows an easy handling of
    the implemented plot methods.",2021-07-22,Matthias Templ,https://github.com/statistikat/VIM,TRUE,https://github.com/statistikat/vim,765508,59,2021-07-22T10:59:08Z,12974.71186440678
vimp,"Calculate point estimates of and valid confidence intervals for
    nonparametric, algorithm-agnostic variable importance measures in high and low dimensions,
    using flexible estimators of the underlying regression functions. For more information
    about the methods, please see Williamson et al. (Biometrics, 2020),  Williamson et al. (arXiv, 2020+) <arXiv:2004.03683>, and Williamson and Feng (ICML, 2020).",2021-08-16,Brian D. Williamson,"https://bdwilliamson.github.io/vimp/,
https://github.com/bdwilliamson/vimp",TRUE,https://github.com/bdwilliamson/vimp,18361,11,2021-08-23T16:22:12Z,1669.1818181818182
VineCopula,"Provides tools for the statistical analysis of vine copula models.
    The package includes tools for parameter estimation, model selection,
    simulation, goodness-of-fit tests, and visualization. Tools for estimation,
    selection and exploratory data analysis of bivariate copula models are also
    provided.",2021-05-12,Thomas Nagler,https://github.com/tnagler/VineCopula,TRUE,https://github.com/tnagler/vinecopula,124088,41,2021-05-11T11:36:06Z,3026.5365853658536
vinereg,"
  Implements D-vine quantile regression models with
  parametric or nonparametric pair-copulas. See 
  Kraus and Czado (2017) <doi:10.1016/j.csda.2016.12.009> and
  Schallhorn et al. (2017) <arXiv:1705.08310>.",2021-05-14,Thomas Nagler,https://tnagler.github.io/vinereg/,TRUE,https://github.com/tnagler/vinereg,14793,4,2021-05-14T18:54:03Z,3698.25
vines,"Implementation of the vine graphical model for building
    high-dimensional probability distributions as a factorization of
    bivariate copulas and marginal density functions. This package
    provides S4 classes for vines (C-vines and D-vines) and methods
    for inference, goodness-of-fit tests, density/distribution
    function evaluation, and simulation.",2016-07-28,Yasser Gonzalez-Fernandez,https://github.com/yasserglez/vines,TRUE,https://github.com/yasserglez/vines,23607,0,2021-06-09T03:39:08Z,NA
vioplot,A violin plot is a combination of a box plot and a kernel density plot. This package allows extensive customisation of violin plots. ,2021-07-27,S. Thomas Kelly,https://github.com/TomKellyGenetics/vioplot,TRUE,https://github.com/tomkellygenetics/vioplot,274867,25,2021-07-27T06:13:52Z,10994.68
vip,"A general framework for constructing variable importance plots from 
  various types of machine learning models in R. Aside from some standard model-
  specific variable importance measures, this package also provides model-
  agnostic approaches that can be applied to any supervised learning algorithm.
  These include 1) an efficient permutation-based variable importance measure, 
  2) variable importance based on Shapley values (Strumbelj and Kononenko, 
  2014) <doi:10.1007/s10115-013-0679-x>, and 3) the variance-based 
  approach described in Greenwell et al. (2018) <arXiv:1805.04755>. A 
  variance-based method for quantifying the relative strength of interaction 
  effects is also included (see the previous reference for details).",2020-12-17,Brandon Greenwell,https://github.com/koalaverse/vip/,TRUE,https://github.com/koalaverse/vip,165205,148,2020-12-15T20:56:53Z,1116.25
viridis,"Color maps designed to improve graph readability for readers with 
    common forms of color blindness and/or color vision deficiency. The color 
    maps are also perceptually-uniform, both in regular form and also when 
    converted to black-and-white for printing. This package also contains 
    'ggplot2' bindings for discrete and continuous color and fill scales. A lean
    version of the package called 'viridisLite' that does not include the 
    'ggplot2' bindings can be found at 
    <https://cran.r-project.org/package=viridisLite>.",2021-05-11,Simon Garnier,"https://sjmgarnier.github.io/viridis/,
https://github.com/sjmgarnier/viridis/",TRUE,https://github.com/sjmgarnier/viridis,10266900,238,2021-05-04T10:10:34Z,43138.23529411765
viridisLite,"Color maps designed to improve graph readability for readers with 
    common forms of color blindness and/or color vision deficiency. The color 
    maps are also perceptually-uniform, both in regular form and also when 
    converted to black-and-white for printing. This is the 'lite' version of the 
    'viridis' package that also contains 'ggplot2' bindings for discrete and 
    continuous color and fill scales and can be found at 
    <https://cran.r-project.org/package=viridis>.",2021-04-13,Simon Garnier,"https://sjmgarnier.github.io/viridisLite/,
https://github.com/sjmgarnier/viridisLite/",TRUE,https://github.com/sjmgarnier/viridislite,16812235,45,2021-04-11T18:02:16Z,373605.22222222225
virtuoso,"Provides users with a simple and convenient
             mechanism to manage and query a 'Virtuoso' database using the 'DBI' (Data-Base Interface)
             compatible 'ODBC' (Open Database Connectivity) interface.
             'Virtuoso' is a high-performance ""universal server,"" which can act
             as both a relational database, supporting standard Structured Query
             Language ('SQL') queries, while also supporting data following the
             Resource Description Framework ('RDF') model for Linked Data.
             'RDF' data can be queried using 'SPARQL' ('SPARQL' Protocol and 'RDF' Query Language)
             queries, a graph-based query that supports semantic reasoning.
             This allows users to leverage the performance of local or remote 'Virtuoso' servers using
             popular 'R' packages such as 'DBI' and 'dplyr', while also providing a 
             high-performance solution for working with large 'RDF' 'triplestores' from 'R.'
             The package also provides helper routines to install, launch, and manage
             a 'Virtuoso' server locally on 'Mac', 'Windows' and 'Linux' platforms using
             the standard interactive installers from the 'R' command-line.  By 
             automatically handling these setup steps, the package can make using 'Virtuoso'
             considerably faster and easier for a most users to deploy in a local
             environment. Managing the bulk import of triples
             from common serializations with a single intuitive command is another key
             feature of this package.  Bulk import performance can be tens to
             hundreds of times faster than the comparable imports using existing 'R' tools,
             including 'rdflib' and 'redland' packages.  ",2021-03-23,Carl Boettiger,https://github.com/ropensci/virtuoso,TRUE,https://github.com/ropensci/virtuoso,6682,8,2021-03-23T02:37:34Z,835.25
virustotal,"Use VirusTotal, a Google service that analyzes files and URLs 
    for viruses, worms, trojans etc., provides category of the content hosted by a 
    domain from a variety of prominent services, provides passive DNS information,
    among other things. See <http://www.virustotal.com> for more information. ",2017-05-01,Gaurav Sood,http://github.com/soodoku/virustotal,TRUE,https://github.com/soodoku/virustotal,14331,10,2021-04-02T15:52:36Z,1433.1
visa,"Provides easy-to-use tools for data analysis and visualization for hyperspectral remote sensing (also known as imaging spectroscopy), with 
    a particular focus on vegetation hyperspectral data analysis. It consists of a set of functions, ranging from the organization of hyperspectral data 
    in the proper data structure for spectral feature selection, calculation of vegetation index, multivariate analysis, as well as to the visualization 
    of spectra and results of analysis in the 'ggplot2' style.",2021-04-20,Kang Yu,https://github.com/kang-yu/visa,TRUE,https://github.com/kang-yu/visa,1408,1,2021-04-20T21:02:00Z,1408
visae,Implementation of Shiny app to visualize adverse events based on the Common Terminology Criteria for Adverse Events using stacked correspondence analysis as described in Diniz et. al (2021) <arXiv:2101.03454>.,2021-01-16,Marcio A. Diniz,NA,TRUE,https://github.com/dnzmarcio/visae,2436,1,2021-08-04T02:43:43Z,2436
viscomplexr,"Functionality for creating phase portraits of functions in the
    complex number plane. Works with R base graphics, whose full 
    functionality is available. Parallel processing is used for optimum 
    performance.",2020-12-11,Peter Biber,"https://peterbiber.github.io/viscomplexr/,
https://github.com/PeterBiber/viscomplexr/",TRUE,https://github.com/peterbiber/viscomplexr,2552,2,2021-04-25T11:46:49Z,1276
visdat,"Create preliminary exploratory data visualisations of an entire 
    dataset to identify problems or unexpected features using 'ggplot2'.",2019-02-15,Nicholas Tierney,"http://visdat.njtierney.com/, https://github.com/ropensci/visdat",TRUE,https://github.com/ropensci/visdat,430298,387,2021-08-06T05:55:49Z,1111.8811369509044
visR,"To enable fit-for-purpose, reusable clinical and medical research 
  focused visualizations and tables with sensible defaults and based on 
  graphical principles as described in: 
  ""Vandemeulebroecke et al. (2018)"" <doi:10.1002/pst.1912>, 
  ""Vandemeulebroecke et al. (2019)"" <doi:10.1002/psp4.12455>, and 
  ""Morris et al. (2019)"" <doi:10.1136/bmjopen-2019-030215>.",2021-06-14,Mark Baillie,https://github.com/openpharma/visR,TRUE,https://github.com/openpharma/visr,1155,105,2021-08-18T16:45:47Z,11
vistime,"A library for creating time based charts, like Gantt or timelines. Possible outputs 
  include 'ggplot2' diagrams, 'plotly.js' graphs, 'Highcharts.js' widgets and data.frames. Results can be
  used in the 'RStudio' viewer pane, in 'RMarkdown' documents or in Shiny apps. In the 
  interactive outputs created by vistime() and hc_vistime(), you can interact with the 
  plot using mouse hover or zoom.",2021-04-10,Sandro Raabe,https://shosaco.github.io/vistime/,TRUE,https://github.com/shosaco/vistime,33532,113,2021-04-16T12:05:52Z,296.7433628318584
vistributions,"Visualize and compute percentiles/probabilities of normal, t, f, chi square 
    and binomial distributions.",2021-05-20,Aravind Hebbali,"https://github.com/rsquaredacademy/vistributions,
https://vistributions.rsquaredacademy.com",TRUE,https://github.com/rsquaredacademy/vistributions,27979,7,2021-07-16T10:57:44Z,3997
Visualize.CRAN.Downloads,Visualize the trends and historical downloads from packages in the 'CRAN' repository. Data is obtained by using the 'API' to query the database from the 'RStudio' 'CRAN' mirror.,2021-04-28,Marcelo Ponce,https://github.com/mponce0/Visualize.CRAN.Downloads,TRUE,https://github.com/mponce0/visualize.cran.downloads,7374,3,2021-04-27T20:07:25Z,2458
vitae,Provides templates and functions to simplify the production and maintenance of curriculum vitae.,2021-02-17,Mitchell OHara-Wild,"https://pkg.mitchelloharawild.com/vitae/,
https://github.com/mitchelloharawild/vitae",TRUE,https://github.com/mitchelloharawild/vitae,36451,705,2021-08-20T02:04:56Z,51.70354609929078
vivo,"Provides an easy to calculate local variable importance measure based on Ceteris Paribus profile 
  and global variable importance measure based on Partial Dependence Profiles.",2020-09-07,Anna Kozak,https://github.com/ModelOriented/vivo,TRUE,https://github.com/modeloriented/vivo,11465,14,2020-09-26T19:15:12Z,818.9285714285714
vizdraws,"Interactive visualization for Bayesian prior and posterior distributions. 
             When both distributions are provided the animation shows a transition from 
             prior to posterior. Finally, the animation splits the distribution using the provided 
             'breaks' into bars that show the probability for each region. 
             If no 'breaks' are providers it will use zero by default.",2021-01-09,Ignacio Martinez,"https://github.com/ignacio82/vizdraws/,
https://vizdraws.martinez.fyi/",TRUE,https://github.com/ignacio82/vizdraws,5600,10,2021-01-17T13:48:14Z,560
vkR,"Provides an interface to the VK API <https://vk.com/dev/methods>.
      VK <https://vk.com/> is the largest European online social networking
      service, based in Russia.",2020-09-29,Dmitriy Sorokin,https://github.com/Dementiy/vkR,TRUE,https://github.com/dementiy/vkr,17271,52,2020-09-25T17:50:00Z,332.13461538461536
vlad,"Contains functions to set up risk-adjusted quality control charts in health care. For the variable life adjusted display (VLAD) proposed by Lovegrove et al. (1997) <doi:10.1016/S0140-6736(97)06507-0> signaling rules derived in Wittenberg et al. (2018) <doi:10.1002/sim.7647> are implemented. Additionally, for the risk-adjusted cumulative sum chart based on log-likelihood ratio statistic introduced by Steiner et al. (2000) <doi:10.1093/biostatistics/1.4.441> average run length and control limits can be computed with fast and accurate Markov chain approximations developed in Knoth et al. (2019) <doi:10.1002/sim.8104>.",2021-02-15,Philipp Wittenberg,https://github.com/wittenberg/vlad,TRUE,https://github.com/wittenberg/vlad,13177,5,2021-05-26T08:12:24Z,2635.4
VLTimeCausality,"A framework to infer causality on a pair of time series of real numbers based on variable-lag Granger causality and transfer entropy. Typically, Granger causality and transfer entropy have an assumption of a fixed and constant time delay between the cause and effect. However, for a non-stationary time series, this assumption is not true. For example, considering two time series of velocity of person A and person B where B follows A. At some time, B stops tying his shoes, then running to catch up A. The fixed-lag assumption is not true in this case. We propose a framework that allows variable-lags between cause and effect in Granger causality and transfer entropy to allow them to deal with variable-lag non-stationary time series. Please see Chainarong Amornbunchornvej, Elena Zheleva, and Tanya Berger-Wolf (2021) <doi:10.1145/3441452> when referring to this package in publications.  ",2021-05-08,Chainarong Amornbunchornvej,https://github.com/DarkEyes/VLTimeSeriesCausality,TRUE,https://github.com/darkeyes/vltimeseriescausality,9587,26,2021-05-08T14:42:35Z,368.7307692307692
volcano3D,"Differential expression (DE) analysis can be used to discover quantitative changes in expression levels between experimental groups. Such results are typically visualised using volcano plots, however in cases where more than two experimental groups are involved, visualising results can become convoluted and it quickly becomes difficult to see the wood for the trees. This package provides easy-to-use functions to extract and visualise outputs from DE between three groups (primarily aimed at 'limma' and 'DESeq2' outputs). We present novel methods to map DE results into polar coordinates to enable users to combine and simultaneously view three sets of results. These graphics also possess optional 'plotly' outputs for interactive and three-dimensional functionality, as seen in Lewis et. al. (2019) <doi:10.1016/j.celrep.2019.07.091>.",2021-03-31,Katriona Goldmann,"https://katrionagoldmann.github.io/volcano3D/index.html,
https://github.com/KatrionaGoldmann/volcano3D",TRUE,https://github.com/katrionagoldmann/volcano3d,6238,9,2021-04-27T17:00:41Z,693.1111111111111
volesti,"Provides an R interface for 'volesti' C++ package. 'volesti' computes estimations of volume 
             of polytopes given by (i) a set of points, (ii) linear inequalities or (iii) Minkowski sum of segments 
             (a.k.a. zonotopes). There are three algorithms for volume estimation as well as algorithms 
             for sampling, rounding and rotating polytopes. Moreover, 'volesti' provides algorithms for 
             estimating copulas useful in computational finance.",2021-07-14,Vissarion Fisikopoulos,NA,TRUE,https://github.com/geomscale/volume_approximation,11935,86,2021-07-28T09:38:25Z,138.77906976744185
vortexR,"Facilitate Post Vortex Simulation Analysis by offering
    tools to collate multiple Vortex (v10) output files into one R object, and
    analyse the collated output statistically. Vortex is a software for
    the development of individual-based model for population dynamic simulation
    (see <https://scti.tools/vortex/>).",2020-04-10,Carlo Pacioni,https://github.com/carlopacioni/vortexR/,TRUE,https://github.com/carlopacioni/vortexr,15529,5,2020-12-22T02:05:49Z,3105.8
VOSONDash,"A 'Shiny' application for the interactive visualisation and
    analysis of networks that also provides a web interface for collecting
    social media data using 'vosonSML'.",2020-07-27,Bryan Gertzel,https://github.com/vosonlab/VOSONDash,TRUE,https://github.com/vosonlab/vosondash,12182,42,2021-07-05T09:24:45Z,290.04761904761904
vosonSML,"A suite of tools for collecting and constructing networks from social media data.
    Provides easy-to-use functions for collecting data across popular platforms (Twitter, YouTube
    and Reddit) and generating different types of networks for analysis.",2020-07-18,Timothy Graham,https://github.com/vosonlab/vosonSML,TRUE,https://github.com/vosonlab/vosonsml,25603,56,2021-06-21T23:28:36Z,457.19642857142856
votesmart,An R interface to the Project 'VoteSmart'<https://justfacts.votesmart.org/> API.,2021-02-22,Amanda Dobbyn,https://github.com/decktools/votesmart/,TRUE,https://github.com/decktools/votesmart,1958,1,2021-05-04T13:49:50Z,1958
VoxR,"Tools for 3D point cloud voxelisation, projection, geometrical and morphological description of trees (DBH, height, volume, crown diameter), analyses of temporal changes between different measurement times, distance based clustering and visualisation of 3D voxel clouds and 2D projection. Most analyses and algorithms provided in the package are based on the concept of space exploration and are described in Lecigne et al. (2018, <doi:10.1093/aob/mcx095>).",2020-09-30,Bastien Lecigne,https://github.com/Blecigne/VoxR,TRUE,https://github.com/blecigne/voxr,15091,2,2020-09-30T05:15:06Z,7545.5
vpc,"Visual predictive checks are a commonly used diagnostic plot in pharmacometrics, showing how certain statistics (percentiles) for observed data compare to those same statistics for data simulated from a model. The package can generate VPCs for continuous, categorical, censored, and (repeated) time-to-event data.",2021-01-11,Ron Keizer,https://github.com/ronkeizer/vpc,TRUE,https://github.com/ronkeizer/vpc,28566,28,2021-04-15T18:24:28Z,1020.2142857142857
vrnmf,"Implements a set of routines to perform structured matrix factorization with minimum volume constraints. The NMF procedure decomposes a matrix X into a product C * D. Given conditions such that the matrix C is non-negative and has sufficiently spread columns, then volume minimization of a matrix D delivers a correct and unique, up to a scale and permutation, solution (C, D). This package provides both an implementation of volume-regularized NMF and ""anchor-free"" NMF, whereby the standard NMF problem is reformulated in the covariance domain. This algorithm was applied in Vladimir B. Seplyarskiy Ruslan A. Soldatov, et al. ""Population sequencing data reveal a compendium of mutational processes in the human germ line"". Science, 12 Aug 2021. <doi:10.1126/science.aba7408>. This package interacts with data available through the 'simulatedNMF' package, which is available in a 'drat' repository. To access this data package, see the instructions at <https://github.com/kharchenkolab/vrnmf>. The size of the 'simulatedNMF' package is approximately 8 MB.",2021-08-20,Evan Biederstedt,https://github.com/kharchenkolab/vrnmf,TRUE,https://github.com/kharchenkolab/vrnmf,170,11,2021-08-20T13:00:37Z,15.454545454545455
vroom,"The goal of 'vroom' is to read and write data (like
    'csv', 'tsv' and 'fwf') quickly. When reading it uses a quick initial
    indexing step, then reads the values lazily , so only the data you
    actually use needs to be read.  The writer formats the data in
    parallel and writes to disk asynchronously from formatting.",2021-08-05,Jim Hester,"https://vroom.r-lib.org, https://github.com/r-lib/vroom",TRUE,https://github.com/r-lib/vroom,1226672,519,2021-08-27T13:34:23Z,2363.529865125241
VSURF,"Three steps variable selection procedure based on random forests.
    Initially developed to handle high dimensional data (for which number of
    variables largely exceeds number of observations), the package is very
    versatile and can treat most dimensions of data, for regression and
    supervised classification problems. First step is dedicated to eliminate
    irrelevant variables from the dataset. Second step aims to select all
    variables related to the response for interpretation purpose. Third step
    refines the selection by eliminating redundancy in the set of variables
    selected by the second step, for prediction purpose.
    Genuer, R. Poggi, J.-M. and Tuleau-Malot, C. (2015)
    <https://journal.r-project.org/archive/2015-2/genuer-poggi-tuleaumalot.pdf>.",2019-07-18,Robin Genuer,https://github.com/robingenuer/VSURF,TRUE,https://github.com/robingenuer/vsurf,29836,21,2021-04-07T13:11:56Z,1420.7619047619048
vtable,"Automatically generates HTML variable documentation including variable names, labels, classes, value labels (if applicable), value ranges, and summary statistics. See the vignette ""vtable"" for a package overview.",2021-08-05,Nick Huntington-Klein,https://nickch-k.github.io/vtable/,TRUE,https://github.com/nickch-k/vtable,26670,20,2021-07-29T21:32:03Z,1333.5
vtreat,"A 'data.frame' processor/conditioner that prepares real-world data for predictive modeling in a statistically sound manner.
    'vtreat' prepares variables so that data has fewer exceptional cases, making
    it easier to safely use models in production. Common problems 'vtreat' defends
    against: 'Inf', 'NA', too many categorical levels, rare categorical levels, and new
    categorical levels (levels seen during application, but not during training). Reference: 
    ""'vtreat': a data.frame Processor for Predictive Modeling"", Zumel, Mount, 2016, <DOI:10.5281/zenodo.1173313>.",2021-06-11,John Mount,"https://github.com/WinVector/vtreat/,
https://winvector.github.io/vtreat/",TRUE,https://github.com/winvector/vtreat,135249,277,2021-06-11T14:31:41Z,488.2635379061372
vtree,"A tool for calculating and drawing ""variable trees"". Variable trees display information about nested subsets of a data frame.",2021-01-17,"Nick Barrowman 
  Sebastian Gatscha","https://github.com/nbarrowman/vtree,
https://nbarrowman.github.io/vtree",TRUE,https://github.com/nbarrowman/vtree,23660,57,2021-07-06T20:44:29Z,415.0877192982456
VulnToolkit,"Contains functions for analysis and summary of tidal datasets. Also provides access to tidal data collected by the National Oceanic and Atmospheric Administration's Center for Operational Oceanographic Products and Services and the Permanent Service for Mean Sea Level. For detailed description and application examples, see Hill, T.D. and S.C. Anisfeld (2021) <doi:10.6084/m9.figshare.14161202.v1> and Hill, T.D. and S.C. Anisfeld (2015) <doi:10.1016/j.ecss.2015.06.004>.",2021-08-02,Troy Hill,https://github.com/troyhill/VulnToolkit,TRUE,https://github.com/troyhill/vulntoolkit,2438,6,2021-08-02T12:39:35Z,406.3333333333333
W2CWM2C,"Set of functions that improves the graphical presentations of the functions: wave.correlation and spin.correlation (waveslim package, Whitcher 2012) and the wave.multiple.correlation and wave.multiple.cross.correlation (wavemulcor package, Fernandez-Macho 2012b). The plot outputs (heatmaps) can be displayed in the screen or can be saved as PNG or JPG images or as PDF or EPS formats. The W2CWM2C package also helps to handle the (input data) multivariate time series easily as a list of N elements (times series) and provides a multivariate data set (dataexample) to exemplify its use. A description of the package was published in a scientific paper: Polanco-Martinez and Fernandez-Macho (2014), <doi:10.1109/MCSE.2014.96>. ",2021-01-08,Josue M. Polanco-Martinez,https://github.com/jomopo/W2CWM2C,TRUE,https://github.com/jomopo/w2cwm2c,16411,0,2021-01-08T20:03:32Z,NA
waiter,"Full screen and partial loading screens for 'Shiny' with spinners, progress bars, and notifications.",2021-07-21,John Coene,"https://waiter.john-coene.com/,
https://github.com/JohnCoene/waiter",TRUE,https://github.com/johncoene/waiter,99772,377,2021-09-03T06:42:00Z,264.6472148541114
wakefield,"Generates random data sets including: data.frames, lists,
        and vectors.",2020-09-13,Tyler Rinker,https://github.com/trinker/wakefield,TRUE,https://github.com/trinker/wakefield,30444,211,2020-09-13T17:16:19Z,144.28436018957345
wal,Read 'Quake' assets including bitmap images and textures in 'wal' file format. This package also provides support for extracting these assets from 'WAD' and 'PAK' file archives. It can also read models in 'MDL' and 'MD2' formats.,2021-01-17,Tim Schäfer,https://github.com/dfsp-spirit/wal,TRUE,https://github.com/dfsp-spirit/wal,3149,1,2021-05-14T11:55:54Z,3149
waldo,"Compare complex R objects and reveal the key
    differences.  Designed particularly for use in testing packages where
    being able to quickly isolate key differences makes understanding test
    failures much easier.",2021-08-23,Hadley Wickham,"https://waldo.r-lib.org, https://github.com/r-lib/waldo",TRUE,https://github.com/r-lib/waldo,5635372,203,2021-08-24T22:52:17Z,27760.453201970442
walker,"Bayesian generalized linear models with time-varying coefficients 
    as in Helske (2020, <arXiv:2009.07063>). Gaussian, Poisson, and binomial 
    observations are supported. The Markov chain Monte Carlo (MCMC) computations are done using 
    Hamiltonian Monte Carlo provided by Stan, using a state space representation 
    of the model in order to marginalise over the coefficients for efficient sampling. 
    For non-Gaussian models, the package uses the importance sampling type estimators based on 
    approximate marginal MCMC as in Vihola, Helske, Franks (2020, <doi:10.1111/sjos.12492>).",2021-04-06,Jouni Helske,https://github.com/helske/walker,TRUE,https://github.com/helske/walker,21866,32,2021-09-01T12:19:28Z,683.3125
warbleR,"Functions aiming to facilitate the analysis of the structure of animal acoustic signals in 'R'. 'warbleR' makes use of the basic sound analysis tools from the package 'seewave', and offers new tools for acoustic structure analysis. The main features of the package are the use of loops to apply tasks through acoustic signals referenced in a selection (annotation) table and the production of spectrograms in image files that allow to organize data and verify acoustic analyzes. The package offers functions to explore, organize and manipulate multiple sound files, explore and download 'Xeno-Canto' recordings, detect signals automatically, create spectrograms of complete recordings or individual signals, run different measures of acoustic signal structure, evaluate the performance of measurement methods, catalog signals, characterize different structural levels in acoustic signals, run statistical analysis of duet coordination and consolidate databases and annotation tables, among others.",2021-03-09,Marcelo Araya-Salas,https://marce10.github.io/warbleR/,TRUE,https://github.com/marce10/warbler,38372,40,2021-08-20T14:44:52Z,959.3
warp,"Tooling to group dates by a variety of periods
    including: yearly, monthly, by second, by week of the month, and more.
    The groups are defined in such a way that they also represent the
    distance between dates in terms of the period. This extracts valuable
    information that can be used in further calculations that rely on a
    specific temporal spacing between observations.",2020-10-21,Davis Vaughan,https://github.com/DavisVaughan/warp,TRUE,https://github.com/davisvaughan/warp,933462,19,2020-10-21T18:05:18Z,49129.57894736842
washdata,"Urban water and sanitation survey dataset collected by Water and
    Sanitation for the Urban Poor (WSUP) with technical support from 
    Valid International. These citywide surveys have been collecting data 
    allowing water and sanitation service levels across the entire city to be 
    characterised, while also allowing more detailed data to be collected in 
    areas of the city of particular interest. These surveys are intended to 
    generate useful information for others working in the water and sanitation
    sector. Current release version includes datasets collected from a survey 
    conducted in Dhaka, Bangladesh in March 2017. This survey in Dhaka is one of 
    a series of surveys to be conducted by WSUP in various
    cities in which they operate including Accra, Ghana; Nakuru, Kenya; 
    Antananarivo, Madagascar; Maputo, Mozambique; and, Lusaka, Zambia. This 
    package will be updated once the surveys in other cities are completed and 
    datasets have been made available.",2020-09-29,Ernest Guevarra,https://github.com/katilingban/washdata/,TRUE,https://github.com/katilingban/washdata,14531,3,2020-09-29T20:07:32Z,4843.666666666667
washex,Gets data from the Washington State Legislature.,2021-04-28,Rohnin Randles,https://github.com/rwrandles/washex-r,TRUE,https://github.com/rwrandles/washex-r,1075,0,2021-04-28T16:48:16Z,NA
waterquality,"The main purpose of waterquality is to quickly and easily convert
    satellite-based reflectance imagery into one or many well-known water quality
    algorithms designed for the detection of harmful algal blooms or the following
    pigment proxies: chlorophyll-a, blue-green algae (phycocyanin), and turbidity.
    Johansen et al. (2019) <doi:10.21079/11681/35053>. ",2020-10-21,Richard Johansen,https://github.com/RAJohansen/waterquality,TRUE,https://github.com/rajohansen/waterquality,12011,26,2021-03-20T17:15:53Z,461.96153846153845
waves,"Originally designed application in the context of resource-limited plant research 
    and breeding programs, 'waves' provides an open-source solution to spectral data processing 
    and model development by bringing useful packages together into a streamlined pipeline. 
    This package is wrapper for functions related to the analysis of point visible and 
    near-infrared reflectance measurements. It includes visualization, filtering, aggregation, 
    preprocessing, cross-validation set formation, model training, and prediction functions to 
    enable open-source association of spectral and reference data. This package is documented 
    in a peer-reviewed manuscript in the Plant Phenome Journal <doi:10.1002/ppj2.20012>.
    Specialized cross-validation schemes are described in detail in Jarquín et al. (2017) 
    <doi:10.3835/plantgenome2016.12.0130>. Example data is from Ikeogu et al. (2017) 
    <doi:10.1371/journal.pone.0188918>.",2021-04-21,Jenna Hershberger,https://github.com/GoreLab/waves,TRUE,https://github.com/gorelab/waves,4137,0,2021-07-12T18:39:29Z,NA
wbacon,"The BACON algorithms are methods for multivariate outlier
    nomination (detection) and robust linear regression by Billor, Hadi,
    and Velleman (2000) <doi:10.1016/S0167-9473(99)00101-2>. The extension
    to weighted problems is due to Beguin and Hulliger (2008)
    <https://www150.statcan.gc.ca/n1/en/catalogue/12-001-X200800110616>.",2021-06-17,Tobias Schoch,https://github.com/tobiasschoch/wbacon,TRUE,https://github.com/tobiasschoch/wbacon,974,0,2021-06-27T15:48:07Z,NA
wbstats,Search and download data from the World Bank Data API.,2020-12-05,Jesse Piburn,https://github.com/nset-ornl/wbstats,TRUE,https://github.com/nset-ornl/wbstats,119498,89,2021-02-27T23:18:31Z,1342.6741573033707
wCorr,"Calculates Pearson, Spearman, polychoric, and polyserial correlation coefficients, in weighted or unweighted form. The package implements tetrachoric correlation as a special case of the polychoric and biserial correlation as a specific case of the polyserial.",2021-05-20,Paul Bailey,https://american-institutes-for-research.github.io/wCorr/,TRUE,https://github.com/american-institutes-for-research/wcorr,28726,0,2021-05-20T18:55:08Z,NA
wdpar,"Fetch and clean data from the World Database on Protected
    Areas (WDPA). Data is obtained from Protected Planet
    <https://www.protectedplanet.net/en>.",2021-09-03,Jeffrey O Hanson,"https://prioritizr.github.io/wdpar/,
https://github.com/prioritizr/wdpar",TRUE,https://github.com/prioritizr/wdpar,16735,22,2021-09-03T02:39:09Z,760.6818181818181
webchem,"Chemical information from around the web. This package interacts 
    with a suite of web services for chemical information. Sources include: Alan
    Wood's Compendium of Pesticide Common Names, Chemical Identifier Resolver,
    ChEBI, Chemical Translation Service, ChemIDplus, ChemSpider, ETOX,
    Flavornet, NIST Chemistry WebBook, OPSIN, PAN Pesticide Database, PubChem,
    SRS, Wikidata.",2021-02-07,Eduard Szöcs,"https://docs.ropensci.org/webchem/,
https://github.com/ropensci/webchem",TRUE,https://github.com/ropensci/webchem,29225,100,2021-03-18T06:40:49Z,292.25
webdriver,"A client for the 'WebDriver' 'API'. It allows driving a
    (probably headless) web browser, and can be used to test web
    applications, including 'Shiny' apps. In theory it works with any
    'WebDriver' implementation, but it was only tested with 'PhantomJS'.",2021-01-12,Ariya Hidayat,https://github.com/rstudio/webdriver,TRUE,https://github.com/rstudio/webdriver,219466,57,2021-06-14T17:51:53Z,3850.280701754386
webex,"Functions for easily creating interactive web pages using
    'R Markdown' that students can use in self-guided learning.",2021-06-10,Dale Barr,https://github.com/psyteachr/webex,TRUE,https://github.com/psyteachr/webex,10283,53,2021-06-09T19:00:21Z,194.0188679245283
WebGestaltR,"The web version WebGestalt <http://www.webgestalt.org> supports 12 organisms, 354 gene identifiers and 321,251 function categories. Users can upload the data and functional categories with their own gene identifiers. In addition to the Over-Representation Analysis, WebGestalt also supports Gene Set Enrichment Analysis and Network Topology Analysis. The user-friendly output report allows interactive and efficient exploration of enrichment results. The WebGestaltR package not only supports all above functions but also can be integrated into other pipeline or simultaneously analyze multiple gene lists.",2020-07-24,Yuxing Liao,https://github.com/bzhanglab/WebGestaltR,TRUE,https://github.com/bzhanglab/webgestaltr,27367,18,2021-05-25T01:56:25Z,1520.388888888889
webglobe,Displays geospatial data on an interactive 3D globe in the web browser.,2020-09-15,Richard Barnes,https://github.com/r-barnes/webglobe/,TRUE,https://github.com/r-barnes/webglobe,12768,28,2020-09-15T16:54:05Z,456
webmockr,"Stubbing and setting expectations on 'HTTP' requests.
    Includes tools for stubbing 'HTTP' requests, including expected
    request conditions and response conditions. Match on
    'HTTP' method, query parameters, request body, headers and
    more. Can be used for unit tests or outside of a testing 
    context.",2021-03-14,Scott Chamberlain,"https://github.com/ropensci/webmockr (devel)
https://books.ropensci.org/http-testing/ (user manual)
https://docs.ropensci.org/webmockr/ (documentation)",TRUE,https://github.com/ropensci/webmockr,71814,38,2021-03-13T20:20:07Z,1889.842105263158
webreadr,"R is used by a vast array of people for a vast array of purposes
    - including web analytics. This package contains functions for consuming and
    munging various common forms of request log, including the Common and Combined
    Web Log formats and various Amazon access logs.",2016-01-23,Oliver Keyes,https://github.com/Ironholds/webreadr,TRUE,https://github.com/ironholds/webreadr,16420,50,2020-10-28T19:52:18Z,328.4
webshot,"Takes screenshots of web pages, including Shiny applications and R
    Markdown documents.",2019-11-22,Winston Chang,https://github.com/wch/webshot/,TRUE,https://github.com/wch/webshot,3855012,197,2021-06-28T17:15:27Z,19568.58883248731
websocket,"Provides a 'WebSocket' client interface for R.
    'WebSocket' is a protocol for low-overhead real-time communication:
    <https://en.wikipedia.org/wiki/WebSocket>.",2021-08-18,Winston Chang,NA,TRUE,https://github.com/rstudio/websocket,234229,73,2021-08-20T14:52:30Z,3208.6164383561645
weed,Makes research involving EMDAT and related datasets easier. These Datasets are manually filled and have several formatting and compatibility issues. Weed aims to resolve these with its functions.,2021-07-07,Ram Kripa,https://github.com/rammkripa/weed,TRUE,https://github.com/rammkripa/weed,1993,3,2021-07-07T15:55:13Z,664.3333333333334
weibulltools,"Provides statistical methods and visualizations that are often 
             used in reliability engineering. Comprises a compact and easily 
             accessible set of methods and visualization tools that make the 
             examination and adjustment as well as the analysis and interpretation 
             of field data (and bench tests) as simple as possible.
             Non-parametric estimators like Median Ranks, 
             Kaplan-Meier (Abernethy, 2006, <ISBN:978-0-9653062-3-2>), 
             Johnson (Johnson, 1964, <ISBN:978-0444403223>), and Nelson-Aalen 
             for failure probability estimation within samples that contain 
             failures as well as censored data are included.   
             The package supports methods like Maximum Likelihood and Rank Regression, 
             (Genschel and Meeker, 2010, <DOI:10.1080/08982112.2010.503447>) 
             for the estimation of multiple parametric lifetime distributions,  
             as well as the computation of confidence intervals of quantiles and 
             probabilities using the delta method related to Fisher's confidence 
             intervals (Meeker and Escobar, 1998, <ISBN:9780471673279>) and the 
             beta-binomial confidence bounds. 
             If desired, mixture model analysis can be done with segmented regression
             and the EM algorithm.
             Besides the well-known Weibull analysis, the package also contains 
             Monte Carlo methods for the correction and completion of imprecisely 
             recorded or unknown lifetime characteristics.
             (Verband der Automobilindustrie e.V. (VDA), 2016, <ISSN:0943-9412>). 
             Plots are created statically ('ggplot2') or interactively ('plotly') and 
             can be customized with functions of the respective visualization package.
             The graphical technique of probability plotting as well as the addition 
             of regression lines and confidence bounds to existing plots are 
             supported. ",2021-01-12,Tim-Gunnar Hensel,"https://tim-tu.github.io/weibulltools,
https://github.com/Tim-TU/weibulltools",TRUE,https://github.com/tim-tu/weibulltools,14710,7,2021-04-13T11:32:12Z,2101.4285714285716
WeightIt,"Generates weights to form equivalent groups in observational studies with point or longitudinal treatments by easing and extending the functionality of the R packages 'twang' for generalized boosted modeling (McCaffrey, Ridgeway & Morral, 2004) <doi:10.1037/1082-989X.9.4.403>, 'CBPS' for covariate balancing propensity score weighting (Imai & Ratkovic, 2014) <doi:10.1111/rssb.12027>, 'ebal' for entropy balancing (Hainmueller, 2012) <doi:10.1093/pan/mpr025>, 'optweight' for optimization-based weights (Zubizarreta, 2015) <doi:10.1080/01621459.2015.1023805>, 'ATE' for empirical balancing calibration weighting (Chan, Yam, & Zhang, 2016) <doi:10.1111/rssb.12129>, 'SuperLearner' for stacked machine learning-based propensity scores (Pirracchio, Petersen, & van der Laan, 2015) <doi:10.1093/aje/kwu253>, among others. Also allows for assessment of weights and checking of covariate balance by interfacing directly with 'cobalt'.",2021-04-03,Noah Greifer,"https://ngreifer.github.io/WeightIt/,
https://github.com/ngreifer/WeightIt",TRUE,https://github.com/ngreifer/weightit,38893,46,2021-05-28T05:36:25Z,845.5
WeightSVM,"Functions for subject/instance weighted support vector machines (SVM). 
    It uses a modified version of 'libsvm' and is compatible with package 'e1071'. ",2020-05-28,Tianchen Xu,https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#weights_for_data_instances,TRUE,https://github.com/zjph602xtc/wsvm,10554,1,2020-12-31T09:59:49Z,10554
wellknown,"Convert 'WKT' to 'GeoJSON' and 'GeoJSON' to 'WKT'. Functions
    included for converting between 'GeoJSON' to 'WKT', creating both
    'GeoJSON' features, and non-features, creating 'WKT' from R objects
    (e.g., lists, data.frames, vectors), and linting 'WKT'.",2021-05-25,Scott Chamberlain,"https://docs.ropensci.org/wellknown/,
https://github.com/ropensci/wellknown",TRUE,https://github.com/ropensci/wellknown,54927,15,2021-05-25T16:54:28Z,3661.8
WeMix,"Run mixed-effects models that include weights at every level. The WeMix package fits a weighted mixed model, also known as a multilevel, mixed, or hierarchical linear model (HLM). The weights could be inverse selection probabilities, such as those developed for an education survey where schools are sampled probabilistically, and then students inside of those schools are sampled probabilistically. Although mixed-effects models are already available in R, WeMix is unique in implementing methods for mixed models using weights at multiple levels. Both linear and logit models are supported. Models may have up to three levels. ",2021-06-06,Paul Bailey,https://american-institutes-for-research.github.io/WeMix/,TRUE,https://github.com/american-institutes-for-research/wemix,23493,1,2021-05-10T14:53:40Z,23493
wesanderson,Palettes generated mostly from 'Wes Anderson' movies.,2018-04-20,Karthik Ram,https://github.com/karthik/wesanderson,TRUE,https://github.com/karthik/wesanderson,306236,1453,2020-10-13T19:52:13Z,210.7611837577426
WhatIf,"Inferences about counterfactuals are essential for prediction,
      answering what if questions, and estimating causal effects.
      However, when the counterfactuals posed are too far from the data at
      hand, conclusions drawn from well-specified statistical analyses
      become based largely on speculation hidden in convenient modeling
      assumptions that few would be willing to defend. Unfortunately,
      standard statistical approaches assume the veracity of the model
      rather than revealing the degree of model-dependence, which makes this
      problem hard to detect. WhatIf offers easy-to-apply methods to
      evaluate counterfactuals that do not require sensitivity testing over
      specified classes of models. If an analysis fails the tests offered
      here, then we know that substantive inferences will be sensitive to at
      least some modeling choices that are not based on empirical evidence,
      no matter what method of inference one chooses to use. WhatIf
      implements the methods for evaluating counterfactuals discussed in
      Gary King and Langche Zeng, 2006, ""The Dangers of Extreme
      Counterfactuals,"" Political Analysis 14 (2) <DOI:10.1093/pan/mpj004>; 
      and Gary King and Langche Zeng, 2007, ""When Can History Be Our Guide? The 
      Pitfalls of Counterfactual Inference,"" International Studies 
      Quarterly 51 (March) <DOI:10.1111/j.1468-2478.2007.00445.x>.",2020-11-14,Heather Stoll,https://gking.harvard.edu/whatif,TRUE,https://github.com/iqss/whatif,26414,12,2020-11-09T20:16:29Z,2201.1666666666665
wheatmap,"Builds complex plots, heatmaps in particular, using natural semantics. Bigger plots can be assembled using directives such as 'LeftOf', 'RightOf', 'TopOf', and 'Beneath' and more. Other features include clustering, dendrograms and integration with 'ggplot2' generated grid objects. This package is particularly designed for bioinformaticians to assemble complex plots for publication.",2018-03-15,Wanding Zhou,https://github.com/zwdzwd/wheatmap,TRUE,https://github.com/zwdzwd/wheatmap,24571,6,2021-08-09T19:03:28Z,4095.1666666666665
whisker,Implements 'Mustache' logicless templating.,2019-08-28,Edwin de Jonge,http://github.com/edwindj/whisker,TRUE,https://github.com/edwindj/whisker,9182266,177,2021-07-02T08:59:22Z,51877.20903954802
WhiteStripe,"Shinohara (2014) <doi:10.1016/j.nicl.2014.08.008>
    introduced 'WhiteStripe', an intensity-based normalization of T1 
    and T2 images, where normal 
    appearing white matter performs well, but requires segmentation.
    This method performs white matter mean and standard deviation
    estimates on data that has been rigidly-registered to the 'MNI'
    template and uses histogram-based methods.",2021-04-07,John Muschelli,NA,TRUE,https://github.com/muschellij2/whitestripe,19391,7,2021-04-07T14:45:50Z,2770.1428571428573
widyr,"Encapsulates the pattern of untidying data into a wide matrix,
  performing some processing, then turning it back into a tidy form. This
  is useful for several operations such as co-occurrence counts,
  correlations, or clustering that are mathematically convenient on wide matrices.",2021-08-12,David Robinson,https://github.com/dgrtwo/widyr,TRUE,https://github.com/dgrtwo/widyr,170269,282,2021-08-12T00:16:57Z,603.7907801418439
wiesbaden,Retrieve and import data from different databases of the Federal Statistical Office of Germany (DESTATIS) using their SOAP XML web service <https://www-genesis.destatis.de/>.,2021-04-20,Moritz Marbach,https://github.com/sumtxt/wiesbaden/,TRUE,https://github.com/sumtxt/wiesbaden,10326,32,2021-06-14T17:07:10Z,322.6875
WikidataR,"An API client for the Wikidata <https://www.wikidata.org/wiki/Wikidata:Main_Page> store of
             semantic data.",2021-07-12,Thomas Shafee,https://github.com/TS404/WikidataR/issues,TRUE,https://github.com/ts404/wikidatar,91966,8,2021-07-13T13:04:08Z,11495.75
wikilake,Scrape lake metadata tables from Wikipedia <https://www.wikipedia.org/>. ,2021-02-09,Joseph Stachelek,https://github.com/jsta/wikilake,TRUE,https://github.com/jsta/wikilake,15104,9,2021-07-22T15:20:34Z,1678.2222222222222
wildlifeDI,"Dynamic interaction refers to spatial-temporal associations
    in the movements of two (or more) animals. This package provides tools for
    calculating a suite of indices used for quantifying dynamic interaction with
    wildlife telemetry data. For more information on each of the methods employed
    see the references within. The package (as of version >= 0.3) also has new tools for
    automating contact analysis in large tracking datasets. The package draws 
    heavily on the classes and methods developed in the 'adehabitat' packages.",2021-06-16,Jed Long,https://github.com/jedalong/wildlifeDI,TRUE,https://github.com/jedalong/wildlifedi,18951,8,2021-06-16T15:01:59Z,2368.875
wildviz,"Fetches data from three disparate data sources and allows user to perform analyses on them. It offers two core components: 1. A robust data retrieval and preparation infrastructure for wildfire, climate, and air quality index data and 2. A simple, informative, and interactive visualizations of the aforementioned datasets for California counties from 2011 through 2015. The sources of data are: wildfire data from Kaggle <https://www.kaggle.com/rtatman/188-million-us-wildfires>, climate data from the National Oceanic and Atmospheric Administration  <https://www.ncdc.noaa.gov/cdo-web/token>, and air quality data from the Environmental Protection Agency <https://aqs.epa.gov/aqsweb/documents/data_api.html>. ",2021-08-23,Bradley Rafferty,https://github.com/bradraff/wildviz,TRUE,https://github.com/bradraff/wildviz,1200,1,2021-08-22T00:27:38Z,1200
winch,"Obtain the native stack trace and fuse it with R's
    stack trace for easier debugging of R packages with native code.",2020-11-16,Kirill Müller,"https://r-prof.github.io/winch/, https://github.com/r-prof/winch",TRUE,https://github.com/r-prof/winch,3962,8,2021-07-29T04:14:09Z,495.25
windsoraiR,"Collect multichannel marketing data from sources such as Google analytics, Facebook Ads, and many others using the 'Windsor.ai' API <https://www.windsor.ai/api-fields/>.",2021-05-10,Novica Nakov,https://github.com/windsor-ai/windsoraiR/,TRUE,https://github.com/windsor-ai/windsorair,1821,1,2021-05-06T12:50:57Z,1821
winfapReader,"Obtain information on peak flow data from the National River Flow Archive (NRFA) in the United Kingdom, either from the Peak Flow Dataset files <https://nrfa.ceh.ac.uk/peak-flow-dataset> once these have been downloaded to the user's computer or using the NRFA's API. These files are in a format suitable for direct use in the 'WINFAP' software, hence the name of the package. ",2021-02-19,Ilaria Prosdocimi,https://ilapros.github.io/winfapReader/,TRUE,https://github.com/ilapros/winfapreader,4961,2,2021-08-31T15:03:17Z,2480.5
wiqid,"Provides simple, fast functions for maximum likelihood and Bayesian estimates of wildlife population parameters, suitable for use with simulated data or bootstraps. Early versions were indeed quick and dirty, but optional error-checking routines and meaningful error messages have been added. Includes single and multi-season occupancy, closed capture population estimation, survival, species richness and distance measures.",2020-06-10,Mike Meredith,https://mmeredith.net/R/wiqid/,TRUE,https://github.com/mikemeredith/wiqid,21597,0,2021-07-12T00:51:40Z,NA
wiseR,"A Shiny application for learning Bayesian Decision Networks from data. This package can be used for probabilistic reasoning (in the observational setting), causal inference (in the presence of interventions) and learning policy decisions (in Decision Network setting). Functionalities include end-to-end implementations for data-preprocessing, structure-learning, exact inference, approximate inference, extending the learned structure to Decision Networks and policy optimization using statistically rigorous methods such as bootstraps, resampling, ensemble-averaging and cross-validation. In addition to Bayesian Decision Networks, it also features correlation networks, community-detection, graph visualizations, graph exports and web-deployment of the learned models as Shiny dashboards.   ",2018-11-29,Tavpritesh Sethi,https://github.com/SAFE-ICU/wiseR,TRUE,https://github.com/safe-icu/wiser,18878,6,2021-07-26T15:27:21Z,3146.3333333333335
wk,"Provides a minimal R and C++ API for parsing
  well-known binary and well-known text representation of
  geometries to and from R-native formats. 
  Well-known binary is compact
  and fast to parse; well-known text is human-readable
  and is useful for writing tests. These formats are only
  useful in R if the information they contain can be 
  accessed in R, for which high-performance functions 
  are provided here.",2021-07-13,Dewey Dunnington,"https://paleolimbot.github.io/wk/,
https://github.com/paleolimbot/wk",TRUE,https://github.com/paleolimbot/wk,395771,26,2021-08-20T20:18:57Z,15221.961538461539
wkutils,"Provides extra utilities for well-known formats in the
  'wk' package that are outside the scope of that package. Utilities
  to parse coordinates from data frames, plot well-known geometry
  vectors, extract meta information from well-known geometry vectors,
  and calculate bounding boxes are provided.",2021-01-11,Dewey Dunnington,"https://paleolimbot.github.io/wkutils/,
https://github.com/paleolimbot/wkutils",TRUE,https://github.com/paleolimbot/wkutils,8385,5,2021-01-15T20:19:44Z,1677
wmm,"Calculate magnetic field at a given location and time according to 
  the World Magnetic Model (WMM). Both the main field and secular variation 
  components are returned. This functionality is useful for physicists and 
  geophysicists who need orthogonal components from WMM. Currently, this package 
  supports annualized time inputs between 2000 and 2025. If desired, users can
  specify which WMM version to use, e.g., the original WMM2015 release or the 
  recent out-of-cycle WMM2015 release. Methods used to implement WMM, including 
  the Gauss coefficients for each release, are described in the following 
  publications: Chulliat et al (2020) <doi:10.25923/ytk1-yx35>,
  Chulliat et al (2019) <doi:10.25921/xhr3-0t19>, 
  Chulliat et al (2015) <doi:10.7289/V5TB14V7>, 
  Maus et al (2010) <https://www.ngdc.noaa.gov/geomag/WMM/data/WMMReports/WMM2010_Report.pdf>, 
  McLean et al (2004) <https://www.ngdc.noaa.gov/geomag/WMM/data/WMMReports/TRWMM_2005.pdf>,
  and Macmillian et al (2000) <https://www.ngdc.noaa.gov/geomag/WMM/data/WMMReports/wmm2000.pdf>.",2021-03-17,Will Frierson,https://github.com/wfrierson/wmm,TRUE,https://github.com/wfrierson/wmm,8016,1,2021-06-01T06:28:28Z,8016
WoodburyMatrix,"A hierarchy of classes and methods for manipulating matrices formed implicitly from the sums of the inverses of other matrices, a situation commonly encountered in spatial statistics and related fields. Enables easy use of the Woodbury matrix identity and the matrix determinant lemma to allow computation (e.g., solving linear systems) without having to form the actual matrix. More information on the underlying linear algebra can be found in Harville, D. A. (1997) <doi:10.1007/b98818>.",2020-08-10,Michael Bertolacci,https://github.com/mbertolacci/WoodburyMatrix,TRUE,https://github.com/mbertolacci/woodburymatrix,4301,1,2020-10-25T03:40:57Z,4301
wooldridge,"Students learning both econometrics and R may find the introduction 
    to both challenging. The wooldridge data package aims to lighten the task by efficiently 
    loading any data set found in the text with a single command. Data sets have been 
    compressed to a fraction of their original size. Documentation files contain page numbers, 
    the original source, time of publication, and notes from the author suggesting avenues for 
    further analysis and research. If one needs an introduction to linear model syntax, a 
    vignette contains R solutions to examples from each chapter of the text. 
    Data sets are from the 7th edition (Wooldridge 2020, ISBN-13: 978-1-337-55886-0), 
    and are backwards compatible with all versions of the text.",2021-06-24,Justin M. Shea,https://justinmshea.github.io/wooldridge/,TRUE,https://github.com/justinmshea/wooldridge,199183,114,2021-07-20T03:11:23Z,1747.219298245614
worcs,"Create reproducible and transparent research projects in 'R'.
    This package is based on the Workflow for Open
    Reproducible Code in Science (WORCS), a step-by-step procedure based on best
    practices for
    Open Science. It includes an 'RStudio' project template, several
    convenience functions, and all dependencies required to make your project
    reproducible and transparent. WORCS is explained in the tutorial paper
    by Van Lissa, Brandmaier, Brinkman, Lamprecht, Struiksma, & Vreede (2020).
    <doi:10.17605/OSF.IO/ZCVBS>.",2021-02-02,Caspar J. van Lissa,https://github.com/cjvanlissa/worcs,TRUE,https://github.com/cjvanlissa/worcs,7088,49,2021-05-15T09:27:25Z,144.6530612244898
word2vec,"Learn vector representations of words by continuous bag of words and skip-gram implementations of the 'word2vec' algorithm. 
    The techniques are detailed in the paper ""Distributed Representations of Words and Phrases and their Compositionality"" by Mikolov et al. (2013), available at <arXiv:1310.4546>.",2021-07-02,Jan Wijffels,https://github.com/bnosac/word2vec,TRUE,https://github.com/bnosac/word2vec,11394,37,2021-07-02T09:13:09Z,307.94594594594594
wordbankr,"Tools for connecting to Wordbank, an open repository for
    developmental vocabulary data. For more information on the
    underlying data, see <http://wordbank.stanford.edu>.",2020-11-13,Mika Braginsky,https://github.com/langcog/wordbankr,TRUE,https://github.com/langcog/wordbankr,14958,16,2021-03-04T02:10:02Z,934.875
wordpiece,"Apply 'Wordpiece' (<arXiv:1609.08144>) tokenization to input text, 
 given an appropriate vocabulary. The 'BERT' (<arXiv:1810.04805>) tokenization 
 conventions are used by default.",2021-02-11,Jonathan Bratt,https://github.com/jonathanbratt/wordpiece,TRUE,https://github.com/jonathanbratt/wordpiece,2233,6,2021-02-08T16:19:12Z,372.1666666666667
wordpredictor,"A framework for developing n-gram models for text prediction.
    It provides data cleaning, data sampling, extracting tokens from text,
    model generation, model evaluation and word prediction. For information on how n-gram models 
    work we referred to: ""Speech and Language Processing""
    <https://web.stanford.edu/~jurafsky/slp3/3.pdf>. For optimizing R code and
    using R6 classes we referred to ""Advanced R"" 
    <https://adv-r.hadley.nz/r6.html>. For writing R extensions we referred to 
    ""R Packages"", <https://r-pkgs.org/index.html>.",2021-06-19,Nadir Latif,"https://github.com/pakjiddat/word-predictor,
https://pakjiddat.github.io/word-predictor/",TRUE,https://github.com/pakjiddat/word-predictor,1056,2,2021-06-19T02:29:33Z,528
wordsalad,"Provides access to various word embedding methods (GloVe, 
    fasttext and word2vec) to extract word vectors using a unified framework to
    increase reproducibility and correctness.",2020-09-23,Emil Hvitfeldt,https://github.com/EmilHvitfeldt/wordsalad,TRUE,https://github.com/emilhvitfeldt/wordsalad,3742,4,2020-09-24T06:59:49Z,935.5
workflowr,"Provides a workflow for your analysis projects by combining
  literate programming ('knitr' and 'rmarkdown') and version control
  ('Git', via 'git2r') to generate a website containing time-stamped,
  versioned, and documented results.",2020-04-30,John Blischak,https://github.com/jdblischak/workflowr,TRUE,https://github.com/jdblischak/workflowr,26441,607,2021-08-04T14:22:14Z,43.560131795716636
workflows,"Managing both a 'parsnip' model and a preprocessor, such as a
    model formula or recipe from 'recipes', can often be challenging. The goal
    of 'workflows' is to streamline this process by bundling the model alongside
    the preprocessor, all within the same object.",2021-07-16,Davis Vaughan,"https://github.com/tidymodels/workflows,
https://workflows.tidymodels.org",TRUE,https://github.com/tidymodels/workflows,392742,129,2021-07-30T14:44:03Z,3044.5116279069766
workflowsets,"A workflow is a combination of a model and preprocessors
    (e.g, a formula, recipe, etc.) (Kuhn and Silge (2021)
    <https://www.tmwr.org/>). In order to try different combinations of
    these, an object can be created that contains many workflows. There
    are functions to create workflows en masse as well as training them
    and visualizing the results.",2021-07-22,Max Kuhn,"https://github.com/tidymodels/workflowsets,
https://workflowsets.tidymodels.org",TRUE,https://github.com/tidymodels/workflowsets,141426,65,2021-08-27T14:06:21Z,2175.7846153846153
workloopR,"Functions for the import, transformation, and analysis of data 
    from muscle physiology experiments. The work loop technique is used to 
    evaluate the mechanical work and power output of muscle. Josephson (1985) 
    <doi:10.1242/jeb.114.1.493> modernized the technique for
    application in comparative biomechanics. Although our initial motivation 
    was to provide functions to analyze work loop experiment data, as we 
    developed the package we incorporated the ability to analyze data from 
    experiments that are often complementary to work loops. There are currently 
    three supported experiment types: work loops, simple twitches, and tetanus 
    trials. Data can be imported directly from .ddf files or via an object 
    constructor function. Through either method, data can then be cleaned or 
    transformed via methods typically used in studies of muscle physiology. 
    Data can then be analyzed to determine the timing and magnitude of force 
    development and relaxation (for isometric trials) or the magnitude of work, 
    net power, and instantaneous power among other things (for work loops). 
    Although we do not provide plotting functions, all resultant objects are 
    designed to be friendly to visualization via either base-R plotting or 
    'tidyverse' functions.
 This package has been peer-reviewed by rOpenSci (v. 1.1.0).",2021-05-06,Vikram B. Baliga,"https://docs.ropensci.org/workloopR/,
https://github.com/ropensci/workloopR/",TRUE,https://github.com/ropensci/workloopr,1367,3,2021-07-28T18:07:12Z,455.6666666666667
worldmet,"Functions to import data from more than 30,000 surface
    meteorological sites around the world managed by the National Oceanic and Atmospheric Administration (NOAA) Integrated Surface
    Database (ISD, see <https://www.ncdc.noaa.gov/isd>).",2021-04-20,David Carslaw,https://davidcarslaw.github.io/worldmet/index.html,TRUE,https://github.com/davidcarslaw/worldmet,26322,33,2021-04-20T12:32:17Z,797.6363636363636
worrms,"Client for World Register of Marine Species
    (<http://www.marinespecies.org/>). Includes functions for each
    of the API methods, including searching for names by name, date and
    common names, searching using external identifiers, fetching
    synonyms, as well as fetching taxonomic children and
    taxonomic classification.",2020-07-08,Scott Chamberlain,"https://docs.ropensci.org/worrms,
https://github.com/ropensci/worrms (devel), https://taxize.dev
(user manual)",TRUE,https://github.com/ropensci/worrms,96342,12,2021-05-12T20:14:36Z,8028.5
wpa,"Opinionated functions that enable easier and faster
    analysis of Workplace Analytics data. There are three main types of functions in 'wpa':
    (i) Standard functions create a 'ggplot' visual or a summary table based on a specific
    Workplace Analytics metric; (2) Report Generation functions generate HTML reports on
    a specific analysis area, e.g. Collaboration; (3) Other miscellaneous functions cover
    more specific applications (e.g. Subject Line text mining) of Workplace Analytics data.
    This package adheres to 'tidyverse' principles and works well with the pipe syntax.
    'wpa' is built with the beginner-to-intermediate R users in mind, and is optimised for
    simplicity. ",2021-09-01,Martin Chan,https://github.com/microsoft/wpa/,TRUE,https://github.com/microsoft/wpa,4364,13,2021-09-03T11:31:03Z,335.6923076923077
wql,"Functions to assist in the processing and
    exploration of data from environmental monitoring programs.
    The package name stands for ""water quality"" and reflects the
    original focus on time series data for physical and chemical
    properties of water, as well as the biota. Intended for
    programs that sample approximately monthly, quarterly or
    annually at discrete stations, a feature of many legacy data
    sets. Most of the functions should be useful for analysis of
    similar-frequency time series regardless of the subject
    matter.",2017-07-04,Alan Jassby,https://github.com/jsta/wql,TRUE,https://github.com/jsta/wql,17305,8,2021-08-04T14:14:31Z,2163.125
wrapr,"Tools for writing and debugging R code. Provides: 
    '%.>%' dot-pipe (an 'S3' configurable pipe), unpack/to (R style multiple assignment/return),
    'build_frame()'/'draw_frame()' ('data.frame' example tools),
    'qc()' (quoting concatenate), 
    ':=' (named map builder), 'let()' (converts non-standard evaluation interfaces to parametric standard
    evaluation interfaces, inspired by 'gtools::strmacro()' and 'base::bquote()'), and more.",2021-06-11,John Mount,"https://github.com/WinVector/wrapr,
https://winvector.github.io/wrapr/",TRUE,https://github.com/winvector/wrapr,330821,128,2021-06-10T21:44:21Z,2584.5390625
wrassp,"A wrapper around Michel Scheffers's 'libassp' (<http://libassp.sourceforge.net/>). 
    The 'libassp' (Advanced Speech Signal Processor) library aims at providing
    functionality for handling speech signal files in most common audio formats
    and for performing analyses common in phonetic science/speech science. This
    includes the calculation of formants, fundamental frequency, root mean
    square, auto correlation, a variety of spectral analyses, zero crossing
    rate, filtering etc. This wrapper provides R with a large subset of
    'libassp's signal processing functions and provides them to the user in a
    (hopefully) user-friendly manner.",2021-05-19,Raphael Winkelmann,https://github.com/IPS-LMU/wrassp,TRUE,https://github.com/ips-lmu/wrassp,23611,20,2021-08-17T12:38:43Z,1180.55
WriteXLS,"Cross-platform Perl based R function to create Excel 2003 (XLS) and Excel 2007 (XLSX)
             files from one or more data frames. Each data frame will be
             written to a separate named worksheet in the Excel spreadsheet.
             The worksheet name will be the name of the data frame it contains
             or can be specified by the user. ",2021-04-01,Marc Schwartz <marc_schwartz@me.com> and various authors for Perl modules listed in each .pm file.,https://github.com/marcschwartz/WriteXLS,TRUE,https://github.com/marcschwartz/writexls,186009,19,2021-04-01T12:47:02Z,9789.947368421053
wrswoR,"A collection of implementations of classical and
    novel algorithms for weighted sampling without replacement.",2020-07-26,Kirill Müller,http://krlmlr.github.io/wrswoR,TRUE,https://github.com/krlmlr/wrswor,22213,17,2021-07-29T04:14:07Z,1306.6470588235295
WRTDStidal,"An adaptation for estuaries (tidal waters) of weighted regression
    on time, discharge, and season to evaluate trends in water quality time series.",2019-11-17,Marcus W. Beck,NA,TRUE,https://github.com/fawda123/wtreg_for_estuaries,15605,0,2021-06-07T14:28:38Z,NA
wru,"Predicts individual race/ethnicity using surname, geolocation,
    and other attributes, such as gender and age. The method utilizes the Bayes'
    Rule to compute the posterior probability of each racial category for any given
    individual. The package implements methods described in Imai and Khanna (2016)
    ""Improving Ecological Inference by Predicting Individual Ethnicity from Voter
    Registration Records"" Political Analysis <DOI:10.1093/pan/mpw001>.",2021-05-17,Kabir Khanna,https://github.com/kosukeimai/wru,TRUE,https://github.com/kosukeimai/wru,23351,77,2021-08-19T17:46:49Z,303.2597402597403
wsrf,"
    A parallel implementation of Weighted Subspace Random Forest.  The
    Weighted Subspace Random Forest algorithm was proposed in the
    International Journal of Data Warehousing and Mining by Baoxun Xu,
    Joshua Zhexue Huang, Graham Williams, Qiang Wang, and Yunming Ye
    (2012) <DOI:10.4018/jdwm.2012040103>.  The algorithm can classify
    very high-dimensional data with random forests built using small
    subspaces.  A novel variable weighting method is used for variable
    subspace selection in place of the traditional random variable
    sampling.This new approach is particularly useful in building
    models from high-dimensional data.",2021-04-28,He Zhao,"https://github.com/SimonYansenZhao/wsrf, https://togaware.com",TRUE,https://github.com/simonyansenzhao/wsrf,33397,10,2021-04-28T02:05:15Z,3339.7
wv,"Provides a series of tools to compute and plot quantities related to classical and robust wavelet variance for time series and regular lattices. More details can be found, for example, in Serroukh, A., Walden, A.T., & Percival, D.B. (2000) <doi:10.2307/2669537> and Guerrier, S. & Molinari, R. (2016) <arXiv:1607.05858>.  ",2020-01-16,Stéphane Guerrier,https://github.com/SMAC-Group/wv,TRUE,https://github.com/smac-group/wv,9082,12,2020-12-12T03:53:49Z,756.8333333333334
WVPlots,"Select data analysis plots, under a standardized calling interface implemented on top of 'ggplot2' and 'plotly'.  
   Plots of interest include: 'ROC', gain curve, scatter plot with marginal distributions, 
   conditioned scatter plot with marginal densities,
   box and stem with matching theoretical distribution, and density with matching theoretical distribution.",2021-01-10,John Mount,"https://github.com/WinVector/WVPlots,
https://winvector.github.io/WVPlots/",TRUE,https://github.com/winvector/wvplots,55858,82,2021-04-10T22:23:49Z,681.1951219512196
x12,The 'X13-ARIMA-SEATS' <https://www.census.gov/srd/www/x13as/> methodology and software is a widely used software and developed by the US Census Bureau. It can be accessed from 'R' with this package and 'X13-ARIMA-SEATS' binaries are provided by the 'R' package 'x13binary'.,2020-11-13,Alexander Kowarik,https://github.com/statistikat/x12,TRUE,https://github.com/statistikat/x12,36009,14,2021-04-29T09:48:20Z,2572.0714285714284
x13binary,"The US Census Bureau provides a seasonal adjustment program now
 called 'X-13ARIMA-SEATS' building on both earlier programs called X-11 and
 X-12 as well as the SEATS program by the Bank of Spain. The US Census Bureau
 offers both source and binary versions -- which this package integrates for
 use by other R packages.",2021-08-12,Dirk Eddelbuettel and Christoph Sax,https://github.com/x13org/x13binary,TRUE,https://github.com/x13org/x13binary,295412,6,2021-08-12T15:09:11Z,49235.333333333336
xaringan,"Create HTML5 slides with R Markdown and the JavaScript library
    'remark.js' (<https://remarkjs.com>).",2021-06-23,Yihui Xie,https://github.com/yihui/xaringan,TRUE,https://github.com/yihui/xaringan,158510,1240,2021-06-24T22:18:01Z,127.83064516129032
xaringanthemer,"Create beautifully color-coordinated and customized
    themes for your 'xaringan' slides, without writing any CSS. Complete
    your slide theme with 'ggplot2' themes that match the font and colors
    used in your slides.  Customized styles can be created directly in
    your slides' 'R Markdown' source file or in a separate external
    script.",2021-06-24,Garrick Aden-Buie,"https://pkg.garrickadenbuie.com/xaringanthemer/,
https://github.com/gadenbuie/xaringanthemer",TRUE,https://github.com/gadenbuie/xaringanthemer,18595,393,2021-07-19T02:05:16Z,47.31552162849873
xfun,Miscellaneous functions commonly used in other packages maintained by 'Yihui Xie'.,2021-08-06,Yihui Xie,https://github.com/yihui/xfun,TRUE,https://github.com/yihui/xfun,26099514,92,2021-08-18T14:41:32Z,283690.3695652174
XiMpLe,"Provides a simple XML tree parser/generator. It includes
        functions to read XML files into R objects, get information out of
        and into nodes, and write R objects back to XML code. It's not as
        powerful as the 'XML' package and doesn't aim to be, but for simple
        XML handling it could be useful. It was originally developed for
        the R GUI and IDE 'RKWard' <https://rkward.kde.org>, to make plugin
        development easier.",2020-09-19,Meik Michalke,https://reaktanz.de/?c=hacking&s=XiMpLe,TRUE,https://github.com/rkward-community/ximple,17574,1,2020-09-19T20:01:07Z,17574
xlsx,Provide R functions to read/write/format Excel 2007 and Excel 97/2000/XP/2003 file formats.,2020-11-10,Cole Arendt,https://github.com/colearendt/xlsx,TRUE,https://github.com/colearendt/xlsx,4005080,70,2020-11-10T19:33:45Z,57215.42857142857
xmeta,"A toolbox for meta-analysis. This package includes (1) a robust multivariate meta-analysis of continuous or binary outcomes; (2) a bivariate Egger's test for detecting small study effects; (3) Galaxy Plot: A New Visualization Tool of Bivariate Meta-Analysis Studies; and (4) a bivariate T&F method accounting for publication bias in bivariate meta-analysis, based on symmetry of the galaxy plot.",2021-02-15,Jiayi Tong,https://github.com/Penncil/xmeta,TRUE,https://github.com/penncil/xmeta,18124,0,2021-02-01T04:49:37Z,NA
xml2,"Work with XML files using a simple, consistent
    interface. Built on top of the 'libxml2' C library.",2020-04-23,Jim Hester,"https://xml2.r-lib.org/, https://github.com/r-lib/xml2",TRUE,https://github.com/r-lib/xml2,17799154,192,2021-05-05T13:41:52Z,92703.92708333333
xmlconvert,"Converts XML documents to R dataframes and dataframes to XML documents.
  A wide variety of options allows for different XML formats and flexible control
  of the conversion process. Results can be exported to CSV and Excel, if desired. 
  Also converts XML data to R lists.",2021-03-27,Joachim Zuckarelli,https://github.com/jsugarelli/xmlconvert/,TRUE,https://github.com/jsugarelli/xmlconvert,5213,17,2021-03-25T21:10:30Z,306.6470588235294
xmlr,"'XML' package for creating and reading and manipulating 'XML', with an object model based on 'Reference Classes'.",2020-05-12,Per Nyfelt,https://github.com/Alipsa/xmlr,TRUE,https://github.com/alipsa/xmlr,5630,7,2020-12-18T13:53:45Z,804.2857142857143
xnet,"Fit a two-step kernel ridge regression model for
        predicting edges in networks, and carry out cross-validation
        using shortcuts for swift and accurate performance assessment
        (Stock et al, 2018 <doi:10.1093/bib/bby095> ).",2020-02-03,Joris Meys,https://github.com/CenterForStatistics-UGent/xnet,TRUE,https://github.com/centerforstatistics-ugent/xnet,7609,10,2021-06-03T13:04:23Z,760.9
xoi,"Analysis of crossover interference in experimental crosses,
    particularly regarding the gamma model. See, for example,
    Broman and Weber (2000) <doi:10.1086/302923>.",2020-02-26,Karl W Broman,https://github.com/kbroman/xoi,TRUE,https://github.com/kbroman/xoi,17021,3,2021-08-05T21:03:02Z,5673.666666666667
xplorerr,"Tools for interactive data exploration built using 'shiny'. Includes apps for descriptive 
    statistics, visualizing probability distributions, inferential statistics, linear regression, 
    logistic regression and RFM analysis.",2021-05-21,Aravind Hebbali,"https://github.com/rsquaredacademy/xplorerr,
https://xplorerr.rsquaredacademy.com/",TRUE,https://github.com/rsquaredacademy/xplorerr,133245,14,2021-05-21T08:36:50Z,9517.5
XPolaris,"The POLARIS database <http://hydrology.cee.duke.edu/POLARIS/> 
    is a 30-meter probabilistic soil series map of the contiguous United 
    States (US). It represents an optimization of the Soil Survey 
    Geographic database. This R package facilitates the access 
    to large amounts of soil data within the US, currently 
    stored online as raster images (TIFF).",2021-06-17,Luiz Moro Rosso,https://github.com/lhmrosso/XPolaris,TRUE,https://github.com/lhmrosso/xpolaris,950,3,2021-06-17T10:10:05Z,316.6666666666667
xpose,"Diagnostics for non-linear mixed-effects (population) 
    models from 'NONMEM' <https://www.iconplc.com/innovation/nonmem/>. 
    'xpose' facilitates data import, creation of numerical run summary 
    and provide 'ggplot2'-based graphics for data exploration and model 
    diagnostics.",2021-06-30,Benjamin Guiastrennec,https://github.com/UUPharmacometrics/xpose,TRUE,https://github.com/uupharmacometrics/xpose,28025,43,2021-06-30T07:30:19Z,651.7441860465116
xpose4,"A model building aid for nonlinear mixed-effects 
    (population) model analysis using NONMEM, facilitating data set 
    checkout, exploration and visualization, model diagnostics, candidate 
    covariate identification and model comparison. The methods are described 
    in Keizer et al. (2013) <doi:10.1038/psp.2013.24>, and Jonsson et al. (1999) 
    <doi:10.1016/s0169-2607(98)00067-4>.",2020-12-18,Andrew C. Hooker,"https://uupharmacometrics.github.io/xpose4/,
https://github.com/UUPharmacometrics/xpose4",TRUE,https://github.com/uupharmacometrics/xpose4,31742,22,2021-03-19T09:21:49Z,1442.8181818181818
xrnet,"Fits hierarchical regularized regression models
    to incorporate potentially informative external data, Weaver and Lewinger (2019) <doi:10.21105/joss.01761>. 
    Utilizes coordinate descent to efficiently fit regularized regression models both with and without
    external information with the most common penalties used in practice (i.e. ridge, lasso, elastic net). 
    Support for standard R matrices, sparse matrices and big.matrix objects.",2020-03-01,Garrett Weaver,https://github.com/USCbiostats/xrnet,TRUE,https://github.com/uscbiostats/xrnet,6905,11,2021-06-01T05:41:51Z,627.7272727272727
xROI,"Digital repeat photography and near-surface remote sensing have been used by environmental scientists to study the environmental change for nearly a decade. However, a user-friendly, reliable, and robust platform to extract color-based statistics and time-series from a large stack of images is still lacking. Here, we present an interactive open-source toolkit, called 'xROI', that facilitate the process time-series extraction and improve the quality of the final data. 'xROI' provides a responsive environment for scientists to interactively a) delineate regions of interest (ROI), b) handle field of view (FOV) shifts, and c) extract and export time series data characterizing image color (i.e. red, green and blue channel digital numbers for the defined ROI). Using 'xROI', user can detect FOV shifts without minimal difficulty. The software gives user the opportunity to readjust the mask files or redraw new ones every time an FOV shift occurs. 'xROI' helps to significantly improve data accuracy and continuity. ",2021-06-02,Bijan Seyednasrollah,NA,TRUE,https://github.com/bnasr/xroi,15308,10,2021-06-02T01:56:04Z,1530.8
xts,"Provide for uniform handling of R's different time-based data classes by extending zoo, maximizing native format information preservation and allowing for user level customization and extension, while simplifying cross-class interoperability.",2020-09-09,Joshua M. Ulrich,https://github.com/joshuaulrich/xts,TRUE,https://github.com/joshuaulrich/xts,11190653,194,2021-06-13T16:13:45Z,57683.778350515466
xutils,"This is a collection of some useful functions when dealing with text data. 
    Currently it only contains a very efficient function of decoding HTML entities
    in character vectors by Rcpp routine.",2021-02-17,Fangzhou Xie,https://github.com/fangzhou-xie/xutils,TRUE,https://github.com/fangzhou-xie/xutils,2050,1,2021-06-16T01:08:05Z,2050
yaConsensus,"Procedures to perform consensus clustering starting from a dissimilarity matrix or a data matrix. It's allowed to select if the subsampling has to be by samples or features. In case of computational heavy load, the procedures can run in parallel.",2021-07-01,Stefano Maria Pagnotta,https://github.com/stefanoMP/yaConsensus,TRUE,https://github.com/stefanomp/yaconsensus,433,0,2021-09-03T10:51:45Z,NA
yager,"Another implementation of general regression neural network in R
    based on Specht (1991) <DOI:10.1109/72.97934>. It is applicable to the 
    functional approximation or the classification. ",2020-10-25,WenSui Liu,https://github.com/statcompute/yager,TRUE,https://github.com/statcompute/yager,7002,10,2020-10-25T20:31:25Z,700.2
yamlet,"A YAML-based
 mechanism for working with table metadata. Supports
 compact syntax for creating, modifying, viewing, exporting,
 importing, displaying, and plotting metadata coded as column 
 attributes. The 'yamlet' dialect is valid 'YAML' with
 defaults and conventions chosen to improve readability. 
 See ?yamlet, ?decorate.data.frame and ?modify.default.
 See ?read_yamlet ?write_yamlet, ?io_csv, and ?ggplot.decorated.",2021-07-18,Tim Bergsma,NA,TRUE,https://github.com/bergsmat/yamlet,13059,1,2021-08-07T17:04:57Z,13059
yamlme,"Setting layout through 'YAML' headers in 'R-Markdown' documents,
    enabling their automatic generation.
    Functions and methods may summarize 'R' objects in automatic reports, for
    instance check-lists and further reports applied to the packages 'taxlist'
    and 'vegtable'.",2021-01-06,Miguel Alvarez,"https://github.com/kamapu/yamlme,
https://kamapu.github.io/rpkg/yamlme/",TRUE,https://github.com/kamapu/yamlme,3093,0,2021-01-12T22:08:47Z,NA
yap,"Another implementation of probabilistic neural network in R
    based on Specht (1990) <DOI:10.1016/0893-6080(90)90049-Q>. It is applicable to the 
    pattern recognition with a N-level response, where N > 2.",2020-10-25,WenSui Liu,https://github.com/statcompute/yap,TRUE,https://github.com/statcompute/yap,6966,7,2020-10-25T20:30:15Z,995.1428571428571
yaps,"Estimate tracks of animals tagged with acoustic transmitters. 'yaps' was introduced in 2017 as a transparent open-source tool to estimate positions of fish (and other aquatic animals) tagged with acoustic transmitters. Based on registrations of acoustic transmitters on hydrophones positioned in a fixed array, 'yaps' enables users to synchronize the collected data (i.e. correcting for drift in the internal clocks of the hydrophones/receivers) and subsequently to estimate tracks of the tagged animals. The paper introducing 'yaps' is available in open access at Baktoft, Gjelland, Økland & Thygesen (2017) <doi:10.1038/s41598-017-14278-z>. Also check out our cookbook with a completely worked through example at Baktoft, Gjelland, Økland, Rehage, Rodemann, Corujo, Viadero & Thygesen (2019) <DOI:10.1101/2019.12.16.877688>. Additional tutorials will eventually make their way onto the project website at <https://baktoft.github.io/yaps/>.",2021-04-13,Henrik Baktoft,"https://github.com/baktoft/yaps, https://baktoft.github.io/yaps/",TRUE,https://github.com/baktoft/yaps,2560,6,2021-04-14T07:26:58Z,426.6666666666667
yardstick,"Tidy tools for quantifying how well model fits to a
    data set such as confusion matrices, class probability curve
    summaries, and regression metrics (e.g., RMSE).",2021-03-28,Davis Vaughan,"https://github.com/tidymodels/yardstick,
https://yardstick.tidymodels.org",TRUE,https://github.com/tidymodels/yardstick,599096,271,2021-08-16T13:49:08Z,2210.6863468634688
yesno,Asks Yes-No questions with variable or custom responses.,2020-07-10,Joe Thorley,https://github.com/poissonconsulting/yesno,TRUE,https://github.com/poissonconsulting/yesno,55124,5,2021-02-15T19:50:06Z,11024.8
yll,"Compute the standard expected years of life lost (YLL),
    as developed by the Global Burden of Disease Study
    (Murray, C.J., Lopez, A.D. and World Health Organization, 1996).
    The YLL is based on comparing the age of death to an external standard life
    expectancy curve. It also computes the average YLL, which highlights premature
    causes of death and brings attention to preventable deaths
    (Aragon et al., 2008).",2018-11-02,Antoine Soetewey,https://github.com/AntoineSoetewey/yll,TRUE,https://github.com/antoinesoetewey/yll,10788,3,2021-01-12T10:45:20Z,3596
ymlthis,"Write 'YAML' front matter for R Markdown and related
    documents. yml_*() functions write 'YAML' and use_*() functions let
    you write the resulting 'YAML' to your clipboard or to .yml files
    related to your project.",2021-03-23,Malcolm Barrett,"https://ymlthis.r-lib.org, https://github.com/r-lib/ymlthis",TRUE,https://github.com/r-lib/ymlthis,16369,141,2021-03-23T14:05:20Z,116.09219858156028
yorkr,"Analyzing performances of cricketers and cricket teams
             based on 'yaml' match data from Cricsheet <https://cricsheet.org/>.",2021-07-10,Tinniam V Ganesh,https://github.com/tvganesh/yorkr/,TRUE,https://github.com/tvganesh/yorkr,19316,14,2021-07-10T10:49:02Z,1379.7142857142858
ypr,"An implementation of equilibrium-based yield per recruit
    methods.  Yield per recruit methods can used to estimate the optimal
    yield for a fish population as described by Walters and Martell (2004)
    <isbn:0-691-11544-3>.  The yield can be based on the number of fish
    caught (or harvested) or biomass caught for all fish or just large
    (trophy) individuals.",2021-07-03,Joe Thorley,https://github.com/poissonconsulting/ypr,TRUE,https://github.com/poissonconsulting/ypr,14077,5,2021-07-03T22:47:30Z,2815.4
zbank,"Interface to the 'ZooBank' API (<http://zoobank.org/Api>) client.
    'ZooBank' (<http://zoobank.org/>) is the official registry of zoological 
    nomenclature. Methods are provided for using each of the API endpoints, 
    including for querying by author, querying for publications, get 
    statistics on 'ZooBank' activity, and more.",2018-10-30,Scott Chamberlain,https://github.com/ropenscilabs/zbank,TRUE,https://github.com/ropenscilabs/zbank,10982,2,2020-11-15T16:03:16Z,5491
zdeskR,"Facilitates making a connection to the 
  'Zendesk' API and executing various queries. You can use it to
  get ticket data and ticket metrics. The 'Zendesk' documentation is 
  available at <https://developer.zendesk.com/rest_api
  /docs/support/introduction>. This package is not supported by 
  'Zendesk' (owner of the software).",2021-03-27,Chris Umphlett,https://github.com/chrisumphlett/zdeskR,TRUE,https://github.com/chrisumphlett/zdeskr,7147,1,2021-08-16T13:45:55Z,7147
zebu,"Implements the estimation of local (and global) association measures: Ducher's Z, pointwise mutual information, normalized pointwise mutual information and chi-squared residuals. The significance of local (and global) association is accessed using p-values estimated by permutations. Finally, using local association subgroup analysis, it identifies if the association between variables is dependent on the value of another variable.",2020-10-06,Olivier M. F. Martin,https://github.com/oliviermfmartin/zebu,TRUE,https://github.com/oliviermfmartin/zebu,14985,0,2020-10-05T17:11:27Z,NA
zeitgebr,"Use behavioural variables to compute period, rhythmicity and other circadian parameters.
  Methods include computation of chi square periodograms (Sokolove and Bushell (1978) <DOI:10.1016/0022-5193(78)90022-X>),
  Lomb-Scargle periodograms (Lomb (1976) <DOI:10.1007/BF00648343>, Scargle (1982) <DOI:10.1086/160554>, Ruf (1999) <DOI:10.1076/brhm.30.2.178.1422>),
  and autocorrelation-based periodograms.",2020-04-25,Quentin Geissmann,https://github.com/rethomics/zeitgebr,TRUE,https://github.com/rethomics/zeitgebr,14023,8,2021-06-11T09:03:11Z,1752.875
Zelig,"A framework that brings together an abundance of common
    statistical models found across packages into a unified interface, and
    provides a common architecture for estimation and interpretation, as well
    as bridging functions to absorb increasingly more models into the
    package. Zelig allows each individual package, for each
    statistical model, to be accessed by a common uniformly structured call and
    set of arguments. Moreover, Zelig automates all the surrounding building
    blocks of a statistical work-flow--procedures and algorithms that may be
    essential to one user's application but which the original package
    developer did not use in their own research and might not themselves
    support. These include bootstrapping, jackknifing, and re-weighting of data.
    In particular, Zelig automatically generates predicted and simulated
    quantities of interest (such as relative risk ratios, average treatment
    effects, first differences and predicted and expected values) to interpret
    and visualize complex models.",2020-12-12,Robert Treacy,https://cran.r-project.org/package=Zelig,TRUE,https://github.com/iqss/zelig,393900,93,2020-12-12T17:00:02Z,4235.4838709677415
zen4R,"Provides an Interface to 'Zenodo' (<https://zenodo.org>) REST API, 
  including management of depositions, attribution of DOIs by 'Zenodo' and 
  upload of files.",2020-09-04,Emmanuel Blondel,https://github.com/eblondel/zen4R,TRUE,https://github.com/eblondel/zen4r,13282,28,2021-07-23T20:49:57Z,474.35714285714283
zenplots,"Graphical tools for visualizing high-dimensional data along a path
 of alternating one- and two-dimensional plots. Note that this
 includes interactive graphics plots based on 'loon' in turn based on 'tcltk'
 (included as part of the standard R distribution).  It also requires 'graph' from Bioconductor.
 For more detail on use and algorithms, see <doi:10.18637/jss.v095.i04>.",2020-11-28,Wayne Oldford,https://github.com/great-northern-diver/zenplots,TRUE,https://github.com/great-northern-diver/zenplots,12551,2,2021-09-03T16:31:18Z,6275.5
zipangu,"Some data treated by the Japanese R user require
    unique operations and processing. These are caused by address, Kanji,
    and traditional year representations. 'zipangu' transforms specific
    to Japan into something more general one.",2021-02-07,Shinya Uryu,"https://uribo.github.io/zipangu/, https://github.com/uribo/zipangu",TRUE,https://github.com/uribo/zipangu,11835,40,2021-08-27T11:15:35Z,295.875
zipcodeR,"Make working with ZIP codes in R painless with an integrated dataset of U.S. ZIP codes and functions for working with them. 
             Search ZIP codes by multiple geographies, including state, county, city & across time zones. Also included are functions for relating
             ZIP codes to Census data, geocoding & distance calculations.",2021-09-02,Gavin Rozzi,"https://github.com/gavinrozzi/zipcodeR/,
https://www.gavinrozzi.com/project/zipcoder/",TRUE,https://github.com/gavinrozzi/zipcoder,18714,54,2021-08-29T00:57:14Z,346.55555555555554
zipsae,"This function produces empirical best linier unbiased predictions (EBLUPs) for Zero-Inflated data and its Relative Standard Error. Small Area Estimation with Zero-Inflated Model (SAE-ZIP) is a model developed for Zero-Inflated data that can lead us to overdispersion situation. To handle this kind of situation, this model is created. The model in this package is based on Small Area Estimation with Zero-Inflated Poisson model proposed by Dian Christien Arisona (2018)<https://repository.ipb.ac.id/handle/123456789/92308>. For the data sample itself, we use combination method between Roberto Benavent and Domingo Morales (2015)<doi:10.1016/j.csda.2015.07.013> and Sabine Krieg, Harm Jan Boonstra and Marc Smeets (2016)<doi:10.1515/jos-2016-0051>.",2021-06-14,Fadheel Wisnu Utomo,https://github.com/dheel/zipsae,TRUE,https://github.com/dheel/zipsae,1301,1,2021-06-14T11:57:02Z,1301
zoltr,"'Zoltar' <https://www.zoltardata.com/> is a website that provides a repository of model forecast results
    in a standardized format and a central location. It supports storing, retrieving, comparing, and analyzing time
    series forecasts for prediction challenges of interest to the modeling community. This package provides functions
    for working with the 'Zoltar' API, including connecting and authenticating, getting information about projects,
    models, and forecasts, deleting and uploading forecast data, and downloading scores.",2020-04-15,Matthew Cornell,"https://github.com/reichlab/zoltr , http://reichlab.io/zoltr/",TRUE,https://github.com/reichlab/zoltr,11439,1,2021-06-28T15:51:25Z,11439
zonebuilder,"Functions, documentation and example data to help divide
    geographic space into discrete polygons (zones).
    The functions are motivated by research into the merits of different zoning systems
    <doi:10.1068/a090169>. A flexible 'ClockBoard' zoning system is
    provided, which breaks-up space by concentric rings
    and radial lines emanating from a central point.
    By default, the diameter of the rings grow according the triangular number sequence
    <doi:10.1080/26375451.2019.1598687> with the first 4 'doughnuts'
    (or 'annuli') measuring 1, 3, 6, and 10 km wide.
    These annuli are subdivided into equal segments (12 by default), creating the
    visual impression of a dartboard. Zones are labelled according to
    distance to the centre and angular distance from North, creating a simple
    geographic zoning and labelling system useful for visualising geographic
    phenomena with a clearly demarcated central location such as cities.",2021-07-12,Robin Lovelace,"https://github.com/zonebuilders/zonebuilder,
https://zonebuilders.github.io/zonebuilder/",TRUE,https://github.com/zonebuilders/zonebuilder,2275,25,2021-08-12T16:07:04Z,91
zscorer,"A tool for calculating z-scores and centiles for weight-for-age, 
    length/height-for-age, weight-for-length/height, BMI-for-age, 
    head circumference-for-age, age circumference-for-age, 
    subscapular skinfold-for-age, triceps skinfold-for-age based on the 
    WHO Child Growth Standards. ",2019-10-19,Mark Myatt,https://github.com/nutriverse/zscorer,TRUE,https://github.com/nutriverse/zscorer,15762,8,2020-12-23T07:30:12Z,1970.25
ztable,"Makes zebra-striped tables (tables with alternating row colors)
    in LaTeX and HTML formats easily from a data.frame, matrix, lm, aov, anova,
    glm, coxph, nls, fitdistr, mytable and cbind.mytable objects.",2020-10-08,Keon-Woong Moon,https://github.com/cardiomoon/ztable,TRUE,https://github.com/cardiomoon/ztable,140240,20,2020-10-08T03:40:18Z,7012
ZVCV,"Stein control variates can be used to improve Monte Carlo estimates of expectations when the derivatives of the log target are available. This package implements a variety of such methods, including zero-variance control variates (ZV-CV, Mira et al. (2013) <doi:10.1007/s11222-012-9344-6>), regularised ZV-CV (South et al., 2018 <arXiv:1811.05073>), control functionals (CF, Oates et al. (2017) <doi:10.1111/rssb.12185>) and semi-exact control functionals (SECF, South et al., 2020 <arXiv:2002.00033>). ZV-CV is a parametric approach that is exact for (low order) polynomial integrands with Gaussian targets. CF is a non-parametric alternative that offers better than the standard Monte Carlo convergence rates. SECF has both a parametric and a non-parametric component and it offers the advantages of both for an additional computational cost. Functions for applying ZV-CV and CF to two estimators for the normalising constant of the posterior distribution in Bayesian statistics are also supplied in this package. The basic requirements for using the package are a set of samples, derivatives and function evaluations. ",2021-06-30,Leah F. South,NA,TRUE,https://github.com/leahprice/zvcv,11464,5,2021-07-02T04:16:05Z,2292.8
